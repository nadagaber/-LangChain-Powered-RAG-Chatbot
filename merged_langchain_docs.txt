
--- api_reference.txt ---

LangChain Python API Reference




















LangChain Python API Reference
#


Welcome to the LangChain Python API reference. This is a reference for all

langchain-x
 packages.


For user guides see 
https://python.langchain.com
.


For the legacy API reference hosted on ReadTheDocs see 
https://api.python.langchain.com/
.




Base packages
#












Core






langchain-core: 0.3.59




core/index.html










Langchain






langchain: 0.3.25




langchain/index.html










Text Splitters






langchain-text-splitters: 0.3.8




text_splitters/index.html










Community






langchain-community: 0.3.23




community/index.html










Experimental






langchain-experimental: 0.3.5rc1




experimental/index.html
















Integrations
#












OpenAI






langchain-openai 0.3.16




openai/index.html










Anthropic






langchain-anthropic 0.3.13




anthropic/index.html










Google VertexAI






langchain-google-vertexai 2.0.23




google_vertexai/index.html










AWS






langchain-aws 0.2.23




aws/index.html










Huggingface






langchain-huggingface 0.2.0




huggingface/index.html










MistralAI






langchain-mistralai 0.2.10




mistralai/index.html








See the full list of integrations in the Section Navigation.






















 On this page
  






Base packages


Integrations


--- docs_concepts.txt ---

Conceptual guide
On this page
Conceptual guide


This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.


We recommend that you go through at least one of the 
Tutorials
 before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.


The conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the 
How-to guides
 and 
Tutorials
. For detailed reference material, please see the 
API reference
.


High level
​




Why LangChain?
: Overview of the value that LangChain provides.


Architecture
: How packages are organized in the LangChain ecosystem.




Concepts
​




Chat models
: LLMs exposed via a chat API that process sequences of messages as input and output a message.


Messages
: The unit of communication in chat models, used to represent model input and output.


Chat history
: A conversation represented as a sequence of messages, alternating between user messages and model responses.


Tools
: A function with an associated schema defining the function's name, description, and the arguments it accepts.


Tool calling
: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.


Structured output
: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.


Memory
: Information about a conversation that is persisted so that it can be used in future conversations.


Multimodality
: The ability to work with data that comes in different forms, such as text, audio, images, and video.


Runnable interface
: The base abstraction that many LangChain components and the LangChain Expression Language are built on.


Streaming
: LangChain streaming APIs for surfacing results as they are generated.


LangChain Expression Language (LCEL)
: A syntax for orchestrating LangChain components. Most useful for simpler applications.


Document loaders
: Load a source as a list of documents.


Retrieval
: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.


Text splitters
: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.


Embedding models
: Models that represent data such as text or images in a vector space.


Vector stores
: Storage of and efficient search over vectors and associated metadata.


Retriever
: A component that returns relevant documents from a knowledge base in response to a query.


Retrieval Augmented Generation (RAG)
: A technique that enhances language models by combining them with external knowledge bases.


Agents
: Use a 
language model
 to choose a sequence of actions to take. Agents can interact with external resources via 
tool
.


Prompt templates
: Component for factoring out the static parts of a model "prompt" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.


Output parsers
: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of 
tool calling
 and 
structured outputs
.


Few-shot prompting
: A technique for improving model performance by providing a few examples of the task to perform in the prompt.


Example selectors
: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.


Async programming
: The basics that one should know to use LangChain in an asynchronous context.


Callbacks
: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.


Tracing
: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.


Evaluation
: The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.


Testing
: The process of verifying that a component of an integration or application works as expected. Testing is essential for ensuring that the application behaves correctly and that changes to the codebase do not introduce new bugs.




Glossary
​




AIMessageChunk
: A partial response from an AI message. Used when streaming responses from a chat model.


AIMessage
: Represents a complete response from an AI model.


astream_events
: Stream granular information from 
LCEL
 chains.


BaseTool
: The base class for all tools in LangChain.


batch
: Use to execute a runnable with batch inputs.


bind_tools
: Allows models to interact with tools.


Caching
: Storing results to avoid redundant calls to a chat model.


Chat models
: Chat models that handle multiple data modalities.


Configurable runnables
: Creating configurable Runnables.


Context window
: The maximum size of input a chat model can process.


Conversation patterns
: Common patterns in chat interactions.


Document
: LangChain's representation of a document.


Embedding models
: Models that generate vector embeddings for various data types.


HumanMessage
: Represents a message from a human user.


InjectedState
: A state injected into a tool function.


InjectedStore
: A store that can be injected into a tool for data persistence.


InjectedToolArg
: Mechanism to inject arguments into tool functions.


input and output types
: Types used for input and output in Runnables.


Integration packages
: Third-party packages that integrate with LangChain.


Integration tests
: Tests that verify the correctness of the interaction between components, usually run with access to the underlying API that powers an integration.


invoke
: A standard method to invoke a Runnable.


JSON mode
: Returning responses in JSON format.


langchain-community
: Community-driven components for LangChain.


langchain-core
: Core langchain package. Includes base interfaces and in-memory implementations.


langchain
: A package for higher level components (e.g., some pre-built chains).


langgraph
: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.


langserve
: Used to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.


LLMs (legacy)
: Older language models that take a string as input and return a string as output.


Managing chat history
: Techniques to maintain and manage the chat history.


OpenAI format
: OpenAI's message format for chat models.


Propagation of RunnableConfig
: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.


rate-limiting
: Client side rate limiting for chat models.


RemoveMessage
: An abstraction used to remove a message from chat history, used primarily in LangGraph.


role
: Represents the role (e.g., user, assistant) of a chat message.


RunnableConfig
: Use to pass run time information to Runnables (e.g., 
run_name
, 
run_id
, 
tags
, 
metadata
, 
max_concurrency
, 
recursion_limit
, 
configurable
).


Standard parameters for chat models
: Parameters such as API key, 
temperature
, and 
max_tokens
.


Standard tests
: A defined set of unit and integration tests that all integrations must pass.


stream
: Use to stream output from a Runnable or a graph.


Tokenization
: The process of converting data into tokens and vice versa.


Tokens
: The basic unit that a language model reads, processes, and generates under the hood.


Tool artifacts
: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.


Tool binding
: Binding tools to models.


@tool
: Decorator for creating tools in LangChain.


Toolkits
: A collection of tools that can be used together.


ToolMessage
: Represents a message that contains the results of a tool execution.


Unit tests
: Tests that verify the correctness of individual components, run in isolation without access to the Internet.


Vector stores
: Datastores specialized for storing and efficiently searching vector embeddings.


with_structured_output
: A helper method for chat models that natively support 
tool calling
 to get structured output matching a given schema specified via Pydantic, JSON schema or a function.


with_types
: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.


Edit this page
Was this page helpful?
Previous
How to create and query vector stores
Next
Agents
High level
Concepts
Glossary


--- docs_concepts_agents.txt ---

Conceptual guide
Agents
On this page
Agents


By themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.


LangGraph
 is an extension of LangChain specifically aimed at creating highly controllable and customizable agents. We recommend that you use LangGraph for building agents.


Please see the following resources for more information:




LangGraph docs on 
common agent architectures


Pre-built agents in LangGraph




Legacy agent concept: AgentExecutor
​


LangChain previously introduced the 
AgentExecutor
 as a runtime for agents.
While it served as an excellent starting point, its limitations became apparent when dealing with more sophisticated and customized agents.
As a result, we're gradually phasing out 
AgentExecutor
 in favor of more flexible solutions in LangGraph.


Transitioning from AgentExecutor to langgraph
​


If you're currently using 
AgentExecutor
, don't worry! We've prepared resources to help you:






For those who still need to use 
AgentExecutor
, we offer a comprehensive guide on 
how to use AgentExecutor
.






However, we strongly recommend transitioning to LangGraph for improved flexibility and control. To facilitate this transition, we've created a detailed 
migration guide
 to help you move from 
AgentExecutor
 to LangGraph seamlessly.




Edit this page
Was this page helpful?
Previous
Conceptual guide
Next
Architecture
Legacy agent concept: AgentExecutor
Transitioning from AgentExecutor to langgraph


--- docs_concepts_architecture.txt ---

Conceptual guide
Architecture
On this page
Architecture


LangChain is a framework that consists of a number of packages.




langchain-core
​


This package contains base abstractions for different components and ways to compose them together.
The interfaces for core components like chat models, vector stores, tools and more are defined here.
No third-party integrations are defined here.
The dependencies are very lightweight.


langchain
​


The main 
langchain
 package contains chains and retrieval strategies that make up an application's cognitive architecture.
These are NOT third-party integrations.
All chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.


Integration packages
​


Popular integrations have their own packages (e.g. 
langchain-openai
, 
langchain-anthropic
, etc) so that they can be properly versioned and appropriately lightweight.


For more information see:




A list 
integrations packages


The 
API Reference
 where you can find detailed information about each of the integration package.




langchain-community
​


This package contains third-party integrations that are maintained by the LangChain community.
Key integration packages are separated out (see above).
This contains integrations for various components (chat models, vector stores, tools, etc).
All dependencies in this package are optional to keep the package as lightweight as possible.


langgraph
​


langgraph
 is an extension of 
langchain
 aimed at building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.


LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.


Further reading


See our LangGraph overview 
here
.


See our LangGraph Academy Course 
here
.




langserve
​


A package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.


important
LangServe is designed to primarily deploy simple Runnables and work with well-known primitives in langchain-core.
If you need a deployment option for LangGraph, you should instead be looking at LangGraph Platform (beta) which will be better suited for deploying LangGraph applications.


For more information, see the 
LangServe documentation
.


LangSmith
​


A developer platform that lets you debug, test, evaluate, and monitor LLM applications.


For more information, see the 
LangSmith documentation
Edit this page
Was this page helpful?
Previous
Agents
Next
Async programming with langchain
langchain-core
langchain
Integration packages
langchain-community
langgraph
langserve
LangSmith


--- docs_concepts_async.txt ---

Conceptual guide
Async programming with langchain
On this page
Async programming with langchain


Prerequisites


Runnable interface


asyncio




LLM based applications often involve a lot of I/O-bound operations, such as making API calls to language models, databases, or other services. Asynchronous programming (or async programming) is a paradigm that allows a program to perform multiple tasks concurrently without blocking the execution of other tasks, improving efficiency and responsiveness, particularly in I/O-bound operations.


note
You are expected to be familiar with asynchronous programming in Python before reading this guide. If you are not, please find appropriate resources online to learn how to program asynchronously in Python.
This guide specifically focuses on what you need to know to work with LangChain in an asynchronous context, assuming that you are already familiar with asynch


Langchain asynchronous APIs
​


Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.


Typically, any method that may perform I/O operations (e.g., making API calls, reading files) will have an asynchronous counterpart.


In LangChain, async implementations are located in the same classes as their synchronous counterparts, with the asynchronous methods having an "a" prefix. For example, the synchronous 
invoke
 method has an asynchronous counterpart called 
ainvoke
.


Many components of LangChain implement the 
Runnable Interface
, which includes support for asynchronous execution. This means that you can run Runnables asynchronously using the 
await
 keyword in Python.


await
 some_runnable
.
ainvoke
(
some_input
)


Other components like 
Embedding Models
 and 
VectorStore
 that do not implement the 
Runnable Interface
 usually still follow the same rule and include the asynchronous version of method in the same class with an "a" prefix.


For example,


await
 some_vectorstore
.
aadd_documents
(
documents
)


Runnables created using the 
LangChain Expression Language (LCEL)
 can also be run asynchronously as they implement
the full 
Runnable Interface
.


For more information, please review the 
API reference
 for the specific component you are using.


Delegation to sync methods
​


Most popular LangChain integrations implement asynchronous support of their APIs. For example, the 
ainvoke
 method of many ChatModel implementations uses the 
httpx.AsyncClient
 to make asynchronous HTTP requests to the model provider's API.


When an asynchronous implementation is not available, LangChain tries to provide a default implementation, even if it incurs
a 
slight
 overhead.


By default, LangChain will delegate the execution of unimplemented asynchronous methods to the synchronous counterparts. LangChain almost always assumes that the synchronous method should be treated as a blocking operation and should be run in a separate thread.
This is done using 
asyncio.loop.run_in_executor
 functionality provided by the 
asyncio
 library. LangChain uses the default executor provided by the 
asyncio
 library, which lazily initializes a thread pool executor with a default number of threads that is reused in the given event loop. While this strategy incurs a slight overhead due to context switching between threads, it guarantees that every asynchronous method has a default implementation that works out of the box.


Performance
​


Async code in LangChain should generally perform relatively well with minimal overhead out of the box, and is unlikely
to be a bottleneck in most applications.


The two main sources of overhead are:




Cost of context switching between threads when 
delegating to synchronous methods
. This can be addressed by providing a native asynchronous implementation.


In 
LCEL
 any "cheap functions" that appear as part of the chain will be either scheduled as tasks on the event loop (if they are async) or run in a separate thread (if they are sync), rather than just be run inline.




The latency overhead you should expect from these is between tens of microseconds to a few milliseconds.


A more common source of performance issues arises from users accidentally blocking the event loop by calling synchronous code in an async context (e.g., calling 
invoke
 rather than 
ainvoke
).


Compatibility
​


LangChain is only compatible with the 
asyncio
 library, which is distributed as part of the Python standard library. It will not work with other async libraries like 
trio
 or 
curio
.


In Python 3.9 and 3.10, 
asyncio's tasks
 did not
accept a 
context
 parameter. Due to this limitation, LangChain cannot automatically propagate the 
RunnableConfig
 down the call chain
in certain scenarios.


If you are experiencing issues with streaming, callbacks or tracing in async code and are using Python 3.9 or 3.10, this is a likely cause.


Please read 
Propagation RunnableConfig
 for more details to learn how to propagate the 
RunnableConfig
 down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).


How to use in ipython and jupyter notebooks
​


As of IPython 7.0, IPython supports asynchronous REPLs. This means that you can use the 
await
 keyword in the IPython REPL and Jupyter Notebooks without any additional setup. For more information, see the 
IPython blog post
.
Edit this page
Was this page helpful?
Previous
Architecture
Next
Callbacks
Langchain asynchronous APIs
Delegation to sync methods
Performance
Compatibility
How to use in ipython and jupyter notebooks


--- docs_concepts_callbacks.txt ---

Conceptual guide
Callbacks
On this page
Callbacks


Prerequisites


Runnable interface




LangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.


You can subscribe to these events by using the 
callbacks
 argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.


Callback events
​


Event
Event Trigger
Associated Method
Chat model start
When a chat model starts
on_chat_model_start
LLM start
When a llm starts
on_llm_start
LLM new token
When an llm OR chat model emits a new token
on_llm_new_token
LLM ends
When an llm OR chat model ends
on_llm_end
LLM errors
When an llm OR chat model errors
on_llm_error
Chain start
When a chain starts running
on_chain_start
Chain end
When a chain ends
on_chain_end
Chain error
When a chain errors
on_chain_error
Tool start
When a tool starts running
on_tool_start
Tool end
When a tool ends
on_tool_end
Tool error
When a tool errors
on_tool_error
Agent action
When an agent takes an action
on_agent_action
Agent finish
When an agent ends
on_agent_finish
Retriever start
When a retriever starts
on_retriever_start
Retriever end
When a retriever ends
on_retriever_end
Retriever error
When a retriever errors
on_retriever_error
Text
When arbitrary text is run
on_text
Retry
When a retry event is run
on_retry


Callback handlers
​


Callback handlers can either be 
sync
 or 
async
:




Sync callback handlers implement the 
BaseCallbackHandler
 interface.


Async callback handlers implement the 
AsyncCallbackHandler
 interface.




During run-time LangChain configures an appropriate callback manager (e.g., 
CallbackManager
 or 
AsyncCallbackManager
 which will be responsible for calling the appropriate method on each "registered" callback handler when the event is triggered.


Passing callbacks
​


The 
callbacks
 property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:




Request time callbacks
: Passed at the time of the request in addition to the input data.
Available on all standard 
Runnable
 objects. These callbacks are INHERITED by all children
of the object they are defined on. For example, 
chain.invoke({"number": 25}, {"callbacks": [handler]})
.


Constructor callbacks
: 
chain = TheNameOfSomeChain(callbacks=[handler])
. These callbacks
are passed as arguments to the constructor of the object. The callbacks are scoped
only to the object they are defined on, and are 
not
 inherited by any children of the object.




warning
Constructor callbacks are scoped only to the object they are defined on. They are 
not
 inherited by children
of the object.


If you're creating a custom chain or runnable, you need to remember to propagate request time
callbacks to any child objects.


Async in Python<=3.10
Any 
RunnableLambda
, a 
RunnableGenerator
, or 
Tool
 that invokes other runnables
and is running 
async
 in python<=3.10, will have to propagate callbacks to child
objects manually. This is because LangChain cannot automatically propagate
callbacks to child objects in this case.
This is a common reason why you may fail to see events being emitted from custom
runnables or tools.


For specifics on how to use callbacks, see the 
relevant how-to guides here
.
Edit this page
Was this page helpful?
Previous
Async programming with langchain
Next
Chat history
Callback events
Callback handlers
Passing callbacks


--- docs_concepts_chat_history.txt ---

Conceptual guide
Chat history
On this page
Chat history


Prerequisites


Messages


Chat models


Tool calling




Chat history is a record of the conversation between the user and the chat model. It is used to maintain context and state throughout the conversation. The chat history is sequence of 
messages
, each of which is associated with a specific 
role
, such as "user", "assistant", "system", or "tool".


Conversation patterns
​




Most conversations start with a 
system message
 that sets the context for the conversation. This is followed by a 
user message
 containing the user's input, and then an 
assistant message
 containing the model's response.


The 
assistant
 may respond directly to the user or if configured with tools request that a 
tool
 be invoked to perform a specific task.


A full conversation often involves a combination of two patterns of alternating messages:




The 
user
 and the 
assistant
 representing a back-and-forth conversation.


The 
assistant
 and 
tool messages
 representing an 
"agentic" workflow
 where the assistant is invoking tools to perform specific tasks.




Managing chat history
​


Since chat models have a maximum limit on input size, it's important to manage chat history and trim it as needed to avoid exceeding the 
context window
.


While processing chat history, it's essential to preserve a correct conversation structure.


Key guidelines for managing chat history:




The conversation should follow one of these structures:




The first message is either a "user" message or a "system" message, followed by a "user" and then an "assistant" message.


The last message should be either a "user" message or a "tool" message containing the result of a tool call.






When using 
tool calling
, a "tool" message should only follow an "assistant" message that requested the tool invocation.




tip
Understanding correct conversation structure is essential for being able to properly implement

memory
 in chat models.


Related resources
​




How to trim messages


Memory guide
 for information on implementing short-term and long-term memory in chat models using 
LangGraph
.


Edit this page
Was this page helpful?
Previous
Callbacks
Next
Chat models
Conversation patterns
Managing chat history
Related resources


--- docs_concepts_chat_models.txt ---

Conceptual guide
Chat models
On this page
Chat models


Overview
​


Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.


Modern LLMs are typically accessed through a chat model interface that takes a list of 
messages
 as input and returns a 
message
 as output.


The newest generation of chat models offer additional capabilities:




Tool calling
: Many popular chat models offer a native 
tool calling
 API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.


Structured output
: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.


Multimodality
: The ability to work with data other than text; for example, images, audio, and video.




Features
​


LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.




Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see 
chat model integrations
 for an up-to-date list of supported models.


Use either LangChain's 
messages
 format or OpenAI format.


Standard 
tool calling API
: standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.


Standard API for 
structuring outputs
 via the 
with_structured_output
 method.


Provides support for 
async programming
, 
efficient batching
, 
a rich streaming API
.


Integration with 
LangSmith
 for monitoring and debugging production-grade applications based on LLMs.


Additional features like standardized 
token usage
, 
rate limiting
, 
caching
 and more.




Integrations
​


LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.


These integrations are one of two types:




Official models
: These are models that are officially supported by LangChain and/or model provider. You can find these models in the 
langchain-<provider>
 packages.


Community models
: There are models that are mostly contributed and supported by the community. You can find these models in the 
langchain-community
 package.




LangChain chat models are named with a convention that prefixes "Chat" to their class names (e.g., 
ChatOllama
, 
ChatAnthropic
, 
ChatOpenAI
, etc.).


Please review the 
chat model integrations
 for a list of supported models.


note
Models that do 
not
 include the prefix "Chat" in their name or include "LLM" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.


Interface
​


LangChain chat models implement the 
BaseChatModel
 interface. Because 
BaseChatModel
 also implements the 
Runnable Interface
, chat models support a 
standard streaming interface
, 
async programming
, optimized 
batching
, and more. Please see the 
Runnable Interface
 for more details.


Many of the key methods of chat models operate on 
messages
 as input and return messages as output.


Chat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the 
standard parameters
 section for more details.


note
In documentation, we will often use the terms "LLM" and "Chat Model" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.
However, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the "Chat" prefix (e.g., 
Ollama
, 
Anthropic
, 
OpenAI
, etc.).
These models implement the 
BaseLLM
 interface and may be named with the "LLM" suffix (e.g., 
OllamaLLM
, 
AnthropicLLM
, 
OpenAILLM
, etc.). Generally, users should not use these models.


Key methods
​


The key methods of a chat model are:




invoke
: The primary method for interacting with a chat model. It takes a list of 
messages
 as input and returns a list of messages as output.


stream
: A method that allows you to stream the output of a chat model as it is generated.


batch
: A method that allows you to batch multiple requests to a chat model together for more efficient processing.


bind_tools
: A method that allows you to bind a tool to a chat model for use in the model's execution context.


with_structured_output
: A wrapper around the 
invoke
 method for models that natively support 
structured output
.




Other important methods can be found in the 
BaseChatModel API Reference
.


Inputs and outputs
​


Modern LLMs are typically accessed through a chat model interface that takes 
messages
 as input and returns 
messages
 as output. Messages are typically associated with a role (e.g., "system", "human", "assistant") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).


LangChain supports two message formats to interact with chat models:




LangChain Message Format
: LangChain's own message format, which is used by default and is used internally by LangChain.


OpenAI's Message Format
: OpenAI's message format.




Standard parameters
​


Many chat models have standardized parameters that can be used to configure the model:


Parameter
Description
model
The name or identifier of the specific AI model you want to use (e.g., 
"gpt-3.5-turbo"
 or 
"gpt-4"
).
temperature
Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.
timeout
The maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesn’t hang indefinitely.
max_tokens
Limits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.
stop
Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.
max_retries
The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.
api_key
The API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.
base_url
The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.
rate_limiter
An optional 
BaseRateLimiter
 to space out requests to avoid exceeding rate limits.  See 
rate-limiting
 below for more details.


Some important things to note:




Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can't be supported on these.


Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. 
langchain-openai
, 
langchain-anthropic
, etc.), they're not enforced on models in 
langchain-community
.




Chat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective 
API reference
 for that model.


Tool calling
​


Chat models can call 
tools
 to perform tasks such as fetching data from a database, making API requests, or running custom code. Please
see the 
tool calling
 guide for more information.


Structured outputs
​


Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. Please read more about
the technique in the 
structured outputs
 guide.


Multimodality
​


Large Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as 
multimodality
.


Currently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.


Context window
​


A chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.


If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the 
memory
.


The size of the input is measured in 
tokens
 which are the unit of processing that the model uses.


Advanced topics
​


Rate-limiting
​


Many chat model providers impose a limit on the number of requests that can be made in a given time period.


If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.


You have a few options to deal with rate limits:




Try to avoid hitting rate limits by spacing out requests: Chat models accept a 
rate_limiter
 parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the 
how to handle rate limits
 for more information on how to use this feature.


Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a 
max_retries
 parameter that can be used to control the number of retries. See the 
standard parameters
 section for more information.


Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.




Caching
​


Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.


The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the 
exact
 inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?


An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.


A semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an 
embedding model
 to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.


However, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.


Please see the 
how to cache chat model responses
 guide for more details.


Related resources
​




How-to guides on using chat models: 
how-to guides
.


List of supported chat models: 
chat model integrations
.




Conceptual guides
​




Messages


Tool calling


Multimodality


Structured outputs


Tokens


Edit this page
Was this page helpful?
Previous
Chat history
Next
Document loaders
Overview
Features
Integrations
Interface
Key methods
Inputs and outputs
Standard parameters
Tool calling
Structured outputs
Multimodality
Context window
Advanced topics
Rate-limiting
Caching
Related resources
Conceptual guides


--- docs_concepts_document_loaders.txt ---

Conceptual guide
Document loaders
On this page
Document loaders




Prerequisites


Document loaders API reference




Document loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.


Integrations
​


You can find available integrations on the 
Document loaders integrations page
.


Interface
​


Documents loaders implement the 
BaseLoader interface
.


Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the 
.load
 method or 
.lazy_load
.


Here's a simple example:


from
 langchain_community
.
document_loaders
.
csv_loader 
import
 CSVLoader
loader 
=
 CSVLoader
(
    
.
.
.
  
# <-- Integration specific parameters here
)
data 
=
 loader
.
load
(
)
API Reference:
CSVLoader


When working with large datasets, you can use the 
.lazy_load
 method:


for
 document 
in
 loader
.
lazy_load
(
)
:
    
print
(
document
)


Related resources
​


Please see the following resources for more information:




How-to guides for document loaders


Document API reference


Document loaders integrations


Edit this page
Was this page helpful?
Previous
Chat models
Next
Embedding models
Integrations
Interface
Related resources


--- docs_concepts_embedding_models.txt ---

Conceptual guide
Embedding models
On this page
Embedding models




Prerequisites


Documents




Note
This conceptual overview focuses on text-based embedding models.
Embedding models can also be 
multimodal
 though such models are not currently supported by LangChain.


Imagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation.
This is the power of embedding models, which lie at the heart of many retrieval systems.
Embedding models transform human language into a format that machines can understand and compare with speed and accuracy.
These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning.
Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.


Key concepts
​




(1) 
Embed text as a vector
: Embeddings transform text into a numerical vector representation.


(2) 
Measure similarity
: Embedding vectors can be compared using simple mathematical operations.


Embedding
​


Historical context
​


The landscape of embedding models has evolved significantly over the years.
A pivotal moment came in 2018 when Google introduced 
BERT (Bidirectional Encoder Representations from Transformers)
.
BERT applied transformer models to embed text as a simple vector representation, which lead to unprecedented performance across various NLP tasks.
However, BERT wasn't optimized for generating sentence embeddings efficiently.
This limitation spurred the creation of 
SBERT (Sentence-BERT)
, which adapted the BERT architecture to generate semantically rich sentence embeddings, easily comparable via similarity metrics like cosine similarity, dramatically reduced the computational overhead for tasks like finding similar sentences.
Today, the embedding model ecosystem is diverse, with numerous providers offering their own implementations.
To navigate this variety, researchers and practitioners often turn to benchmarks like the Massive Text Embedding Benchmark (MTEB) 
here
 for objective comparisons.


Further reading


See the 
seminal BERT paper
.


See Cameron Wolfe's 
excellent review
 of embedding models.


See the 
Massive Text Embedding Benchmark (MTEB)
 leaderboard for a comprehensive overview of embedding models.




Interface
​


LangChain provides a universal interface for working with them, providing standard methods for common operations.
This common interface simplifies interaction with various embedding providers through two central methods:




embed_documents
: For embedding multiple texts (documents)


embed_query
: For embedding a single text (query)




This distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself).
To illustrate, here's a practical example using LangChain's 
.embed_documents
 method to embed a list of strings:


from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings_model 
=
 OpenAIEmbeddings
(
)
embeddings 
=
 embeddings_model
.
embed_documents
(
    
[
        
"Hi there!"
,
        
"Oh, hello!"
,
        
"What's your name?"
,
        
"My friends call me World"
,
        
"Hello World!"
    
]
)
len
(
embeddings
)
,
 
len
(
embeddings
[
0
]
)
(
5
,
 
1536
)
API Reference:
OpenAIEmbeddings


For convenience, you can also use the 
embed_query
 method to embed a single text:


query_embedding 
=
 embeddings_model
.
embed_query
(
"What is the meaning of life?"
)


Further reading


See the full list of 
LangChain embedding model integrations
.


See these 
how-to guides
 for working with embedding models.




Integrations
​


LangChain offers many embedding model integrations which you can find 
on the embedding models
 integrations page.


Measure similarity
​


Each embedding is essentially a set of coordinates, often in a high-dimensional space.
In this space, the position of each point (embedding) reflects the meaning of its corresponding text.
Just as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space.
This allows for intuitive comparisons between different pieces of text.
By reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure.
Some common similarity metrics include:




Cosine Similarity
: Measures the cosine of the angle between two vectors.


Euclidean Distance
: Measures the straight-line distance between two points.


Dot Product
: Measures the projection of one vector onto another.




The choice of similarity metric should be chosen based on the model.
As an example, 
OpenAI suggests cosine similarity for their embeddings
, which can be easily implemented:


import
 numpy 
as
 np
def
 
cosine_similarity
(
vec1
,
 vec2
)
:
    dot_product 
=
 np
.
dot
(
vec1
,
 vec2
)
    norm_vec1 
=
 np
.
linalg
.
norm
(
vec1
)
    norm_vec2 
=
 np
.
linalg
.
norm
(
vec2
)
    
return
 dot_product 
/
 
(
norm_vec1 
*
 norm_vec2
)
similarity 
=
 cosine_similarity
(
query_result
,
 document_result
)
print
(
"Cosine Similarity:"
,
 similarity
)


Further reading


See Simon Willison’s 
nice blog post and video
 on embeddings and similarity metrics.


See 
this documentation
 from Google on similarity metrics to consider with embeddings.


See Pinecone's 
blog post
 on similarity metrics.


See OpenAI's 
FAQ
 on what similarity metric to use with OpenAI embeddings.


Edit this page
Was this page helpful?
Previous
Document loaders
Next
Evaluation
Key concepts
Embedding
Historical context
Interface
Integrations
Measure similarity


--- docs_concepts_evaluation.txt ---

Conceptual guide
Evaluation
Evaluation




Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.
This process is vital for building reliable applications.




LangSmith
 helps with this process in a few ways:




It makes it easier to create and curate datasets via its tracing and annotation features


It provides an evaluation framework that helps you define metrics and run your app against your dataset


It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code




To learn more, check out 
this LangSmith guide
.
Edit this page
Was this page helpful?
Previous
Embedding models
Next
Example selectors


--- docs_concepts_example_selectors.txt ---

Conceptual guide
Example selectors
On this page
Example selectors


Prerequisites


Chat models


Few-shot prompting




Overview
​


One common prompting technique for achieving better performance is to include examples as part of the prompt. This is known as 
few-shot prompting
.


This gives the 
language model
 concrete examples of how it should behave.
Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.


Example Selectors
 are classes responsible for selecting and then formatting examples into prompts.


Related resources
​




Example selector how-to guides


Edit this page
Was this page helpful?
Previous
Evaluation
Next
Few-shot prompting
Overview
Related resources


--- docs_concepts_few_shot_prompting.txt ---

Conceptual guide
Few-shot prompting
On this page
Few-shot prompting


Prerequisites


Chat models




Overview
​


One of the most effective ways to improve model performance is to give a model examples of
what you want it to do. The technique of adding example inputs and expected outputs
to a model prompt is known as "few-shot prompting". The technique is based on the

Language Models are Few-Shot Learners
 paper.
There are a few things to think about when doing few-shot prompting:




How are examples generated?


How many examples are in each prompt?


How are examples selected at runtime?


How are examples formatted in the prompt?




Here are the considerations for each.


1. Generating examples
​


The first and most important step of few-shot prompting is coming up with a good dataset of examples. Good examples should be relevant at runtime, clear, informative, and provide information that was not already known to the model.


At a high-level, the basic ways to generate examples are:




Manual: a person/people generates examples they think are useful.


Better model: a better (presumably more expensive/slower) model's responses are used as examples for a worse (presumably cheaper/faster) model.


User feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples).


LLM feedback: same as user feedback but the process is automated by having models evaluate themselves.




Which approach is best depends on your task. For tasks where a small number of core principles need to be understood really well, it can be valuable hand-craft a few really good examples.
For tasks where the space of correct behaviors is broader and more nuanced, it can be useful to generate many examples in a more automated fashion so that there's a higher likelihood of there being some highly relevant examples for any runtime input.


Single-turn v.s. multi-turn examples


Another dimension to think about when generating examples is what the example is actually showing.


The simplest types of examples just have a user input and an expected model output. These are single-turn examples.


One more complex type of example is where the example is an entire conversation, usually in which a model initially responds incorrectly and a user then tells the model how to correct its answer.
This is called a multi-turn example. Multi-turn examples can be useful for more nuanced tasks where it's useful to show common errors and spell out exactly why they're wrong and what should be done instead.


2. Number of examples
​


Once we have a dataset of examples, we need to think about how many examples should be in each prompt.
The key tradeoff is that more examples generally improve performance, but larger prompts increase costs and latency.
And beyond some threshold having too many examples can start to confuse the model.
Finding the right number of examples is highly dependent on the model, the task, the quality of the examples, and your cost and latency constraints.
Anecdotally, the better the model is the fewer examples it needs to perform well and the more quickly you hit steeply diminishing returns on adding more examples.
But, the best/only way to reliably answer this question is to run some experiments with different numbers of examples.


3. Selecting examples
​


Assuming we are not adding our entire example dataset into each prompt, we need to have a way of selecting examples from our dataset based on a given input. We can do this:




Randomly


By (semantic or keyword-based) similarity of the inputs


Based on some other constraints, like token size




LangChain has a number of 
ExampleSelectors
 which make it easy to use any of these techniques.


Generally, selecting by semantic similarity leads to the best model performance. But how important this is is again model and task specific, and is something worth experimenting with.


4. Formatting examples
​


Most state-of-the-art models these days are chat models, so we'll focus on formatting examples for those. Our basic options are to insert the examples:




In the system prompt as a string


As their own messages




If we insert our examples into the system prompt as a string, we'll need to make sure it's clear to the model where each example begins and which parts are the input versus output. Different models respond better to different syntaxes, like 
ChatML
, XML, TypeScript, etc.


If we insert our examples as messages, where each example is represented as a sequence of Human, AI messages, we might want to also assign 
names
 to our messages like 
"example_user"
 and 
"example_assistant"
 to make it clear that these messages correspond to different actors than the latest input message.


Formatting tool call examples


One area where formatting examples as messages can be tricky is when our example outputs have tool calls. This is because different models have different constraints on what types of message sequences are allowed when any tool calls are generated.




Some models require that any AIMessage with tool calls be immediately followed by ToolMessages for every tool call,


Some models additionally require that any ToolMessages be immediately followed by an AIMessage before the next HumanMessage,


Some models require that tools are passed into the model if there are any tool calls / ToolMessages in the chat history.




These requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy ToolMessages / AIMessages to the end of each example with generic contents to satisfy the API constraints.
In these cases it's especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models.


You can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks 
here
.
Edit this page
Was this page helpful?
Previous
Example selectors
Next
Conceptual guide
Overview
1. Generating examples
2. Number of examples
3. Selecting examples
4. Formatting examples


--- docs_concepts_key_value_stores.txt ---

Conceptual guide
Key-value stores
On this page
Key-value stores


Overview
​


LangChain provides a key-value store interface for storing and retrieving data.


LangChain includes a 
BaseStore
 interface,
which allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a
more specific 
BaseStore[str, bytes]
 instance that stores binary data (referred to as a 
ByteStore
), and internally take care of
encoding and decoding data for their specific needs.


This means that as a user, you only need to think about one type of store rather than different ones for different types of data.


Usage
​


The key-value store interface in LangChain is used primarily for:






Caching 
embeddings
 via 
CachedBackedEmbeddings
 to avoid recomputing embeddings for repeated queries or when re-indexing content.






As a simple 
Document
 persistence layer in some retrievers.






Please see these how-to guides for more information:




How to cache embeddings guide
.


How to retriever using multiple vectors per document
.




Interface
​


All 
BaseStores
 support the following interface. Note that the interface allows for modifying 
multiple
 key-value pairs at once:




mget(key: Sequence[str]) -> List[Optional[bytes]]
: get the contents of multiple keys, returning 
None
 if the key does not exist


mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None
: set the contents of multiple keys


mdelete(key: Sequence[str]) -> None
: delete multiple keys


yield_keys(prefix: Optional[str] = None) -> Iterator[str]
: yield all keys in the store, optionally filtering by a prefix




Integrations
​


Please reference the 
stores integration page
 for a list of available key-value store integrations.
Edit this page
Was this page helpful?
Previous
Conceptual guide
Next
LangChain Expression Language (LCEL)
Overview
Usage
Interface
Integrations


--- docs_concepts_lcel.txt ---

Conceptual guide
LangChain Expression Language (LCEL)
On this page
LangChain Expression Language (LCEL)


Prerequisites


Runnable Interface




The 
L
ang
C
hain 
E
xpression 
L
anguage (LCEL) takes a 
declarative
 approach to building new 
Runnables
 from existing Runnables.


This means that you describe what 
should
 happen, rather than 
how
 it should happen, allowing LangChain to optimize the run-time execution of the chains.


We often refer to a 
Runnable
 created using LCEL as a "chain". It's important to remember that a "chain" is 
Runnable
 and it implements the full 
Runnable Interface
.


note


The 
LCEL cheatsheet
 shows common patterns that involve the Runnable interface and LCEL expressions.


Please see the following list of 
how-to guides
 that cover common tasks with LCEL.


A list of built-in 
Runnables
 can be found in the 
LangChain Core API Reference
. Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.




Benefits of LCEL
​


LangChain optimizes the run-time execution of chains built with LCEL in a number of ways:




Optimized parallel execution
: Run Runnables in parallel using 
RunnableParallel
 or run multiple inputs through a given chain in parallel using the 
Runnable Batch API
. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.


Guaranteed Async support
: Any chain built with LCEL can be run asynchronously using the 
Runnable Async API
. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.


Simplify streaming
: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a 
chat model
 or 
llm
 comes out).




Other benefits include:




Seamless LangSmith tracing

As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.
With LCEL, 
all
 steps are automatically logged to 
LangSmith
 for maximum observability and debuggability.


Standard API
: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.


Deployable with LangServe
: Chains built with LCEL can be deployed using for production use.




Should I use LCEL?
​


LCEL is an 
orchestration solution
 -- it allows LangChain to handle run-time execution of chains in an optimized way.


While we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of 
LangGraph
.


In LangGraph, users define graphs that specify the application's flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.


Here are some guidelines:




If you are making a single LLM call, you don't need LCEL; instead call the underlying 
chat model
 directly.


If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you're taking advantage of the LCEL benefits.


If you're building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use 
LangGraph
 instead. Remember that you can always use LCEL within individual nodes in LangGraph.




Composition Primitives
​


LCEL
 chains are built by composing existing 
Runnables
 together. The two main composition primitives are 
RunnableSequence
 and 
RunnableParallel
.


Many other composition primitives (e.g., 
RunnableAssign
) can be thought of as variations of these two primitives.


note
You can find a list of all composition primitives in the 
LangChain Core API Reference
.


RunnableSequence
​


RunnableSequence
 is a composition primitive that allows you "chain" multiple runnables sequentially, with the output of one runnable serving as the input to the next.


from
 langchain_core
.
runnables 
import
 RunnableSequence
chain 
=
 RunnableSequence
(
[
runnable1
,
 runnable2
]
)
API Reference:
RunnableSequence


Invoking the 
chain
 with some input:


final_output 
=
 chain
.
invoke
(
some_input
)


corresponds to the following:


output1 
=
 runnable1
.
invoke
(
some_input
)
final_output 
=
 runnable2
.
invoke
(
output1
)


note
runnable1
 and 
runnable2
 are placeholders for any 
Runnable
 that you want to chain together.


RunnableParallel
​


RunnableParallel
 is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.


from
 langchain_core
.
runnables 
import
 RunnableParallel
chain 
=
 RunnableParallel
(
{
    
"key1"
:
 runnable1
,
    
"key2"
:
 runnable2
,
}
)
API Reference:
RunnableParallel


Invoking the 
chain
 with some input:


final_output 
=
 chain
.
invoke
(
some_input
)


Will yield a 
final_output
 dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.


{
    
"key1"
:
 runnable1
.
invoke
(
some_input
)
,
    
"key2"
:
 runnable2
.
invoke
(
some_input
)
,
}


Recall, that the runnables are executed in parallel, so while the result is the same as
dictionary comprehension shown above, the execution time is much faster.


note
RunnableParallel
supports both synchronous and asynchronous execution (as all 
Runnables
 do).


For synchronous execution, 
RunnableParallel
 uses a 
ThreadPoolExecutor
 to run the runnables concurrently.


For asynchronous execution, 
RunnableParallel
 uses 
asyncio.gather
 to run the runnables concurrently.




Composition Syntax
​


The usage of 
RunnableSequence
 and 
RunnableParallel
 is so common that we created a shorthand syntax for using them. This helps
to make the code more readable and concise.


The 
|
 operator
​


We have 
overloaded
 the 
|
 operator to create a 
RunnableSequence
 from two 
Runnables
.


chain 
=
 runnable1 
|
 runnable2


is Equivalent to:


chain 
=
 RunnableSequence
(
[
runnable1
,
 runnable2
]
)


The 
.pipe
 method
​


If you have moral qualms with operator overloading, you can use the 
.pipe
 method instead. This is equivalent to the 
|
 operator.


chain 
=
 runnable1
.
pipe
(
runnable2
)


Coercion
​


LCEL applies automatic type coercion to make it easier to compose chains.


If you do not understand the type coercion, you can always use the 
RunnableSequence
 and 
RunnableParallel
 classes directly.


This will make the code more verbose, but it will also make it more explicit.


Dictionary to RunnableParallel
​


Inside an LCEL expression, a dictionary is automatically converted to a 
RunnableParallel
.


For example, the following code:


mapping 
=
 
{
    
"key1"
:
 runnable1
,
    
"key2"
:
 runnable2
,
}
chain 
=
 mapping 
|
 runnable3


It gets automatically converted to the following:


chain 
=
 RunnableSequence
(
[
RunnableParallel
(
mapping
)
,
 runnable3
]
)


caution
You have to be careful because the 
mapping
 dictionary is not a 
RunnableParallel
 object, it is just a dictionary. This means that the following code will raise an 
AttributeError
:
mapping
.
invoke
(
some_input
)


Function to RunnableLambda
​


Inside an LCEL expression, a function is automatically converted to a 
RunnableLambda
.


def some_func(x):
    return x
chain = some_func | runnable1


It gets automatically converted to the following:


chain 
=
 RunnableSequence
(
[
RunnableLambda
(
some_func
)
,
 runnable1
]
)


caution
You have to be careful because the lambda function is not a 
RunnableLambda
 object, it is just a function. This means that the following code will raise an 
AttributeError
:
lambda
 x
:
 x 
+
 
1
.
invoke
(
some_input
)


Legacy chains
​


LCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as 
LLMChain
 and

ConversationalRetrievalChain
. Many of these legacy chains hide important details like prompts, and as a wider variety
of viable models emerge, customization has become more and more important.


If you are currently using one of these legacy chains, please see 
this guide for guidance on how to migrate
.


For guides on how to do specific tasks with LCEL, check out 
the relevant how-to guides
.
Edit this page
Was this page helpful?
Previous
Key-value stores
Next
Messages
Benefits of LCEL
Should I use LCEL?
Composition Primitives
RunnableSequence
RunnableParallel
Composition Syntax
The 
|
 operator
The 
.pipe
 method
Coercion
Legacy chains


--- docs_concepts_messages.txt ---

Conceptual guide
Messages
On this page
Messages


Prerequisites


Chat Models




Overview
​


Messages are the unit of communication in 
chat models
. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.


Each message has a 
role
 (e.g., "user", "assistant") and 
content
 (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.


LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.


What is inside a message?
​


A message typically consists of the following pieces of information:




Role
: The role of the message (e.g., "user", "assistant").


Content
: The content of the message (e.g., text, multimodal data).


Additional metadata: id, name, 
token usage
 and other model-specific metadata.




Role
​


Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.


Role
Description
system
Used to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.
user
Represents input from a user interacting with the model, usually in the form of text or other interactive input.
assistant
Represents a response from the model, which can include text or a request to invoke tools.
tool
A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support 
tool calling
.
function
 (legacy)
This is a legacy role, corresponding to OpenAI's legacy function-calling API. 
tool
 role should be used instead.


Content
​


The content of a message text or a list of dictionaries representing 
multimodal data
 (e.g., images, audio, video). The exact format of the content can vary between different chat model providers.


Currently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.


For more information see:




SystemMessage
 -- for content which should be passed to direct the conversation


HumanMessage
 -- for content in the input from the user.


AIMessage
 -- for content in the response from the model.


Multimodality
 -- for more information on multimodal content.




Other Message Data
​


Depending on the chat model provider, messages can include other data such as:




ID
: An optional unique identifier for the message.


Name
: An optional 
name
 property which allows differentiate between different entities/speakers with the same role. Not all models support this!


Metadata
: Additional information about the message, such as timestamps, token usage, etc.


Tool Calls
: A request made by the model to call one or more tools> See 
tool calling
 for more information.




Conversation Structure
​


The sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response.


For example, a typical conversation structure might look like this:




User Message
: "Hello, how are you?"


Assistant Message
: "I'm doing well, thank you for asking."


User Message
: "Can you tell me a joke?"


Assistant Message
: "Sure! Why did the scarecrow win an award? Because he was outstanding in his field!"




Please read the 
chat history
 guide for more information on managing chat history and ensuring that the conversation structure is correct.


LangChain Messages
​


LangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.


LangChain messages are Python objects that subclass from a 
BaseMessage
.


The five main message types are:




SystemMessage
: corresponds to 
system
 role


HumanMessage
: corresponds to 
user
 role


AIMessage
: corresponds to 
assistant
 role


AIMessageChunk
: corresponds to 
assistant
 role, used for 
streaming
 responses


ToolMessage
: corresponds to 
tool
 role




Other important messages include:




RemoveMessage
 -- does not correspond to any role. This is an abstraction, mostly used in 
LangGraph
 to manage chat history.


Legacy
 
FunctionMessage
: corresponds to the 
function
 role in OpenAI's 
legacy
 function-calling API.




You can find more information about 
messages
 in the 
API Reference
.


SystemMessage
​


A 
SystemMessage
 is used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., "This is a conversation about cooking").


Different chat providers may support system message in one of the following ways:




Through a "system" message role
: In this case, a system message is included as part of the message sequence with the role explicitly set as "system."


Through a separate API parameter for system instructions
: Instead of being included as a message, system instructions are passed via a dedicated API parameter.


No support for system messages
: Some models do not support system messages at all.




Most major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the provider’s capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter.


If no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message's content into a HumanMessage or raise an exception if that is not possible. However, this behavior is not yet consistently enforced across all implementations, and if using a less popular implementation of a chat model (e.g., an implementation from the 
langchain-community
 package) it is recommended to check the specific documentation for that model.


HumanMessage
​


The 
HumanMessage
 corresponds to the 
"user"
 role. A human message represents input from a user interacting with the model.


Text Content
​


Most chat models expect the user input to be in the form of text.


from
 langchain_core
.
messages 
import
 HumanMessage
model
.
invoke
(
[
HumanMessage
(
content
=
"Hello, how are you?"
)
]
)
API Reference:
HumanMessage


tip
When invoking a chat model with a string as input, LangChain will automatically convert the string into a 
HumanMessage
 object. This is mostly useful for quick testing.
model
.
invoke
(
"Hello, how are you?"
)


Multi-modal Content
​


Some chat models accept multimodal inputs, such as images, audio, video, or files like PDFs.


Please see the 
multimodality
 guide for more information.


AIMessage
​


AIMessage
 is used to represent a message with the role 
"assistant"
. This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment.


from
 langchain_core
.
messages 
import
 HumanMessage
ai_message 
=
 model
.
invoke
(
[
HumanMessage
(
"Tell me a joke"
)
]
)
ai_message 
# <-- AIMessage
API Reference:
HumanMessage


An 
AIMessage
 has the following attributes. The attributes which are 
standardized
 are the ones that LangChain attempts to standardize across different chat model providers. 
raw
 fields are specific to the model provider and may vary.


Attribute
Standardized/Raw
Description
content
Raw
Usually a string, but can be a list of content blocks. See 
content
 for details.
tool_calls
Standardized
Tool calls associated with the message. See 
tool calling
 for details.
invalid_tool_calls
Standardized
Tool calls with parsing errors associated with the message. See 
tool calling
 for details.
usage_metadata
Standardized
Usage metadata for a message, such as 
token counts
. See 
Usage Metadata API Reference
.
id
Standardized
An optional unique identifier for the message, ideally provided by the provider/model that created the message.
response_metadata
Raw
Response metadata, e.g., response headers, logprobs, token counts.


content
​


The 
content
 property of an 
AIMessage
 represents the response generated by the chat model.


The content is either:




text
 -- the norm for virtually all chat models.


A 
list of dictionaries
 -- Each dictionary represents a content block and is associated with a 
type
.




Used by Anthropic for surfacing agent thought process when doing 
tool calling
.


Used by OpenAI for audio outputs. Please see 
multi-modal content
 for more information.








important
The 
content
 property is 
not
 standardized across different chat model providers, mostly because there are
still few examples to generalize from.


AIMessageChunk
​


It is common to 
stream
 responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.


It is returned from the 
stream
, 
astream
 and 
astream_events
 methods of the chat model.


For example,


for
 chunk 
in
 model
.
stream
(
[
HumanMessage
(
"what color is the sky?"
)
]
)
:
    
print
(
chunk
)


AIMessageChunk
 follows nearly the same structure as 
AIMessage
, but uses a different 
ToolCallChunk

to be able to stream tool calling in a standardized manner.


Aggregating
​


AIMessageChunks
 support the 
+
 operator to merge them into a single 
AIMessage
. This is useful when you want to display the final response to the user.


ai_message 
=
 chunk1 
+
 chunk2 
+
 chunk3 
+
 
.
.
.


ToolMessage
​


This represents a message with role "tool", which contains the result of 
calling a tool
. In addition to 
role
 and 
content
, this message has:




a 
tool_call_id
 field which conveys the id of the call to the tool that was called to produce this result.


an 
artifact
 field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.




Please see 
tool calling
 for more information.


RemoveMessage
​


This is a special message type that does not correspond to any roles. It is used
for managing chat history in 
LangGraph
.


Please see the following for more information on how to use the 
RemoveMessage
:




Memory conceptual guide


How to delete messages




(Legacy) FunctionMessage
​


This is a legacy message type, corresponding to OpenAI's legacy function-calling API. 
ToolMessage
 should be used instead to correspond to the updated tool-calling API.


OpenAI Format
​


Inputs
​


Chat models also accept OpenAI's format as 
inputs
 to chat models:


chat_model
.
invoke
(
[
    
{
        
"role"
:
 
"user"
,
        
"content"
:
 
"Hello, how are you?"
,
    
}
,
    
{
        
"role"
:
 
"assistant"
,
        
"content"
:
 
"I'm doing well, thank you for asking."
,
    
}
,
    
{
        
"role"
:
 
"user"
,
        
"content"
:
 
"Can you tell me a joke?"
,
    
}
]
)


Outputs
​


At the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you
need OpenAI format for the output as well.


The 
convert_to_openai_messages
 utility function can be used to convert from LangChain messages to OpenAI format.
Edit this page
Was this page helpful?
Previous
LangChain Expression Language (LCEL)
Next
Multimodality
Overview
What is inside a message?
Role
Content
Other Message Data
Conversation Structure
LangChain Messages
SystemMessage
HumanMessage
AIMessage
AIMessageChunk
ToolMessage
RemoveMessage
(Legacy) FunctionMessage
OpenAI Format
Inputs
Outputs


--- docs_concepts_multimodality.txt ---

Conceptual guide
Multimodality
On this page
Multimodality


Overview
​


Multimodality
 refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various components, allowing models and systems to handle and process a mix of these data types seamlessly.




Chat Models
: These could, in theory, accept and generate multimodal inputs and outputs, handling a variety of data types like text, images, audio, and video.


Embedding Models
: Embedding Models can represent multimodal content, embedding various forms of data—such as text, images, and audio—into vector spaces.


Vector Stores
: Vector stores could search over embeddings that represent multimodal data, enabling retrieval across different types of information.




Multimodality in chat models
​


Pre-requisites


Chat models


Messages




LangChain supports multimodal data as input to chat models:




Following provider-specific formats


Adhering to a cross-provider standard (see 
how-to guides
 for detail)




How to use multimodal models
​




Use the 
chat model integration table
 to identify which models support multimodality.


Reference the 
relevant how-to guides
 for specific examples of how to use multimodal models.




What kind of multimodality is supported?
​


Inputs
​


Some models can accept multimodal inputs, such as images, audio, video, or files.
The types of multimodal inputs supported depend on the model provider. For instance,

OpenAI
,

Anthropic
, and

Google Gemini

support documents like PDFs as inputs.


The gist of passing multimodal inputs to a chat model is to use content blocks that
specify a type and corresponding data. For example, to pass an image to a chat model
as URL:


from
 langchain_core
.
messages 
import
 HumanMessage
message 
=
 HumanMessage
(
    content
=
[
        
{
"type"
:
 
"text"
,
 
"text"
:
 
"Describe the weather in this image:"
}
,
        
{
            
"type"
:
 
"image"
,
            
"source_type"
:
 
"url"
,
            
"url"
:
 
"https://..."
,
        
}
,
    
]
,
)
response 
=
 model
.
invoke
(
[
message
]
)
API Reference:
HumanMessage


We can also pass the image as in-line data:


from
 langchain_core
.
messages 
import
 HumanMessage
message 
=
 HumanMessage
(
    content
=
[
        
{
"type"
:
 
"text"
,
 
"text"
:
 
"Describe the weather in this image:"
}
,
        
{
            
"type"
:
 
"image"
,
            
"source_type"
:
 
"base64"
,
            
"data"
:
 
"<base64 string>"
,
            
"mime_type"
:
 
"image/jpeg"
,
        
}
,
    
]
,
)
response 
=
 model
.
invoke
(
[
message
]
)
API Reference:
HumanMessage


To pass a PDF file as in-line data (or URL, as supported by providers such as
Anthropic), just change 
"type"
 to 
"file"
 and 
"mime_type"
 to 
"application/pdf"
.


See the 
how-to guides
 for more detail.


Most chat models that support multimodal 
image
 inputs also accept those values in
OpenAI's 
Chat Completions format
:


from
 langchain_core
.
messages 
import
 HumanMessage
message 
=
 HumanMessage
(
    content
=
[
        
{
"type"
:
 
"text"
,
 
"text"
:
 
"Describe the weather in this image:"
}
,
        
{
"type"
:
 
"image_url"
,
 
"image_url"
:
 
{
"url"
:
 image_url
}
}
,
    
]
,
)
response 
=
 model
.
invoke
(
[
message
]
)
API Reference:
HumanMessage


Otherwise, chat models will typically accept the native, provider-specific content
block format. See 
chat model integrations
 for detail
on specific providers.


Outputs
​


Some chat models support multimodal outputs, such as images and audio. Multimodal
outputs will appear as part of the 
AIMessage

response object. See for example:




Generating 
audio outputs
 with OpenAI;


Generating 
image outputs
 with Google Gemini.




Tools
​


Currently, no chat model is designed to work 
directly
 with multimodal data in a 
tool call request
 or 
ToolMessage
 result.


However, a chat model can easily interact with multimodal data by invoking tools with references (e.g., a URL) to the multimodal data, rather than the data itself. For example, any model capable of 
tool calling
 can be equipped with tools to download and process images, audio, or video.


Multimodality in embedding models
​


Prerequisites


Embedding Models




Embeddings
 are vector representations of data used for tasks like similarity search and retrieval.


The current 
embedding interface
 used in LangChain is optimized entirely for text-based data, and will 
not
 work with multimodal data.


As use cases involving multimodal search and retrieval tasks become more common, we expect to expand the embedding interface to accommodate other data types like images, audio, and video.


Multimodality in vector stores
​


Prerequisites


Vector stores




Vector stores are databases for storing and retrieving embeddings, which are typically used in search and retrieval tasks. Similar to embeddings, vector stores are currently optimized for text-based data.


As use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video.
Edit this page
Was this page helpful?
Previous
Messages
Next
Output parsers
Overview
Multimodality in chat models
How to use multimodal models
What kind of multimodality is supported?
Multimodality in embedding models
Multimodality in vector stores


--- docs_concepts_output_parsers.txt ---

Conceptual guide
Output parsers
Output parsers




note
The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.
More and more models are supporting function (or tool) calling, which handles this automatically.
It is recommended to use function/tool calling rather than output parsing.
See documentation for that 
here
.


Output parser
 is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.
Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.


LangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:




Name
: The name of the output parser


Supports Streaming
: Whether the output parser supports streaming.


Has Format Instructions
: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.


Calls LLM
: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.


Input Type
: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.


Output Type
: The output type of the object returned by the parser.


Description
: Our commentary on this output parser and when to use it.




Name
Supports Streaming
Has Format Instructions
Calls LLM
Input Type
Output Type
Description
Str
✅
str
 | 
Message
String
Parses texts from message objects. Useful for handling variable formats of message content (e.g., extracting text from content blocks).
JSON
✅
✅
str
 | 
Message
JSON object
Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.
XML
✅
✅
str
 | 
Message
dict
Returns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).
CSV
✅
✅
str
 | 
Message
List[str]
Returns a list of comma separated values.
OutputFixing
✅
str
 | 
Message
Wraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.
RetryWithError
✅
str
 | 
Message
Wraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.
Pydantic
✅
str
 | 
Message
pydantic.BaseModel
Takes a user defined Pydantic model and returns data in that format.
YAML
✅
str
 | 
Message
pydantic.BaseModel
Takes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.
PandasDataFrame
✅
str
 | 
Message
dict
Useful for doing operations with pandas DataFrames.
Enum
✅
str
 | 
Message
Enum
Parses response into one of the provided enum values.
Datetime
✅
str
 | 
Message
datetime.datetime
Parses response into a datetime string.
Structured
✅
str
 | 
Message
Dict[str, str]
An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.


For specifics on how to use output parsers, see the 
relevant how-to guides here
.
Edit this page
Was this page helpful?
Previous
Multimodality
Next
Prompt Templates


--- docs_concepts_prompt_templates.txt ---

Conceptual guide
Prompt Templates
On this page
Prompt Templates


Prompt templates help to translate user input and parameters into instructions for a language model.
This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.


Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.


Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.
The reason this PromptValue exists is to make it easy to switch between strings and messages.


There are a few different types of prompt templates:


String PromptTemplates
​


These prompt templates are used to format a single string, and generally are used for simpler inputs.
For example, a common way to construct and use a PromptTemplate is as follows:


from
 langchain_core
.
prompts 
import
 PromptTemplate
prompt_template 
=
 PromptTemplate
.
from_template
(
"Tell me a joke about {topic}"
)
prompt_template
.
invoke
(
{
"topic"
:
 
"cats"
}
)
API Reference:
PromptTemplate


ChatPromptTemplates
​


These prompt templates are used to format a list of messages. These "templates" consist of a list of templates themselves.
For example, a common way to construct and use a ChatPromptTemplate is as follows:


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
prompt_template 
=
 ChatPromptTemplate
(
[
    
(
"system"
,
 
"You are a helpful assistant"
)
,
    
(
"user"
,
 
"Tell me a joke about {topic}"
)
]
)
prompt_template
.
invoke
(
{
"topic"
:
 
"cats"
}
)
API Reference:
ChatPromptTemplate


In the above example, this ChatPromptTemplate will construct two messages when called.
The first is a system message, that has no variables to format.
The second is a HumanMessage, and will be formatted by the 
topic
 variable the user passes in.


MessagesPlaceholder
​




This prompt template is responsible for adding a list of messages in a particular place.
In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.
But what if we wanted the user to pass in a list of messages that we would slot into a particular spot?
This is how you use MessagesPlaceholder.


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 MessagesPlaceholder
from
 langchain_core
.
messages 
import
 HumanMessage
prompt_template 
=
 ChatPromptTemplate
(
[
    
(
"system"
,
 
"You are a helpful assistant"
)
,
    MessagesPlaceholder
(
"msgs"
)
]
)
prompt_template
.
invoke
(
{
"msgs"
:
 
[
HumanMessage
(
content
=
"hi!"
)
]
}
)
API Reference:
ChatPromptTemplate
 | 
MessagesPlaceholder
 | 
HumanMessage


This will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.
If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).
This is useful for letting a list of messages be slotted into a particular spot.


An alternative way to accomplish the same thing without using the 
MessagesPlaceholder
 class explicitly is:


prompt_template 
=
 ChatPromptTemplate
(
[
    
(
"system"
,
 
"You are a helpful assistant"
)
,
    
(
"placeholder"
,
 
"{msgs}"
)
 
# <-- This is the changed part
]
)


For specifics on how to use prompt templates, see the 
relevant how-to guides here
.
Edit this page
Was this page helpful?
Previous
Output parsers
Next
Retrieval augmented generation (RAG)
String PromptTemplates
ChatPromptTemplates
MessagesPlaceholder


--- docs_concepts_rag.txt ---

Conceptual guide
Retrieval augmented generation (RAG)
On this page
Retrieval augmented generation (RAG)


Prerequisites


Retrieval




Overview
​


Retrieval Augmented Generation (RAG) is a powerful technique that enhances 
language models
 by combining them with external knowledge bases.
RAG addresses 
a key limitation of models
: models rely on fixed training datasets, which can lead to outdated or incomplete information.
When given a query, RAG systems first search a knowledge base for relevant information.
The system then incorporates this retrieved information into the model's prompt.
The model uses the provided context to generate a response to the query.
By bridging the gap between vast language models and dynamic, targeted information retrieval, RAG is a powerful technique for building more capable and reliable AI systems.


Key concepts
​




(1) 
Retrieval system
: Retrieve relevant information from a knowledge base.


(2) 
Adding external knowledge
: Pass retrieved information to a model.


Retrieval system
​


Model's have internal knowledge that is often fixed, or at least not updated frequently due to the high cost of training.
This limits their ability to answer questions about current events, or to provide specific domain knowledge.
To address this, there are various knowledge injection techniques like 
fine-tuning
 or continued pre-training.
Both are 
costly
 and often 
poorly suited
 for factual retrieval.
Using a retrieval system offers several advantages:




Up-to-date information
: RAG can access and utilize the latest data, keeping responses current.


Domain-specific expertise
: With domain-specific knowledge bases, RAG can provide answers in specific domains.


Reduced hallucination
: Grounding responses in retrieved facts helps minimize false or invented information.


Cost-effective knowledge integration
: RAG offers a more efficient alternative to expensive model fine-tuning.




Further reading
See our conceptual guide on 
retrieval
.


Adding external knowledge
​


With a retrieval system in place, we need to pass knowledge from this system to the model.
A RAG pipeline typically achieves this following these steps:




Receive an input query.


Use the retrieval system to search for relevant information based on the query.


Incorporate the retrieved information into the prompt sent to the LLM.


Generate a response that leverages the retrieved context.




As an example, here's a simple RAG workflow that passes information from a 
retriever
 to a 
chat model
:


from
 langchain_openai 
import
 ChatOpenAI
from
 langchain_core
.
messages 
import
 SystemMessage
,
 HumanMessage
# Define a system prompt that tells the model how to use the retrieved context
system_prompt 
=
 
"""You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
Use three sentences maximum and keep the answer concise.
Context: {context}:"""
    
# Define a question
question 
=
 
"""What are the main components of an LLM-powered autonomous agent system?"""
# Retrieve relevant documents
docs 
=
 retriever
.
invoke
(
question
)
# Combine the documents into a single string
docs_text 
=
 
""
.
join
(
d
.
page_content 
for
 d 
in
 docs
)
# Populate the system prompt with the retrieved context
system_prompt_fmt 
=
 system_prompt
.
format
(
context
=
docs_text
)
# Create a model
model 
=
 ChatOpenAI
(
model
=
"gpt-4o"
,
 temperature
=
0
)
 
# Generate a response
questions 
=
 model
.
invoke
(
[
SystemMessage
(
content
=
system_prompt_fmt
)
,
                          HumanMessage
(
content
=
question
)
]
)
API Reference:
ChatOpenAI
 | 
SystemMessage
 | 
HumanMessage


Further reading
RAG a deep area with many possible optimization and design choices:


See 
this excellent blog
 from Cameron Wolfe for a comprehensive overview and history of RAG.


See our 
RAG how-to guides
.


See our RAG 
tutorials
.


See our RAG from Scratch course, with 
code
 and 
video playlist
.


Also, see our RAG from Scratch course 
on Freecodecamp
.


Edit this page
Was this page helpful?
Previous
Prompt Templates
Next
Retrieval
Overview
Key concepts
Retrieval system
Adding external knowledge


--- docs_concepts_retrieval.txt ---

Conceptual guide
Retrieval
On this page
Retrieval


Prerequisites


Retrievers


Vector stores


Embeddings


Text splitters




Security
Some of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).
There are inherent risks in doing this.
Make sure that your database connection permissions are scoped as narrowly as possible for your application's needs.
This will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.
For more on general security best practices, see our 
security guide
.


Overview
​


Retrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.
These systems accommodate various data formats:




Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.


Structured data is typically housed in relational or graph databases with defined schemas.




Despite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.
Models play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.
This translation enables more intuitive and flexible interactions with complex data structures.


Key concepts
​




(1) 
Query analysis
: A process where models transform or construct search queries to optimize retrieval.


(2) 
Information retrieval
: Search queries are used to fetch information from various retrieval systems.


Query analysis
​


While users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.
Query analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:




Query Re-writing
: Queries can be re-written or expanded to improve semantic or lexical searches.


Query Construction
: Search indexes may require structured queries (e.g., SQL for databases).




Query analysis employs models to transform or construct optimized search queries from raw user input.


Query re-writing
​


Retrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.
To achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.
This transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.
Here are some key benefits of using models for query analysis in unstructured data retrieval:




Query Clarification
: Models can rephrase ambiguous or poorly worded queries for clarity.


Semantic Understanding
: They can capture the intent behind a query, going beyond literal keyword matching.


Query Expansion
: Models can generate related terms or concepts to broaden the search scope.


Complex Query Handling
: They can break down multi-part questions into simpler sub-queries.




Various techniques have been developed to leverage models for query re-writing, including:


Name
When to use
Description
Multi-query
When you want to ensure high recall in retrieval by providing multiple phrasings of a question.
Rewrite the user question with multiple phrasings, retrieve documents for each rewritten question, return the unique documents for all queries.
Decomposition
When a question can be broken down into smaller subproblems.
Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).
Step-back
When a higher-level conceptual understanding is required.
First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. 
Paper
.
HyDE
If you have challenges retrieving relevant documents using the raw user inputs.
Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. 
Paper
.


As an example, query decomposition can simply be accomplished using prompting and a structured output that enforces a list of sub-questions.
These can then be run sequentially or in parallel on a downstream retrieval system.


from
 typing 
import
 List
from
 pydantic 
import
 BaseModel
,
 Field
from
 langchain_openai 
import
 ChatOpenAI
from
 langchain_core
.
messages 
import
 SystemMessage
,
 HumanMessage
# Define a pydantic model to enforce the output structure
class
 
Questions
(
BaseModel
)
:
    questions
:
 List
[
str
]
 
=
 Field
(
        description
=
"A list of sub-questions related to the input query."
    
)
# Create an instance of the model and enforce the output structure
model 
=
 ChatOpenAI
(
model
=
"gpt-4o"
,
 temperature
=
0
)
 
structured_model 
=
 model
.
with_structured_output
(
Questions
)
# Define the system prompt
system 
=
 
"""You are a helpful assistant that generates multiple sub-questions related to an input question. \n
The goal is to break down the input into a set of sub-problems / sub-questions that can be answered independently. \n"""
# Pass the question to the model
question 
=
 
"""What are the main components of an LLM-powered autonomous agent system?"""
questions 
=
 structured_model
.
invoke
(
[
SystemMessage
(
content
=
system
)
]
+
[
HumanMessage
(
content
=
question
)
]
)
API Reference:
ChatOpenAI
 | 
SystemMessage
 | 
HumanMessage


tip
See our RAG from Scratch videos for a few different specific approaches:


Multi-query


Decomposition


Step-back


HyDE




Query construction
​


Query analysis also can focus on translating natural language queries into specialized query languages or filters.
This translation is crucial for effectively interacting with various types of databases that house structured or semi-structured data.






Structured Data examples
: For relational and graph databases, Domain-Specific Languages (DSLs) are used to query data.




Text-to-SQL
: 
Converts natural language to SQL
 for relational databases.


Text-to-Cypher
: 
Converts natural language to Cypher
 for graph databases.








Semi-structured Data examples
: For vectorstores, queries can combine semantic search with metadata filtering.




Natural Language to Metadata Filters
: Converts user queries into 
appropriate metadata filters
.








These approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:


Name
When to Use
Description
Self Query
If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.
This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).
Text to SQL
If users are asking questions that require information housed in a relational database, accessible via SQL.
This uses an LLM to transform user input into a SQL query.
Text-to-Cypher
If users are asking questions that require information housed in a graph database, accessible via Cypher.
This uses an LLM to transform user input into a Cypher query.


As an example, here is how to use the 
SelfQueryRetriever
 to convert natural language queries into metadata filters.


metadata_field_info 
=
 schema_for_metadata 
document_content_description 
=
 
"Brief summary of a movie"
llm 
=
 ChatOpenAI
(
temperature
=
0
)
retriever 
=
 SelfQueryRetriever
.
from_llm
(
    llm
,
    vectorstore
,
    document_content_description
,
    metadata_field_info
,
)


Further reading


See our tutorials on 
text-to-SQL
, 
text-to-Cypher
, and 
query analysis for metadata filters
.


See our 
blog post overview
.


See our RAG from Scratch video on 
query construction
.




Information retrieval
​


Common retrieval systems
​


Lexical search indexes
​


Many search engines are based upon matching words in a query to the words in each document.
This approach is called lexical retrieval, using search 
algorithms that are typically based upon word frequencies
.
The intution is simple: a word appears frequently both in the user’s query and a particular document, then this document might be a good match.


The particular data structure used to implement this is often an 
inverted index
.
This types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.
Using this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.

BM25
 and 
TF-IDF
 are 
two popular lexical search algorithms
.


Further reading


See the 
BM25
 retriever integration.


See the 
Elasticsearch
 retriever integration.




Vector indexes
​


Vector indexes are an alternative way to index and store unstructured data.
See our conceptual guide on 
vectorstores
 for a detailed overview.


In short, rather than using word frequencies, vectorstores use an 
embedding model
 to compress documents into high-dimensional vector representation.
This allows for efficient similarity search over embedding vectors using simple mathematical operations like cosine similarity.


Further reading


See our 
how-to guide
 for more details on working with vectorstores.


See our 
list of vectorstore integrations
.


See Cameron Wolfe's 
blog post
 on the basics of vector search.




Relational databases
​


Relational databases are a fundamental type of structured data storage used in many applications.
They organize data into tables with predefined schemas, where each table represents an entity or relationship.
Data is stored in rows (records) and columns (attributes), allowing for efficient querying and manipulation through SQL (Structured Query Language).
Relational databases excel at maintaining data integrity, supporting complex queries, and handling relationships between different data entities.


Further reading


See our 
tutorial
 for working with SQL databases.


See our 
SQL database toolkit
.




Graph databases
​


Graph databases are a specialized type of database designed to store and manage highly interconnected data.
Unlike traditional relational databases, graph databases use a flexible structure consisting of nodes (entities), edges (relationships), and properties.
This structure allows for efficient representation and querying of complex, interconnected data.
Graph databases store data in a graph structure, with nodes, edges, and properties.
They are particularly useful for storing and querying complex relationships between data points, such as social networks, supply-chain management, fraud detection, and recommendation services


Further reading


See our 
tutorial
 for working with graph databases.


See our 
list of graph database integrations
.


See Neo4j's 
starter kit for LangChain
.




Retriever
​


LangChain provides a unified interface for interacting with various retrieval systems through the 
retriever
 concept. The interface is straightforward:




Input: A query (string)


Output: A list of documents (standardized LangChain 
Document
 objects)




You can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for databases that typically require structured query languages.
For example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) to be transformed into a SQL query behind the scenes.
Regardless of the underlying retrieval system, all retrievers in LangChain share a common interface. You can use them with the simple 
invoke
 method:


docs 
=
 retriever
.
invoke
(
query
)


Further reading


See our 
conceptual guide on retrievers
.


See our 
how-to guide
 on working with retrievers.


Edit this page
Was this page helpful?
Previous
Retrieval augmented generation (RAG)
Next
Retrievers
Overview
Key concepts
Query analysis
Query re-writing
Query construction
Information retrieval
Common retrieval systems
Retriever


--- docs_concepts_retrievers.txt ---

Conceptual guide
Retrievers
On this page
Retrievers




Prerequisites


Vector stores


Embeddings


Text splitters




Overview
​


Many different types of retrieval systems exist, including vectorstores, graph databases, and relational databases.
With the rise on popularity of large language models, retrieval systems have become an important component in AI application (e.g., 
RAG
).
Because of their importance and variability, LangChain provides a uniform interface for interacting with different types of retrieval systems.
The LangChain 
retriever
 interface is straightforward:




Input: A query (string)


Output: A list of documents (standardized LangChain 
Document
 objects)




Key concept
​




All retrievers implement a simple interface for retrieving documents using natural language queries.


Interface
​


The only requirement for a retriever is the ability to accepts a query and return documents.
In particular, 
LangChain's retriever class
 only requires that the 
_get_relevant_documents
 method is implemented, which takes a 
query: str
 and returns a list of 
Document
 objects that are most relevant to the query.
The underlying logic used to get relevant documents is specified by the retriever and can be whatever is most useful for the application.


A LangChain retriever is a 
runnable
, which is a standard interface is for LangChain components.
This means that it has a few common methods, including 
invoke
, that are used to interact with it. A retriever can be invoked with a query:


docs 
=
 retriever
.
invoke
(
query
)


Retrievers return a list of 
Document
 objects, which have two attributes:




page_content
: The content of this document. Currently is a string.


metadata
: Arbitrary metadata associated with this document (e.g., document id, file name, source, etc).




Further reading


See our 
how-to guide
 on building your own custom retriever.




Common types
​


Despite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.


Search apis
​


It's important to note that retrievers don't need to actually 
store
 documents.
For example, we can be built retrievers on top of search APIs that simply return search results!
See our retriever integrations with 
Amazon Kendra
 or 
Wikipedia Search
.


Relational or graph database
​


Retrievers can be built on top of relational or graph databases.
In these cases, 
query analysis
 techniques to construct a structured query from natural language is critical.
For example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) retriever to be transformed into a SQL query behind the scenes.


Further reading


See our 
tutorial
 for context on how to build a retreiver using a SQL database and text-to-SQL.


See our 
tutorial
 for context on how to build a retreiver using a graph database and text-to-Cypher.




Lexical search
​


As discussed in our conceptual review of 
retrieval
, many search engines are based upon matching words in a query to the words in each document.

BM25
 and 
TF-IDF
 are 
two popular lexical search algorithms
.
LangChain has retrievers for many popular lexical search algorithms / engines.


Further reading


See the 
BM25
 retriever integration.


See the 
TF-IDF
 retriever integration.


See the 
Elasticsearch
 retriever integration.




Vector store
​


Vector stores
 are a powerful and efficient way to index and retrieve unstructured data.
A vectorstore can be used as a retriever by calling the 
as_retriever()
 method.


vectorstore 
=
 MyVectorStore
(
)
retriever 
=
 vectorstore
.
as_retriever
(
)


Advanced retrieval patterns
​


Ensemble
​


Because the retriever interface is so simple, returning a list of 
Document
 objects given a search query, it is possible to combine multiple retrievers using ensembling.
This is particularly useful when you have multiple retrievers that are good at finding different types of relevant documents.
It is easy to create an 
ensemble retriever
 that combines multiple retrievers with linear weighted scores:


# Initialize the ensemble retriever
ensemble_retriever 
=
 EnsembleRetriever
(
    retrievers
=
[
bm25_retriever
,
 vector_store_retriever
]
,
 weights
=
[
0.5
,
 
0.5
]
)


When ensembling, how do we combine search results from many retrievers?
This motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as 
Reciprocal Rank Fusion (RRF)
.


Source document retention
​


Many retrievers utilize some kind of index to make documents easily searchable.
The process of indexing can include a transformation step (e.g., vectorstores often use document splitting).
Whatever transformation is used, can be very useful to retain a link between the 
transformed document
 and the original, giving the retriever the ability to return the 
original
 document.




This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small chunk size for indexing documents in a vectorstore.
If you return 
only
 the chunks as the retrieval result, then the model will have lost the original document context for the chunks.


LangChain has two different retrievers that can be used to address this challenge.
The 
Multi-Vector
 retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document.
The 
ParentDocument
 retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.


Name
Index Type
Uses an LLM
When to Use
Description
ParentDocument
Vector store + Document Store
No
If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.
This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks).
Multi Vector
Vector store + Document Store
Sometimes during indexing
If you are able to extract information from documents that you think is more relevant to index than the text itself.
This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.


Further reading


See our 
how-to guide
 on using the ParentDocument retriever.


See our 
how-to guide
 on using the MultiVector retriever.


See our RAG from Scratch video on the 
multi vector retriever
.


Edit this page
Was this page helpful?
Previous
Retrieval
Next
Runnable interface
Overview
Key concept
Interface
Common types
Search apis
Relational or graph database
Lexical search
Vector store
Advanced retrieval patterns
Ensemble
Source document retention


--- docs_concepts_runnables.txt ---

Conceptual guide
Runnable interface
On this page
Runnable interface


The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as 
language models
, 
output parsers
, 
retrievers
, 
compiled LangGraph graphs
 and more.


This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.


Related Resources


The 
"Runnable" Interface API Reference
 provides a detailed overview of the Runnable interface and its methods.


A list of built-in 
Runnables
 can be found in the 
LangChain Core API Reference
. Many of these Runnables are useful when composing custom "chains" in LangChain using the 
LangChain Expression Language (LCEL)
.




Overview of runnable interface
​


The Runnable way defines a standard interface that allows a Runnable component to be:




Invoked
: A single input is transformed into an output.


Batched
: Multiple inputs are efficiently transformed into outputs.


Streamed
: Outputs are streamed as they are produced.


Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.


Composed: Multiple Runnables can be composed to work together using 
the LangChain Expression Language (LCEL)
 to create complex pipelines.




Please review the 
LCEL Cheatsheet
 for some common patterns that involve the Runnable interface and LCEL expressions.




Optimized parallel execution (batch)
​




LangChain Runnables offer a built-in 
batch
 (and 
batch_as_completed
) API that allow you to process multiple inputs in parallel.


Using these methods can significantly improve performance when needing to process multiple independent inputs, as the
processing can be done in parallel instead of sequentially.


The two batching options are:




batch
: Process multiple inputs in parallel, returning results in the same order as the inputs.


batch_as_completed
: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.




The default implementation of 
batch
 and 
batch_as_completed
 use a thread pool executor to run the 
invoke
 method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.


Some Runnables may provide their own implementations of 
batch
 and 
batch_as_completed
 that are optimized for their specific use case (e.g.,
rely on a 
batch
 API provided by a model provider).


note
The async versions of 
abatch
 and 
abatch_as_completed
 relies on asyncio's 
gather
 and 
as_completed
 functions to run the 
ainvoke
 method in parallel.


tip
When processing a large number of inputs using 
batch
 or 
batch_as_completed
, users may want to control the maximum number of parallel calls. This can be done by setting the 
max_concurrency
 attribute in the 
RunnableConfig
 dictionary. See the 
RunnableConfig
 for more information.
Chat Models also have a built-in 
rate limiter
 that can be used to control the rate at which requests are made.


Asynchronous support
​




Runnables expose an asynchronous API, allowing them to be called using the 
await
 syntax in Python. Asynchronous methods can be identified by the "a" prefix (e.g., 
ainvoke
, 
abatch
, 
astream
, 
abatch_as_completed
).


Please refer to the 
Async Programming with LangChain
 guide for more details.


Streaming APIs
​




Streaming is critical in making applications based on LLMs feel responsive to end-users.


Runnables expose the following three streaming APIs:




sync 
stream
 and async 
astream
: yields the output a Runnable as it is generated.


The async 
astream_events
: a more advanced streaming API that allows streaming intermediate steps and final output


The 
legacy
 async 
astream_log
: a legacy streaming API that streams intermediate steps and final output




Please refer to the 
Streaming Conceptual Guide
 for more details on how to stream in LangChain.


Input and output types
​


Every 
Runnable
 is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.


Runnable methods that result in the execution of the Runnable (e.g., 
invoke
, 
batch
, 
stream
, 
astream_events
) work with these input and output types.




invoke: Accepts an input and returns an output.


batch: Accepts a list of inputs and returns a list of outputs.


stream: Accepts an input and returns a generator that yields outputs.




The 
input type
 and 
output type
 vary by component:


Component
Input Type
Output Type
Prompt
dictionary
PromptValue
ChatModel
a string, list of chat messages or a PromptValue
ChatMessage
LLM
a string, list of chat messages or a PromptValue
String
OutputParser
the output of an LLM or ChatModel
Depends on the parser
Retriever
a string
List of Documents
Tool
a string or dictionary, depending on the tool
Depends on the tool


Please refer to the individual component documentation for more information on the input and output types and how to use them.


Inspecting schemas
​


note
This is an advanced feature that is unnecessary for most users. You should probably
skip this section unless you have a specific need to inspect the schema of a Runnable.


In more advanced use cases, you may want to programmatically 
inspect
 the Runnable and determine what input and output types the Runnable expects and produces.


The Runnable interface provides methods to get the 
JSON Schema
 of the input and output types of a Runnable, as well as 
Pydantic schemas
 for the input and output types.


These APIs are mostly used internally for unit-testing and by 
LangServe
 which uses the APIs for input validation and generation of 
OpenAPI documentation
.


In addition, to the input and output types, some Runnables have been set up with additional run time configuration options.
There are corresponding APIs to get the Pydantic Schema and JSON Schema of the configuration options for the Runnable.
Please see the 
Configurable Runnables
 section for more information.


Method
Description
get_input_schema
Gives the Pydantic Schema of the input schema for the Runnable.
get_output_schema
Gives the Pydantic Schema of the output schema for the Runnable.
config_schema
Gives the Pydantic Schema of the config schema for the Runnable.
get_input_jsonschema
Gives the JSONSchema of the input schema for the Runnable.
get_output_jsonschema
Gives the JSONSchema of the output schema for the Runnable.
get_config_jsonschema
Gives the JSONSchema of the config schema for the Runnable.


With_types
​


LangChain will automatically try to infer the input and output types of a Runnable based on available information.


Currently, this inference does not work well for more complex Runnables that are built using 
LCEL
 composition, and the inferred input and / or output types may be incorrect. In these cases, we recommend that users override the inferred input and output types using the 
with_types
 method (
API Reference
).


RunnableConfig
​


Any of the methods that are used to execute the runnable (e.g., 
invoke
, 
batch
, 
stream
, 
astream_events
) accept a second argument called

RunnableConfig
 (
API Reference
). This argument is a dictionary that contains configuration for the Runnable that will be used
at run time during the execution of the runnable.


A 
RunnableConfig
 can have any of the following properties defined:


Attribute
Description
run_name
Name used for the given Runnable (not inherited).
run_id
Unique identifier for this call. sub-calls will get their own unique run ids.
tags
Tags for this call and any sub-calls.
metadata
Metadata for this call and any sub-calls.
callbacks
Callbacks for this call and any sub-calls.
max_concurrency
Maximum number of parallel calls to make (e.g., used by batch).
recursion_limit
Maximum number of times a call can recurse (e.g., used by Runnables that return Runnables)
configurable
Runtime values for configurable attributes of the Runnable.


Passing 
config
 to the 
invoke
 method is done like so:


some_runnable
.
invoke
(
   some_input
,
 
   config
=
{
      
'run_name'
:
 
'my_run'
,
 
      
'tags'
:
 
[
'tag1'
,
 
'tag2'
]
,
 
      
'metadata'
:
 
{
'key'
:
 
'value'
}
      
   
}
)


Propagation of RunnableConfig
​


Many 
Runnables
 are composed of other Runnables, and it is important that the 
RunnableConfig
 is propagated to all sub-calls made by the Runnable. This allows providing run time configuration values to the parent Runnable that are inherited by all sub-calls.


If this were not the case, it would be impossible to set and propagate 
callbacks
 or other configuration values like 
tags
 and 
metadata
 which
are expected to be inherited by all sub-calls.


There are two main patterns by which new 
Runnables
 are created:






Declaratively using 
LangChain Expression Language (LCEL)
:


chain 
=
 prompt 
|
 chat_model 
|
 output_parser






Using a 
custom Runnable
  (e.g., 
RunnableLambda
) or using the 
@tool
 decorator:


def
 
foo
(
input
)
:
    
# Note that .invoke() is used directly here
    
return
 bar_runnable
.
invoke
(
input
)
foo_runnable 
=
 RunnableLambda
(
foo
)






LangChain will try to propagate 
RunnableConfig
 automatically for both of the patterns.


For handling the second pattern, LangChain relies on Python's 
contextvars
.


In Python 3.11 and above, this works out of the box, and you do not need to do anything special to propagate the 
RunnableConfig
 to the sub-calls.


In Python 3.9 and 3.10, if you are using 
async code
, you need to manually pass the 
RunnableConfig
 through to the 
Runnable
 when invoking it.


This is due to a limitation in 
asyncio's tasks
  in Python 3.9 and 3.10 which did
not accept a 
context
 argument.


Propagating the 
RunnableConfig
 manually is done like so:


async
 
def
 
foo
(
input
,
 config
)
:
 
# <-- Note the config argument
    
return
 
await
 bar_runnable
.
ainvoke
(
input
,
 config
=
config
)
    
foo_runnable 
=
 RunnableLambda
(
foo
)


caution
When using Python 3.10 or lower and writing async code, 
RunnableConfig
 cannot be propagated
automatically, and you will need to do it manually! This is a common pitfall when
attempting to stream data using 
astream_events
 and 
astream_log
 as these methods
rely on proper propagation of 
callbacks
 defined inside of 
RunnableConfig
.


Setting custom run name, tags, and metadata
​


The 
run_name
, 
tags
, and 
metadata
 attributes of the 
RunnableConfig
 dictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable.


The 
run_name
 is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.


The 
tags
 and 
metadata
 attributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.


Using these attributes can be useful for tracking and debugging runs, as they will be surfaced in 
LangSmith
 as trace attributes that you can
filter and search on.


The attributes will also be propagated to 
callbacks
, and will appear in streaming APIs like 
astream_events
 as part of each event in the stream.


Related


How-to trace with LangChain




Setting run id
​


note
This is an advanced feature that is unnecessary for most users.


You may need to set a custom 
run_id
 for a given run, in case you want
to reference it later or correlate it with other systems.


The 
run_id
 MUST be a valid UUID string and 
unique
 for each run. It is used to identify
the parent run, sub-class will get their own unique run ids automatically.


To set a custom 
run_id
, you can pass it as a key-value pair in the 
config
 dictionary when invoking the Runnable:


import
 uuid
run_id 
=
 uuid
.
uuid4
(
)
some_runnable
.
invoke
(
   some_input
,
 
   config
=
{
      
'run_id'
:
 run_id
   
}
)
# Do something with the run_id


Setting recursion limit
​


note
This is an advanced feature that is unnecessary for most users.


Some Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a 
recursion_limit
 in the 
RunnableConfig
 dictionary. This will limit the number of times a Runnable can recurse.


Setting max concurrency
​


If using the 
batch
 or 
batch_as_completed
 methods, you can set the 
max_concurrency
 attribute in the 
RunnableConfig
 dictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.


tip
If you're trying to rate limit the number of requests made by a 
Chat Model
, you can use the built-in 
rate limiter
 instead of setting 
max_concurrency
, which will be more effective.
See the 
How to handle rate limits
 guide for more information.


Setting configurable
​


The 
configurable
 field is used to pass runtime values for configurable attributes of the Runnable.


It is used frequently in 
LangGraph
 with

LangGraph Persistence

and 
memory
.


It is used for a similar purpose in 
RunnableWithMessageHistory
 to specify either
a 
session_id
 / 
conversation_id
 to keep track of conversation history.


In addition, you can use it to specify any custom configuration options to pass to any 
Configurable Runnable
 that they create.


Setting callbacks
  ​


Use this option to configure 
callbacks
 for the runnable at
runtime. The callbacks will be passed to all sub-calls made by the runnable.


some_runnable
.
invoke
(
   some_input
,
   
{
      
"callbacks"
:
 
[
         SomeCallbackHandler
(
)
,
         AnotherCallbackHandler
(
)
,
      
]
   
}
)


Please read the 
Callbacks Conceptual Guide
 for more information on how to use callbacks in LangChain.


important
If you're using Python 3.9 or 3.10 in an async environment, you must propagate
the 
RunnableConfig
 manually to sub-calls in some cases. Please see the

Propagating RunnableConfig
 section for more information.


Creating a runnable from a function
​


You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using 
LangChain Expression Language (LCEL)
 to compose
multiple Runnables and you need to add custom processing logic in one of the steps.


There are two ways to create a custom Runnable from a function:




RunnableLambda
: Use this for simple transformations where streaming is not required.


RunnableGenerator
: use this for more complex transformations when streaming is needed.




See the 
How to run custom functions
 guide for more information on how to use 
RunnableLambda
 and 
RunnableGenerator
.


important
Users should not try to subclass Runnables to create a new custom Runnable. It is
much more complex and error-prone than simply using 
RunnableLambda
 or 
RunnableGenerator
.


Configurable runnables
​


note
This is an advanced feature that is unnecessary for most users.
It helps with configuration of large "chains" created using the 
LangChain Expression Language (LCEL)

and is leveraged by 
LangServe
 for deployed Runnables.


Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.


To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:




configurable_fields
: This method allows you to configure specific 
attributes
 in a Runnable. For example, the 
temperature
 attribute of a chat model.


configurable_alternatives
: This method enables you to specify 
alternative
 Runnables that can be run during runtime. For example, you could specify a list of different chat models that can be used.




See the 
How to configure runtime chain internals
 guide for more information on how to configure runtime chain internals.
Edit this page
Was this page helpful?
Previous
Retrievers
Next
Streaming
Overview of runnable interface
Optimized parallel execution (batch)
Asynchronous support
Streaming APIs
Input and output types
Inspecting schemas
RunnableConfig
Propagation of RunnableConfig
Setting custom run name, tags, and metadata
Setting run id
Setting recursion limit
Setting max concurrency
Setting configurable
Setting callbacks
Creating a runnable from a function
Configurable runnables


--- docs_concepts_streaming.txt ---

Conceptual guide
Streaming
On this page
Streaming


Prerequisites


Runnable Interface


Chat Models




Streaming
 is crucial for enhancing the responsiveness of applications built on 
LLMs
. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.


Overview
​


Generating full responses from 
LLMs
 often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.


In this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.


What to stream in LLM applications
​


In applications involving LLMs, several types of data can be streamed to improve user experience by reducing perceived latency and increasing transparency. These include:


1. Streaming LLM outputs
​


The most common and critical data to stream is the output generated by the LLM itself. LLMs often take time to generate full responses, and by streaming the output in real-time, users can see partial results as they are produced. This provides immediate feedback and helps reduce the wait time for users.


2. Streaming pipeline or workflow progress
​


Beyond just streaming LLM output, it’s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:






In LangGraph Workflows:

With 
LangGraph
, workflows are composed of nodes and edges that represent various steps. Streaming here involves tracking changes to the 
graph state
 as individual 
nodes
 request updates. This allows for more granular monitoring of which node in the workflow is currently active, giving real-time updates about the status of the workflow as it progresses through different stages.






In LCEL Pipelines:

Streaming updates from an 
LCEL
 pipeline involves capturing progress from individual 
sub-runnables
. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline's progress.






Streaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.


3. Streaming custom data
​


In some cases, you may need to stream 
custom data
 that goes beyond the information provided by the pipeline or workflow structure. This custom information is injected within a specific step in the workflow, whether that step is a tool or a LangGraph node. For example, you could stream updates about what a tool is doing in real-time or the progress through a LangGraph node. This granular data, which is emitted directly from within the step, provides more detailed insights into the execution of the workflow and is especially useful in complex processes where more visibility is needed.


Streaming APIs
​


LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the 
Runnable Interface
, including 
LLMs
, 
compiled LangGraph graphs
, and any Runnable generated with 
LCEL
.




sync 
stream
 and async 
astream
: Use to stream outputs from individual Runnables (e.g., a chat model) as they are generated or stream any workflow created with LangGraph.


The async only 
astream_events
: Use this API to get access to custom events and intermediate outputs from LLM  applications built entirely with 
LCEL
. Note that this API is available, but not needed when working with LangGraph.




note
In addition, there is a 
legacy
 async 
astream_log
 API. This API is not recommended for new projects it is more complex and less feature-rich than the other streaming APIs.


stream()
 and 
astream()
​


The 
stream()
 method returns an iterator that yields chunks of output synchronously as they are produced. You can use a 
for
 loop to process each chunk in real-time. For example, when using an LLM, this allows the output to be streamed incrementally as it is generated, reducing the wait time for users.


The type of chunk yielded by the 
stream()
 and 
astream()
 methods depends on the component being streamed. For example, when streaming from an 
LLM
 each component will be an 
AIMessageChunk
; however, for other components, the chunk may be different.


The 
stream()
 method returns an iterator that yields these chunks as they are produced. For example,


for
 chunk 
in
 component
.
stream
(
some_input
)
:
    
# IMPORTANT: Keep the processing of each chunk as efficient as possible.
    
# While you're processing the current chunk, the upstream component is
    
# waiting to produce the next one. For example, if working with LangGraph,
    
# graph execution is paused while the current chunk is being processed.
    
# In extreme cases, this could even result in timeouts (e.g., when llm outputs are
    
# streamed from an API that has a timeout).
    
print
(
chunk
)


The 
asynchronous version
, 
astream()
, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.


Usage with chat models
​


When using 
stream()
 or 
astream()
 with chat models, the output is streamed as 
AIMessageChunks
 as it is generated by the LLM. This allows you to present or process the LLM's output incrementally as it's being produced, which is particularly useful in interactive applications or interfaces.


Usage with LangGraph
​


LangGraph
 compiled graphs are 
Runnables
 and support the standard streaming APIs.


When using the 
stream
 and 
astream
 methods with LangGraph, you can choose 
one or more
 
streaming mode
 which allow you to control the type of output that is streamed. The available streaming modes are:




"values"
: Emit all values of the 
state
 for each step.


"updates"
: Emit only the node name(s) and updates that were returned by the node(s) after each step.


"debug"
: Emit debug events for each step.


"messages"
: Emit LLM 
messages
 
token-by-token
.


"custom"
: Emit custom output written using 
LangGraph's StreamWriter
.




For more information, please see:




LangGraph streaming conceptual guide
 for more information on how to stream when working with LangGraph.


LangGraph streaming how-to guides
 for specific examples of streaming in LangGraph.




Usage with LCEL
​


If you compose multiple Runnables using 
LangChain’s Expression Language (LCEL)
, the 
stream()
 and 
astream()
 methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally. 
LCEL
 tries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.


astream_events
​




tip
Use the 
astream_events
 API to access custom data and intermediate outputs from LLM applications built entirely with 
LCEL
.
While this API is available for use with 
LangGraph
 as well, it is usually not necessary when working with LangGraph, as the 
stream
 and 
astream
 methods provide comprehensive streaming capabilities for LangGraph graphs.


For chains constructed using 
LCEL
, the 
.stream()
 method only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of the chain alongside the final output. For example, you may want to return sources alongside the final generation when building a chat-over-documents app.


There are ways to do this 
using callbacks
, or by constructing your chain in such a way that it passes intermediate
values to the end with something like chained 
.assign()
 calls, but LangChain also includes an

.astream_events()
 method that combines the flexibility of callbacks with the ergonomics of 
.stream()
. When called, it returns an iterator
which yields 
various types of events
 that you can filter and process according
to the needs of your project.


Here's one small example that prints just events containing streamed chat model output:


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_anthropic 
import
 ChatAnthropic
model 
=
 ChatAnthropic
(
model
=
"claude-3-sonnet-20240229"
)
prompt 
=
 ChatPromptTemplate
.
from_template
(
"tell me a joke about {topic}"
)
parser 
=
 StrOutputParser
(
)
chain 
=
 prompt 
|
 model 
|
 parser
async
 
for
 event 
in
 chain
.
astream_events
(
{
"topic"
:
 
"parrot"
}
)
:
    kind 
=
 event
[
"event"
]
    
if
 kind 
==
 
"on_chat_model_stream"
:
        
print
(
event
,
 end
=
"|"
,
 flush
=
True
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
ChatAnthropic


You can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!


See 
this guide
 for more detailed information on how to use 
.astream_events()
, including a table listing available events.


Writing custom data to the stream
​


To write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:




LangGraph's 
StreamWriter
 can be used to write custom data that will surface through 
stream
 and 
astream
 APIs when working with LangGraph. 
Important
 this is a LangGraph feature, so it is not available when working with pure LCEL. See 
how to streaming custom data
 for more information.


dispatch_events
 / 
adispatch_events
 can be used to write custom data that will be surfaced through the 
astream_events
 API. See 
how to dispatch custom callback events
 for more information.




"Auto-Streaming" Chat Models
​


LangChain simplifies streaming from 
chat models
 by automatically enabling streaming mode in certain cases, even when you’re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming 
invoke
 method but still want to stream the entire application, including intermediate results from the chat model.


How It Works
​


When you call the 
invoke
 (or 
ainvoke
) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application.


Under the hood, it'll have 
invoke
 (or 
ainvoke
) use the 
stream
 (or 
astream
) method to generate its output. The result of the invocation will be the same as far as the code that was using 
invoke
 is concerned; however, while the chat model is being streamed, LangChain will take care of invoking 
on_llm_new_token
 events in LangChain's 
callback system
. These callback events
allow LangGraph 
stream
/
astream
 and 
astream_events
 to surface the chat model's output in real-time.


Example:


def
 
node
(
state
)
:
    
.
.
.
    
# The code below uses the invoke method, but LangChain will 
    
# automatically switch to streaming mode
    
# when it detects that the overall 
    
# application is being streamed.
    ai_message 
=
 model
.
invoke
(
state
[
"messages"
]
)
    
.
.
.
for
 chunk 
in
 compiled_graph
.
stream
(
.
.
.
,
 mode
=
"messages"
)
:
 
    
.
.
.


Async Programming
​


LangChain offers both synchronous (sync) and asynchronous (async) versions of many of its methods. The async methods are typically prefixed with an "a" (e.g., 
ainvoke
, 
astream
). When writing async code, it's crucial to consistently use these asynchronous methods to ensure non-blocking behavior and optimal performance.


If streaming data fails to appear in real-time, please ensure that you are using the correct async methods for your workflow.


Please review the 
async programming in LangChain guide
 for more information on writing async code with LangChain.


Related Resources
​


Please see the following how-to guides for specific examples of streaming in LangChain:




LangGraph conceptual guide on streaming


LangGraph streaming how-to guides


How to stream runnables
: This how-to guide goes over common streaming patterns with LangChain components (e.g., chat models) and with 
LCEL
.


How to stream chat models


How to stream tool calls




For writing custom data to the stream, please see the following resources:




If using LangGraph, see 
how to stream custom data
.


If using LCEL, see 
how to dispatch custom callback events
.


Edit this page
Was this page helpful?
Previous
Runnable interface
Next
Structured outputs
Overview
What to stream in LLM applications
1. Streaming LLM outputs
2. Streaming pipeline or workflow progress
3. Streaming custom data
Streaming APIs
stream()
 and 
astream()
astream_events
Writing custom data to the stream
"Auto-Streaming" Chat Models
How It Works
Async Programming
Related Resources


--- docs_concepts_structured_outputs.txt ---

Conceptual guide
Structured outputs
On this page
Structured outputs


Overview
​


For many applications, such as chatbots, models need to respond to users directly in natural language.
However, there are scenarios where we need models to output in a 
structured format
.
For example, we might want to store the model output in a database and ensure that the output conforms to the database schema.
This need motivates the concept of structured output, where models can be instructed to respond with a particular output structure.




Key concepts
​


(1) Schema definition:
 The output structure is represented as a schema, which can be defined in several ways.

(2) Returning structured output:
 The model is given this schema, and is instructed to return output that conforms to it.


Recommended usage
​


This pseudocode illustrates the recommended workflow when using structured output.
LangChain provides a method, 
with_structured_output()
, that automates the process of binding the schema to the 
model
 and parsing the output.
This helper function is available for all model providers that support structured output.


# Define schema
schema 
=
 
{
"foo"
:
 
"bar"
}
# Bind schema to model
model_with_structure 
=
 model
.
with_structured_output
(
schema
)
# Invoke the model to produce structured output that matches the schema
structured_output 
=
 model_with_structure
.
invoke
(
user_input
)


Schema definition
​


The central concept is that the output structure of model responses needs to be represented in some way.
While types of objects you can use depend on the model you're working with, there are common types of objects that are typically allowed or recommended for structured output in Python.


The simplest and most common format for structured output is a JSON-like structure, which in Python can be represented as a dictionary (dict) or list (list).
JSON objects (or dicts in Python) are often used directly when the tool requires raw, flexible, and minimal-overhead structured data.


{
  
"answer"
:
 
"The answer to the user's question"
,
  
"followup_question"
:
 
"A followup question the user could ask"
}


As a second example, 
Pydantic
 is particularly useful for defining structured output schemas because it offers type hints and validation.
Here's an example of a Pydantic schema:


from
 pydantic 
import
 BaseModel
,
 Field
class
 
ResponseFormatter
(
BaseModel
)
:
    
"""Always use this tool to structure your response to the user."""
    answer
:
 
str
 
=
 Field
(
description
=
"The answer to the user's question"
)
    followup_question
:
 
str
 
=
 Field
(
description
=
"A followup question the user could ask"
)


Returning structured output
​


With a schema defined, we need a way to instruct the model to use it.
While one approach is to include this schema in the prompt and 
ask nicely
 for the model to use it, this is not recommended.
Several more powerful methods that utilizes native features in the model provider's API are available.


Using tool calling
​


Many 
model providers support
 tool calling, a concept discussed in more detail in our 
tool calling guide
.
In short, tool calling involves binding a tool to a model and, when appropriate, the model can 
decide
 to call this tool and ensure its response conforms to the tool's schema.
With this in mind, the central concept is straightforward: 
simply bind our schema to a model as a tool!

Here is an example using the 
ResponseFormatter
 schema defined above:


from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
model
=
"gpt-4o"
,
 temperature
=
0
)
# Bind responseformatter schema as a tool to the model
model_with_tools 
=
 model
.
bind_tools
(
[
ResponseFormatter
]
)
# Invoke the model
ai_msg 
=
 model_with_tools
.
invoke
(
"What is the powerhouse of the cell?"
)
API Reference:
ChatOpenAI


The arguments of the tool call are already extracted as a dictionary.
This dictionary can be optionally parsed into a Pydantic object, matching our original 
ResponseFormatter
 schema.


# Get the tool call arguments
ai_msg
.
tool_calls
[
0
]
[
"args"
]
{
'answer'
:
 
"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy."
,
 
'followup_question'
:
 
'What is the function of ATP in the cell?'
}
# Parse the dictionary into a pydantic object
pydantic_object 
=
 ResponseFormatter
.
model_validate
(
ai_msg
.
tool_calls
[
0
]
[
"args"
]
)


JSON mode
​


In addition to tool calling, some model providers support a feature called 
JSON mode
.
This supports JSON schema definition as input and enforces the model to produce a conforming JSON output.
You can find a table of model providers that support JSON mode 
here
.
Here is an example of how to use JSON mode with OpenAI:


from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
model
=
"gpt-4o"
)
.
with_structured_output
(
method
=
"json_mode"
)
ai_msg 
=
 model
.
invoke
(
"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]"
)
ai_msg
{
'random_ints'
:
 
[
45
,
 
67
,
 
12
,
 
34
,
 
89
,
 
23
,
 
78
,
 
56
,
 
90
,
 
11
]
}
API Reference:
ChatOpenAI


Structured output method
​


There are a few challenges when producing structured output with the above methods:


(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.


(2) In addition, the model needs to be instructed to 
always
 use the tool when we want to enforce structured output, which is a provider specific setting.


(3) When JSON mode is used, the output needs to be parsed into a JSON object.


With these challenges in mind, LangChain provides a helper function (
with_structured_output()
) to streamline the process.




This both binds the schema to the model as a tool and parses the output to the specified output schema.


# Bind the schema to the model
model_with_structure 
=
 model
.
with_structured_output
(
ResponseFormatter
)
# Invoke the model
structured_output 
=
 model_with_structure
.
invoke
(
"What is the powerhouse of the cell?"
)
# Get back the pydantic object
structured_output
ResponseFormatter
(
answer
=
"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy."
,
 followup_question
=
'What is the function of ATP in the cell?'
)


Further reading
For more details on usage, see our 
how-to guide
.
Edit this page
Was this page helpful?
Previous
Streaming
Next
Testing
Overview
Key concepts
Recommended usage
Schema definition
Returning structured output
Using tool calling
JSON mode
Structured output method


--- docs_concepts_testing.txt ---

Conceptual guide
Testing
On this page
Testing




Testing is a critical part of the development process that ensures your code works as expected and meets the desired quality standards.


In the LangChain ecosystem, we have 2 main types of tests: 
unit tests
 and 
integration tests
.


For integrations that implement standard LangChain abstractions, we have a set of 
standard tests
 (both unit and integration) that help maintain compatibility between different components and ensure reliability of high-usage ones.


Unit Tests
​


Definition
: Unit tests are designed to validate the smallest parts of your code—individual functions or methods—ensuring they work as expected in isolation. They do not rely on external systems or integrations.


Example
: Testing the 
convert_langchain_aimessage_to_dict
 function to confirm it correctly converts an AI message to a dictionary format:


from
 langchain_core
.
messages 
import
 AIMessage
,
 ToolCall
,
 convert_to_openai_messages
def
 
test_convert_to_openai_messages
(
)
:
    ai_message 
=
 AIMessage
(
        content
=
"Let me call that tool for you!"
,
        tool_calls
=
[
            ToolCall
(
name
=
'parrot_multiply_tool'
,
 
id
=
'1'
,
 args
=
{
'a'
:
 
2
,
 
'b'
:
 
3
}
)
,
        
]
    
)
    
    result 
=
 convert_to_openai_messages
(
ai_message
)
    
    expected 
=
 
{
        
"role"
:
 
"assistant"
,
        
"tool_calls"
:
 
[
            
{
                
"type"
:
 
"function"
,
                
"id"
:
 
"1"
,
                
"function"
:
 
{
                    
"name"
:
 
"parrot_multiply_tool"
,
                    
"arguments"
:
 
'{"a": 2, "b": 3}'
,
                
}
,
            
}
        
]
,
        
"content"
:
 
"Let me call that tool for you!"
,
    
}
    
assert
 result 
==
 expected  
# Ensure conversion matches expected output
API Reference:
AIMessage
 | 
ToolCall
 | 
convert_to_openai_messages




Integration Tests
​


Definition
: Integration tests validate that multiple components or systems work together as expected. For tools or integrations relying on external services, these tests often ensure end-to-end functionality.


Example
: Testing 
ParrotMultiplyTool
 with access to an API service that multiplies two numbers and adds 80:


def
 
test_integration_with_service
(
)
:
    tool 
=
 ParrotMultiplyTool
(
)
    result 
=
 tool
.
invoke
(
{
"a"
:
 
2
,
 
"b"
:
 
3
}
)
    
assert
 result 
==
 
86




Standard Tests
​


Definition
: Standard tests are pre-defined tests provided by LangChain to ensure consistency and reliability across all tools and integrations. They include both unit and integration test templates tailored for LangChain components.


Example
: Subclassing LangChain's 
ToolsUnitTests
 or 
ToolsIntegrationTests
 to automatically run standard tests:


from
 langchain_tests
.
unit_tests 
import
 ToolsUnitTests
class
 
TestParrotMultiplyToolUnit
(
ToolsUnitTests
)
:
    
@property
    
def
 
tool_constructor
(
self
)
:
        
return
 ParrotMultiplyTool
    
def
 
tool_invoke_params_example
(
self
)
:
        
return
 
{
"a"
:
 
2
,
 
"b"
:
 
3
}
API Reference:
ToolsUnitTests


To learn more, check out our guide on 
how to add standard tests to an integration
.
Edit this page
Was this page helpful?
Previous
Structured outputs
Next
String-in, string-out llms
Unit Tests
Integration Tests
Standard Tests


--- docs_concepts_text_llms.txt ---

Conceptual guide
String-in, string-out llms
String-in, string-out llms


tip
You are probably looking for the 
Chat Model Concept Guide
 page for more information.


LangChain has implementations for older language models that take a string as input and return a string as output. These models are typically named without the "Chat" prefix (e.g., 
Ollama
, 
Anthropic
, 
OpenAI
, etc.), and may include the "LLM" suffix (e.g., 
OllamaLLM
, 
AnthropicLLM
, 
OpenAILLM
, etc.). These models implement the 
BaseLLM
 interface.


Users should be using almost exclusively the newer 
Chat Models
 as most
model providers have adopted a chat like interface for interacting with language models.
Edit this page
Was this page helpful?
Previous
Testing
Next
Text splitters


--- docs_concepts_text_splitters.txt ---

Conceptual guide
Text splitters
On this page
Text splitters




Prerequisites


Documents


Tokenization(/docs/concepts/tokens)




Overview
​


Document splitting is often a crucial preprocessing step for many applications.
It involves breaking down large texts into smaller, manageable chunks.
This process offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of models, and improving the quality of text representations used in retrieval systems.
There are several strategies for splitting documents, each with its own advantages.


Key concepts
​




Text splitters split documents into smaller chunks for use in downstream applications.


Why split documents?
​


There are several reasons to split documents:




Handling non-uniform document lengths
: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.


Overcoming model limitations
: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.


Improving representation quality
: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.


Enhancing retrieval precision
: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.


Optimizing computational resources
: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.




Now, the next question is 
how
 to split the documents into chunks! There are several strategies, each with its own advantages.


Further reading


See Greg Kamradt's 
chunkviz
 to visualize different splitting strategies discussed below.




Approaches
​


Length-based
​


The most intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit.
Key benefits of length-based splitting:




Straightforward implementation


Consistent chunk sizes


Easily adaptable to different model requirements




Types of length-based splitting:




Token-based
: Splits text based on the number of tokens, which is useful when working with language models.


Character-based
: Splits text based on the number of characters, which can be more consistent across different types of text.




Example implementation using LangChain's 
CharacterTextSplitter
 with token-based splitting:


from
 langchain_text_splitters 
import
 CharacterTextSplitter
text_splitter 
=
 CharacterTextSplitter
.
from_tiktoken_encoder
(
    encoding_name
=
"cl100k_base"
,
 chunk_size
=
100
,
 chunk_overlap
=
0
)
texts 
=
 text_splitter
.
split_text
(
document
)
API Reference:
CharacterTextSplitter


Further reading


See the how-to guide for 
token-based
 splitting.


See the how-to guide for 
character-based
 splitting.




Text-structured based
​


Text is naturally organized into hierarchical units such as paragraphs, sentences, and words.
We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.
LangChain's 
RecursiveCharacterTextSplitter
 implements this concept:




The 
RecursiveCharacterTextSplitter
 attempts to keep larger units (e.g., paragraphs) intact.


If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).


This process continues down to the word level if necessary.




Here is example usage:


from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
100
,
 chunk_overlap
=
0
)
texts 
=
 text_splitter
.
split_text
(
document
)
API Reference:
RecursiveCharacterTextSplitter


Further reading


See the how-to guide for 
recursive text splitting
.




Document-structured based
​


Some documents have an inherent structure, such as HTML, Markdown, or JSON files.
In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text.
Key benefits of structure-based splitting:




Preserves the logical organization of the document


Maintains context within each chunk


Can be more effective for downstream tasks like retrieval or summarization




Examples of structure-based splitting:




Markdown
: Split based on headers (e.g., #, ##, ###)


HTML
: Split using tags


JSON
: Split by object or array elements


Code
: Split by functions, classes, or logical blocks




Further reading


See the how-to guide for 
Markdown splitting
.


See the how-to guide for 
Recursive JSON splitting
.


See the how-to guide for 
Code splitting
.


See the how-to guide for 
HTML splitting
.




Semantic meaning based
​


Unlike the previous methods, semantic-based splitting actually considers the 
content
 of the text.
While other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text's semantics.
There are several ways to implement this, but conceptually the approach is split text when there are significant changes in text 
meaning
.
As an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences:




Start with the first few sentences and generate an embedding.


Move to the next group of sentences and generate another embedding (e.g., using a sliding window approach).


Compare the embeddings to find significant differences, which indicate potential "break points" between semantic sections.




This technique helps create chunks that are more semantically coherent, potentially improving the quality of downstream tasks like retrieval or summarization.


Further reading


See the how-to guide for 
splitting text based on semantic meaning
.


See Greg Kamradt's 
notebook
 showcasing semantic splitting.


Edit this page
Was this page helpful?
Previous
String-in, string-out llms
Next
Tokens
Overview
Key concepts
Why split documents?
Approaches
Length-based
Text-structured based
Document-structured based
Semantic meaning based


--- docs_concepts_tokens.txt ---

Conceptual guide
Tokens
On this page
Tokens


Modern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use to break down input and generate output. In this section, we'll discuss what tokens are and how they are used by language models.


What is a token?
​


A 
token
 is the basic unit that a language model reads, processes, and generates. These units can vary based on how the model provider defines them, but in general, they could represent:




A whole word (e.g., "apple"),


A part of a word (e.g., "app"),


Or other linguistic components such as punctuation or spaces.




The way the model tokenizes the input depends on its 
tokenizer algorithm
, which converts the input into tokens. Similarly, the model’s output comes as a stream of tokens, which is then decoded back into human-readable text.


How tokens work in language models
​


The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, language models focus on 
tokens
, which represent meaningful linguistic units. Here's how the process works:






Input Tokenization
: When you provide a model with a prompt (e.g., "LangChain is cool!"), the tokenizer algorithm splits the text into tokens. For example, the sentence could be tokenized into parts like 
["Lang", "Chain", " is", " cool", "!"]
. Note that token boundaries don’t always align with word boundaries.







Processing
: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, capturing context and meaning from the input.






Output Generation
: The model generates new tokens one by one. These output tokens are then decoded back into human-readable text.






Using tokens instead of raw characters allows the model to focus on linguistically meaningful units, which helps it capture grammar, structure, and context more effectively.


Tokens don’t have to be text
​


Although tokens are most commonly used to represent text, they don’t have to be limited to textual data. Tokens can also serve as abstract representations of 
multi-modal data
, such as:




Images
,


Audio
,


Video
,


And other types of data.




At the time of writing, virtually no models support 
multi-modal output
, and only a few models can handle 
multi-modal inputs
 (e.g., text combined with images or audio). However, as advancements in AI continue, we expect 
multi-modality
 to become much more common. This would allow models to process and generate a broader range of media, significantly expanding the scope of what tokens can represent and how models can interact with diverse types of data.


note
In principle, 
anything that can be represented as a sequence of tokens
 could be modeled in a similar way. For example, 
DNA sequences
—which are composed of a series of nucleotides (A, T, C, G)—can be tokenized and modeled to capture patterns, make predictions, or generate sequences. This flexibility allows transformer-based models to handle diverse types of sequential data, further broadening their potential applications across various domains, including bioinformatics, signal processing, and other fields that involve structured or unstructured sequences.


Please see the 
multimodality
 section for more information on multi-modal inputs and outputs.


Why not use characters?
​


Using tokens instead of individual characters makes models both more efficient and better at understanding context and grammar. Tokens represent meaningful units, like whole words or parts of words, allowing models to capture language structure more effectively than by processing raw characters. Token-level processing also reduces the number of units the model has to handle, leading to faster computation.


In contrast, character-level processing would require handling a much larger sequence of input, making it harder for the model to learn relationships and context. Tokens enable models to focus on linguistic meaning, making them more accurate and efficient in generating responses.


How tokens correspond to text
​


Please see this post from 
OpenAI
 for more details on how tokens are counted and how they correspond to text.


According to the OpenAI post, the approximate token counts for English text are as follows:




1 token ~= 4 chars in English


1 token ~= ¾ words


100 tokens ~= 75 words


Edit this page
Was this page helpful?
Previous
Text splitters
Next
Tool calling
What is a token?
How tokens work in language models
Tokens don’t have to be text
Why not use characters?
How tokens correspond to text


--- docs_concepts_tool_calling.txt ---

Conceptual guide
Tool calling
On this page
Tool calling


Prerequisites


Tools


Chat Models




Overview
​


Many AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.
But what about cases where we want a model to also interact 
directly
 with systems, such as databases or an API?
These systems often have a particular input schema; for example, APIs frequently have a required payload structure.
This need motivates the concept of 
tool calling
. You can use 
tool calling
 to request model responses that match a particular schema.


info
You will sometimes hear the term 
function calling
. We use this term interchangeably with 
tool calling
.




Key concepts
​


(1) Tool Creation:
 Use the 
@tool
 decorator to create a 
tool
. A tool is an association between a function and its schema.

(2) Tool Binding:
 The tool needs to be connected to a model that supports tool calling. This gives the model awareness of the tool and the associated input schema required by the tool.

(3) Tool Calling:
 When appropriate, the model can decide to call a tool and ensure its response conforms to the tool's input schema.

(4) Tool Execution:
 The tool can be executed using the arguments provided by the model.




Recommended usage
​


This pseudocode illustrates the recommended workflow for using tool calling.
Created tools are passed to 
.bind_tools()
 method as a list.
This model can be called, as usual. If a tool call is made, model's response will contain the tool call arguments.
The tool call arguments can be passed directly to the tool.


# Tool creation
tools 
=
 
[
my_tool
]
# Tool binding
model_with_tools 
=
 model
.
bind_tools
(
tools
)
# Tool calling 
response 
=
 model_with_tools
.
invoke
(
user_input
)


Tool creation
​


The recommended way to create a tool is using the 
@tool
 decorator.


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply a and b."""
    
return
 a 
*
 b
API Reference:
tool


Further reading


See our conceptual guide on 
tools
 for more details.


See our 
model integrations
 that support tool calling.


See our 
how-to guide
 on tool calling.




Tool binding
​


Many
 
model providers
 support tool calling.


tip
See our 
model integration page
 for a list of providers that support tool calling.


The central concept to understand is that LangChain provides a standardized interface for connecting tools to models.
The 
.bind_tools()
 method can be used to specify which tools are available for a model to call.


model_with_tools 
=
 model
.
bind_tools
(
tools_list
)


As a specific example, let's take a function 
multiply
 and bind it as a tool to a model that supports tool calling.


def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply a and b.
    Args:
        a: first int
        b: second int
    """
    
return
 a 
*
 b
llm_with_tools 
=
 tool_calling_model
.
bind_tools
(
[
multiply
]
)


Tool calling
​




A key principle of tool calling is that the model decides when to use a tool based on the input's relevance. The model doesn't always need to call a tool.
For example, given an unrelated input, the model would not call the tool:


result 
=
 llm_with_tools
.
invoke
(
"Hello world!"
)


The result would be an 
AIMessage
 containing the model's response in natural language (e.g., "Hello!").
However, if we pass an input 
relevant to the tool
, the model should choose to call it:


result 
=
 llm_with_tools
.
invoke
(
"What is 2 multiplied by 3?"
)


As before, the output 
result
 will be an 
AIMessage
.
But, if the tool was called, 
result
 will have a 
tool_calls
 attribute.
This attribute includes everything needed to execute the tool, including the tool name and input arguments:


result.tool_calls
{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'xxx', 'type': 'tool_call'}


For more details on usage, see our 
how-to guides
!


Tool execution
​


Tools
 implement the 
Runnable
 interface, which means that they can be invoked (e.g., 
tool.invoke(args)
) directly.


LangGraph
 offers pre-built components (e.g., 
ToolNode
) that will often invoke the tool in behalf of the user.


Further reading


See our 
how-to guide
 on tool calling.


See the 
LangGraph documentation on using ToolNode
.




Best practices
​


When designing 
tools
 to be used by a model, it is important to keep in mind that:




Models that have explicit 
tool-calling APIs
 will be better at tool calling than non-fine-tuned models.


Models will perform better if the tools have well-chosen names and descriptions.


Simple, narrowly scoped tools are easier for models to use than complex tools.


Asking the model to select from a large list of tools poses challenges for the model.


Edit this page
Was this page helpful?
Previous
Tokens
Next
Tools
Overview
Key concepts
Recommended usage
Tool creation
Tool binding
Tool calling
Tool execution
Best practices


--- docs_concepts_tools.txt ---

Conceptual guide
Tools
On this page
Tools


Prerequisites


Chat models




Overview
​


The 
tool
 abstraction in LangChain associates a Python 
function
 with a 
schema
 that defines the function's 
name
, 
description
 and 
expected arguments
.


Tools
 can be passed to 
chat models
 that support 
tool calling
 allowing the model to request the execution of a specific function with specific inputs.


Key concepts
​




Tools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.


Create tools using the 
@tool
 decorator, which simplifies the process of tool creation, supporting the following:




Automatically infer the tool's 
name
, 
description
 and 
expected arguments
, while also supporting customization.


Defining tools that return 
artifacts
 (e.g. images, dataframes, etc.)


Hiding input arguments from the schema (and hence from the model) using 
injected tool arguments
.








Tool interface
​


The tool interface is defined in the 
BaseTool
 class which is a subclass of the 
Runnable Interface
.


The key attributes that correspond to the tool's 
schema
:




name
: The name of the tool.


description
: A description of what the tool does.


args
: Property that returns the JSON schema for the tool's arguments.




The key methods to execute the function associated with the 
tool
:




invoke
: Invokes the tool with the given arguments.


ainvoke
: Invokes the tool with the given arguments, asynchronously. Used for 
async programming with Langchain
.




Create tools using the 
@tool
 decorator
​


The recommended way to create tools is using the 
@tool
 decorator. This decorator is designed to simplify the process of tool creation and should be used in most cases. After defining a function, you can decorate it with 
@tool
 to create a tool that implements the 
Tool Interface
.


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
   
"""Multiply two numbers."""
   
return
 a 
*
 b
API Reference:
tool


For more details on how to create tools, see the 
how to create custom tools
 guide.


note
LangChain has a few other ways to create tools; e.g., by sub-classing the 
BaseTool
 class or by using 
StructuredTool
. These methods are shown in the 
how to create custom tools guide
, but
we generally recommend using the 
@tool
 decorator for most cases.


Use the tool directly
​


Once you have defined a tool, you can use it directly by calling the function. For example, to use the 
multiply
 tool defined above:


multiply
.
invoke
(
{
"a"
:
 
2
,
 
"b"
:
 
3
}
)


Inspect
​


You can also inspect the tool's schema and other properties:


print
(
multiply
.
name
)
 
# multiply
print
(
multiply
.
description
)
 
# Multiply two numbers.
print
(
multiply
.
args
)
 
# {
# 'type': 'object', 
# 'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 
# 'required': ['a', 'b']
# }


note
If you're using pre-built LangChain or LangGraph components like 
create_react_agent
,you might not need to interact with tools directly. However, understanding how to use them can be valuable for debugging and testing. Additionally, when building custom LangGraph workflows, you may find it necessary to work with tools directly.


Configuring the schema
​


The 
@tool
 decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).


Please see the 
API reference for @tool
 for more details and review the 
how to create custom tools
 guide for examples.


Tool artifacts
​


Tools
 are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.


@tool
(
response_format
=
"content_and_artifact"
)
def
 
some_tool
(
.
.
.
)
 
-
>
 Tuple
[
str
,
 Any
]
:
    
"""Tool that does something."""
    
.
.
.
    
return
 
'Message for chat model'
,
 some_artifact 


See 
how to return artifacts from tools
 for more details.


Special type annotations
​


There are a number of special type annotations that can be used in the tool's function signature to configure the run time behavior of the tool.


The following type annotations will end up 
removing
 the argument from the tool's schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.




InjectedToolArg
: Value should be injected manually at runtime using 
.invoke
 or 
.ainvoke
.


RunnableConfig
: Pass in the RunnableConfig object to the tool.


InjectedState
: Pass in the overall state of the LangGraph graph to the tool.


InjectedStore
: Pass in the LangGraph store object to the tool.




You can also use the 
Annotated
 type with a string literal to provide a 
description
 for the corresponding argument that 
WILL
 be exposed in the tool's schema.




Annotated[..., "string literal"]
 -- Adds a description to the argument that will be exposed in the tool's schema.




InjectedToolArg
​


There are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the 
InjectedToolArg
 annotation, which allows certain parameters to be hidden from the tool's schema.


For example, if a tool requires a 
user_id
 to be injected dynamically at runtime, it can be structured in this way:


from
 langchain_core
.
tools 
import
 tool
,
 InjectedToolArg
@tool
def
 
user_specific_tool
(
input_data
:
 
str
,
 user_id
:
 InjectedToolArg
)
 
-
>
 
str
:
    
"""Tool that processes input data."""
    
return
 
f"User 
{
user_id
}
 processed 
{
input_data
}
"
API Reference:
tool
 | 
InjectedToolArg


Annotating the 
user_id
 argument with 
InjectedToolArg
 tells LangChain that this argument should not be exposed as part of the
tool's schema.


See 
how to pass run time values to tools
 for more details on how to use 
InjectedToolArg
.


RunnableConfig
​


You can use the 
RunnableConfig
 object to pass custom run time values to tools.


If you need to access the 
RunnableConfig
 object from within a tool. This can be done by using the 
RunnableConfig
 annotation in the tool's function signature.


from
 langchain_core
.
runnables 
import
 RunnableConfig
@tool
async
 
def
 
some_func
(
.
.
.
,
 config
:
 RunnableConfig
)
 
-
>
 
.
.
.
:
    
"""Tool that does something."""
    
# do something with config
    
.
.
.
await
 some_func
.
ainvoke
(
.
.
.
,
 config
=
{
"configurable"
:
 
{
"value"
:
 
"some_value"
}
}
)
API Reference:
RunnableConfig


The 
config
 will not be part of the tool's schema and will be injected at runtime with appropriate values.


note
You may need to access the 
config
 object to manually propagate it to subclass. This happens if you're working with python 3.9 / 3.10 in an 
async
 environment and need to manually propagate the 
config
 object to sub-calls.
Please read 
Propagation RunnableConfig
 for more details to learn how to propagate the 
RunnableConfig
 down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).


InjectedState
​


Please see the 
InjectedState
 documentation for more details.


InjectedStore
​


Please see the 
InjectedStore
 documentation for more details.


Best practices
​


When designing tools to be used by models, keep the following in mind:




Tools that are well-named, correctly-documented and properly type-hinted are easier for models to use.


Design simple and narrowly scoped tools, as they are easier for models to use correctly.


Use chat models that support 
tool-calling
 APIs to take advantage of tools.




Toolkits
​




LangChain has a concept of 
toolkits
. This a very thin abstraction that groups tools together that
are designed to be used together for specific tasks.


Interface
​


All Toolkits expose a 
get_tools
 method which returns a list of tools. You can therefore do:


# Initialize a toolkit
toolkit 
=
 ExampleTookit
(
.
.
.
)
# Get list of tools
tools 
=
 toolkit
.
get_tools
(
)


Related resources
​


See the following resources for more information:




API Reference for @tool


How to create custom tools


How to pass run time values to tools


All LangChain tool how-to guides


Additional how-to guides that show usage with LangGraph


Tool integrations, see the 
tool integration docs
.


Edit this page
Was this page helpful?
Previous
Tool calling
Next
Tracing
Overview
Key concepts
Tool interface
Create tools using the 
@tool
 decorator
Use the tool directly
Inspect
Configuring the schema
Tool artifacts
Special type annotations
InjectedToolArg
RunnableConfig
InjectedState
InjectedStore
Best practices
Toolkits
Interface
Related resources


--- docs_concepts_tracing.txt ---

Conceptual guide
Tracing
Tracing




A trace is essentially a series of steps that your application takes to go from input to output.
Traces contain individual steps called 
runs
. These can be individual calls from a model, retriever,
tool, or sub-chains.
Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.


For a deeper dive, check out 
this LangSmith conceptual guide
.
Edit this page
Was this page helpful?
Previous
Tools
Next
Vector stores


--- docs_concepts_vectorstores.txt ---

Conceptual guide
Vector stores
On this page
Vector stores




Prerequisites


Embeddings


Text splitters




Note
This conceptual overview focuses on text-based indexing and retrieval for simplicity.
However, embedding models can be 
multi-modal

and vector stores can be used to store and retrieve a variety of data types beyond text.


Overview
​


Vector stores are specialized data stores that enable indexing and retrieving information based on vector representations.


These vectors, called 
embeddings
, capture the semantic meaning of data that has been embedded.


Vector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.




Integrations
​


LangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.


Please see the 
full list of LangChain vectorstore integrations
.


Interface
​


LangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.


The interface consists of basic methods for writing, deleting and searching for documents in the vector store.


The key methods are:




add_documents
: Add a list of texts to the vector store.


delete
: Delete a list of documents from the vector store.


similarity_search
: Search for similar documents to a given query.




Initialization
​


Most vectors in LangChain accept an embedding model as an argument when initializing the vector store.


We will use LangChain's 
InMemoryVectorStore
 implementation to illustrate the API.


from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
# Initialize with an embedding model
vector_store 
=
 InMemoryVectorStore
(
embedding
=
SomeEmbeddingModel
(
)
)
API Reference:
InMemoryVectorStore


Adding documents
​


To add documents, use the 
add_documents
 method.


This API works with a list of 
Document
 objects.

Document
 objects all have 
page_content
 and 
metadata
 attributes, making them a universal way to store unstructured text and associated metadata.


from
 langchain_core
.
documents 
import
 Document
document_1 
=
 Document
(
    page_content
=
"I had chocolate chip pancakes and scrambled eggs for breakfast this morning."
,
    metadata
=
{
"source"
:
 
"tweet"
}
,
)
document_2 
=
 Document
(
    page_content
=
"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees."
,
    metadata
=
{
"source"
:
 
"news"
}
,
)
documents 
=
 
[
document_1
,
 document_2
]
vector_store
.
add_documents
(
documents
=
documents
)
API Reference:
Document


You should usually provide IDs for the documents you add to the vector store, so
that instead of adding the same document multiple times, you can update the existing document.


vector_store
.
add_documents
(
documents
=
documents
,
 ids
=
[
"doc1"
,
 
"doc2"
]
)


Delete
​


To delete documents, use the 
delete
 method which takes a list of document IDs to delete.


vector_store
.
delete
(
ids
=
[
"doc1"
]
)


Search
​


Vector stores embed and store the documents that added.
If we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones.
This captures two important concepts: first, there needs to be a way to measure the similarity between the query and 
any
 
embedded
 document.
Second, there needs to be an algorithm to efficiently perform this similarity search across 
all
 embedded documents.


Similarity metrics
​


A critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:




Cosine Similarity
: Measures the cosine of the angle between two vectors.


Euclidean Distance
: Measures the straight-line distance between two points.


Dot Product
: Measures the projection of one vector onto another.




The choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer
to the documentation of the specific vectorstore you are using to see what similarity metrics are supported.


Further reading


See 
this documentation
 from Google on similarity metrics to consider with embeddings.


See Pinecone's 
blog post
 on similarity metrics.


See OpenAI's 
FAQ
 on what similarity metric to use with OpenAI embeddings.




Similarity search
​


Given a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over 
all
 the embedded documents to find the most similar ones.
There are various ways to do this. As an example, many vectorstores implement 
HNSW (Hierarchical Navigable Small World)
, a graph-based index structure that allows for efficient similarity search.
Regardless of the search algorithm used under the hood, the LangChain vectorstore interface has a 
similarity_search
 method for all integrations.
This will take the search query, create an embedding, find similar documents, and return them as a list of 
Documents
.


query 
=
 
"my query"
docs 
=
 vectorstore
.
similarity_search
(
query
)


Many vectorstores support search parameters to be passed with the 
similarity_search
 method. See the documentation for the specific vectorstore you are using to see what parameters are supported.
As an example 
Pinecone
 several parameters that are important general concepts:
Many vectorstores support 
the 
k
, which controls the number of Documents to return, and 
filter
, which allows for filtering documents by metadata.




query (str) – Text to look up documents similar to.


k (int) – Number of Documents to return. Defaults to 4.


filter (dict | None) – Dictionary of argument(s) to filter on metadata




Further reading


See the 
how-to guide
 for more details on how to use the 
similarity_search
 method.


See the 
integrations page
 for more details on arguments that can be passed in to the 
similarity_search
 method for specific vectorstores.




Metadata filtering
​


While vectorstore implement a search algorithm to efficiently search over 
all
 the embedded documents to find the most similar ones, many also support filtering on metadata.
Metadata filtering helps narrow down the search by applying specific conditions such as retrieving documents from a particular source or date range. These two concepts work well together:




Semantic search
: Query the unstructured data directly, often via embedding or keyword similarity.


Metadata search
: Apply structured query to the metadata, filtering specific documents.




Vector store support for metadata filtering is typically dependent on the underlying vector store implementation.


Here is example usage with 
Pinecone
, showing that we filter for all documents that have the metadata key 
source
 with value 
tweet
.


vectorstore
.
similarity_search
(
    
"LangChain provides abstractions to make working with LLMs easy"
,
    k
=
2
,
    
filter
=
{
"source"
:
 
"tweet"
}
,
)


Further reading


See Pinecone's 
documentation
 on filtering with metadata.


See the 
list of LangChain vectorstore integrations
 that support metadata filtering.




Advanced search and retrieval techniques
​


While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, 
maximal marginal relevance
 is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.
As a second example, some 
vector stores
 offer built-in 
hybrid-search
 to combine keyword and semantic similarity search, which marries the benefits of both approaches.
At the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with 
similarity_search
.
See this 
how-to guide on hybrid search
 for more details.


Name
When to use
Description
Hybrid search
When combining keyword-based and semantic similarity.
Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. 
Paper
.
Maximal Marginal Relevance (MMR)
When needing to diversify search results.
MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.
Edit this page
Was this page helpful?
Previous
Tracing
Next
Why LangChain?
Overview
Integrations
Interface
Initialization
Adding documents
Delete
Search
Similarity metrics
Similarity search
Metadata filtering
Advanced search and retrieval techniques


--- docs_concepts_why_langchain.txt ---

Conceptual guide
Why LangChain?
On this page
Why LangChain?


The goal of 
langchain
 the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.
While LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.
This page will talk about the LangChain ecosystem as a whole.
Most of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!


Features
​


There are several primary needs that LangChain aims to address:






Standardized component interfaces:
 The growing number of 
models
 and 
related components
 for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.
This diversity can make it challenging for developers to switch between providers or combine components when building applications.
LangChain exposes a standard interface for key components, making it easy to switch between providers.






Orchestration:
 As applications become more complex, combining multiple components and models, there's 
a growing need to efficiently connect these elements into control flows
 that can 
accomplish diverse tasks
.

Orchestration
 is crucial for building such applications.






Observability and evaluation:
 As applications become more complex, it becomes increasingly difficult to understand what is happening within them.
Furthermore, the pace of development can become rate-limited by the 
paradox of choice
.
For example, developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost.

Observability
 and evaluations can help developers monitor their applications and rapidly answer these types of questions with confidence.






Standardized component interfaces
​


LangChain provides common interfaces for components that are central to many AI applications.
As an example, all 
chat models
 implement the 
BaseChatModel
 interface.
This provides a standard way to interact with chat models, supporting important but often provider-specific features like 
tool calling
 and 
structured outputs
.


Example: chat models
​


Many 
model providers
 support 
tool calling
, a critical feature for many applications (e.g., 
agents
), that allows a developer to request model responses that match a particular schema.
The APIs for each provider differ.
LangChain's 
chat model
 interface provides a common way to bind 
tools
 to a model in order to support 
tool calling
:


# Tool creation
tools 
=
 
[
my_tool
]
# Tool binding
model_with_tools 
=
 model
.
bind_tools
(
tools
)


Similarly, getting models to produce 
structured outputs
 is an extremely common use case.
Providers support different approaches for this, including 
JSON mode or tool calling
, with different APIs.
LangChain's 
chat model
 interface provides a common way to produce structured outputs using the 
with_structured_output()
 method:


# Define schema
schema 
=
 
.
.
.
# Bind schema to model
model_with_structure 
=
 model
.
with_structured_output
(
schema
)


Example: retrievers
​


In the context of 
RAG
 and LLM application components, LangChain's 
retriever
 interface provides a standard way to connect to many different types of data services or databases (e.g., 
vector stores
 or databases).
The underlying implementation of the retriever depends on the type of data store or database you are connecting to, but all retrievers implement the 
runnable interface
, meaning they can be invoked in a common manner.


documents 
=
 my_retriever
.
invoke
(
"What is the meaning of life?"
)


Orchestration
​


While standardization for individual components is useful, we've increasingly seen that developers want to 
combine
 components into more complex applications.
This motivates the need for 
orchestration
.
There are several common characteristics of LLM applications that this orchestration layer should support:




Complex control flow:
 The application requires complex patterns such as cycles (e.g., a loop that reiterates until a condition is met).


Persistence
:
 The application needs to maintain 
short-term and / or long-term memory
.


Human-in-the-loop
:
 The application needs human interaction, e.g., pausing, reviewing, editing, approving certain steps.




The recommended way to orchestrate components for complex applications is 
LangGraph
.
LangGraph is a library that gives developers a high degree of control by expressing the flow of the application as a set of nodes and edges.
LangGraph comes with built-in support for 
persistence
, 
human-in-the-loop
, 
memory
, and other features.
It's particularly well suited for building 
agents
 or 
multi-agent
 applications.
Importantly, individual LangChain components can be used as LangGraph nodes, but you can also use LangGraph 
without
 using LangChain components.


Further reading
Have a look at our free course, 
Introduction to LangGraph
, to learn more about how to use LangGraph to build complex applications.


Observability and evaluation
​


The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice.
Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost.
High quality tracing and evaluations can help you rapidly answer these types of questions with confidence.

LangSmith
 is our platform that supports observability and evaluation for AI applications.
See our conceptual guides on 
evaluations
 and 
tracing
 for more details.


Further reading
See our video playlist on 
LangSmith tracing and evaluations
 for more details.


Conclusion
​


LangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:




Ease of swapping providers:
 It allows you to swap out different component providers without having to change the underlying code.


Advanced features:
 It provides common methods for more advanced features, such as 
streaming
 and 
tool calling
.




LangGraph
 makes it possible to orchestrate complex applications (e.g., 
agents
) and provide features like including 
persistence
, 
human-in-the-loop
, or 
memory
.


LangSmith
 makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.
Edit this page
Was this page helpful?
Previous
Vector stores
Next
v0.3
Features
Standardized component interfaces
Example: chat models
Example: retrievers
Orchestration
Observability and evaluation
Conclusion


--- docs_contributing.txt ---

Welcome Contributors
On this page
Welcome Contributors


Hi there! Thank you for your interest in contributing to LangChain.
As an open-source project in a fast developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes.


Tutorials
​


More coming soon! We are working on tutorials to help you make your first contribution to the project.




Make your first docs PR




How-to Guides
​




Documentation
: Help improve our docs, including this one!


Code
: Help us write code, fix bugs, or improve our infrastructure.


Integrations
: Help us integrate with your favorite vendors and tools.


Standard Tests
: Ensure your integration passes an expected set of tests.




Reference
​




Repository Structure
: Understand the high level structure of the repository.


Review Process
: Learn about the review process for pull requests.


Frequently Asked Questions (FAQ)
: Get answers to common questions about contributing.




Community
​


💭 GitHub Discussions
​


We have a 
discussions
 page where users can ask usage questions, discuss design decisions, and propose new features.


If you are able to help answer questions, please do so! This will allow the maintainers to spend more time focused on development and bug fixing.


🚩 GitHub Issues
​


Our 
issues
 page is kept up to date with bugs, improvements, and feature requests.


There is a 
taxonomy of labels

to help with sorting and discovery of issues of interest. Please use these to help
organize issues. Check out the 
Help Wanted

and 
Good First Issue

tags for recommendations.


If you start working on an issue, please assign it to yourself.


If you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature.
If two issues are related, or blocking, please link them rather than combining them.


We will try to keep these issues as up-to-date as possible, though
with the rapid rate of development in this field some may get out of date.
If you notice this happening, please let us know.


📢 Community Slack
​


We have a 
community slack
 where you can ask questions, get help, and discuss the project with other contributors and users.


🙋 Getting Help
​


Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please
ask in 
community slack
 or open a 
discussion on GitHub
.


In a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase.
If you are finding these difficult (or even just annoying) to work with, feel free to ask in 
community slack
!
Edit this page
Was this page helpful?
Next
Tutorials
Tutorials
How-to Guides
Reference
Community
💭 GitHub Discussions
🚩 GitHub Issues
📢 Community Slack
🙋 Getting Help


--- docs_get_started_introduction.txt ---

Introduction
On this page
Introduction


LangChain
 is a framework for developing applications powered by large language models (LLMs).


LangChain simplifies every stage of the LLM application lifecycle:




Development
: Build your applications using LangChain's open-source 
components
 and 
third-party integrations
.
Use 
LangGraph
 to build stateful agents with first-class streaming and human-in-the-loop support.


Productionization
: Use 
LangSmith
 to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.


Deployment
: Turn your LangGraph applications into production-ready APIs and Assistants with 
LangGraph Platform
.








LangChain implements a standard interface for large language models and related
technologies, such as embedding models and vector stores, and integrates with
hundreds of providers. See the 
integrations
 page for
more.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


model
.
invoke
(
"Hello, world!"
)


note
These docs focus on the Python LangChain library. 
Head here
 for docs on the JavaScript LangChain library.


Architecture
​


The LangChain framework consists of multiple open-source libraries. Read more in the

Architecture
 page.




langchain-core
: Base abstractions for chat models and other components.


Integration packages
 (e.g. 
langchain-openai
, 
langchain-anthropic
, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.


langchain
: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.


langchain-community
: Third-party integrations that are community maintained.


langgraph
: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See 
LangGraph documentation
.




Guides
​


Tutorials
​


If you're looking to build something specific or are more of a hands-on learner, check out our 
tutorials section
.
This is the best place to get started.


These are the best ones to get started with:




Build a Simple LLM Application


Build a Chatbot


Build an Agent


Introduction to LangGraph




Explore the full list of LangChain tutorials 
here
, and check out other 
LangGraph tutorials here
. To learn more about LangGraph, check out our first LangChain Academy course, 
Introduction to LangGraph
, available 
here
.


How-to guides
​


Here
 you’ll find short answers to “How do I….?” types of questions.
These how-to guides don’t cover topics in depth – you’ll find that material in the 
Tutorials
 and the 
API Reference
.
However, these guides will help you quickly accomplish common tasks using 
chat models
,

vector stores
, and other common LangChain components.


Check out 
LangGraph-specific how-tos here
.


Conceptual guide
​


Introductions to all the key parts of LangChain you’ll need to know! 
Here
 you'll find high level explanations of all LangChain concepts.


For a deeper dive into LangGraph concepts, check out 
this page
.


Integrations
​


LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.
If you're looking to get up and running quickly with 
chat models
, 
vector stores
,
or other LangChain components from a specific provider, check out our growing list of 
integrations
.


API reference
​


Head to the reference section for full documentation of all classes and methods in the LangChain Python packages.


Ecosystem
​


🦜🛠️ LangSmith
​


Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.


🦜🕸️ LangGraph
​


Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.


Additional resources
​


Versions
​


See what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.


Security
​


Read up on 
security
 best practices to make sure you're developing safely with LangChain.


Contributing
​


Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.
Edit this page
Was this page helpful?
Next
Tutorials
Architecture
Guides
Tutorials
How-to guides
Conceptual guide
Integrations
API reference
Ecosystem
🦜🛠️ LangSmith
🦜🕸️ LangGraph
Additional resources
Versions
Security
Contributing


--- docs_how_to.txt ---

How-to guides
On this page
How-to guides


Here you’ll find answers to “How do I….?” types of questions.
These guides are 
goal-oriented
 and 
concrete
; they're meant to help you complete a specific task.
For conceptual explanations see the 
Conceptual guide
.
For end-to-end walkthroughs see 
Tutorials
.
For comprehensive descriptions of every class and function see the 
API Reference
.


Installation
​




How to: install LangChain packages


How to: use LangChain with different Pydantic versions




Key features
​


This highlights functionality that is core to using LangChain.




How to: return structured data from a model


How to: use a model to call tools


How to: stream runnables


How to: debug your LLM apps




Components
​


These are the core building blocks you can use when building applications.


Chat models
​


Chat Models
 are newer forms of language models that take messages in and output a message.
See 
supported integrations
 for details on getting started with chat models from a specific provider.




How to: do function/tool calling


How to: get models to return structured output


How to: cache model responses


How to: get log probabilities


How to: create a custom chat model class


How to: stream a response back


How to: track token usage


How to: track response metadata across providers


How to: use chat model to call tools


How to: stream tool calls


How to: handle rate limits


How to: few shot prompt tool behavior


How to: bind model-specific formatted tools


How to: force a specific tool call


How to: work with local models


How to: init any model in one line


How to: pass multimodal data directly to models




Messages
​


Messages
 are the input and output of chat models. They have some 
content
 and a 
role
, which describes the source of the message.




How to: trim messages


How to: filter messages


How to: merge consecutive messages of the same type




Prompt templates
​


Prompt Templates
 are responsible for formatting user input into a format that can be passed to a language model.




How to: use few shot examples


How to: use few shot examples in chat models


How to: partially format prompt templates


How to: compose prompts together


How to: use multimodal prompts




Example selectors
​


Example Selectors
 are responsible for selecting the correct few shot examples to pass to the prompt.




How to: use example selectors


How to: select examples by length


How to: select examples by semantic similarity


How to: select examples by semantic ngram overlap


How to: select examples by maximal marginal relevance


How to: select examples from LangSmith few-shot datasets




LLMs
​


What LangChain calls 
LLMs
 are older forms of language models that take a string in and output a string.




How to: cache model responses


How to: create a custom LLM class


How to: stream a response back


How to: track token usage


How to: work with local models




Output parsers
​


Output Parsers
 are responsible for taking the output of an LLM and parsing into more structured format.




How to: parse text from message objects


How to: use output parsers to parse an LLM response into structured format


How to: parse JSON output


How to: parse XML output


How to: parse YAML output


How to: retry when output parsing errors occur


How to: try to fix errors in output parsing


How to: write a custom output parser class




Document loaders
​


Document Loaders
 are responsible for loading documents from a variety of sources.




How to: load PDF files


How to: load web pages


How to: load CSV data


How to: load data from a directory


How to: load HTML data


How to: load JSON data


How to: load Markdown data


How to: load Microsoft Office data


How to: write a custom document loader




Text splitters
​


Text Splitters
 take a document and split into chunks that can be used for retrieval.




How to: recursively split text


How to: split HTML


How to: split by character


How to: split code


How to: split Markdown by headers


How to: recursively split JSON


How to: split text into semantic chunks


How to: split by tokens




Embedding models
​


Embedding Models
 take a piece of text and create a numerical representation of it.
See 
supported integrations
 for details on getting started with embedding models from a specific provider.




How to: embed text data


How to: cache embedding results


How to: create a custom embeddings class




Vector stores
​


Vector stores
 are databases that can efficiently store and retrieve embeddings.
See 
supported integrations
 for details on getting started with vector stores from a specific provider.




How to: use a vector store to retrieve data




Retrievers
​


Retrievers
 are responsible for taking a query and returning relevant documents.




How to: use a vector store to retrieve data


How to: generate multiple queries to retrieve data for


How to: use contextual compression to compress the data retrieved


How to: write a custom retriever class


How to: add similarity scores to retriever results


How to: combine the results from multiple retrievers


How to: reorder retrieved results to mitigate the "lost in the middle" effect


How to: generate multiple embeddings per document


How to: retrieve the whole document for a chunk


How to: generate metadata filters


How to: create a time-weighted retriever


How to: use hybrid vector and keyword retrieval




Indexing
​


Indexing is the process of keeping your vectorstore in-sync with the underlying data source.




How to: reindex data to keep your vectorstore in-sync with the underlying data source




Tools
​


LangChain 
Tools
 contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer 
here
 for a list of pre-built tools.




How to: create tools


How to: use built-in tools and toolkits


How to: use chat models to call tools


How to: pass tool outputs to chat models


How to: pass run time values to tools


How to: add a human-in-the-loop for tools


How to: handle tool errors


How to: force models to call a tool


How to: disable parallel tool calling


How to: access the 
RunnableConfig
 from a tool


How to: stream events from a tool


How to: return artifacts from a tool


How to: convert Runnables to tools


How to: add ad-hoc tool calling capability to models


How to: pass in runtime secrets




Multimodal
​




How to: pass multimodal data directly to models


How to: use multimodal prompts




Agents
​


note
For in depth how-to guides for agents, please check out 
LangGraph
 documentation.




How to: use legacy LangChain Agents (AgentExecutor)


How to: migrate from legacy LangChain agents to LangGraph




Callbacks
​


Callbacks
 allow you to hook into the various stages of your LLM application's execution.




How to: pass in callbacks at runtime


How to: attach callbacks to a module


How to: pass callbacks into a module constructor


How to: create custom callback handlers


How to: use callbacks in async environments


How to: dispatch custom callback events




Custom
​


All of LangChain components can easily be extended to support your own versions.




How to: create a custom chat model class


How to: create a custom LLM class


How to: create a custom embeddings class


How to: write a custom retriever class


How to: write a custom document loader


How to: write a custom output parser class


How to: create custom callback handlers


How to: define a custom tool


How to: dispatch custom callback events




Serialization
​




How to: save and load LangChain objects




Use cases
​


These guides cover use-case specific details.


Q&A with RAG
​


Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.
For a high-level tutorial on RAG, check out 
this guide
.




How to: add chat history


How to: stream


How to: return sources


How to: return citations


How to: do per-user retrieval




Extraction
​


Extraction is when you use LLMs to extract structured information from unstructured text.
For a high level tutorial on extraction, check out 
this guide
.




How to: use reference examples


How to: handle long text


How to: do extraction without using function calling




Chatbots
​


Chatbots involve using an LLM to have a conversation.
For a high-level tutorial on building chatbots, check out 
this guide
.




How to: manage memory


How to: do retrieval


How to: use tools


How to: manage large chat history




Query analysis
​


Query Analysis is the task of using an LLM to generate a query to send to a retriever.
For a high-level tutorial on query analysis, check out 
this guide
.




How to: add examples to the prompt


How to: handle cases where no queries are generated


How to: handle multiple queries


How to: handle multiple retrievers


How to: construct filters


How to: deal with high cardinality categorical variables




Q&A over SQL + CSV
​


You can use LLMs to do question answering over tabular data.
For a high-level tutorial, check out 
this guide
.




How to: use prompting to improve results


How to: do query validation


How to: deal with large databases


How to: deal with CSV files




Q&A over graph databases
​


You can use an LLM to do question answering over graph databases.
For a high-level tutorial, check out 
this guide
.




How to: add a semantic layer over the database


How to: construct knowledge graphs




Summarization
​


LLMs can summarize and otherwise distill desired information from text, including
large volumes of text. For a high-level tutorial, check out 
this guide
.




How to: summarize text in a single LLM call


How to: summarize text through parallelization


How to: summarize text through iterative refinement




LangChain Expression Language (LCEL)
​


Should I use LCEL?
LCEL is an orchestration solution. See our

concepts page
 for recommendations on when to
use LCEL.


LangChain Expression Language
 is a way to create arbitrary custom chains. It is built on the 
Runnable
 protocol.


LCEL cheatsheet
: For a quick overview of how to use the main LCEL primitives.


Migration guide
: For migrating legacy chain abstractions to LCEL.




How to: chain runnables


How to: stream runnables


How to: invoke runnables in parallel


How to: add default invocation args to runnables


How to: turn any function into a runnable


How to: pass through inputs from one chain step to the next


How to: configure runnable behavior at runtime


How to: add message history (memory) to a chain


How to: route between sub-chains


How to: create a dynamic (self-constructing) chain


How to: inspect runnables


How to: add fallbacks to a runnable


How to: pass runtime secrets to a runnable




LangGraph
​


LangGraph is an extension of LangChain aimed at
building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.


LangGraph documentation is currently hosted on a separate site.
You can peruse 
LangGraph how-to guides here
.


LangSmith
​


LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.


LangSmith documentation is hosted on a separate site.
You can peruse 
LangSmith how-to guides here
, but we'll highlight a few sections that are particularly
relevant to LangChain below:


Evaluation
​




Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.


To learn more, check out the 
LangSmith evaluation how-to guides
.


Tracing
​




Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.




How to: trace with LangChain


How to: add metadata and tags to traces




You can see general tracing-related how-tos 
in this section of the LangSmith docs
.
Edit this page
Was this page helpful?
Previous
Summarize Text
Next
How-to guides
Installation
Key features
Components
Chat models
Messages
Prompt templates
Example selectors
LLMs
Output parsers
Document loaders
Text splitters
Embedding models
Vector stores
Retrievers
Indexing
Tools
Multimodal
Agents
Callbacks
Custom
Serialization
Use cases
Q&A with RAG
Extraction
Chatbots
Query analysis
Q&A over SQL + CSV
Q&A over graph databases
Summarization
LangChain Expression Language (LCEL)
LangGraph
LangSmith
Evaluation
Tracing


--- docs_how_to_MultiQueryRetriever.txt ---

How-to guides
How to use the MultiQueryRetriever
On this page
How to use the MultiQueryRetriever


Distance-based 
vector database
 retrieval 
embeds
 (represents) queries in high-dimensional space and finds similar embedded documents based on a distance metric. But, retrieval may produce different results with subtle changes in query wording, or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.


The 
MultiQueryRetriever
 automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the 
MultiQueryRetriever
 can mitigate some of the limitations of the distance-based retrieval and get a richer set of results.


Let's build a vectorstore using the 
LLM Powered Autonomous Agents
 blog post by Lilian Weng from the 
RAG tutorial
:


# Build a sample vectorDB
from
 langchain_chroma 
import
 Chroma
from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
# Load blog post
loader 
=
 WebBaseLoader
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
)
data 
=
 loader
.
load
(
)
# Split
text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
500
,
 chunk_overlap
=
0
)
splits 
=
 text_splitter
.
split_documents
(
data
)
# VectorDB
embedding 
=
 OpenAIEmbeddings
(
)
vectordb 
=
 Chroma
.
from_documents
(
documents
=
splits
,
 embedding
=
embedding
)
API Reference:
WebBaseLoader
 | 
OpenAIEmbeddings
 | 
RecursiveCharacterTextSplitter


USER_AGENT environment variable not set, consider setting it to identify your requests.


Simple usage
​


Specify the LLM to use for query generation, and the retriever will do the rest.


from
 langchain
.
retrievers
.
multi_query 
import
 MultiQueryRetriever
from
 langchain_openai 
import
 ChatOpenAI
question 
=
 
"What are the approaches to Task Decomposition?"
llm 
=
 ChatOpenAI
(
temperature
=
0
)
retriever_from_llm 
=
 MultiQueryRetriever
.
from_llm
(
    retriever
=
vectordb
.
as_retriever
(
)
,
 llm
=
llm
)
API Reference:
MultiQueryRetriever
 | 
ChatOpenAI


# Set logging for the queries
import
 logging
logging
.
basicConfig
(
)
logging
.
getLogger
(
"langchain.retrievers.multi_query"
)
.
setLevel
(
logging
.
INFO
)


unique_docs 
=
 retriever_from_llm
.
invoke
(
question
)
len
(
unique_docs
)


INFO:langchain.retrievers.multi_query:Generated queries: ['1. How can Task Decomposition be achieved through different methods?', '2. What strategies are commonly used for Task Decomposition?', '3. What are the various ways to break down tasks in Task Decomposition?']


5


Note that the underlying queries generated by the 
retriever
 are logged at the 
INFO
 level.


Supplying your own prompt
​


Under the hood, 
MultiQueryRetriever
 generates queries using a specific 
prompt
. To customize this prompt:




Make a 
PromptTemplate
 with an input variable for the question;


Implement an 
output parser
 like the one below to split the result into a list of queries.




The prompt and output parser together must support the generation of a list of queries.


from
 typing 
import
 List
from
 langchain_core
.
output_parsers 
import
 BaseOutputParser
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 pydantic 
import
 BaseModel
,
 Field
# Output parser will split the LLM result into a list of queries
class
 
LineListOutputParser
(
BaseOutputParser
[
List
[
str
]
]
)
:
    
"""Output parser for a list of lines."""
    
def
 
parse
(
self
,
 text
:
 
str
)
 
-
>
 List
[
str
]
:
        lines 
=
 text
.
strip
(
)
.
split
(
"\n"
)
        
return
 
list
(
filter
(
None
,
 lines
)
)
  
# Remove empty lines
output_parser 
=
 LineListOutputParser
(
)
QUERY_PROMPT 
=
 PromptTemplate
(
    input_variables
=
[
"question"
]
,
    template
=
"""You are an AI language model assistant. Your task is to generate five 
    different versions of the given user question to retrieve relevant documents from a vector 
    database. By generating multiple perspectives on the user question, your goal is to help
    the user overcome some of the limitations of the distance-based similarity search. 
    Provide these alternative questions separated by newlines.
    Original question: {question}"""
,
)
llm 
=
 ChatOpenAI
(
temperature
=
0
)
# Chain
llm_chain 
=
 QUERY_PROMPT 
|
 llm 
|
 output_parser
# Other inputs
question 
=
 
"What are the approaches to Task Decomposition?"
API Reference:
BaseOutputParser
 | 
PromptTemplate


# Run
retriever 
=
 MultiQueryRetriever
(
    retriever
=
vectordb
.
as_retriever
(
)
,
 llm_chain
=
llm_chain
,
 parser_key
=
"lines"
)
  
# "lines" is the key (attribute name) of the parsed output
# Results
unique_docs 
=
 retriever
.
invoke
(
"What does the course say about regression?"
)
len
(
unique_docs
)


INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide insights on regression from the course material?', '2. How is regression discussed in the course content?', '3. What information does the course offer regarding regression?', '4. In what way is regression covered in the course?', "5. What are the course's teachings on regression?"]


9
Edit this page
Was this page helpful?
Previous
Custom Document Loader
Next
How to add scores to retriever results


--- docs_how_to_add_scores_retriever.txt ---

How-to guides
How to add scores to retriever results
On this page
How to add scores to retriever results


Retrievers
 will return sequences of 
Document
 objects, which by default include no information about the process that retrieved them (e.g., a similarity score against a query). Here we demonstrate how to add retrieval scores to the 
.metadata
 of documents:




From 
vectorstore retrievers
;


From higher-order LangChain retrievers, such as 
SelfQueryRetriever
 or 
MultiVectorRetriever
.




For (1), we will implement a short wrapper function around the corresponding 
vector store
. For (2), we will update a method of the corresponding class.


Create vector store
​


First we populate a vector store with some data. We will use a 
PineconeVectorStore
, but this guide is compatible with any LangChain vector store that implements a 
.similarity_search_with_score
 method.


from
 langchain_core
.
documents 
import
 Document
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_pinecone 
import
 PineconeVectorStore
docs 
=
 
[
    Document
(
        page_content
=
"A bunch of scientists bring back dinosaurs and mayhem breaks loose"
,
        metadata
=
{
"year"
:
 
1993
,
 
"rating"
:
 
7.7
,
 
"genre"
:
 
"science fiction"
}
,
    
)
,
    Document
(
        page_content
=
"Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."
,
        metadata
=
{
"year"
:
 
2010
,
 
"director"
:
 
"Christopher Nolan"
,
 
"rating"
:
 
8.2
}
,
    
)
,
    Document
(
        page_content
=
"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"
,
        metadata
=
{
"year"
:
 
2006
,
 
"director"
:
 
"Satoshi Kon"
,
 
"rating"
:
 
8.6
}
,
    
)
,
    Document
(
        page_content
=
"A bunch of normal-sized women are supremely wholesome and some men pine after them"
,
        metadata
=
{
"year"
:
 
2019
,
 
"director"
:
 
"Greta Gerwig"
,
 
"rating"
:
 
8.3
}
,
    
)
,
    Document
(
        page_content
=
"Toys come alive and have a blast doing so"
,
        metadata
=
{
"year"
:
 
1995
,
 
"genre"
:
 
"animated"
}
,
    
)
,
    Document
(
        page_content
=
"Three men walk into the Zone, three men walk out of the Zone"
,
        metadata
=
{
            
"year"
:
 
1979
,
            
"director"
:
 
"Andrei Tarkovsky"
,
            
"genre"
:
 
"thriller"
,
            
"rating"
:
 
9.9
,
        
}
,
    
)
,
]
vectorstore 
=
 PineconeVectorStore
.
from_documents
(
    docs
,
 index_name
=
"sample"
,
 embedding
=
OpenAIEmbeddings
(
)
)
API Reference:
Document
 | 
OpenAIEmbeddings
 | 
PineconeVectorStore


Retriever
​


To obtain scores from a vector store retriever, we wrap the underlying vector store's 
.similarity_search_with_score
 method in a short function that packages scores into the associated document's metadata.


We add a 
@chain
 decorator to the function to create a 
Runnable
 that can be used similarly to a typical retriever.


from
 typing 
import
 List
from
 langchain_core
.
documents 
import
 Document
from
 langchain_core
.
runnables 
import
 chain
@chain
def
 
retriever
(
query
:
 
str
)
 
-
>
 List
[
Document
]
:
    docs
,
 scores 
=
 
zip
(
*
vectorstore
.
similarity_search_with_score
(
query
)
)
    
for
 doc
,
 score 
in
 
zip
(
docs
,
 scores
)
:
        doc
.
metadata
[
"score"
]
 
=
 score
    
return
 docs
API Reference:
Document
 | 
chain


result 
=
 retriever
.
invoke
(
"dinosaur"
)
result


(Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993.0, 'score': 0.84429127}),
 Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995.0, 'score': 0.792038262}),
 Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979.0, 'score': 0.751571238}),
 Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006.0, 'score': 0.747471571}))


Note that similarity scores from the retrieval step are included in the metadata of the above documents.


SelfQueryRetriever
​


SelfQueryRetriever
 will use a LLM to generate a query that is potentially structured-- for example, it can construct filters for the retrieval on top of the usual semantic-similarity driven selection. See 
this guide
 for more detail.


SelfQueryRetriever
 includes a short (1 - 2 line) method 
_get_docs_with_query
 that executes the 
vectorstore
 search. We can subclass 
SelfQueryRetriever
 and override this method to propagate similarity scores.


First, following the 
how-to guide
, we will need to establish some metadata on which to filter:


from
 langchain
.
chains
.
query_constructor
.
base 
import
 AttributeInfo
from
 langchain
.
retrievers
.
self_query
.
base 
import
 SelfQueryRetriever
from
 langchain_openai 
import
 ChatOpenAI
metadata_field_info 
=
 
[
    AttributeInfo
(
        name
=
"genre"
,
        description
=
"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']"
,
        
type
=
"string"
,
    
)
,
    AttributeInfo
(
        name
=
"year"
,
        description
=
"The year the movie was released"
,
        
type
=
"integer"
,
    
)
,
    AttributeInfo
(
        name
=
"director"
,
        description
=
"The name of the movie director"
,
        
type
=
"string"
,
    
)
,
    AttributeInfo
(
        name
=
"rating"
,
 description
=
"A 1-10 rating for the movie"
,
 
type
=
"float"
    
)
,
]
document_content_description 
=
 
"Brief summary of a movie"
llm 
=
 ChatOpenAI
(
temperature
=
0
)
API Reference:
AttributeInfo
 | 
SelfQueryRetriever
 | 
ChatOpenAI


We then override the 
_get_docs_with_query
 to use the 
similarity_search_with_score
 method of the underlying vector store:


from
 typing 
import
 Any
,
 Dict
class
 
CustomSelfQueryRetriever
(
SelfQueryRetriever
)
:
    
def
 
_get_docs_with_query
(
        self
,
 query
:
 
str
,
 search_kwargs
:
 Dict
[
str
,
 Any
]
    
)
 
-
>
 List
[
Document
]
:
        
"""Get docs, adding score information."""
        docs
,
 scores 
=
 
zip
(
            
*
self
.
vectorstore
.
similarity_search_with_score
(
query
,
 
**
search_kwargs
)
        
)
        
for
 doc
,
 score 
in
 
zip
(
docs
,
 scores
)
:
            doc
.
metadata
[
"score"
]
 
=
 score
        
return
 docs


Invoking this retriever will now include similarity scores in the document metadata. Note that the underlying structured-query capabilities of 
SelfQueryRetriever
 are retained.


retriever 
=
 CustomSelfQueryRetriever
.
from_llm
(
    llm
,
    vectorstore
,
    document_content_description
,
    metadata_field_info
,
)
result 
=
 retriever
.
invoke
(
"dinosaur movie with rating less than 8"
)
result


(Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993.0, 'score': 0.84429127}),)


MultiVectorRetriever
​


MultiVectorRetriever
 allows you to associate multiple vectors with a single document. This can be useful in a number of applications. For example, we can index small chunks of a larger document and run the retrieval on the chunks, but return the larger "parent" document when invoking the retriever. 
ParentDocumentRetriever
, a subclass of 
MultiVectorRetriever
, includes convenience methods for populating a vector store to support this. Further applications are detailed in this 
how-to guide
.


To propagate similarity scores through this retriever, we can again subclass 
MultiVectorRetriever
 and override a method. This time we will override 
_get_relevant_documents
.


First, we prepare some fake data. We generate fake "whole documents" and store them in a document store; here we will use a simple 
InMemoryStore
.


from
 langchain
.
storage 
import
 InMemoryStore
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
# The storage layer for the parent documents
docstore 
=
 InMemoryStore
(
)
fake_whole_documents 
=
 
[
    
(
"fake_id_1"
,
 Document
(
page_content
=
"fake whole document 1"
)
)
,
    
(
"fake_id_2"
,
 Document
(
page_content
=
"fake whole document 2"
)
)
,
]
docstore
.
mset
(
fake_whole_documents
)
API Reference:
InMemoryStore
 | 
RecursiveCharacterTextSplitter


Next we will add some fake "sub-documents" to our vector store. We can link these sub-documents to the parent documents by populating the 
"doc_id"
 key in its metadata.


docs 
=
 
[
    Document
(
        page_content
=
"A snippet from a larger document discussing cats."
,
        metadata
=
{
"doc_id"
:
 
"fake_id_1"
}
,
    
)
,
    Document
(
        page_content
=
"A snippet from a larger document discussing discourse."
,
        metadata
=
{
"doc_id"
:
 
"fake_id_1"
}
,
    
)
,
    Document
(
        page_content
=
"A snippet from a larger document discussing chocolate."
,
        metadata
=
{
"doc_id"
:
 
"fake_id_2"
}
,
    
)
,
]
vectorstore
.
add_documents
(
docs
)


['62a85353-41ff-4346-bff7-be6c8ec2ed89',
 '5d4a0e83-4cc5-40f1-bc73-ed9cbad0ee15',
 '8c1d9a56-120f-45e4-ba70-a19cd19a38f4']


To propagate the scores, we subclass 
MultiVectorRetriever
 and override its 
_get_relevant_documents
 method. Here we will make two changes:




We will add similarity scores to the metadata of the corresponding "sub-documents" using the 
similarity_search_with_score
 method of the underlying vector store as above;


We will include a list of these sub-documents in the metadata of the retrieved parent document. This surfaces what snippets of text were identified by the retrieval, together with their corresponding similarity scores.




from
 collections 
import
 defaultdict
from
 langchain
.
retrievers 
import
 MultiVectorRetriever
from
 langchain_core
.
callbacks 
import
 CallbackManagerForRetrieverRun
class
 
CustomMultiVectorRetriever
(
MultiVectorRetriever
)
:
    
def
 
_get_relevant_documents
(
        self
,
 query
:
 
str
,
 
*
,
 run_manager
:
 CallbackManagerForRetrieverRun
    
)
 
-
>
 List
[
Document
]
:
        
"""Get documents relevant to a query.
        Args:
            query: String to find relevant documents for
            run_manager: The callbacks handler to use
        Returns:
            List of relevant documents
        """
        results 
=
 self
.
vectorstore
.
similarity_search_with_score
(
            query
,
 
**
self
.
search_kwargs
        
)
        
# Map doc_ids to list of sub-documents, adding scores to metadata
        id_to_doc 
=
 defaultdict
(
list
)
        
for
 doc
,
 score 
in
 results
:
            doc_id 
=
 doc
.
metadata
.
get
(
"doc_id"
)
            
if
 doc_id
:
                doc
.
metadata
[
"score"
]
 
=
 score
                id_to_doc
[
doc_id
]
.
append
(
doc
)
        
# Fetch documents corresponding to doc_ids, retaining sub_docs in metadata
        docs 
=
 
[
]
        
for
 _id
,
 sub_docs 
in
 id_to_doc
.
items
(
)
:
            docstore_docs 
=
 self
.
docstore
.
mget
(
[
_id
]
)
            
if
 docstore_docs
:
                
if
 doc 
:=
 docstore_docs
[
0
]
:
                    doc
.
metadata
[
"sub_docs"
]
 
=
 sub_docs
                    docs
.
append
(
doc
)
        
return
 docs
API Reference:
MultiVectorRetriever
 | 
CallbackManagerForRetrieverRun


Invoking this retriever, we can see that it identifies the correct parent document, including the relevant snippet from the sub-document with similarity score.


retriever 
=
 CustomMultiVectorRetriever
(
vectorstore
=
vectorstore
,
 docstore
=
docstore
)
retriever
.
invoke
(
"cat"
)


[Document(page_content='fake whole document 1', metadata={'sub_docs': [Document(page_content='A snippet from a larger document discussing cats.', metadata={'doc_id': 'fake_id_1', 'score': 0.831276655})]})]
Edit this page
Was this page helpful?
Previous
How to use the MultiQueryRetriever
Next
Caching
Create vector store
Retriever
SelfQueryRetriever
MultiVectorRetriever


--- docs_how_to_agent_executor.txt ---

How-to guides
Build an Agent with AgentExecutor (Legacy)
On this page
Build an Agent with AgentExecutor (Legacy)


important
This section will cover building with the legacy LangChain AgentExecutor. These are fine for getting started, but past a certain point, you will likely want flexibility and control that they do not offer. For working with more advanced agents, we'd recommend checking out 
LangGraph Agents
 or the 
migration guide


By themselves, language models can't take actions - they just output text.
A big use case for LangChain is creating 
agents
.
Agents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be.
The results of those actions can then be fed back into the agent and it determines whether more actions are needed, or whether it is okay to finish.


In this tutorial, we will build an agent that can interact with multiple different tools: one being a local database, the other being a search engine. You will be able to ask this agent questions, watch it call tools, and have conversations with it.


Concepts
​


Concepts we will cover are:




Using 
language models
, in particular their tool calling ability


Creating a 
Retriever
 to expose specific information to our agent


Using a Search 
Tool
 to look up things online


Chat History
, which allows a chatbot to "remember" past interactions and take them into account when responding to follow-up questions.


Debugging and tracing your application using 
LangSmith




Setup
​


Jupyter Notebook
​


This guide (and most of the other guides in the documentation) uses 
Jupyter notebooks
 and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.


This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See 
here
 for instructions on how to install.


Installation
​


To install LangChain run:




Pip
Conda
pip install langchain
conda install langchain -c conda-forge


For more details, see our 
Installation guide
.


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with 
LangSmith
.


After you sign up at the link above, make sure to set your environment variables to start logging traces:


export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."


Or, if in a notebook, you can set them with:


import
 getpass
import
 os
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Define tools
​


We first need to create the tools we want to use. We will use two tools: 
Tavily
 (to search online) and then a retriever over a local index we will create


Tavily
​


We have a built-in tool in LangChain to easily use Tavily search engine as tool.
Note that this requires an API key - they have a free tier, but if you don't have one or don't want to create one, you can always ignore this step.


Once you create your API key, you will need to export that as:


export TAVILY_API_KEY="..."


from
 langchain_community
.
tools
.
tavily_search 
import
 TavilySearchResults
API Reference:
TavilySearchResults


search 
=
 TavilySearchResults
(
max_results
=
2
)


search
.
invoke
(
"what is the weather in SF"
)


[{'url': 'https://www.weatherapi.com/',
  'content': "{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1714000492, 'localtime': '2024-04-24 16:14'}, 'current': {'last_updated_epoch': 1713999600, 'last_updated': '2024-04-24 16:00', 'temp_c': 15.6, 'temp_f': 60.1, 'is_day': 1, 'condition': {'text': 'Overcast', 'icon': '//cdn.weatherapi.com/weather/64x64/day/122.png', 'code': 1009}, 'wind_mph': 10.5, 'wind_kph': 16.9, 'wind_degree': 330, 'wind_dir': 'NNW', 'pressure_mb': 1018.0, 'pressure_in': 30.06, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 72, 'cloud': 100, 'feelslike_c': 15.6, 'feelslike_f': 60.1, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 14.8, 'gust_kph': 23.8}}"},
 {'url': 'https://www.weathertab.com/en/c/e/04/united-states/california/san-francisco/',
  'content': 'San Francisco Weather Forecast for Apr 2024 - Risk of Rain Graph. Rain Risk Graph: Monthly Overview. Bar heights indicate rain risk percentages. Yellow bars mark low-risk days, while black and grey bars signal higher risks. Grey-yellow bars act as buffers, advising to keep at least one day clear from the riskier grey and black days, guiding ...'}]


Retriever
​


We will also create a retriever over some data of our own. For a deeper explanation of each step here, see 
this tutorial
.


from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
loader 
=
 WebBaseLoader
(
"https://docs.smith.langchain.com/overview"
)
docs 
=
 loader
.
load
(
)
documents 
=
 RecursiveCharacterTextSplitter
(
    chunk_size
=
1000
,
 chunk_overlap
=
200
)
.
split_documents
(
docs
)
vector 
=
 FAISS
.
from_documents
(
documents
,
 OpenAIEmbeddings
(
)
)
retriever 
=
 vector
.
as_retriever
(
)
API Reference:
WebBaseLoader
 | 
FAISS
 | 
OpenAIEmbeddings
 | 
RecursiveCharacterTextSplitter


retriever
.
invoke
(
"how to upload a dataset"
)
[
0
]


Document(page_content='# The data to predict and grade over    evaluators=[exact_match], # The evaluators to score the results    experiment_prefix="sample-experiment", # The name of the experiment    metadata={      "version": "1.0.0",      "revision_id": "beta"    },)import { Client, Run, Example } from \'langsmith\';import { runOnDataset } from \'langchain/smith\';import { EvaluationResult } from \'langsmith/evaluation\';const client = new Client();// Define dataset: these are your test casesconst datasetName = "Sample Dataset";const dataset = await client.createDataset(datasetName, {    description: "A sample dataset in LangSmith."});await client.createExamples({    inputs: [        { postfix: "to LangSmith" },        { postfix: "to Evaluations in LangSmith" },    ],    outputs: [        { output: "Welcome to LangSmith" },        { output: "Welcome to Evaluations in LangSmith" },    ],    datasetId: dataset.id,});// Define your evaluatorconst exactMatch = async ({ run, example }: { run: Run; example?:', metadata={'source': 'https://docs.smith.langchain.com/overview', 'title': 'Getting started with LangSmith | \uf8ffü¶úÔ∏è\uf8ffüõ†Ô∏è LangSmith', 'description': 'Introduction', 'language': 'en'})


Now that we have populated our index that we will do doing retrieval over, we can easily turn it into a tool (the format needed for an agent to properly use it)


from
 langchain
.
tools
.
retriever 
import
 create_retriever_tool
API Reference:
create_retriever_tool


retriever_tool 
=
 create_retriever_tool
(
    retriever
,
    
"langsmith_search"
,
    
"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!"
,
)


Tools
​


Now that we have created both, we can create a list of tools that we will use downstream.


tools 
=
 
[
search
,
 retriever_tool
]


Using Language Models
​


Next, let's learn how to use a language model by to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4"
,
 model_provider
=
"openai"
)


You can call the language model by passing in a list of messages. By default, the response is a 
content
 string.


from
 langchain_core
.
messages 
import
 HumanMessage
response 
=
 model
.
invoke
(
[
HumanMessage
(
content
=
"hi!"
)
]
)
response
.
content
API Reference:
HumanMessage


'Hello! How can I assist you today?'


We can now see what it is like to enable this model to do tool calling. In order to enable that we use 
.bind_tools
 to give the language model knowledge of these tools


model_with_tools 
=
 model
.
bind_tools
(
tools
)


We can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the 
content
 field as well as the 
tool_calls
 field.


response 
=
 model_with_tools
.
invoke
(
[
HumanMessage
(
content
=
"Hi!"
)
]
)
print
(
f"ContentString: 
{
response
.
content
}
"
)
print
(
f"ToolCalls: 
{
response
.
tool_calls
}
"
)


ContentString: Hello! How can I assist you today?
ToolCalls: []


Now, let's try calling it with some input that would expect a tool to be called.


response 
=
 model_with_tools
.
invoke
(
[
HumanMessage
(
content
=
"What's the weather in SF?"
)
]
)
print
(
f"ContentString: 
{
response
.
content
}
"
)
print
(
f"ToolCalls: 
{
response
.
tool_calls
}
"
)


ContentString: 
ToolCalls: [{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_4HteVahXkRAkWjp6dGXryKZX'}]


We can see that there's now no content, but there is a tool call! It wants us to call the Tavily Search tool.


This isn't calling that tool yet - it's just telling us to. In order to actually calll it, we'll want to create our agent.


Create the agent
​


Now that we have defined the tools and the LLM, we can create the agent. We will be using a tool calling agent - for more information on this type of agent, as well as other options, see 
this guide
.


We can first choose the prompt we want to use to guide the agent.


If you want to see the contents of this prompt and have access to LangSmith, you can go to:


https://smith.langchain.com/hub/hwchase17/openai-functions-agent


from
 langchain 
import
 hub
# Get the prompt to use - you can modify this!
prompt 
=
 hub
.
pull
(
"hwchase17/openai-functions-agent"
)
prompt
.
messages
API Reference:
hub


[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),
 MessagesPlaceholder(variable_name='chat_history', optional=True),
 HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),
 MessagesPlaceholder(variable_name='agent_scratchpad')]


Now, we can initialize the agent with the LLM, the prompt, and the tools. The agent is responsible for taking in input and deciding what actions to take. Crucially, the Agent does not execute those actions - that is done by the AgentExecutor (next step). For more information about how to think about these components, see our 
conceptual guide
.


Note that we are passing in the 
model
, not 
model_with_tools
. That is because 
create_tool_calling_agent
 will call 
.bind_tools
 for us under the hood.


from
 langchain
.
agents 
import
 create_tool_calling_agent
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
)
API Reference:
create_tool_calling_agent


Finally, we combine the agent (the brains) with the tools inside the AgentExecutor (which will repeatedly call the agent and execute tools).


from
 langchain
.
agents 
import
 AgentExecutor
agent_executor 
=
 AgentExecutor
(
agent
=
agent
,
 tools
=
tools
)
API Reference:
AgentExecutor


Run the agent
​


We can now run the agent on a few queries! Note that for now, these are all 
stateless
 queries (it won't remember previous interactions).


First up, let's how it responds when there's no need to call a tool:


agent_executor
.
invoke
(
{
"input"
:
 
"hi!"
}
)


{'input': 'hi!', 'output': 'Hello! How can I assist you today?'}


In order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the 
LangSmith trace


Let's now try it out on an example where it should be invoking the retriever


agent_executor
.
invoke
(
{
"input"
:
 
"how can langsmith help with testing?"
}
)


{'input': 'how can langsmith help with testing?',
 'output': 'LangSmith is a platform that aids in building production-grade Language Learning Model (LLM) applications. It can assist with testing in several ways:\n\n1. **Monitoring and Evaluation**: LangSmith allows close monitoring and evaluation of your application. This helps you to ensure the quality of your application and deploy it with confidence.\n\n2. **Tracing**: LangSmith has tracing capabilities that can be beneficial for debugging and understanding the behavior of your application.\n\n3. **Evaluation Capabilities**: LangSmith has built-in tools for evaluating the performance of your LLM. \n\n4. **Prompt Hub**: This is a prompt management tool built into LangSmith that can help in testing different prompts and their responses.\n\nPlease note that to use LangSmith, you would need to install it and create an API key. The platform offers Python and Typescript SDKs for utilization. It works independently and does not require the use of LangChain.'}


Let's take a look at the 
LangSmith trace
 to make sure it's actually calling that.


Now let's try one where it needs to call the search tool:


agent_executor
.
invoke
(
{
"input"
:
 
"whats the weather in sf?"
}
)


{'input': 'whats the weather in sf?',
 'output': 'The current weather in San Francisco is partly cloudy with a temperature of 16.1°C (61.0°F). The wind is coming from the WNW at a speed of 10.5 mph. The humidity is at 67%. [source](https://www.weatherapi.com/)'}


We can check out the 
LangSmith trace
 to make sure it's calling the search tool effectively.


Adding in memory
​


As mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in previous 
chat_history
. Note: it needs to be called 
chat_history
 because of the prompt we are using. If we use a different prompt, we could change the variable name


# Here we pass in an empty list of messages for chat_history because it is the first message in the chat
agent_executor
.
invoke
(
{
"input"
:
 
"hi! my name is bob"
,
 
"chat_history"
:
 
[
]
}
)


{'input': 'hi! my name is bob',
 'chat_history': [],
 'output': 'Hello Bob! How can I assist you today?'}


from
 langchain_core
.
messages 
import
 AIMessage
,
 HumanMessage
API Reference:
AIMessage
 | 
HumanMessage


agent_executor
.
invoke
(
    
{
        
"chat_history"
:
 
[
            HumanMessage
(
content
=
"hi! my name is bob"
)
,
            AIMessage
(
content
=
"Hello Bob! How can I assist you today?"
)
,
        
]
,
        
"input"
:
 
"what's my name?"
,
    
}
)


{'chat_history': [HumanMessage(content='hi! my name is bob'),
  AIMessage(content='Hello Bob! How can I assist you today?')],
 'input': "what's my name?",
 'output': 'Your name is Bob. How can I assist you further?'}


If we want to keep track of these messages automatically, we can wrap this in a RunnableWithMessageHistory. For more information on how to use this, see 
this guide
.


from
 langchain_community
.
chat_message_histories 
import
 ChatMessageHistory
from
 langchain_core
.
chat_history 
import
 BaseChatMessageHistory
from
 langchain_core
.
runnables
.
history 
import
 RunnableWithMessageHistory
store 
=
 
{
}
def
 
get_session_history
(
session_id
:
 
str
)
 
-
>
 BaseChatMessageHistory
:
    
if
 session_id 
not
 
in
 store
:
        store
[
session_id
]
 
=
 ChatMessageHistory
(
)
    
return
 store
[
session_id
]
API Reference:
ChatMessageHistory
 | 
BaseChatMessageHistory
 | 
RunnableWithMessageHistory


Because we have multiple inputs, we need to specify two things:




input_messages_key
: The input key to use to add to the conversation history.


history_messages_key
: The key to add the loaded messages into.




agent_with_chat_history 
=
 RunnableWithMessageHistory
(
    agent_executor
,
    get_session_history
,
    input_messages_key
=
"input"
,
    history_messages_key
=
"chat_history"
,
)


agent_with_chat_history
.
invoke
(
    
{
"input"
:
 
"hi! I'm bob"
}
,
    config
=
{
"configurable"
:
 
{
"session_id"
:
 
"<foo>"
}
}
,
)


{'input': "hi! I'm bob",
 'chat_history': [],
 'output': 'Hello Bob! How can I assist you today?'}


agent_with_chat_history
.
invoke
(
    
{
"input"
:
 
"what's my name?"
}
,
    config
=
{
"configurable"
:
 
{
"session_id"
:
 
"<foo>"
}
}
,
)


{'input': "what's my name?",
 'chat_history': [HumanMessage(content="hi! I'm bob"),
  AIMessage(content='Hello Bob! How can I assist you today?')],
 'output': 'Your name is Bob.'}


Example LangSmith trace: 
https://smith.langchain.com/public/98c8d162-60ae-4493-aa9f-992d87bd0429/r


Conclusion
​


That's a wrap! In this quick start we covered how to create a simple agent. Agents are a complex topic, and there's lot to learn!


important
This section covered building with LangChain Agents. They are fine for getting started, but past a certain point you will likely want flexibility and control which they do not offer. To develop more advanced agents, we recommend checking out 
LangGraph


If you want to continue using LangChain agents, some good advanced guides are:




How to use LangGraph's built-in versions of 
AgentExecutor


How to create a custom agent


How to stream responses from an agent


How to return structured output from an agent


Edit this page
Was this page helpful?
Previous
How to add ad-hoc tool calling capability to LLMs and Chat Models
Next
How to construct knowledge graphs
Concepts
Setup
Jupyter Notebook
Installation
LangSmith
Define tools
Tavily
Retriever
Tools
Using Language Models
Create the agent
Run the agent
Adding in memory
Conclusion


--- docs_how_to_assign.txt ---

How-to guides
How to add values to a chain's state
On this page
How to add values to a chain's state


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Expression Language (LCEL)


Chaining runnables


Calling runnables in parallel


Custom functions


Passing data through




An alternate way of 
passing data through
 steps of a chain is to leave the current values of the chain state unchanged while assigning a new value under a given key. The 
RunnablePassthrough.assign()
 static method takes an input value and adds the extra arguments passed to the assign function.


This is useful in the common 
LangChain Expression Language
 pattern of additively creating a dictionary to use as input to a later step.


Here's an example:


%
pip install 
-
-
upgrade 
-
-
quiet langchain langchain
-
openai
import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


from
 langchain_core
.
runnables 
import
 RunnableParallel
,
 RunnablePassthrough
runnable 
=
 RunnableParallel
(
    extra
=
RunnablePassthrough
.
assign
(
mult
=
lambda
 x
:
 x
[
"num"
]
 
*
 
3
)
,
    modified
=
lambda
 x
:
 x
[
"num"
]
 
+
 
1
,
)
runnable
.
invoke
(
{
"num"
:
 
1
}
)
API Reference:
RunnableParallel
 | 
RunnablePassthrough


{'extra': {'num': 1, 'mult': 3}, 'modified': 2}


Let's break down what's happening here.




The input to the chain is 
{"num": 1}
. This is passed into a 
RunnableParallel
, which invokes the runnables it is passed in parallel with that input.


The value under the 
extra
 key is invoked. 
RunnablePassthrough.assign()
 keeps the original keys in the input dict (
{"num": 1}
), and assigns a new key called 
mult
. The value is 
lambda x: x["num"] * 3)
, which is 
3
. Thus, the result is 
{"num": 1, "mult": 3}
.


{"num": 1, "mult": 3}
 is returned to the 
RunnableParallel
 call, and is set as the value to the key 
extra
.


At the same time, the 
modified
 key is called. The result is 
2
, since the lambda extracts a key called 
"num"
 from its input and adds one.




Thus, the result is 
{'extra': {'num': 1, 'mult': 3}, 'modified': 2}
.


Streaming
​


One convenient feature of this method is that it allows values to pass through as soon as they are available. To show this off, we'll use 
RunnablePassthrough.assign()
 to immediately return source docs in a retrieval chain:


from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
,
 OpenAIEmbeddings
vectorstore 
=
 FAISS
.
from_texts
(
    
[
"harrison worked at kensho"
]
,
 embedding
=
OpenAIEmbeddings
(
)
)
retriever 
=
 vectorstore
.
as_retriever
(
)
template 
=
 
"""Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt 
=
 ChatPromptTemplate
.
from_template
(
template
)
model 
=
 ChatOpenAI
(
)
generation_chain 
=
 prompt 
|
 model 
|
 StrOutputParser
(
)
retrieval_chain 
=
 
{
    
"context"
:
 retriever
,
    
"question"
:
 RunnablePassthrough
(
)
,
}
 
|
 RunnablePassthrough
.
assign
(
output
=
generation_chain
)
stream 
=
 retrieval_chain
.
stream
(
"where did harrison work?"
)
for
 chunk 
in
 stream
:
    
print
(
chunk
)
API Reference:
FAISS
 | 
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI
 | 
OpenAIEmbeddings


{'question': 'where did harrison work?'}
{'context': [Document(page_content='harrison worked at kensho')]}
{'output': ''}
{'output': 'H'}
{'output': 'arrison'}
{'output': ' worked'}
{'output': ' at'}
{'output': ' Kens'}
{'output': 'ho'}
{'output': '.'}
{'output': ''}


We can see that the first chunk contains the original 
"question"
 since that is immediately available. The second chunk contains 
"context"
 since the retriever finishes second. Finally, the output from the 
generation_chain
 streams in chunks as soon as it is available.


Next steps
​


Now you've learned how to pass data through your chains to help to help format the data flowing through your chains.


To learn more, see the other how-to guides on runnables in this section.
Edit this page
Was this page helpful?
Previous
How to handle multiple retrievers when doing query analysis
Next
How to construct filters for query analysis
Streaming
Next steps


--- docs_how_to_binding.txt ---

How-to guides
How to add default invocation args to a Runnable
On this page
How to add default invocation args to a Runnable


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Expression Language (LCEL)


Chaining runnables


Tool calling




Sometimes we want to invoke a 
Runnable
 within a 
RunnableSequence
 with constant arguments that are not part of the output of the preceding Runnable in the sequence, and which are not part of the user input. We can use the 
Runnable.bind()
 method to set these arguments ahead of time.


Binding stop sequences
​


Suppose we have a simple prompt + model chain:


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"Write out the following equation using algebraic symbols then solve it. Use the format\n\nEQUATION:...\nSOLUTION:...\n\n"
,
        
)
,
        
(
"human"
,
 
"{equation_statement}"
)
,
    
]
)
model 
=
 ChatOpenAI
(
temperature
=
0
)
runnable 
=
 
(
    
{
"equation_statement"
:
 RunnablePassthrough
(
)
}
 
|
 prompt 
|
 model 
|
 StrOutputParser
(
)
)
print
(
runnable
.
invoke
(
"x raised to the third plus seven equals 12"
)
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI


EQUATION: x^3 + 7 = 12
SOLUTION: 
Subtract 7 from both sides:
x^3 = 5
Take the cube root of both sides:
x = ∛5


and want to call the model with certain 
stop
 words so that we shorten the output as is useful in certain types of prompting techniques. While we can pass some arguments into the constructor, other runtime args use the 
.bind()
 method as follows:


runnable 
=
 
(
    
{
"equation_statement"
:
 RunnablePassthrough
(
)
}
    
|
 prompt
    
|
 model
.
bind
(
stop
=
"SOLUTION"
)
    
|
 StrOutputParser
(
)
)
print
(
runnable
.
invoke
(
"x raised to the third plus seven equals 12"
)
)


EQUATION: x^3 + 7 = 12


What you can bind to a Runnable will depend on the extra parameters you can pass when invoking it.


Attaching OpenAI tools
​


Another common use-case is tool calling. While you should generally use the 
.bind_tools()
 method for tool-calling models, you can also bind provider-specific args directly if you want lower level control:


tools 
=
 
[
    
{
        
"type"
:
 
"function"
,
        
"function"
:
 
{
            
"name"
:
 
"get_current_weather"
,
            
"description"
:
 
"Get the current weather in a given location"
,
            
"parameters"
:
 
{
                
"type"
:
 
"object"
,
                
"properties"
:
 
{
                    
"location"
:
 
{
                        
"type"
:
 
"string"
,
                        
"description"
:
 
"The city and state, e.g. San Francisco, CA"
,
                    
}
,
                    
"unit"
:
 
{
"type"
:
 
"string"
,
 
"enum"
:
 
[
"celsius"
,
 
"fahrenheit"
]
}
,
                
}
,
                
"required"
:
 
[
"location"
]
,
            
}
,
        
}
,
    
}
]


model 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
.
bind
(
tools
=
tools
)
model
.
invoke
(
"What's the weather in SF, NYC and LA?"
)


AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z0OU2CytqENVrRTI6T8DkI3u', 'function': {'arguments': '{"location": "San Francisco, CA", "unit": "celsius"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw', 'function': {'arguments': '{"location": "New York, NY", "unit": "celsius"}', 'name': 'get_current_weather'}, 'type': 'function'}, {'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH', 'function': {'arguments': '{"location": "Los Angeles, CA", "unit": "celsius"}', 'name': 'get_current_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 85, 'total_tokens': 169}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_77a673219d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d57ad5fa-b52a-4822-bc3e-74f838697e18-0', tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'San Francisco, CA', 'unit': 'celsius'}, 'id': 'call_z0OU2CytqENVrRTI6T8DkI3u'}, {'name': 'get_current_weather', 'args': {'location': 'New York, NY', 'unit': 'celsius'}, 'id': 'call_ft96IJBh0cMKkQWrZjNg4bsw'}, {'name': 'get_current_weather', 'args': {'location': 'Los Angeles, CA', 'unit': 'celsius'}, 'id': 'call_tfbtGgCLmuBuWgZLvpPwvUMH'}])


Next steps
​


You now know how to bind runtime arguments to a Runnable.


To learn more, see the other how-to guides on runnables in this section, including:




Using configurable fields and alternatives
 to change parameters of a step in a chain, or even swap out entire steps, at runtime


Edit this page
Was this page helpful?
Previous
How to stream chat model responses
Next
How to add retrieval to chatbots
Binding stop sequences
Attaching OpenAI tools
Next steps


--- docs_how_to_caching_embeddings.txt ---

How-to guides
Caching
On this page
Caching


Embeddings
 can be stored or temporarily cached to avoid needing to recompute them.


Caching embeddings can be done using a 
CacheBackedEmbeddings
. The cache backed embedder is a wrapper around an embedder that caches
embeddings in a key-value store. The text is hashed and the hash is used as the key in the cache.


The main supported way to initialize a 
CacheBackedEmbeddings
 is 
from_bytes_store
. It takes the following parameters:




underlying_embedder: The embedder to use for embedding.


document_embedding_cache: Any 
ByteStore
 for caching document embeddings.


batch_size: (optional, defaults to 
None
) The number of documents to embed between store updates.


namespace: (optional, defaults to 
""
) The namespace to use for document cache. This namespace is used to avoid collisions with other caches. For example, set it to the name of the embedding model used.


query_embedding_cache: (optional, defaults to 
None
 or not caching) A 
ByteStore
 for caching query embeddings, or 
True
 to use the same store as 
document_embedding_cache
.




Attention
:




Be sure to set the 
namespace
 parameter to avoid collisions of the same text embedded using different embeddings models.


CacheBackedEmbeddings
 does not cache query embeddings by default. To enable query caching, one needs to specify a 
query_embedding_cache
.




from
 langchain
.
embeddings 
import
 CacheBackedEmbeddings
API Reference:
CacheBackedEmbeddings


Using with a Vector Store
​


First, let's see an example that uses the local file system for storing embeddings and uses FAISS vector store for retrieval.


%
pip install 
-
-
upgrade 
-
-
quiet  langchain
-
openai faiss
-
cpu


from
 langchain
.
storage 
import
 LocalFileStore
from
 langchain_community
.
document_loaders 
import
 TextLoader
from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 CharacterTextSplitter
underlying_embeddings 
=
 OpenAIEmbeddings
(
)
store 
=
 LocalFileStore
(
"./cache/"
)
cached_embedder 
=
 CacheBackedEmbeddings
.
from_bytes_store
(
    underlying_embeddings
,
 store
,
 namespace
=
underlying_embeddings
.
model
)
API Reference:
LocalFileStore
 | 
TextLoader
 | 
FAISS
 | 
OpenAIEmbeddings
 | 
CharacterTextSplitter


The cache is empty prior to embedding:


list
(
store
.
yield_keys
(
)
)


[]


Load the document, split it into chunks, embed each chunk and load it into the vector store.


raw_documents 
=
 TextLoader
(
"state_of_the_union.txt"
)
.
load
(
)
text_splitter 
=
 CharacterTextSplitter
(
chunk_size
=
1000
,
 chunk_overlap
=
0
)
documents 
=
 text_splitter
.
split_documents
(
raw_documents
)


Create the vector store:


%
%
time
db 
=
 FAISS
.
from_documents
(
documents
,
 cached_embedder
)


CPU times: user 218 ms, sys: 29.7 ms, total: 248 ms
Wall time: 1.02 s


If we try to create the vector store again, it'll be much faster since it does not need to re-compute any embeddings.


%
%
time
db2 
=
 FAISS
.
from_documents
(
documents
,
 cached_embedder
)


CPU times: user 15.7 ms, sys: 2.22 ms, total: 18 ms
Wall time: 17.2 ms


And here are some of the embeddings that got created:


list
(
store
.
yield_keys
(
)
)
[
:
5
]


['text-embedding-ada-00217a6727d-8916-54eb-b196-ec9c9d6ca472',
 'text-embedding-ada-0025fc0d904-bd80-52da-95c9-441015bfb438',
 'text-embedding-ada-002e4ad20ef-dfaa-5916-9459-f90c6d8e8159',
 'text-embedding-ada-002ed199159-c1cd-5597-9757-f80498e8f17b',
 'text-embedding-ada-0021297d37a-2bc1-5e19-bf13-6c950f075062']


Swapping the 
ByteStore


In order to use a different 
ByteStore
, just use it when creating your 
CacheBackedEmbeddings
. Below, we create an equivalent cached embeddings object, except using the non-persistent 
InMemoryByteStore
 instead:


from
 langchain
.
embeddings 
import
 CacheBackedEmbeddings
from
 langchain
.
storage 
import
 InMemoryByteStore
store 
=
 InMemoryByteStore
(
)
cached_embedder 
=
 CacheBackedEmbeddings
.
from_bytes_store
(
    underlying_embeddings
,
 store
,
 namespace
=
underlying_embeddings
.
model
)
API Reference:
CacheBackedEmbeddings
 | 
InMemoryByteStore
Edit this page
Was this page helpful?
Previous
How to add scores to retriever results
Next
How to use callbacks in async environments
Using with a Vector Store


--- docs_how_to_callbacks_async.txt ---

How-to guides
How to use callbacks in async environments
On this page
How to use callbacks in async environments


Prerequisites
This guide assumes familiarity with the following concepts:


Callbacks


Custom callback handlers




If you are planning to use the async APIs, it is recommended to use and extend 
AsyncCallbackHandler
 to avoid blocking the event.


warning
If you use a sync 
CallbackHandler
 while using an async method to run your LLM / Chain / Tool / Agent, it will still work. However, under the hood, it will be called with 
run_in_executor
 which can cause issues if your 
CallbackHandler
 is not thread-safe.


danger
If you're on 
python<=3.10
, you need to remember to propagate 
config
 or 
callbacks
 when invoking other 
runnable
 from within a 
RunnableLambda
, 
RunnableGenerator
 or 
@tool
. If you do not do this,
the callbacks will not be propagated to the child runnables being invoked.


import
 asyncio
from
 typing 
import
 Any
,
 Dict
,
 List
from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_core
.
callbacks 
import
 AsyncCallbackHandler
,
 BaseCallbackHandler
from
 langchain_core
.
messages 
import
 HumanMessage
from
 langchain_core
.
outputs 
import
 LLMResult
class
 
MyCustomSyncHandler
(
BaseCallbackHandler
)
:
    
def
 
on_llm_new_token
(
self
,
 token
:
 
str
,
 
**
kwargs
)
 
-
>
 
None
:
        
print
(
f"Sync handler being called in a `thread_pool_executor`: token: 
{
token
}
"
)
class
 
MyCustomAsyncHandler
(
AsyncCallbackHandler
)
:
    
"""Async callback handler that can be used to handle callbacks from langchain."""
    
async
 
def
 
on_llm_start
(
        self
,
 serialized
:
 Dict
[
str
,
 Any
]
,
 prompts
:
 List
[
str
]
,
 
**
kwargs
:
 Any
    
)
 
-
>
 
None
:
        
"""Run when chain starts running."""
        
print
(
"zzzz...."
)
        
await
 asyncio
.
sleep
(
0.3
)
        class_name 
=
 serialized
[
"name"
]
        
print
(
"Hi! I just woke up. Your llm is starting"
)
    
async
 
def
 
on_llm_end
(
self
,
 response
:
 LLMResult
,
 
**
kwargs
:
 Any
)
 
-
>
 
None
:
        
"""Run when chain ends running."""
        
print
(
"zzzz...."
)
        
await
 asyncio
.
sleep
(
0.3
)
        
print
(
"Hi! I just woke up. Your llm is ending"
)
# To enable streaming, we pass in `streaming=True` to the ChatModel constructor
# Additionally, we pass in a list with our custom handler
chat 
=
 ChatAnthropic
(
    model
=
"claude-3-sonnet-20240229"
,
    max_tokens
=
25
,
    streaming
=
True
,
    callbacks
=
[
MyCustomSyncHandler
(
)
,
 MyCustomAsyncHandler
(
)
]
,
)
await
 chat
.
agenerate
(
[
[
HumanMessage
(
content
=
"Tell me a joke"
)
]
]
)
API Reference:
ChatAnthropic
 | 
AsyncCallbackHandler
 | 
BaseCallbackHandler
 | 
HumanMessage
 | 
LLMResult


zzzz....
Hi! I just woke up. Your llm is starting
Sync handler being called in a `thread_pool_executor`: token: Here
Sync handler being called in a `thread_pool_executor`: token: 's
Sync handler being called in a `thread_pool_executor`: token:  a
Sync handler being called in a `thread_pool_executor`: token:  little
Sync handler being called in a `thread_pool_executor`: token:  joke
Sync handler being called in a `thread_pool_executor`: token:  for
Sync handler being called in a `thread_pool_executor`: token:  you
Sync handler being called in a `thread_pool_executor`: token: :
Sync handler being called in a `thread_pool_executor`: token: 
Why
Sync handler being called in a `thread_pool_executor`: token:  can
Sync handler being called in a `thread_pool_executor`: token: 't
Sync handler being called in a `thread_pool_executor`: token:  a
Sync handler being called in a `thread_pool_executor`: token:  bicycle
Sync handler being called in a `thread_pool_executor`: token:  stan
Sync handler being called in a `thread_pool_executor`: token: d up
Sync handler being called in a `thread_pool_executor`: token:  by
Sync handler being called in a `thread_pool_executor`: token:  itself
Sync handler being called in a `thread_pool_executor`: token: ?
Sync handler being called in a `thread_pool_executor`: token:  Because
Sync handler being called in a `thread_pool_executor`: token:  it
Sync handler being called in a `thread_pool_executor`: token: 's
Sync handler being called in a `thread_pool_executor`: token:  two
Sync handler being called in a `thread_pool_executor`: token: -
Sync handler being called in a `thread_pool_executor`: token: tire
zzzz....
Hi! I just woke up. Your llm is ending


LLMResult(generations=[[ChatGeneration(text="Here's a little joke for you:\n\nWhy can't a bicycle stand up by itself? Because it's two-tire", message=AIMessage(content="Here's a little joke for you:\n\nWhy can't a bicycle stand up by itself? Because it's two-tire", id='run-8afc89e8-02c0-4522-8480-d96977240bd4-0'))]], llm_output={}, run=[RunInfo(run_id=UUID('8afc89e8-02c0-4522-8480-d96977240bd4'))])


Next steps
​


You've now learned how to create your own custom callback handlers.


Next, check out the other how-to guides in this section, such as 
how to attach callbacks to a runnable
.
Edit this page
Was this page helpful?
Previous
Caching
Next
How to attach callbacks to a runnable
Next steps


--- docs_how_to_callbacks_attach.txt ---

How-to guides
How to attach callbacks to a runnable
On this page
How to attach callbacks to a runnable


Prerequisites
This guide assumes familiarity with the following concepts:


Callbacks


Custom callback handlers


Chaining runnables


Attach runtime arguments to a Runnable




If you are composing a chain of runnables and want to reuse callbacks across multiple executions, you can attach callbacks with the 
.with_config()
 method. This saves you the need to pass callbacks in each time you invoke the chain.


important
with_config()
 binds a configuration which will be interpreted as 
runtime
 configuration. So these callbacks will propagate to all child components.


Here's an example:


from
 typing 
import
 Any
,
 Dict
,
 List
from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_core
.
callbacks 
import
 BaseCallbackHandler
from
 langchain_core
.
messages 
import
 BaseMessage
from
 langchain_core
.
outputs 
import
 LLMResult
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
class
 
LoggingHandler
(
BaseCallbackHandler
)
:
    
def
 
on_chat_model_start
(
        self
,
 serialized
:
 Dict
[
str
,
 Any
]
,
 messages
:
 List
[
List
[
BaseMessage
]
]
,
 
**
kwargs
    
)
 
-
>
 
None
:
        
print
(
"Chat model started"
)
    
def
 
on_llm_end
(
self
,
 response
:
 LLMResult
,
 
**
kwargs
)
 
-
>
 
None
:
        
print
(
f"Chat model ended, response: 
{
response
}
"
)
    
def
 
on_chain_start
(
        self
,
 serialized
:
 Dict
[
str
,
 Any
]
,
 inputs
:
 Dict
[
str
,
 Any
]
,
 
**
kwargs
    
)
 
-
>
 
None
:
        
print
(
f"Chain 
{
serialized
.
get
(
'name'
)
}
 started"
)
    
def
 
on_chain_end
(
self
,
 outputs
:
 Dict
[
str
,
 Any
]
,
 
**
kwargs
)
 
-
>
 
None
:
        
print
(
f"Chain ended, outputs: 
{
outputs
}
"
)
callbacks 
=
 
[
LoggingHandler
(
)
]
llm 
=
 ChatAnthropic
(
model
=
"claude-3-sonnet-20240229"
)
prompt 
=
 ChatPromptTemplate
.
from_template
(
"What is 1 + {number}?"
)
chain 
=
 prompt 
|
 llm
chain_with_callbacks 
=
 chain
.
with_config
(
callbacks
=
callbacks
)
chain_with_callbacks
.
invoke
(
{
"number"
:
 
"2"
}
)
API Reference:
ChatAnthropic
 | 
BaseCallbackHandler
 | 
BaseMessage
 | 
LLMResult
 | 
ChatPromptTemplate


Chain RunnableSequence started
Chain ChatPromptTemplate started
Chain ended, outputs: messages=[HumanMessage(content='What is 1 + 2?')]
Chat model started
Chat model ended, response: generations=[[ChatGeneration(text='1 + 2 = 3', message=AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01NTYMsH9YxkoWsiPYs4Lemn', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-d6bcfd72-9c94-466d-bac0-f39e456ad6e3-0'))]] llm_output={'id': 'msg_01NTYMsH9YxkoWsiPYs4Lemn', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}} run=None
Chain ended, outputs: content='1 + 2 = 3' response_metadata={'id': 'msg_01NTYMsH9YxkoWsiPYs4Lemn', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}} id='run-d6bcfd72-9c94-466d-bac0-f39e456ad6e3-0'


AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01NTYMsH9YxkoWsiPYs4Lemn', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-d6bcfd72-9c94-466d-bac0-f39e456ad6e3-0')


The bound callbacks will run for all nested module runs.


Next steps
​


You've now learned how to attach callbacks to a chain.


Next, check out the other how-to guides in this section, such as how to 
pass callbacks in at runtime
.
Edit this page
Was this page helpful?
Previous
How to use callbacks in async environments
Next
How to propagate callbacks  constructor
Next steps


--- docs_how_to_callbacks_constructor.txt ---

How-to guides
How to propagate callbacks  constructor
On this page
How to propagate callbacks  constructor


Prerequisites
This guide assumes familiarity with the following concepts:


Callbacks


Custom callback handlers




Most LangChain modules allow you to pass 
callbacks
 directly into the constructor (i.e., initializer). In this case, the callbacks will only be called for that instance (and any nested runs).


warning
Constructor callbacks are scoped only to the object they are defined on. They are 
not
 inherited by children of the object. This can lead to confusing behavior,
and it's generally better to pass callbacks as a run time argument.


Here's an example:


from
 typing 
import
 Any
,
 Dict
,
 List
from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_core
.
callbacks 
import
 BaseCallbackHandler
from
 langchain_core
.
messages 
import
 BaseMessage
from
 langchain_core
.
outputs 
import
 LLMResult
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
class
 
LoggingHandler
(
BaseCallbackHandler
)
:
    
def
 
on_chat_model_start
(
        self
,
 serialized
:
 Dict
[
str
,
 Any
]
,
 messages
:
 List
[
List
[
BaseMessage
]
]
,
 
**
kwargs
    
)
 
-
>
 
None
:
        
print
(
"Chat model started"
)
    
def
 
on_llm_end
(
self
,
 response
:
 LLMResult
,
 
**
kwargs
)
 
-
>
 
None
:
        
print
(
f"Chat model ended, response: 
{
response
}
"
)
    
def
 
on_chain_start
(
        self
,
 serialized
:
 Dict
[
str
,
 Any
]
,
 inputs
:
 Dict
[
str
,
 Any
]
,
 
**
kwargs
    
)
 
-
>
 
None
:
        
print
(
f"Chain 
{
serialized
.
get
(
'name'
)
}
 started"
)
    
def
 
on_chain_end
(
self
,
 outputs
:
 Dict
[
str
,
 Any
]
,
 
**
kwargs
)
 
-
>
 
None
:
        
print
(
f"Chain ended, outputs: 
{
outputs
}
"
)
callbacks 
=
 
[
LoggingHandler
(
)
]
llm 
=
 ChatAnthropic
(
model
=
"claude-3-sonnet-20240229"
,
 callbacks
=
callbacks
)
prompt 
=
 ChatPromptTemplate
.
from_template
(
"What is 1 + {number}?"
)
chain 
=
 prompt 
|
 llm
chain
.
invoke
(
{
"number"
:
 
"2"
}
)
API Reference:
ChatAnthropic
 | 
BaseCallbackHandler
 | 
BaseMessage
 | 
LLMResult
 | 
ChatPromptTemplate


Chat model started
Chat model ended, response: generations=[[ChatGeneration(text='1 + 2 = 3', message=AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01CdKsRmeS9WRb8BWnHDEHm7', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-2d7fdf2a-7405-4e17-97c0-67e6b2a65305-0'))]] llm_output={'id': 'msg_01CdKsRmeS9WRb8BWnHDEHm7', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}} run=None


AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01CdKsRmeS9WRb8BWnHDEHm7', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-2d7fdf2a-7405-4e17-97c0-67e6b2a65305-0')


You can see that we only see events from the chat model run - no chain events from the prompt or broader chain.


Next steps
​


You've now learned how to pass callbacks into a constructor.


Next, check out the other how-to guides in this section, such as how to 
pass callbacks at runtime
.
Edit this page
Was this page helpful?
Previous
How to attach callbacks to a runnable
Next
How to dispatch custom callback events
Next steps


--- docs_how_to_callbacks_custom_events.txt ---

How-to guides
How to dispatch custom callback events
On this page
How to dispatch custom callback events


Prerequisites
This guide assumes familiarity with the following concepts:


Callbacks


Custom callback handlers


Astream Events API
 the 
astream_events
 method will surface custom callback events.




In some situations, you may want to dispatch a custom callback event from within a 
Runnable
 so it can be surfaced
in a custom callback handler or via the 
Astream Events API
.


For example, if you have a long running tool with multiple steps, you can dispatch custom events between the steps and use these custom events to monitor progress.
You could also surface these custom events to an end user of your application to show them how the current task is progressing.


To dispatch a custom event you need to decide on two attributes for the event: the 
name
 and the 
data
.


Attribute
Type
Description
name
str
A user defined name for the event.
data
Any
The data associated with the event. This can be anything, though we suggest making it JSON serializable.


important


Dispatching custom callback events requires 
langchain-core>=0.2.15
.


Custom callback events can only be dispatched from within an existing 
Runnable
.


If using 
astream_events
, you must use 
version='v2'
 to see custom events.


Sending or rendering custom callbacks events in LangSmith is not yet supported.




COMPATIBILITY
LangChain cannot automatically propagate configuration, including callbacks necessary for astream_events(), to child runnables if you are running async code in python<=3.10. This is a common reason why you may fail to see events being emitted from custom runnables or tools.
If you are running python<=3.10, you will need to manually propagate the 
RunnableConfig
 object to the child runnable in async environments. For an example of how to manually propagate the config, see the implementation of the 
bar
 RunnableLambda below.
If you are running python>=3.11, the 
RunnableConfig
 will automatically propagate to child runnables in async environment. However, it is still a good idea to propagate the 
RunnableConfig
 manually if your code may run in other Python versions.


Astream Events API
​


The most useful way to consume custom events is via the 
Astream Events API
.


We can use the 
async
 
adispatch_custom_event
 API to emit custom events in an async setting.


important
To see custom events via the astream events API, you need to use the newer 
v2
 API of 
astream_events
.


from
 langchain_core
.
callbacks
.
manager 
import
 
(
    adispatch_custom_event
,
)
from
 langchain_core
.
runnables 
import
 RunnableLambda
from
 langchain_core
.
runnables
.
config 
import
 RunnableConfig
@RunnableLambda
async
 
def
 
foo
(
x
:
 
str
)
 
-
>
 
str
:
    
await
 adispatch_custom_event
(
"event1"
,
 
{
"x"
:
 x
}
)
    
await
 adispatch_custom_event
(
"event2"
,
 
5
)
    
return
 x
async
 
for
 event 
in
 foo
.
astream_events
(
"hello world"
,
 version
=
"v2"
)
:
    
print
(
event
)
API Reference:
adispatch_custom_event
 | 
RunnableLambda
 | 
RunnableConfig


{'event': 'on_chain_start', 'data': {'input': 'hello world'}, 'name': 'foo', 'tags': [], 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'metadata': {}, 'parent_ids': []}
{'event': 'on_custom_event', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'event1', 'tags': [], 'metadata': {}, 'data': {'x': 'hello world'}, 'parent_ids': []}
{'event': 'on_custom_event', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'event2', 'tags': [], 'metadata': {}, 'data': 5, 'parent_ids': []}
{'event': 'on_chain_stream', 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'foo', 'tags': [], 'metadata': {}, 'data': {'chunk': 'hello world'}, 'parent_ids': []}
{'event': 'on_chain_end', 'data': {'output': 'hello world'}, 'run_id': 'f354ffe8-4c22-4881-890a-c1cad038a9a6', 'name': 'foo', 'tags': [], 'metadata': {}, 'parent_ids': []}


In python <= 3.10, you must propagate the config manually!


from
 langchain_core
.
callbacks
.
manager 
import
 
(
    adispatch_custom_event
,
)
from
 langchain_core
.
runnables 
import
 RunnableLambda
from
 langchain_core
.
runnables
.
config 
import
 RunnableConfig
@RunnableLambda
async
 
def
 
bar
(
x
:
 
str
,
 config
:
 RunnableConfig
)
 
-
>
 
str
:
    
"""An example that shows how to manually propagate config.
    You must do this if you're running python<=3.10.
    """
    
await
 adispatch_custom_event
(
"event1"
,
 
{
"x"
:
 x
}
,
 config
=
config
)
    
await
 adispatch_custom_event
(
"event2"
,
 
5
,
 config
=
config
)
    
return
 x
async
 
for
 event 
in
 bar
.
astream_events
(
"hello world"
,
 version
=
"v2"
)
:
    
print
(
event
)
API Reference:
adispatch_custom_event
 | 
RunnableLambda
 | 
RunnableConfig


{'event': 'on_chain_start', 'data': {'input': 'hello world'}, 'name': 'bar', 'tags': [], 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'metadata': {}, 'parent_ids': []}
{'event': 'on_custom_event', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'event1', 'tags': [], 'metadata': {}, 'data': {'x': 'hello world'}, 'parent_ids': []}
{'event': 'on_custom_event', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'event2', 'tags': [], 'metadata': {}, 'data': 5, 'parent_ids': []}
{'event': 'on_chain_stream', 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'bar', 'tags': [], 'metadata': {}, 'data': {'chunk': 'hello world'}, 'parent_ids': []}
{'event': 'on_chain_end', 'data': {'output': 'hello world'}, 'run_id': 'c787b09d-698a-41b9-8290-92aaa656f3e7', 'name': 'bar', 'tags': [], 'metadata': {}, 'parent_ids': []}


Async Callback Handler
​


You can also consume the dispatched event via an async callback handler.


from
 typing 
import
 Any
,
 Dict
,
 List
,
 Optional
from
 uuid 
import
 UUID
from
 langchain_core
.
callbacks 
import
 AsyncCallbackHandler
from
 langchain_core
.
callbacks
.
manager 
import
 
(
    adispatch_custom_event
,
)
from
 langchain_core
.
runnables 
import
 RunnableLambda
from
 langchain_core
.
runnables
.
config 
import
 RunnableConfig
class
 
AsyncCustomCallbackHandler
(
AsyncCallbackHandler
)
:
    
async
 
def
 
on_custom_event
(
        self
,
        name
:
 
str
,
        data
:
 Any
,
        
*
,
        run_id
:
 UUID
,
        tags
:
 Optional
[
List
[
str
]
]
 
=
 
None
,
        metadata
:
 Optional
[
Dict
[
str
,
 Any
]
]
 
=
 
None
,
        
**
kwargs
:
 Any
,
    
)
 
-
>
 
None
:
        
print
(
            
f"Received event 
{
name
}
 with data: 
{
data
}
, with tags: 
{
tags
}
, with metadata: 
{
metadata
}
 and run_id: 
{
run_id
}
"
        
)
@RunnableLambda
async
 
def
 
bar
(
x
:
 
str
,
 config
:
 RunnableConfig
)
 
-
>
 
str
:
    
"""An example that shows how to manually propagate config.
    You must do this if you're running python<=3.10.
    """
    
await
 adispatch_custom_event
(
"event1"
,
 
{
"x"
:
 x
}
,
 config
=
config
)
    
await
 adispatch_custom_event
(
"event2"
,
 
5
,
 config
=
config
)
    
return
 x
async_handler 
=
 AsyncCustomCallbackHandler
(
)
await
 foo
.
ainvoke
(
1
,
 
{
"callbacks"
:
 
[
async_handler
]
,
 
"tags"
:
 
[
"foo"
,
 
"bar"
]
}
)
API Reference:
AsyncCallbackHandler
 | 
adispatch_custom_event
 | 
RunnableLambda
 | 
RunnableConfig


Received event event1 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: a62b84be-7afd-4829-9947-7165df1f37d9
Received event event2 with data: 5, with tags: ['foo', 'bar'], with metadata: {} and run_id: a62b84be-7afd-4829-9947-7165df1f37d9


1


Sync Callback Handler
​


Let's see how to emit custom events in a sync environment using 
dispatch_custom_event
.


You 
must
 call 
dispatch_custom_event
 from within an existing 
Runnable
.


from
 typing 
import
 Any
,
 Dict
,
 List
,
 Optional
from
 uuid 
import
 UUID
from
 langchain_core
.
callbacks 
import
 BaseCallbackHandler
from
 langchain_core
.
callbacks
.
manager 
import
 
(
    dispatch_custom_event
,
)
from
 langchain_core
.
runnables 
import
 RunnableLambda
from
 langchain_core
.
runnables
.
config 
import
 RunnableConfig
class
 
CustomHandler
(
BaseCallbackHandler
)
:
    
def
 
on_custom_event
(
        self
,
        name
:
 
str
,
        data
:
 Any
,
        
*
,
        run_id
:
 UUID
,
        tags
:
 Optional
[
List
[
str
]
]
 
=
 
None
,
        metadata
:
 Optional
[
Dict
[
str
,
 Any
]
]
 
=
 
None
,
        
**
kwargs
:
 Any
,
    
)
 
-
>
 
None
:
        
print
(
            
f"Received event 
{
name
}
 with data: 
{
data
}
, with tags: 
{
tags
}
, with metadata: 
{
metadata
}
 and run_id: 
{
run_id
}
"
        
)
@RunnableLambda
def
 
foo
(
x
:
 
int
,
 config
:
 RunnableConfig
)
 
-
>
 
int
:
    dispatch_custom_event
(
"event1"
,
 
{
"x"
:
 x
}
)
    dispatch_custom_event
(
"event2"
,
 
{
"x"
:
 x
}
)
    
return
 x
handler 
=
 CustomHandler
(
)
foo
.
invoke
(
1
,
 
{
"callbacks"
:
 
[
handler
]
,
 
"tags"
:
 
[
"foo"
,
 
"bar"
]
}
)
API Reference:
BaseCallbackHandler
 | 
dispatch_custom_event
 | 
RunnableLambda
 | 
RunnableConfig


Received event event1 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: 27b5ce33-dc26-4b34-92dd-08a89cb22268
Received event event2 with data: {'x': 1}, with tags: ['foo', 'bar'], with metadata: {} and run_id: 27b5ce33-dc26-4b34-92dd-08a89cb22268


1


Next steps
​


You've seen how to emit custom events, you can check out the more in depth guide for 
astream events
 which is the easiest way to leverage custom events.
Edit this page
Was this page helpful?
Previous
How to propagate callbacks  constructor
Next
How to pass callbacks in at runtime
Astream Events API
Async Callback Handler
Sync Callback Handler
Next steps


--- docs_how_to_callbacks_runtime.txt ---

How-to guides
How to pass callbacks in at runtime
On this page
How to pass callbacks in at runtime


Prerequisites
This guide assumes familiarity with the following concepts:


Callbacks


Custom callback handlers




In many cases, it is advantageous to pass in handlers instead when running the object. When we pass through 
CallbackHandlers
 using the 
callbacks
 keyword arg when executing a run, those callbacks will be issued by all nested objects involved in the execution. For example, when a handler is passed through to an Agent, it will be used for all callbacks related to the agent and all the objects involved in the agent's execution, in this case, the Tools and LLM.


This prevents us from having to manually attach the handlers to each individual nested object. Here's an example:


from
 typing 
import
 Any
,
 Dict
,
 List
from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_core
.
callbacks 
import
 BaseCallbackHandler
from
 langchain_core
.
messages 
import
 BaseMessage
from
 langchain_core
.
outputs 
import
 LLMResult
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
class
 
LoggingHandler
(
BaseCallbackHandler
)
:
    
def
 
on_chat_model_start
(
        self
,
 serialized
:
 Dict
[
str
,
 Any
]
,
 messages
:
 List
[
List
[
BaseMessage
]
]
,
 
**
kwargs
    
)
 
-
>
 
None
:
        
print
(
"Chat model started"
)
    
def
 
on_llm_end
(
self
,
 response
:
 LLMResult
,
 
**
kwargs
)
 
-
>
 
None
:
        
print
(
f"Chat model ended, response: 
{
response
}
"
)
    
def
 
on_chain_start
(
        self
,
 serialized
:
 Dict
[
str
,
 Any
]
,
 inputs
:
 Dict
[
str
,
 Any
]
,
 
**
kwargs
    
)
 
-
>
 
None
:
        
print
(
f"Chain 
{
serialized
.
get
(
'name'
)
}
 started"
)
    
def
 
on_chain_end
(
self
,
 outputs
:
 Dict
[
str
,
 Any
]
,
 
**
kwargs
)
 
-
>
 
None
:
        
print
(
f"Chain ended, outputs: 
{
outputs
}
"
)
callbacks 
=
 
[
LoggingHandler
(
)
]
llm 
=
 ChatAnthropic
(
model
=
"claude-3-sonnet-20240229"
)
prompt 
=
 ChatPromptTemplate
.
from_template
(
"What is 1 + {number}?"
)
chain 
=
 prompt 
|
 llm
chain
.
invoke
(
{
"number"
:
 
"2"
}
,
 config
=
{
"callbacks"
:
 callbacks
}
)
API Reference:
ChatAnthropic
 | 
BaseCallbackHandler
 | 
BaseMessage
 | 
LLMResult
 | 
ChatPromptTemplate


Chain RunnableSequence started
Chain ChatPromptTemplate started
Chain ended, outputs: messages=[HumanMessage(content='What is 1 + 2?')]
Chat model started
Chat model ended, response: generations=[[ChatGeneration(text='1 + 2 = 3', message=AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01D8Tt5FdtBk5gLTfBPm2tac', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-bb0dddd8-85f3-4e6b-8553-eaa79f859ef8-0'))]] llm_output={'id': 'msg_01D8Tt5FdtBk5gLTfBPm2tac', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}} run=None
Chain ended, outputs: content='1 + 2 = 3' response_metadata={'id': 'msg_01D8Tt5FdtBk5gLTfBPm2tac', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}} id='run-bb0dddd8-85f3-4e6b-8553-eaa79f859ef8-0'


AIMessage(content='1 + 2 = 3', response_metadata={'id': 'msg_01D8Tt5FdtBk5gLTfBPm2tac', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 13}}, id='run-bb0dddd8-85f3-4e6b-8553-eaa79f859ef8-0')


If there are already existing callbacks associated with a module, these will run in addition to any passed in at runtime.


Next steps
​


You've now learned how to pass callbacks at runtime.


Next, check out the other how-to guides in this section, such as how to 
pass callbacks into a module constructor
.
Edit this page
Was this page helpful?
Previous
How to dispatch custom callback events
Next
How to split by character
Next steps


--- docs_how_to_character_text_splitter.txt ---

How-to guides
How to split by character
How to split by character


This is the simplest method. This 
splits
 based on a given character sequence, which defaults to 
"\n\n"
. Chunk length is measured by number of characters.




How the text is split: by single character separator.


How the chunk size is measured: by number of characters.




To obtain the string content directly, use 
.split_text
.


To create LangChain 
Document
 objects (e.g., for use in downstream tasks), use 
.create_documents
.


%
pip install 
-
qU langchain
-
text
-
splitters


from
 langchain_text_splitters 
import
 CharacterTextSplitter
# Load an example document
with
 
open
(
"state_of_the_union.txt"
)
 
as
 f
:
    state_of_the_union 
=
 f
.
read
(
)
text_splitter 
=
 CharacterTextSplitter
(
    separator
=
"\n\n"
,
    chunk_size
=
1000
,
    chunk_overlap
=
200
,
    length_function
=
len
,
    is_separator_regex
=
False
,
)
texts 
=
 text_splitter
.
create_documents
(
[
state_of_the_union
]
)
print
(
texts
[
0
]
)
API Reference:
CharacterTextSplitter


page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n\nLast year COVID-19 kept us apart. This year we are finally together again. \n\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n\nWith a duty to one another to the American people to the Constitution. \n\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \n\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n\nHe met the Ukrainian people. \n\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'


Use 
.create_documents
 to propagate metadata associated with each document to the output chunks:


metadatas 
=
 
[
{
"document"
:
 
1
}
,
 
{
"document"
:
 
2
}
]
documents 
=
 text_splitter
.
create_documents
(
    
[
state_of_the_union
,
 state_of_the_union
]
,
 metadatas
=
metadatas
)
print
(
documents
[
0
]
)


page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n\nLast year COVID-19 kept us apart. This year we are finally together again. \n\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n\nWith a duty to one another to the American people to the Constitution. \n\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \n\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n\nHe met the Ukrainian people. \n\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.' metadata={'document': 1}


Use 
.split_text
 to obtain the string content directly:


text_splitter
.
split_text
(
state_of_the_union
)
[
0
]


'Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n\nLast year COVID-19 kept us apart. This year we are finally together again. \n\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n\nWith a duty to one another to the American people to the Constitution. \n\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \n\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \n\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \n\nHe met the Ukrainian people. \n\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.'
Edit this page
Was this page helpful?
Previous
How to pass callbacks in at runtime
Next
How to cache chat model responses


--- docs_how_to_chat_model_caching.txt ---

How-to guides
How to cache chat model responses
On this page
How to cache chat model responses


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


LLMs




LangChain provides an optional caching layer for 
chat models
. This is useful for two main reasons:




It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. This is especially useful during app development.


It can speed up your application by reducing the number of API calls you make to the LLM provider.




This guide will walk you through how to enable this in your apps.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


# <!-- ruff: noqa: F821 -->
from
 langchain_core
.
globals
 
import
 set_llm_cache
API Reference:
set_llm_cache


In Memory Cache
​


This is an ephemeral cache that stores model calls in memory. It will be wiped when your environment restarts, and is not shared across processes.


%
%
time
from
 langchain_core
.
caches 
import
 InMemoryCache
set_llm_cache
(
InMemoryCache
(
)
)
# The first time, it is not yet in cache, so it should take longer
llm
.
invoke
(
"Tell me a joke"
)
API Reference:
InMemoryCache


CPU times: user 645 ms, sys: 214 ms, total: 859 ms
Wall time: 829 ms


AIMessage(content="Why don't scientists trust atoms?\n\nBecause they make up everything!", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')


%
%
time
# The second time it is, so it goes faster
llm
.
invoke
(
"Tell me a joke"
)


CPU times: user 822 µs, sys: 288 µs, total: 1.11 ms
Wall time: 1.06 ms


AIMessage(content="Why don't scientists trust atoms?\n\nBecause they make up everything!", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 11, 'total_tokens': 24}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-b6836bdd-8c30-436b-828f-0ac5fc9ab50e-0')


SQLite Cache
​


This cache implementation uses a 
SQLite
 database to store responses, and will last across process restarts.


!rm 
.
langchain
.
db


# We can do the same thing with a SQLite cache
from
 langchain_community
.
cache 
import
 SQLiteCache
set_llm_cache
(
SQLiteCache
(
database_path
=
".langchain.db"
)
)
API Reference:
SQLiteCache


%
%
time
# The first time, it is not yet in cache, so it should take longer
llm
.
invoke
(
"Tell me a joke"
)


CPU times: user 9.91 ms, sys: 7.68 ms, total: 17.6 ms
Wall time: 657 ms


AIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-39d9e1e8-7766-4970-b1d8-f50213fd94c5-0')


%
%
time
# The second time it is, so it goes faster
llm
.
invoke
(
"Tell me a joke"
)


CPU times: user 52.2 ms, sys: 60.5 ms, total: 113 ms
Wall time: 127 ms


AIMessage(content='Why did the scarecrow win an award? Because he was outstanding in his field!', id='run-39d9e1e8-7766-4970-b1d8-f50213fd94c5-0')


Next steps
​


You've now learned how to cache model responses to save time and money.


Next, check out the other how-to guides chat models in this section, like 
how to get a model to return structured output
 or 
how to create your own custom chat model
.
Edit this page
Was this page helpful?
Previous
How to split by character
Next
How to handle rate limits
In Memory Cache
SQLite Cache
Next steps


--- docs_how_to_chat_model_rate_limiting.txt ---

How-to guides
How to handle rate limits
On this page
How to handle rate limits


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


LLMs




You may find yourself in a situation where you are getting rate limited by the model provider API because you're making too many requests.


For example, this might happen if you are running many parallel queries to benchmark the chat model on a test dataset.


If you are facing such a situation, you can use a rate limiter to help match the rate at which you're making request to the rate allowed
by the API.


Requires 
langchain-core >= 0.2.24
This functionality was added in 
langchain-core == 0.2.24
. Please make sure your package is up to date.


Initialize a rate limiter
​


Langchain comes with a built-in in memory rate limiter. This rate limiter is thread safe and can be shared by multiple threads in the same process.


The provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size
of the requests.


from
 langchain_core
.
rate_limiters 
import
 InMemoryRateLimiter
rate_limiter 
=
 InMemoryRateLimiter
(
    requests_per_second
=
0.1
,
  
# <-- Super slow! We can only make a request once every 10 seconds!!
    check_every_n_seconds
=
0.1
,
  
# Wake up every 100 ms to check whether allowed to make a request,
    max_bucket_size
=
10
,
  
# Controls the maximum burst size.
)
API Reference:
InMemoryRateLimiter


Choose a model
​


Choose any model and pass to it the rate_limiter via the 
rate_limiter
 attribute.


import
 os
import
 time
from
 getpass 
import
 getpass
if
 
"ANTHROPIC_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"ANTHROPIC_API_KEY"
]
 
=
 getpass
(
)
from
 langchain_anthropic 
import
 ChatAnthropic
model 
=
 ChatAnthropic
(
model_name
=
"claude-3-opus-20240229"
,
 rate_limiter
=
rate_limiter
)
API Reference:
ChatAnthropic


Let's confirm that the rate limiter works. We should only be able to invoke the model once per 10 seconds.


for
 _ 
in
 
range
(
5
)
:
    tic 
=
 time
.
time
(
)
    model
.
invoke
(
"hello"
)
    toc 
=
 time
.
time
(
)
    
print
(
toc 
-
 tic
)


11.599073648452759
10.7502121925354
10.244257926940918
8.83088755607605
11.645203590393066
Edit this page
Was this page helpful?
Previous
How to cache chat model responses
Next
How to init any model in one line
Initialize a rate limiter
Choose a model


--- docs_how_to_chat_models_universal_init.txt ---

How-to guides
How to init any model in one line
On this page
How to init any model in one line


Many LLM applications let end users specify what model provider and model they want the application to be powered by. This requires writing some logic to initialize different 
chat models
 based on some user configuration. The 
init_chat_model()
 helper method makes it easy to initialize a number of different model integrations without having to worry about import paths and class names.


Supported models
See the 
init_chat_model()
 API reference for a full list of supported integrations.
Make sure you have the 
integration packages
 installed for any model providers you want to support. E.g. you should have 
langchain-openai
 installed to init an OpenAI model.


%
pip install 
-
qU langchain
>=
0.2
.8
 langchain
-
openai langchain
-
anthropic langchain
-
google
-
vertexai


Basic usage
​


from
 langchain
.
chat_models 
import
 init_chat_model
# Returns a langchain_openai.ChatOpenAI instance.
gpt_4o 
=
 init_chat_model
(
"gpt-4o"
,
 model_provider
=
"openai"
,
 temperature
=
0
)
# Returns a langchain_anthropic.ChatAnthropic instance.
claude_opus 
=
 init_chat_model
(
    
"claude-3-opus-20240229"
,
 model_provider
=
"anthropic"
,
 temperature
=
0
)
# Returns a langchain_google_vertexai.ChatVertexAI instance.
gemini_15 
=
 init_chat_model
(
    
"gemini-1.5-pro"
,
 model_provider
=
"google_vertexai"
,
 temperature
=
0
)
# Since all model integrations implement the ChatModel interface, you can use them in the same way.
print
(
"GPT-4o: "
 
+
 gpt_4o
.
invoke
(
"what's your name"
)
.
content 
+
 
"\n"
)
print
(
"Claude Opus: "
 
+
 claude_opus
.
invoke
(
"what's your name"
)
.
content 
+
 
"\n"
)
print
(
"Gemini 1.5: "
 
+
 gemini_15
.
invoke
(
"what's your name"
)
.
content 
+
 
"\n"
)
API Reference:
init_chat_model


/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_95293/571506279.py:4: LangChainBetaWarning: The function `init_chat_model` is in beta. It is actively being worked on, so the API may change.
  gpt_4o = init_chat_model("gpt-4o", model_provider="openai", temperature=0)
``````output
GPT-4o: I'm an AI created by OpenAI, and I don't have a personal name. How can I assist you today?
``````output
Claude Opus: My name is Claude. It's nice to meet you!
``````output
Gemini 1.5: I am a large language model, trained by Google. 
I don't have a name like a person does. You can call me Bard if you like! 😊


Inferring model provider
​


For common and distinct model names 
init_chat_model()
 will attempt to infer the model provider. See the 
API reference
 for a full list of inference behavior. E.g. any model that starts with 
gpt-3...
 or 
gpt-4...
 will be inferred as using model provider 
openai
.


gpt_4o 
=
 init_chat_model
(
"gpt-4o"
,
 temperature
=
0
)
claude_opus 
=
 init_chat_model
(
"claude-3-opus-20240229"
,
 temperature
=
0
)
gemini_15 
=
 init_chat_model
(
"gemini-1.5-pro"
,
 temperature
=
0
)


Creating a configurable model
​


You can also create a runtime-configurable model by specifying 
configurable_fields
. If you don't specify a 
model
 value, then "model" and "model_provider" be configurable by default.


configurable_model 
=
 init_chat_model
(
temperature
=
0
)
configurable_model
.
invoke
(
    
"what's your name"
,
 config
=
{
"configurable"
:
 
{
"model"
:
 
"gpt-4o"
}
}
)


AIMessage(content="I'm an AI created by OpenAI, and I don't have a personal name. How can I assist you today?", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 11, 'total_tokens': 34}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-b41df187-4627-490d-af3c-1c96282d3eb0-0', usage_metadata={'input_tokens': 11, 'output_tokens': 23, 'total_tokens': 34})


configurable_model
.
invoke
(
    
"what's your name"
,
 config
=
{
"configurable"
:
 
{
"model"
:
 
"claude-3-5-sonnet-20240620"
}
}
)


AIMessage(content="My name is Claude. It's nice to meet you!", additional_kwargs={}, response_metadata={'id': 'msg_01Fx9P74A7syoFkwE73CdMMY', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 11, 'output_tokens': 15}}, id='run-a0fd2bbd-3b7e-46bf-8d69-a48c7e60b03c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26})


Configurable model with default values
​


We can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:


first_llm 
=
 init_chat_model
(
    model
=
"gpt-4o"
,
    temperature
=
0
,
    configurable_fields
=
(
"model"
,
 
"model_provider"
,
 
"temperature"
,
 
"max_tokens"
)
,
    config_prefix
=
"first"
,
  
# useful when you have a chain with multiple models
)
first_llm
.
invoke
(
"what's your name"
)


AIMessage(content="I'm an AI created by OpenAI, and I don't have a personal name. How can I assist you today?", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 11, 'total_tokens': 34}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-3380f977-4b89-4f44-bc02-b64043b3166f-0', usage_metadata={'input_tokens': 11, 'output_tokens': 23, 'total_tokens': 34})


first_llm
.
invoke
(
    
"what's your name"
,
    config
=
{
        
"configurable"
:
 
{
            
"first_model"
:
 
"claude-3-5-sonnet-20240620"
,
            
"first_temperature"
:
 
0.5
,
            
"first_max_tokens"
:
 
100
,
        
}
    
}
,
)


AIMessage(content="My name is Claude. It's nice to meet you!", additional_kwargs={}, response_metadata={'id': 'msg_01EFKSWpmsn2PSYPQa4cNHWb', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 11, 'output_tokens': 15}}, id='run-3c58f47c-41b9-4e56-92e7-fb9602e3787c-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26})


Using a configurable model declaratively
​


We can call declarative operations like 
bind_tools
, 
with_structured_output
, 
with_configurable
, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.


from
 pydantic 
import
 BaseModel
,
 Field
class
 
GetWeather
(
BaseModel
)
:
    
"""Get the current weather in a given location"""
    location
:
 
str
 
=
 Field
(
.
.
.
,
 description
=
"The city and state, e.g. San Francisco, CA"
)
class
 
GetPopulation
(
BaseModel
)
:
    
"""Get the current population in a given location"""
    location
:
 
str
 
=
 Field
(
.
.
.
,
 description
=
"The city and state, e.g. San Francisco, CA"
)
llm 
=
 init_chat_model
(
temperature
=
0
)
llm_with_tools 
=
 llm
.
bind_tools
(
[
GetWeather
,
 GetPopulation
]
)
llm_with_tools
.
invoke
(
    
"what's bigger in 2024 LA or NYC"
,
 config
=
{
"configurable"
:
 
{
"model"
:
 
"gpt-4o"
}
}
)
.
tool_calls


[{'name': 'GetPopulation',
  'args': {'location': 'Los Angeles, CA'},
  'id': 'call_Ga9m8FAArIyEjItHmztPYA22',
  'type': 'tool_call'},
 {'name': 'GetPopulation',
  'args': {'location': 'New York, NY'},
  'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',
  'type': 'tool_call'}]


llm_with_tools
.
invoke
(
    
"what's bigger in 2024 LA or NYC"
,
    config
=
{
"configurable"
:
 
{
"model"
:
 
"claude-3-5-sonnet-20240620"
}
}
,
)
.
tool_calls


[{'name': 'GetPopulation',
  'args': {'location': 'Los Angeles, CA'},
  'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',
  'type': 'tool_call'},
 {'name': 'GetPopulation',
  'args': {'location': 'New York City, NY'},
  'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',
  'type': 'tool_call'}]
Edit this page
Was this page helpful?
Previous
How to handle rate limits
Next
How to track token usage in ChatModels
Basic usage
Inferring model provider
Creating a configurable model
Configurable model with default values
Using a configurable model declaratively


--- docs_how_to_chat_streaming.txt ---

How-to guides
How to stream chat model responses
On this page
How to stream chat model responses


All 
chat models
 implement the 
Runnable interface
, which comes with a 
default
 implementations of standard runnable methods (i.e. 
ainvoke
, 
batch
, 
abatch
, 
stream
, 
astream
, 
astream_events
).


The 
default
 streaming implementation provides an
Iterator
 (or 
AsyncIterator
 for asynchronous streaming) that yields a single value: the final output from the underlying chat model provider.


tip
The 
default
 implementation does 
not
 provide support for token-by-token streaming, but it ensures that the the model can be swapped in for any other model as it supports the same standard interface.


The ability to stream the output token-by-token depends on whether the provider has implemented proper streaming support.


See which 
integrations support token-by-token streaming here
.


Sync streaming
​


Below we use a 
|
 to help visualize the delimiter between tokens.


from
 langchain_anthropic
.
chat_models 
import
 ChatAnthropic
chat 
=
 ChatAnthropic
(
model
=
"claude-3-haiku-20240307"
)
for
 chunk 
in
 chat
.
stream
(
"Write me a 1 verse song about goldfish on the moon"
)
:
    
print
(
chunk
.
content
,
 end
=
"|"
,
 flush
=
True
)
API Reference:
ChatAnthropic


Here| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|
Floating| up| in| the| star|ry| night|,|
Fins| a|-|gl|im|mer| in| the| pale| moon|light|.|
Gol|dfish| swimming|,| peaceful| an|d free|,|
Se|ren|ely| |drif|ting| across| the| lunar| sea|.|


Async Streaming
​


from
 langchain_anthropic
.
chat_models 
import
 ChatAnthropic
chat 
=
 ChatAnthropic
(
model
=
"claude-3-haiku-20240307"
)
async
 
for
 chunk 
in
 chat
.
astream
(
"Write me a 1 verse song about goldfish on the moon"
)
:
    
print
(
chunk
.
content
,
 end
=
"|"
,
 flush
=
True
)
API Reference:
ChatAnthropic


Here| is| a| |1| |verse| song| about| gol|dfish| on| the| moon|:|
Floating| up| above| the| Earth|,|
Gol|dfish| swim| in| alien| m|irth|.|
In| their| bowl| of| lunar| dust|,|
Gl|it|tering| scales| reflect| the| trust|
Of| swimming| free| in| this| new| worl|d,|
Where| their| aqu|atic| dream|'s| unf|ur|le|d.|


Astream events
​


Chat models also support the standard 
astream events
 method.


This method is useful if you're streaming output from a larger LLM application that contains multiple steps (e.g., an LLM chain composed of a prompt, llm and parser).


from
 langchain_anthropic
.
chat_models 
import
 ChatAnthropic
chat 
=
 ChatAnthropic
(
model
=
"claude-3-haiku-20240307"
)
idx 
=
 
0
async
 
for
 event 
in
 chat
.
astream_events
(
    
"Write me a 1 verse song about goldfish on the moon"
)
:
    idx 
+=
 
1
    
if
 idx 
>=
 
5
:
  
# Truncate the output
        
print
(
"...Truncated"
)
        
break
    
print
(
event
)
API Reference:
ChatAnthropic


{'event': 'on_chat_model_start', 'data': {'input': 'Write me a 1 verse song about goldfish on the moon'}, 'name': 'ChatAnthropic', 'tags': [], 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e', usage_metadata={'input_tokens': 21, 'output_tokens': 2, 'total_tokens': 23, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content="Here's", additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '1d430164-52b1-4d00-8c00-b16460f7737e', 'name': 'ChatAnthropic', 'tags': [], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-haiku-20240307', 'ls_model_type': 'chat', 'ls_temperature': None, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=' a short one-verse song', additional_kwargs={}, response_metadata={}, id='run-1d430164-52b1-4d00-8c00-b16460f7737e')}, 'parent_ids': []}
...Truncated
Edit this page
Was this page helpful?
Previous
How to invoke runnables in parallel
Next
How to add default invocation args to a Runnable
Sync streaming
Async Streaming
Astream events


--- docs_how_to_chat_token_usage_tracking.txt ---

How-to guides
How to track token usage in ChatModels
On this page
How to track token usage in ChatModels


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models




Tracking 
token
 usage to calculate cost is an important part of putting your app in production. This guide goes over how to obtain this information from your LangChain model calls.


This guide requires 
langchain-anthropic
 and 
langchain-openai >= 0.3.11
.


%
pip install 
-
qU langchain
-
anthropic langchain
-
openai


A note on streaming with OpenAI
OpenAI's Chat Completions API does not stream token usage statistics by default (see API reference

here
).
To recover token counts when streaming with 
ChatOpenAI
 or 
AzureChatOpenAI
, set 
stream_usage=True
 as
demonstrated in this guide.


Using LangSmith
​


You can use 
LangSmith
 to help track token usage in your LLM application. See the 
LangSmith quick start guide
.


Using AIMessage.usage_metadata
​


A number of model providers return token usage information as part of the chat generation response. When available, this information will be included on the 
AIMessage
 objects produced by the corresponding model.


LangChain 
AIMessage
 objects include a 
usage_metadata
 attribute. When populated, this attribute will be a 
UsageMetadata
 dictionary with standard keys (e.g., 
"input_tokens"
 and 
"output_tokens"
). They will also include information on cached token usage and tokens from multi-modal data.


Examples:


OpenAI
:


from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
model
=
"gpt-4o-mini"
)
openai_response 
=
 llm
.
invoke
(
"hello"
)
openai_response
.
usage_metadata
API Reference:
init_chat_model


{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}


Anthropic
:


from
 langchain_anthropic 
import
 ChatAnthropic
llm 
=
 ChatAnthropic
(
model
=
"claude-3-haiku-20240307"
)
anthropic_response 
=
 llm
.
invoke
(
"hello"
)
anthropic_response
.
usage_metadata
API Reference:
ChatAnthropic


{'input_tokens': 8, 'output_tokens': 12, 'total_tokens': 20}


Streaming
​


Some providers support token count metadata in a streaming context.


OpenAI
​


For example, OpenAI will return a message 
chunk
 at the end of a stream with token usage information. This behavior is supported by 
langchain-openai >= 0.1.9
 and can be enabled by setting 
stream_usage=True
. This attribute can also be set when 
ChatOpenAI
 is instantiated.


note
By default, the last message chunk in a stream will include a 
"finish_reason"
 in the message's 
response_metadata
 attribute. If we include token usage in streaming mode, an additional chunk containing usage metadata will be added to the end of the stream, such that 
"finish_reason"
 appears on the second to last message chunk.


llm 
=
 init_chat_model
(
model
=
"gpt-4o-mini"
)
aggregate 
=
 
None
for
 chunk 
in
 llm
.
stream
(
"hello"
,
 stream_usage
=
True
)
:
    
print
(
chunk
)
    aggregate 
=
 chunk 
if
 aggregate 
is
 
None
 
else
 aggregate 
+
 chunk


content='' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content='Hello' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content='!' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content=' How' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content=' can' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content=' I' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content=' assist' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content=' you' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content=' today' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content='?' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content='' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini'} id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623'
content='' id='run-adb20c31-60c7-43a2-99b2-d4a53ca5f623' usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}


Note that the usage metadata will be included in the sum of the individual message chunks:


print
(
aggregate
.
content
)
print
(
aggregate
.
usage_metadata
)


Hello! How can I assist you today?
{'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17}


To disable streaming token counts for OpenAI, set 
stream_usage
 to False, or omit it from the parameters:


aggregate 
=
 
None
for
 chunk 
in
 llm
.
stream
(
"hello"
)
:
    
print
(
chunk
)


content='' id='run-8e758550-94b0-4cca-a298-57482793c25d'
content='Hello' id='run-8e758550-94b0-4cca-a298-57482793c25d'
content='!' id='run-8e758550-94b0-4cca-a298-57482793c25d'
content=' How' id='run-8e758550-94b0-4cca-a298-57482793c25d'
content=' can' id='run-8e758550-94b0-4cca-a298-57482793c25d'
content=' I' id='run-8e758550-94b0-4cca-a298-57482793c25d'
content=' assist' id='run-8e758550-94b0-4cca-a298-57482793c25d'
content=' you' id='run-8e758550-94b0-4cca-a298-57482793c25d'
content=' today' id='run-8e758550-94b0-4cca-a298-57482793c25d'
content='?' id='run-8e758550-94b0-4cca-a298-57482793c25d'
content='' response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini'} id='run-8e758550-94b0-4cca-a298-57482793c25d'


You can also enable streaming token usage by setting 
stream_usage
 when instantiating the chat model. This can be useful when incorporating chat models into LangChain 
chains
: usage metadata can be monitored when 
streaming intermediate steps
 or using tracing software such as 
LangSmith
.


See the below example, where we return output structured to a desired schema, but can still observe token usage streamed from intermediate steps.


from
 pydantic 
import
 BaseModel
,
 Field
class
 
Joke
(
BaseModel
)
:
    
"""Joke to tell user."""
    setup
:
 
str
 
=
 Field
(
description
=
"question to set up a joke"
)
    punchline
:
 
str
 
=
 Field
(
description
=
"answer to resolve the joke"
)
llm 
=
 init_chat_model
(
    model
=
"gpt-4o-mini"
,
    stream_usage
=
True
,
)
# Under the hood, .with_structured_output binds tools to the
# chat model and appends a parser.
structured_llm 
=
 llm
.
with_structured_output
(
Joke
)
async
 
for
 event 
in
 structured_llm
.
astream_events
(
"Tell me a joke"
)
:
    
if
 event
[
"event"
]
 
==
 
"on_chat_model_end"
:
        
print
(
f'Token usage: 
{
event
[
"data"
]
[
"output"
]
.
usage_metadata
}
\n'
)
    
elif
 event
[
"event"
]
 
==
 
"on_chain_end"
 
and
 event
[
"name"
]
 
==
 
"RunnableSequence"
:
        
print
(
event
[
"data"
]
[
"output"
]
)
    
else
:
        
pass


Token usage: {'input_tokens': 79, 'output_tokens': 23, 'total_tokens': 102}
setup='Why was the math book sad?' punchline='Because it had too many problems.'


Token usage is also visible in the corresponding 
LangSmith trace
 in the payload from the chat model.


Using callbacks
​


Requires 
langchain-core>=0.3.49


LangChain implements a callback handler and context manager that will track token usage across calls of any chat model that returns 
usage_metadata
.


There are also some API-specific callback context managers that maintain pricing for different models, allowing for cost estimation in real time. They are currently only implemented for the OpenAI API and Bedrock Anthropic API, and are available in 
langchain-community
:




get_openai_callback


get_bedrock_anthropic_callback




Below, we demonstrate the general-purpose usage metadata callback manager. We can track token usage through configuration or as a context manager.


Tracking token usage through configuration
​


To track token usage through configuration, instantiate a 
UsageMetadataCallbackHandler
 and pass it into the config:


from
 langchain
.
chat_models 
import
 init_chat_model
from
 langchain_core
.
callbacks 
import
 UsageMetadataCallbackHandler
llm_1 
=
 init_chat_model
(
model
=
"openai:gpt-4o-mini"
)
llm_2 
=
 init_chat_model
(
model
=
"anthropic:claude-3-5-haiku-latest"
)
callback 
=
 UsageMetadataCallbackHandler
(
)
result_1 
=
 llm_1
.
invoke
(
"Hello"
,
 config
=
{
"callbacks"
:
 
[
callback
]
}
)
result_2 
=
 llm_2
.
invoke
(
"Hello"
,
 config
=
{
"callbacks"
:
 
[
callback
]
}
)
callback
.
usage_metadata
API Reference:
init_chat_model
 | 
UsageMetadataCallbackHandler


{'gpt-4o-mini-2024-07-18': {'input_tokens': 8,
  'output_tokens': 10,
  'total_tokens': 18,
  'input_token_details': {'audio': 0, 'cache_read': 0},
  'output_token_details': {'audio': 0, 'reasoning': 0}},
 'claude-3-5-haiku-20241022': {'input_tokens': 8,
  'output_tokens': 21,
  'total_tokens': 29,
  'input_token_details': {'cache_read': 0, 'cache_creation': 0}}}


Tracking token usage using a context manager
​


You can also use 
get_usage_metadata_callback
 to create a context manager and aggregate usage metadata there:


from
 langchain
.
chat_models 
import
 init_chat_model
from
 langchain_core
.
callbacks 
import
 get_usage_metadata_callback
llm_1 
=
 init_chat_model
(
model
=
"openai:gpt-4o-mini"
)
llm_2 
=
 init_chat_model
(
model
=
"anthropic:claude-3-5-haiku-latest"
)
with
 get_usage_metadata_callback
(
)
 
as
 cb
:
    llm_1
.
invoke
(
"Hello"
)
    llm_2
.
invoke
(
"Hello"
)
    
print
(
cb
.
usage_metadata
)
API Reference:
init_chat_model
 | 
get_usage_metadata_callback


{'gpt-4o-mini-2024-07-18': {'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'claude-3-5-haiku-20241022': {'input_tokens': 8, 'output_tokens': 21, 'total_tokens': 29, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}}


Either of these methods will aggregate token usage across multiple calls to each model. For example, you can use it in an 
agent
 to track token usage across repeated calls to one model:


%
pip install 
-
qU langgraph


from
 langgraph
.
prebuilt 
import
 create_react_agent
# Create a tool
def
 
get_weather
(
location
:
 
str
)
 
-
>
 
str
:
    
"""Get the weather at a location."""
    
return
 
"It's sunny."
callback 
=
 UsageMetadataCallbackHandler
(
)
tools 
=
 
[
get_weather
]
agent 
=
 create_react_agent
(
"openai:gpt-4o-mini"
,
 tools
)
for
 step 
in
 agent
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 
"What's the weather in Boston?"
}
]
}
,
    stream_mode
=
"values"
,
    config
=
{
"callbacks"
:
 
[
callback
]
}
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
print
(
f"\nTotal usage: 
{
callback
.
usage_metadata
}
"
)
API Reference:
create_react_agent


================================[1m Human Message [0m=================================
What's the weather in Boston?
==================================[1m Ai Message [0m==================================
Tool Calls:
  get_weather (call_izMdhUYpp9Vhx7DTNAiybzGa)
 Call ID: call_izMdhUYpp9Vhx7DTNAiybzGa
  Args:
    location: Boston
=================================[1m Tool Message [0m=================================
Name: get_weather
It's sunny.
==================================[1m Ai Message [0m==================================
The weather in Boston is sunny.
Total usage: {'gpt-4o-mini-2024-07-18': {'input_token_details': {'audio': 0, 'cache_read': 0}, 'input_tokens': 125, 'total_tokens': 149, 'output_tokens': 24, 'output_token_details': {'audio': 0, 'reasoning': 0}}}


Next steps
​


You've now seen a few examples of how to track token usage for supported providers.


Next, check out the other how-to guides chat models in this section, like 
how to get a model to return structured output
 or 
how to add caching to your chat models
.
Edit this page
Was this page helpful?
Previous
How to init any model in one line
Next
How to add tools to chatbots
Using LangSmith
Using AIMessage.usage_metadata
Streaming
Using callbacks
Tracking token usage through configuration
Tracking token usage using a context manager
Next steps


--- docs_how_to_chatbots_memory.txt ---

How-to guides
How to add memory to chatbots
On this page
How to add memory to chatbots


A key feature of chatbots is their ability to use the content of previous conversational turns as context. This state management can take several forms, including:




Simply stuffing previous messages into a chat model prompt.


The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.


More complex modifications like synthesizing summaries for long running conversations.




We'll go into more detail on a few techniques below!


note
This how-to guide previously built a chatbot using 
RunnableWithMessageHistory
. You can access this version of the guide in the 
v0.2 docs
.
As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of 
LangGraph persistence
 to incorporate 
memory
 into new LangChain applications.
If your code is already relying on 
RunnableWithMessageHistory
 or 
BaseChatMessageHistory
, you do 
not
 need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses 
RunnableWithMessageHistory
 will continue to work as expected.
Please see 
How to migrate to LangGraph Memory
 for more details.


Setup
​


You'll need to install a few packages, and have your OpenAI API key set as an environment variable named 
OPENAI_API_KEY
:


%
pip install 
-
-
upgrade 
-
-
quiet langchain langchain
-
openai langgraph
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"OpenAI API Key:"
)


OpenAI API Key: ········


Let's also set up a chat model that we'll use for the below examples.


from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
API Reference:
ChatOpenAI


Message passing
​


The simplest form of memory is simply passing chat history messages into a chain. Here's an example:


from
 langchain_core
.
messages 
import
 AIMessage
,
 HumanMessage
,
 SystemMessage
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 MessagesPlaceholder
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        SystemMessage
(
            content
=
"You are a helpful assistant. Answer all questions to the best of your ability."
        
)
,
        MessagesPlaceholder
(
variable_name
=
"messages"
)
,
    
]
)
chain 
=
 prompt 
|
 model
ai_msg 
=
 chain
.
invoke
(
    
{
        
"messages"
:
 
[
            HumanMessage
(
                content
=
"Translate from English to French: I love programming."
            
)
,
            AIMessage
(
content
=
"J'adore la programmation."
)
,
            HumanMessage
(
content
=
"What did you just say?"
)
,
        
]
,
    
}
)
print
(
ai_msg
.
content
)
API Reference:
AIMessage
 | 
HumanMessage
 | 
SystemMessage
 | 
ChatPromptTemplate
 | 
MessagesPlaceholder


I said, "I love programming" in French: "J'adore la programmation."


We can see that by passing the previous conversation into a chain, it can use it as context to answer questions. This is the basic concept underpinning chatbot memory - the rest of the guide will demonstrate convenient techniques for passing or reformatting messages.


Automatic history management
​


The previous examples pass messages to the chain (and model) explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also provides a way to build applications that have memory using LangGraph's 
persistence
. You can 
enable persistence
 in LangGraph applications by providing a 
checkpointer
 when compiling the graph.


from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
graph 
import
 START
,
 MessagesState
,
 StateGraph
workflow 
=
 StateGraph
(
state_schema
=
MessagesState
)
# Define the function that calls the model
def
 
call_model
(
state
:
 MessagesState
)
:
    system_prompt 
=
 
(
        
"You are a helpful assistant. "
        
"Answer all questions to the best of your ability."
    
)
    messages 
=
 
[
SystemMessage
(
content
=
system_prompt
)
]
 
+
 state
[
"messages"
]
    response 
=
 model
.
invoke
(
messages
)
    
return
 
{
"messages"
:
 response
}
# Define the node and edge
workflow
.
add_node
(
"model"
,
 call_model
)
workflow
.
add_edge
(
START
,
 
"model"
)
# Add simple in-memory checkpointer
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
checkpointer
=
memory
)
API Reference:
MemorySaver
 | 
StateGraph


We'll pass the latest input to the conversation here and let LangGraph keep track of the conversation history using the checkpointer:


app
.
invoke
(
    
{
"messages"
:
 
[
HumanMessage
(
content
=
"Translate to French: I love programming."
)
]
}
,
    config
=
{
"configurable"
:
 
{
"thread_id"
:
 
"1"
}
}
,
)


{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),
  AIMessage(content="J'aime programmer.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39})]}


app
.
invoke
(
    
{
"messages"
:
 
[
HumanMessage
(
content
=
"What did I just ask you?"
)
]
}
,
    config
=
{
"configurable"
:
 
{
"thread_id"
:
 
"1"
}
}
,
)


{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='be5e7099-3149-4293-af49-6b36c8ccd71b'),
  AIMessage(content="J'aime programmer.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 35, 'total_tokens': 39, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a753d7a-b97b-4d01-a661-626be6f41b38-0', usage_metadata={'input_tokens': 35, 'output_tokens': 4, 'total_tokens': 39}),
  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='c667529b-7c41-4cc0-9326-0af47328b816'),
  AIMessage(content='You asked me to translate "I love programming" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 54, 'total_tokens': 67, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-134a7ea0-d3a4-4923-bd58-25e5a43f6a1f-0', usage_metadata={'input_tokens': 54, 'output_tokens': 13, 'total_tokens': 67})]}


Modifying chat history
​


Modifying stored chat messages can help your chatbot handle a variety of situations. Here are some examples:


Trimming messages
​


LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the 
app
 we declared above:


demo_ephemeral_chat_history 
=
 
[
    HumanMessage
(
content
=
"Hey there! I'm Nemo."
)
,
    AIMessage
(
content
=
"Hello!"
)
,
    HumanMessage
(
content
=
"How are you today?"
)
,
    AIMessage
(
content
=
"Fine thanks!"
)
,
]
app
.
invoke
(
    
{
        
"messages"
:
 demo_ephemeral_chat_history
        
+
 
[
HumanMessage
(
content
=
"What's my name?"
)
]
    
}
,
    config
=
{
"configurable"
:
 
{
"thread_id"
:
 
"2"
}
}
,
)


{'messages': [HumanMessage(content="Hey there! I'm Nemo.", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),
  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),
  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),
  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),
  HumanMessage(content="What's my name?", additional_kwargs={}, response_metadata={}, id='c933eca3-5fd8-4651-af16-20fe2d49c216'),
  AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 63, 'total_tokens': 68, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-a0b21acc-9dbb-4fb6-a953-392020f37d88-0', usage_metadata={'input_tokens': 63, 'output_tokens': 5, 'total_tokens': 68})]}


We can see the app remembers the preloaded name.


But let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in 
trim_messages
 util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 "token" and keep only the last two messages:


from
 langchain_core
.
messages 
import
 trim_messages
from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
graph 
import
 START
,
 MessagesState
,
 StateGraph
# Define trimmer
# count each message as 1 "token" (token_counter=len) and keep only the last two messages
trimmer 
=
 trim_messages
(
strategy
=
"last"
,
 max_tokens
=
2
,
 token_counter
=
len
)
workflow 
=
 StateGraph
(
state_schema
=
MessagesState
)
# Define the function that calls the model
def
 
call_model
(
state
:
 MessagesState
)
:
    trimmed_messages 
=
 trimmer
.
invoke
(
state
[
"messages"
]
)
    system_prompt 
=
 
(
        
"You are a helpful assistant. "
        
"Answer all questions to the best of your ability."
    
)
    messages 
=
 
[
SystemMessage
(
content
=
system_prompt
)
]
 
+
 trimmed_messages
    response 
=
 model
.
invoke
(
messages
)
    
return
 
{
"messages"
:
 response
}
# Define the node and edge
workflow
.
add_node
(
"model"
,
 call_model
)
workflow
.
add_edge
(
START
,
 
"model"
)
# Add simple in-memory checkpointer
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
checkpointer
=
memory
)
API Reference:
trim_messages
 | 
MemorySaver
 | 
StateGraph


Let's call this new app and check the response


app
.
invoke
(
    
{
        
"messages"
:
 demo_ephemeral_chat_history
        
+
 
[
HumanMessage
(
content
=
"What is my name?"
)
]
    
}
,
    config
=
{
"configurable"
:
 
{
"thread_id"
:
 
"3"
}
}
,
)


{'messages': [HumanMessage(content="Hey there! I'm Nemo.", additional_kwargs={}, response_metadata={}, id='6b4cab70-ce18-49b0-bb06-267bde44e037'),
  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='ba3714f4-8876-440b-a651-efdcab2fcb4c'),
  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='08d032c0-1577-4862-a3f2-5c1b90687e21'),
  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='21790e16-db05-4537-9a6b-ecad0fcec436'),
  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='a22ab7c5-8617-4821-b3e9-a9e7dca1ff78'),
  AIMessage(content="I'm sorry, but I don't have access to personal information about you unless you share it with me. How can I assist you today?", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 39, 'total_tokens': 66, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7b32d72-9f57-4705-be7e-43bf1c3d293b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 27, 'total_tokens': 66})]}


We can see that 
trim_messages
 was called and only the two most recent messages will be passed to the model. In this case, this means that the model forgot the name we gave it.


Check out our 
how to guide on trimming messages
 for more.


Summary memory
​


We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history:


demo_ephemeral_chat_history 
=
 
[
    HumanMessage
(
content
=
"Hey there! I'm Nemo."
)
,
    AIMessage
(
content
=
"Hello!"
)
,
    HumanMessage
(
content
=
"How are you today?"
)
,
    AIMessage
(
content
=
"Fine thanks!"
)
,
]


And now, let's update the model-calling function to distill previous interactions into a summary:


from
 langchain_core
.
messages 
import
 HumanMessage
,
 RemoveMessage
from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
graph 
import
 START
,
 MessagesState
,
 StateGraph
workflow 
=
 StateGraph
(
state_schema
=
MessagesState
)
# Define the function that calls the model
def
 
call_model
(
state
:
 MessagesState
)
:
    system_prompt 
=
 
(
        
"You are a helpful assistant. "
        
"Answer all questions to the best of your ability. "
        
"The provided chat history includes a summary of the earlier conversation."
    
)
    system_message 
=
 SystemMessage
(
content
=
system_prompt
)
    message_history 
=
 state
[
"messages"
]
[
:
-
1
]
  
# exclude the most recent user input
    
# Summarize the messages if the chat history reaches a certain size
    
if
 
len
(
message_history
)
 
>=
 
4
:
        last_human_message 
=
 state
[
"messages"
]
[
-
1
]
        
# Invoke the model to generate conversation summary
        summary_prompt 
=
 
(
            
"Distill the above chat messages into a single summary message. "
            
"Include as many specific details as you can."
        
)
        summary_message 
=
 model
.
invoke
(
            message_history 
+
 
[
HumanMessage
(
content
=
summary_prompt
)
]
        
)
        
# Delete messages that we no longer want to show up
        delete_messages 
=
 
[
RemoveMessage
(
id
=
m
.
id
)
 
for
 m 
in
 state
[
"messages"
]
]
        
# Re-add user message
        human_message 
=
 HumanMessage
(
content
=
last_human_message
.
content
)
        
# Call the model with summary & response
        response 
=
 model
.
invoke
(
[
system_message
,
 summary_message
,
 human_message
]
)
        message_updates 
=
 
[
summary_message
,
 human_message
,
 response
]
 
+
 delete_messages
    
else
:
        message_updates 
=
 model
.
invoke
(
[
system_message
]
 
+
 state
[
"messages"
]
)
    
return
 
{
"messages"
:
 message_updates
}
# Define the node and edge
workflow
.
add_node
(
"model"
,
 call_model
)
workflow
.
add_edge
(
START
,
 
"model"
)
# Add simple in-memory checkpointer
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
checkpointer
=
memory
)
API Reference:
HumanMessage
 | 
RemoveMessage
 | 
MemorySaver
 | 
StateGraph


Let's see if it remembers the name we gave it:


app
.
invoke
(
    
{
        
"messages"
:
 demo_ephemeral_chat_history
        
+
 
[
HumanMessage
(
"What did I say my name was?"
)
]
    
}
,
    config
=
{
"configurable"
:
 
{
"thread_id"
:
 
"4"
}
}
,
)


{'messages': [AIMessage(content="Nemo greeted me, and I responded positively, indicating that I'm doing well.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 60, 'total_tokens': 76, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee42f98d-907d-4bad-8f16-af2db789701d-0', usage_metadata={'input_tokens': 60, 'output_tokens': 16, 'total_tokens': 76}),
  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='788555ea-5b1f-4c29-a2f2-a92f15d147be'),
  AIMessage(content='You mentioned that your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 67, 'total_tokens': 75, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-099a43bd-a284-4969-bb6f-0be486614cd8-0', usage_metadata={'input_tokens': 67, 'output_tokens': 8, 'total_tokens': 75})]}


Note that invoking the app again will keep accumulating the history until it reaches the specified number of messages (four in our case). At that point we will generate another summary generated from the initial summary plus new messages and so on.
Edit this page
Was this page helpful?
Previous
How to use a vectorstore as a retriever
Next
How to use example selectors
Setup
Message passing
Automatic history management
Modifying chat history
Trimming messages
Summary memory


--- docs_how_to_chatbots_retrieval.txt ---

How-to guides
How to add retrieval to chatbots
On this page
How to add retrieval to chatbots


Retrieval
 is a common technique chatbots use to augment their responses with data outside a chat model's training data. This section will cover how to implement retrieval in the context of chatbots, but it's worth noting that retrieval is a very subtle and deep topic - we encourage you to explore 
other parts of the documentation
 that go into greater depth!


Setup
​


You'll need to install a few packages, and have your OpenAI API key set as an environment variable named 
OPENAI_API_KEY
:


%
pip install 
-
qU langchain langchain
-
openai langchain
-
chroma beautifulsoup4
# Set env var OPENAI_API_KEY or load from a .env file:
import
 dotenv
dotenv
.
load_dotenv
(
)


[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.
You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m
[0mNote: you may need to restart the kernel to use updated packages.


True


Let's also set up a chat model that we'll use for the below examples.


from
 langchain_openai 
import
 ChatOpenAI
chat 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0.2
)
API Reference:
ChatOpenAI


Creating a retriever
​


We'll use 
the LangSmith documentation
 as source material and store the content in a 
vector store
 for later retrieval. Note that this example will gloss over some of the specifics around parsing and storing a data source - you can see more 
in-depth documentation on creating retrieval systems here
.


Let's use a document loader to pull text from the docs:


from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
loader 
=
 WebBaseLoader
(
"https://docs.smith.langchain.com/overview"
)
data 
=
 loader
.
load
(
)
API Reference:
WebBaseLoader


Next, we split it into smaller chunks that the LLM's context window can handle and store it in a vector database:


from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
500
,
 chunk_overlap
=
0
)
all_splits 
=
 text_splitter
.
split_documents
(
data
)
API Reference:
RecursiveCharacterTextSplitter


Then we embed and store those chunks in a vector database:


from
 langchain_chroma 
import
 Chroma
from
 langchain_openai 
import
 OpenAIEmbeddings
vectorstore 
=
 Chroma
.
from_documents
(
documents
=
all_splits
,
 embedding
=
OpenAIEmbeddings
(
)
)
API Reference:
OpenAIEmbeddings


And finally, let's create a retriever from our initialized vectorstore:


# k is the number of chunks to retrieve
retriever 
=
 vectorstore
.
as_retriever
(
k
=
4
)
docs 
=
 retriever
.
invoke
(
"Can LangSmith help test my LLM applications?"
)
docs


[Document(page_content='Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
 Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
 Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
 Document(page_content="does that affect the output?\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'})]


We can see that invoking the retriever above results in some parts of the LangSmith docs that contain information about testing that our chatbot can use as context when answering questions. And now we've got a retriever that can return related data from the LangSmith docs!


Document chains
​


Now that we have a retriever that can return LangChain docs, let's create a chain that can use them as context to answer questions. We'll use a 
create_stuff_documents_chain
 helper function to "stuff" all of the input documents into the prompt. It will also handle formatting the docs as strings.


In addition to a chat model, the function also expects a prompt that has a 
context
 variables, as well as a placeholder for chat history messages named 
messages
. We'll create an appropriate prompt and pass it as shown below:


from
 langchain
.
chains
.
combine_documents 
import
 create_stuff_documents_chain
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 MessagesPlaceholder
SYSTEM_TEMPLATE 
=
 
"""
Answer the user's questions based on the below context. 
If the context doesn't contain any relevant information to the question, don't make something up and just say "I don't know":
<context>
{context}
</context>
"""
question_answering_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            SYSTEM_TEMPLATE
,
        
)
,
        MessagesPlaceholder
(
variable_name
=
"messages"
)
,
    
]
)
document_chain 
=
 create_stuff_documents_chain
(
chat
,
 question_answering_prompt
)
API Reference:
create_stuff_documents_chain
 | 
ChatPromptTemplate
 | 
MessagesPlaceholder


We can invoke this 
document_chain
 by itself to answer questions. Let's use the docs we retrieved above and the same question, 
how can langsmith help with testing?
:


from
 langchain_core
.
messages 
import
 HumanMessage
document_chain
.
invoke
(
    
{
        
"context"
:
 docs
,
        
"messages"
:
 
[
            HumanMessage
(
content
=
"Can LangSmith help test my LLM applications?"
)
        
]
,
    
}
)
API Reference:
HumanMessage


'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'


Looks good! For comparison, we can try it with no context docs and compare the result:


document_chain
.
invoke
(
    
{
        
"context"
:
 
[
]
,
        
"messages"
:
 
[
            HumanMessage
(
content
=
"Can LangSmith help test my LLM applications?"
)
        
]
,
    
}
)


"I don't know about LangSmith's specific capabilities for testing LLM applications. It's best to reach out to LangSmith directly to inquire about their services and how they can assist with testing your LLM applications."


We can see that the LLM does not return any results.


Retrieval chains
​


Let's combine this document chain with the retriever. Here's one way this can look:


from
 typing 
import
 Dict
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
def
 
parse_retriever_input
(
params
:
 Dict
)
:
    
return
 params
[
"messages"
]
[
-
1
]
.
content
retrieval_chain 
=
 RunnablePassthrough
.
assign
(
    context
=
parse_retriever_input 
|
 retriever
,
)
.
assign
(
    answer
=
document_chain
,
)
API Reference:
RunnablePassthrough


Given a list of input messages, we extract the content of the last message in the list and pass that to the retriever to fetch some documents. Then, we pass those documents as context to our document chain to generate a final response.


Invoking this chain combines both steps outlined above:


retrieval_chain
.
invoke
(
    
{
        
"messages"
:
 
[
            HumanMessage
(
content
=
"Can LangSmith help test my LLM applications?"
)
        
]
,
    
}
)


{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],
 'context': [Document(page_content='Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
  Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
  Document(page_content="does that affect the output?\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'})],
 'answer': 'Yes, LangSmith can help test and evaluate your LLM applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}


Looks good!


Query transformation
​


Our retrieval chain is capable of answering questions about LangSmith, but there's a problem - chatbots interact with users conversationally, and therefore have to deal with followup questions.


The chain in its current form will struggle with this. Consider a followup question to our original question like 
Tell me more!
. If we invoke our retriever with that query directly, we get documents irrelevant to LLM application testing:


retriever
.
invoke
(
"Tell me more!"
)


[Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
 Document(page_content='playground. Here, you can modify the prompt and re-run it to observe the resulting changes to the output - as many times as needed!Currently, this feature supports only OpenAI and Anthropic models and works for LLM and Chat Model calls. We plan to extend its functionality to more LLM types, chains, agents, and retrievers in the future.What is the exact sequence of events?\u200bIn complicated chains and agents, it can often be hard to understand what is going on under the hood. What calls are being', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
 Document(page_content='however, there is still no complete substitute for human review to get the utmost quality and reliability from your application.', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
 Document(page_content='Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'})]


This is because the retriever has no innate concept of state, and will only pull documents most similar to the query given. To solve this, we can transform the query into a standalone query without any external references an LLM.


Here's an example:


from
 langchain_core
.
messages 
import
 AIMessage
,
 HumanMessage
query_transform_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        MessagesPlaceholder
(
variable_name
=
"messages"
)
,
        
(
            
"user"
,
            
"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else."
,
        
)
,
    
]
)
query_transformation_chain 
=
 query_transform_prompt 
|
 chat
query_transformation_chain
.
invoke
(
    
{
        
"messages"
:
 
[
            HumanMessage
(
content
=
"Can LangSmith help test my LLM applications?"
)
,
            AIMessage
(
                content
=
"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
            
)
,
            HumanMessage
(
content
=
"Tell me more!"
)
,
        
]
,
    
}
)
API Reference:
AIMessage
 | 
HumanMessage


AIMessage(content='"LangSmith LLM application testing and evaluation"')


Awesome! That transformed query would pull up context documents related to LLM application testing.


Let's add this to our retrieval chain. We can wrap our retriever as follows:


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
runnables 
import
 RunnableBranch
query_transforming_retriever_chain 
=
 RunnableBranch
(
    
(
        
lambda
 x
:
 
len
(
x
.
get
(
"messages"
,
 
[
]
)
)
 
==
 
1
,
        
# If only one message, then we just pass that message's content to retriever
        
(
lambda
 x
:
 x
[
"messages"
]
[
-
1
]
.
content
)
 
|
 retriever
,
    
)
,
    
# If messages, then we pass inputs to LLM chain to transform the query, then pass to retriever
    query_transform_prompt 
|
 chat 
|
 StrOutputParser
(
)
 
|
 retriever
,
)
.
with_config
(
run_name
=
"chat_retriever_chain"
)
API Reference:
StrOutputParser
 | 
RunnableBranch


Then, we can use this query transformation chain to make our retrieval chain better able to handle such followup questions:


SYSTEM_TEMPLATE 
=
 
"""
Answer the user's questions based on the below context. 
If the context doesn't contain any relevant information to the question, don't make something up and just say "I don't know":
<context>
{context}
</context>
"""
question_answering_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            SYSTEM_TEMPLATE
,
        
)
,
        MessagesPlaceholder
(
variable_name
=
"messages"
)
,
    
]
)
document_chain 
=
 create_stuff_documents_chain
(
chat
,
 question_answering_prompt
)
conversational_retrieval_chain 
=
 RunnablePassthrough
.
assign
(
    context
=
query_transforming_retriever_chain
,
)
.
assign
(
    answer
=
document_chain
,
)


Awesome! Let's invoke this new chain with the same inputs as earlier:


conversational_retrieval_chain
.
invoke
(
    
{
        
"messages"
:
 
[
            HumanMessage
(
content
=
"Can LangSmith help test my LLM applications?"
)
,
        
]
    
}
)


{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?')],
 'context': [Document(page_content='Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
  Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
  Document(page_content="does that affect the output?\u200bSo you notice a bad output, and you go into LangSmith to see what's going on. You find the faulty LLM call and are now looking at the exact input. You want to try changing a word or a phrase to see what happens -- what do you do?We constantly ran into this issue. Initially, we copied the prompt to a playground of sorts. But this got annoying, so we built a playground of our own! When examining an LLM call, you can click the Open in Playground button to access this", metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'})],
 'answer': 'Yes, LangSmith can help test and evaluate LLM (Language Model) applications. It simplifies the initial setup, and you can use it to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'}


conversational_retrieval_chain
.
invoke
(
    
{
        
"messages"
:
 
[
            HumanMessage
(
content
=
"Can LangSmith help test my LLM applications?"
)
,
            AIMessage
(
                content
=
"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
            
)
,
            HumanMessage
(
content
=
"Tell me more!"
)
,
        
]
,
    
}
)


{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'),
  AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'),
  HumanMessage(content='Tell me more!')],
 'context': [Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
  Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
  Document(page_content='Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}),
  Document(page_content='LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'})],
 'answer': 'LangSmith simplifies the initial setup for building reliable LLM applications, but it acknowledges that there is still work needed to bring the performance of prompts, chains, and agents up to the level where they are reliable enough to be used in production. It also provides the capability to manually review and annotate runs through annotation queues, allowing you to select runs based on criteria like model type or automatic evaluation scores for human review. This feature is particularly useful for assessing subjective qualities that automatic evaluators struggle with.'}


You can check out 
this LangSmith trace
 to see the internal query transformation step for yourself.


Streaming
​


Because this chain is constructed with LCEL, you can use familiar methods like 
.stream()
 with it:


stream 
=
 conversational_retrieval_chain
.
stream
(
    
{
        
"messages"
:
 
[
            HumanMessage
(
content
=
"Can LangSmith help test my LLM applications?"
)
,
            AIMessage
(
                content
=
"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise."
            
)
,
            HumanMessage
(
content
=
"Tell me more!"
)
,
        
]
,
    
}
)
for
 chunk 
in
 stream
:
    
print
(
chunk
)


{'messages': [HumanMessage(content='Can LangSmith help test my LLM applications?'), AIMessage(content='Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.'), HumanMessage(content='Tell me more!')]}
{'context': [Document(page_content='LangSmith Overview and User Guide | 🦜️🛠️ LangSmith', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}), Document(page_content='You can also quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs.Monitoring\u200bAfter all this, your app might finally ready to go in production. LangSmith can also be used to monitor your application in much the same way that you used for debugging. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. Each run can also be', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}), Document(page_content='Skip to main content🦜️🛠️ LangSmith DocsPython DocsJS/TS DocsSearchGo to AppLangSmithOverviewTracingTesting & EvaluationOrganizationsHubLangSmith CookbookOverviewOn this pageLangSmith Overview and User GuideBuilding reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.Over the past two months, we at LangChain', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'}), Document(page_content='LangSmith makes it easy to manually review and annotate runs through annotation queues.These queues allow you to select any runs based on criteria like model type or automatic evaluation scores, and queue them up for human review. As a reviewer, you can then quickly step through the runs, viewing the input, output, and any existing tags before adding your own feedback.We often use this for a couple of reasons:To assess subjective qualities that automatic evaluators struggle with, like', metadata={'description': 'Building reliable LLM applications can be challenging. LangChain simplifies the initial setup, but there is still work needed to bring the performance of prompts, chains and agents up the level where they are reliable enough to be used in production.', 'language': 'en', 'source': 'https://docs.smith.langchain.com/overview', 'title': 'LangSmith Overview and User Guide | 🦜️🛠️ LangSmith'})]}
{'answer': ''}
{'answer': 'Lang'}
{'answer': 'Smith'}
{'answer': ' simpl'}
{'answer': 'ifies'}
{'answer': ' the'}
{'answer': ' initial'}
{'answer': ' setup'}
{'answer': ' for'}
{'answer': ' building'}
{'answer': ' reliable'}
{'answer': ' L'}
{'answer': 'LM'}
{'answer': ' applications'}
{'answer': '.'}
{'answer': ' It'}
{'answer': ' provides'}
{'answer': ' features'}
{'answer': ' for'}
{'answer': ' manually'}
{'answer': ' reviewing'}
{'answer': ' and'}
{'answer': ' annot'}
{'answer': 'ating'}
{'answer': ' runs'}
{'answer': ' through'}
{'answer': ' annotation'}
{'answer': ' queues'}
{'answer': ','}
{'answer': ' allowing'}
{'answer': ' you'}
{'answer': ' to'}
{'answer': ' select'}
{'answer': ' runs'}
{'answer': ' based'}
{'answer': ' on'}
{'answer': ' criteria'}
{'answer': ' like'}
{'answer': ' model'}
{'answer': ' type'}
{'answer': ' or'}
{'answer': ' automatic'}
{'answer': ' evaluation'}
{'answer': ' scores'}
{'answer': ','}
{'answer': ' and'}
{'answer': ' queue'}
{'answer': ' them'}
{'answer': ' up'}
{'answer': ' for'}
{'answer': ' human'}
{'answer': ' review'}
{'answer': '.'}
{'answer': ' As'}
{'answer': ' a'}
{'answer': ' reviewer'}
{'answer': ','}
{'answer': ' you'}
{'answer': ' can'}
{'answer': ' quickly'}
{'answer': ' step'}
{'answer': ' through'}
{'answer': ' the'}
{'answer': ' runs'}
{'answer': ','}
{'answer': ' view'}
{'answer': ' the'}
{'answer': ' input'}
{'answer': ','}
{'answer': ' output'}
{'answer': ','}
{'answer': ' and'}
{'answer': ' any'}
{'answer': ' existing'}
{'answer': ' tags'}
{'answer': ' before'}
{'answer': ' adding'}
{'answer': ' your'}
{'answer': ' own'}
{'answer': ' feedback'}
{'answer': '.'}
{'answer': ' This'}
{'answer': ' can'}
{'answer': ' be'}
{'answer': ' particularly'}
{'answer': ' useful'}
{'answer': ' for'}
{'answer': ' assessing'}
{'answer': ' subjective'}
{'answer': ' qualities'}
{'answer': ' that'}
{'answer': ' automatic'}
{'answer': ' evalu'}
{'answer': 'ators'}
{'answer': ' struggle'}
{'answer': ' with'}
{'answer': '.'}
{'answer': ''}


Further reading
​


This guide only scratches the surface of retrieval techniques. For more on different ways of ingesting, preparing, and retrieving the most relevant data, check out the relevant how-to guides 
here
.
Edit this page
Was this page helpful?
Previous
How to add default invocation args to a Runnable
Next
How to use few shot examples in chat models
Setup
Creating a retriever
Document chains
Retrieval chains
Query transformation
Streaming
Further reading


--- docs_how_to_chatbots_tools.txt ---

How-to guides
How to add tools to chatbots
On this page
How to add tools to chatbots


Prerequisites
This guide assumes familiarity with the following concepts:


Chatbots


Agents


Chat history




This section will cover how to create conversational agents: chatbots that can interact with other systems and APIs using tools.


note
This how-to guide previously built a chatbot using 
RunnableWithMessageHistory
. You can access this version of the guide in the 
v0.2 docs
.
As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of 
LangGraph persistence
 to incorporate 
memory
 into new LangChain applications.
If your code is already relying on 
RunnableWithMessageHistory
 or 
BaseChatMessageHistory
, you do 
not
 need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses 
RunnableWithMessageHistory
 will continue to work as expected.
Please see 
How to migrate to LangGraph Memory
 for more details.


Setup
​


For this guide, we'll be using a 
tool calling agent
 with a single tool for searching the web. The default will be powered by 
Tavily
, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.


You'll need to 
sign up for an account
 on the Tavily website, and install the following packages:


%
pip install 
-
-
upgrade 
-
-
quiet langchain
-
community langchain
-
openai tavily
-
python langgraph
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"OpenAI API Key:"
)
if
 
not
 os
.
environ
.
get
(
"TAVILY_API_KEY"
)
:
    os
.
environ
[
"TAVILY_API_KEY"
]
 
=
 getpass
.
getpass
(
"Tavily API Key:"
)


OpenAI API Key: ········
Tavily API Key: ········


You will also need your OpenAI key set as 
OPENAI_API_KEY
 and your Tavily API key set as 
TAVILY_API_KEY
.


Creating an agent
​


Our end goal is to create an agent that can respond conversationally to user questions while looking up information as needed.


First, let's initialize Tavily and an OpenAI 
chat model
 capable of tool calling:


from
 langchain_community
.
tools
.
tavily_search 
import
 TavilySearchResults
from
 langchain_openai 
import
 ChatOpenAI
tools 
=
 
[
TavilySearchResults
(
max_results
=
1
)
]
# Choose the LLM that will drive the agent
# Only certain models support this
model 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
API Reference:
TavilySearchResults
 | 
ChatOpenAI


To make our agent conversational, we can also specify a prompt. Here's an example:


prompt 
=
 
(
    
"You are a helpful assistant. "
    
"You may not need to use tools for every query - the user may just want to chat!"
)


Great! Now let's assemble our agent using LangGraph's prebuilt 
create_react_agent
, which allows you to create a 
tool-calling agent
:


from
 langgraph
.
prebuilt 
import
 create_react_agent
# prompt allows you to preprocess the inputs to the model inside ReAct agent
# in this case, since we're passing a prompt string, we'll just always add a SystemMessage
# with this prompt string before any other messages sent to the model
agent 
=
 create_react_agent
(
model
,
 tools
,
 prompt
=
prompt
)
API Reference:
create_react_agent


Running the agent
​


Now that we've set up our agent, let's try interacting with it! It can handle both trivial queries that require no lookup:


from
 langchain_core
.
messages 
import
 HumanMessage
agent
.
invoke
(
{
"messages"
:
 
[
HumanMessage
(
content
=
"I'm Nemo!"
)
]
}
)
API Reference:
HumanMessage


{'messages': [HumanMessage(content="I'm Nemo!", additional_kwargs={}, response_metadata={}, id='39e715c7-bd1c-426f-8e14-c05586b3d221'),
  AIMessage(content='Hi Nemo! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 107, 'total_tokens': 118, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-6937c944-d702-40bb-9a9f-4141ddde9f78-0', usage_metadata={'input_tokens': 107, 'output_tokens': 11, 'total_tokens': 118})]}


Or, it can use of the passed search tool to get up to date information if needed:


agent
.
invoke
(
    
{
        
"messages"
:
 
[
            HumanMessage
(
                content
=
"What is the current conservation status of the Great Barrier Reef?"
            
)
        
]
,
    
}
)


{'messages': [HumanMessage(content='What is the current conservation status of the Great Barrier Reef?', additional_kwargs={}, response_metadata={}, id='a74cc581-8ad5-4401-b3a5-f028d69e4b21'),
  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_aKOItwvAb4DHQCwaasKphGHq', 'function': {'arguments': '{"query":"current conservation status of the Great Barrier Reef 2023"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 116, 'total_tokens': 144, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-267ff8a8-d866-4ae5-9534-ad87ebbdc954-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current conservation status of the Great Barrier Reef 2023'}, 'id': 'call_aKOItwvAb4DHQCwaasKphGHq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 116, 'output_tokens': 28, 'total_tokens': 144}),
  ToolMessage(content='[{"url": "https://www.aims.gov.au/monitoring-great-barrier-reef/gbr-condition-summary-2023-24", "content": "This report summarises the condition of coral reefs in the Northern, Central and Southern\xa0Great Barrier Reef (GBR) from the Long-Term Monitoring Program (LTMP) surveys of 94 reefs conducted between August\xa02023 and June 2024 (reported as ‘2024’). Over the past 38 years of monitoring by the Australian Institute of Marine Science (AIMS), hard coral cover on reefs of the GBR has decreased and increased in response to cycles of disturbance and recovery. It is relatively rare for GBR reefs to have 75% to 100% hard coral cover and AIMS defines >30% – 50% hard coral cover as a high value, based on historical surveys across the GBR."}]', name='tavily_search_results_json', id='05b3fab7-9ac8-42bb-9612-ff2a896dbb67', tool_call_id='call_aKOItwvAb4DHQCwaasKphGHq', artifact={'query': 'current conservation status of the Great Barrier Reef 2023', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Annual Summary Report of Coral Reef Condition 2023/24', 'url': 'https://www.aims.gov.au/monitoring-great-barrier-reef/gbr-condition-summary-2023-24', 'content': 'This report summarises the condition of coral reefs in the Northern, Central and Southern\xa0Great Barrier Reef (GBR) from the Long-Term Monitoring Program (LTMP) surveys of 94 reefs conducted between August\xa02023 and June 2024 (reported as ‘2024’). Over the past 38 years of monitoring by the Australian Institute of Marine Science (AIMS), hard coral cover on reefs of the GBR has decreased and increased in response to cycles of disturbance and recovery. It is relatively rare for GBR reefs to have 75% to 100% hard coral cover and AIMS defines >30% – 50% hard coral cover as a high value, based on historical surveys across the GBR.', 'score': 0.95991266, 'raw_content': None}], 'response_time': 4.22}),
  AIMessage(content='The current conservation status of the Great Barrier Reef (GBR) indicates ongoing challenges and fluctuations in coral health. According to a report from the Australian Institute of Marine Science (AIMS), the condition of coral reefs in the GBR has been monitored over the years, showing cycles of disturbance and recovery. \n\nAs of the latest surveys conducted between August 2023 and June 2024, hard coral cover on the GBR has experienced both decreases and increases. AIMS defines a hard coral cover of over 30% to 50% as high value, but it is relatively rare for GBR reefs to achieve 75% to 100% hard coral cover.\n\nFor more detailed information, you can refer to the [AIMS report](https://www.aims.gov.au/monitoring-great-barrier-reef/gbr-condition-summary-2023-24).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 337, 'total_tokens': 511, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-bec32925-0dba-445d-8b55-87358ef482bb-0', usage_metadata={'input_tokens': 337, 'output_tokens': 174, 'total_tokens': 511})]}


Conversational responses
​


Because our prompt contains a placeholder for chat history messages, our agent can also take previous interactions into account and respond conversationally like a standard chatbot:


from
 langchain_core
.
messages 
import
 AIMessage
,
 HumanMessage
agent
.
invoke
(
    
{
        
"messages"
:
 
[
            HumanMessage
(
content
=
"I'm Nemo!"
)
,
            AIMessage
(
content
=
"Hello Nemo! How can I assist you today?"
)
,
            HumanMessage
(
content
=
"What is my name?"
)
,
        
]
,
    
}
)
API Reference:
AIMessage
 | 
HumanMessage


{'messages': [HumanMessage(content="I'm Nemo!", additional_kwargs={}, response_metadata={}, id='2c8e58bf-ad20-45a4-940b-84393c6b3a03'),
  AIMessage(content='Hello Nemo! How can I assist you today?', additional_kwargs={}, response_metadata={}, id='5e014114-7e9d-42c3-b63e-a662b3a49bef'),
  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='d92be4e1-6497-4037-9a9a-83d3e7b760d5'),
  AIMessage(content='Your name is Nemo!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 130, 'total_tokens': 136, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-17db96f8-8dbd-4f25-a80d-e4e872967641-0', usage_metadata={'input_tokens': 130, 'output_tokens': 6, 'total_tokens': 136})]}


If preferred, you can also add memory to the LangGraph agent to manage the history of messages. Let's redeclare it this way:


from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
memory 
=
 MemorySaver
(
)
agent 
=
 create_react_agent
(
model
,
 tools
,
 prompt
=
prompt
,
 checkpointer
=
memory
)
API Reference:
MemorySaver


agent
.
invoke
(
    
{
"messages"
:
 
[
HumanMessage
(
"I'm Nemo!"
)
]
}
,
    config
=
{
"configurable"
:
 
{
"thread_id"
:
 
"1"
}
}
,
)


{'messages': [HumanMessage(content="I'm Nemo!", additional_kwargs={}, response_metadata={}, id='117b2cfc-c6cc-449c-bba9-26fc545d0afa'),
  AIMessage(content='Hi Nemo! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 107, 'total_tokens': 118, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba16cc0b-fba1-4ec5-9d99-e010c3b702d0-0', usage_metadata={'input_tokens': 107, 'output_tokens': 11, 'total_tokens': 118})]}


And then if we rerun our wrapped agent executor:


agent
.
invoke
(
    
{
"messages"
:
 
[
HumanMessage
(
"What is my name?"
)
]
}
,
    config
=
{
"configurable"
:
 
{
"thread_id"
:
 
"1"
}
}
,
)


{'messages': [HumanMessage(content="I'm Nemo!", additional_kwargs={}, response_metadata={}, id='117b2cfc-c6cc-449c-bba9-26fc545d0afa'),
  AIMessage(content='Hi Nemo! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 107, 'total_tokens': 118, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba16cc0b-fba1-4ec5-9d99-e010c3b702d0-0', usage_metadata={'input_tokens': 107, 'output_tokens': 11, 'total_tokens': 118}),
  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='53ac8d34-99bb-43a7-9103-80e26b7ee6cc'),
  AIMessage(content='Your name is Nemo!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 130, 'total_tokens': 136, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-b3f224a5-902a-4973-84ff-9b683615b0e2-0', usage_metadata={'input_tokens': 130, 'output_tokens': 6, 'total_tokens': 136})]}


This 
LangSmith trace
 shows what's going on under the hood.


Further reading
​


For more on how to build agents, check these 
LangGraph
 guides:




agents conceptual guide


agents tutorials


create_react_agent




For more on tool usage, you can also check out 
this use case section
.
Edit this page
Was this page helpful?
Previous
How to track token usage in ChatModels
Next
How to split code
Setup
Creating an agent
Running the agent
Conversational responses
Further reading


--- docs_how_to_code_splitter.txt ---

How-to guides
How to split code
On this page
How to split code


RecursiveCharacterTextSplitter
 includes pre-built lists of separators that are useful for 
splitting text
 in a specific programming language.


Supported languages are stored in the 
langchain_text_splitters.Language
 enum. They include:


"cpp",
"go",
"java",
"kotlin",
"js",
"ts",
"php",
"proto",
"python",
"rst",
"ruby",
"rust",
"scala",
"swift",
"markdown",
"latex",
"html",
"sol",
"csharp",
"cobol",
"c",
"lua",
"perl",
"haskell"


To view the list of separators for a given language, pass a value from this enum into


RecursiveCharacterTextSplitter
.
get_separators_for_language


To instantiate a splitter that is tailored for a specific language, pass a value from the enum into


RecursiveCharacterTextSplitter
.
from_language


Below we demonstrate examples for the various languages.


%
pip install 
-
qU langchain
-
text
-
splitters


from
 langchain_text_splitters 
import
 
(
    Language
,
    RecursiveCharacterTextSplitter
,
)
API Reference:
Language
 | 
RecursiveCharacterTextSplitter


To view the full list of supported languages:


[
e
.
value 
for
 e 
in
 Language
]


['cpp',
 'go',
 'java',
 'kotlin',
 'js',
 'ts',
 'php',
 'proto',
 'python',
 'rst',
 'ruby',
 'rust',
 'scala',
 'swift',
 'markdown',
 'latex',
 'html',
 'sol',
 'csharp',
 'cobol',
 'c',
 'lua',
 'perl',
 'haskell']


You can also see the separators used for a given language:


RecursiveCharacterTextSplitter
.
get_separators_for_language
(
Language
.
PYTHON
)


['\nclass ', '\ndef ', '\n\tdef ', '\n\n', '\n', ' ', '']


Python
​


Here's an example using the PythonTextSplitter:


PYTHON_CODE 
=
 
"""
def hello_world():
    print("Hello, World!")
# Call the function
hello_world()
"""
python_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
PYTHON
,
 chunk_size
=
50
,
 chunk_overlap
=
0
)
python_docs 
=
 python_splitter
.
create_documents
(
[
PYTHON_CODE
]
)
python_docs


[Document(page_content='def hello_world():\n    print("Hello, World!")'),
 Document(page_content='# Call the function\nhello_world()')]


JS
​


Here's an example using the JS text splitter:


JS_CODE 
=
 
"""
function helloWorld() {
  console.log("Hello, World!");
}
// Call the function
helloWorld();
"""
js_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
JS
,
 chunk_size
=
60
,
 chunk_overlap
=
0
)
js_docs 
=
 js_splitter
.
create_documents
(
[
JS_CODE
]
)
js_docs


[Document(page_content='function helloWorld() {\n  console.log("Hello, World!");\n}'),
 Document(page_content='// Call the function\nhelloWorld();')]


TS
​


Here's an example using the TS text splitter:


TS_CODE 
=
 
"""
function helloWorld(): void {
  console.log("Hello, World!");
}
// Call the function
helloWorld();
"""
ts_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
TS
,
 chunk_size
=
60
,
 chunk_overlap
=
0
)
ts_docs 
=
 ts_splitter
.
create_documents
(
[
TS_CODE
]
)
ts_docs


[Document(page_content='function helloWorld(): void {'),
 Document(page_content='console.log("Hello, World!");\n}'),
 Document(page_content='// Call the function\nhelloWorld();')]


Markdown
​


Here's an example using the Markdown text splitter:


markdown_text 
=
 
"""
# 🦜️🔗 LangChain
⚡ Building applications with LLMs through composability ⚡
## What is LangChain?
# Hopefully this code block isn't split
LangChain is a framework for...
As an open-source project in a rapidly developing field, we are extremely open to contributions.
"""


md_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
MARKDOWN
,
 chunk_size
=
60
,
 chunk_overlap
=
0
)
md_docs 
=
 md_splitter
.
create_documents
(
[
markdown_text
]
)
md_docs


[Document(metadata={}, page_content='# 🦜️🔗 LangChain'),
 Document(metadata={}, page_content='⚡ Building applications with LLMs through composability ⚡'),
 Document(metadata={}, page_content='## What is LangChain?'),
 Document(metadata={}, page_content="# Hopefully this code block isn't split"),
 Document(metadata={}, page_content='LangChain is a framework for...'),
 Document(metadata={}, page_content='As an open-source project in a rapidly developing field, we'),
 Document(metadata={}, page_content='are extremely open to contributions.')]


Latex
​


Here's an example on Latex text:


latex_text 
=
 
"""
\documentclass{article}
\begin{document}
\maketitle
\section{Introduction}
Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis.
\subsection{History of LLMs}
The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.
\subsection{Applications of LLMs}
LLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.
\end{document}
"""


latex_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
MARKDOWN
,
 chunk_size
=
60
,
 chunk_overlap
=
0
)
latex_docs 
=
 latex_splitter
.
create_documents
(
[
latex_text
]
)
latex_docs


[Document(page_content='\\documentclass{article}\n\n\x08egin{document}\n\n\\maketitle'),
 Document(page_content='\\section{Introduction}'),
 Document(page_content='Large language models (LLMs) are a type of machine learning'),
 Document(page_content='model that can be trained on vast amounts of text data to'),
 Document(page_content='generate human-like language. In recent years, LLMs have'),
 Document(page_content='made significant advances in a variety of natural language'),
 Document(page_content='processing tasks, including language translation, text'),
 Document(page_content='generation, and sentiment analysis.'),
 Document(page_content='\\subsection{History of LLMs}'),
 Document(page_content='The earliest LLMs were developed in the 1980s and 1990s,'),
 Document(page_content='but they were limited by the amount of data that could be'),
 Document(page_content='processed and the computational power available at the'),
 Document(page_content='time. In the past decade, however, advances in hardware and'),
 Document(page_content='software have made it possible to train LLMs on massive'),
 Document(page_content='datasets, leading to significant improvements in'),
 Document(page_content='performance.'),
 Document(page_content='\\subsection{Applications of LLMs}'),
 Document(page_content='LLMs have many applications in industry, including'),
 Document(page_content='chatbots, content creation, and virtual assistants. They'),
 Document(page_content='can also be used in academia for research in linguistics,'),
 Document(page_content='psychology, and computational linguistics.'),
 Document(page_content='\\end{document}')]


HTML
​


Here's an example using an HTML text splitter:


html_text 
=
 
"""
<!DOCTYPE html>
<html>
    <head>
        <title>🦜️🔗 LangChain</title>
        <style>
            body {
                font-family: Arial, sans-serif;
            }
            h1 {
                color: darkblue;
            }
        </style>
    </head>
    <body>
        <div>
            <h1>🦜️🔗 LangChain</h1>
            <p>⚡ Building applications with LLMs through composability ⚡</p>
        </div>
        <div>
            As an open-source project in a rapidly developing field, we are extremely open to contributions.
        </div>
    </body>
</html>
"""


html_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
HTML
,
 chunk_size
=
60
,
 chunk_overlap
=
0
)
html_docs 
=
 html_splitter
.
create_documents
(
[
html_text
]
)
html_docs


[Document(page_content='<!DOCTYPE html>\n<html>'),
 Document(page_content='<head>\n        <title>🦜️🔗 LangChain</title>'),
 Document(page_content='<style>\n            body {\n                font-family: Aria'),
 Document(page_content='l, sans-serif;\n            }\n            h1 {'),
 Document(page_content='color: darkblue;\n            }\n        </style>\n    </head'),
 Document(page_content='>'),
 Document(page_content='<body>'),
 Document(page_content='<div>\n            <h1>🦜️🔗 LangChain</h1>'),
 Document(page_content='<p>⚡ Building applications with LLMs through composability ⚡'),
 Document(page_content='</p>\n        </div>'),
 Document(page_content='<div>\n            As an open-source project in a rapidly dev'),
 Document(page_content='eloping field, we are extremely open to contributions.'),
 Document(page_content='</div>\n    </body>\n</html>')]


Solidity
​


Here's an example using the Solidity text splitter:


SOL_CODE 
=
 
"""
pragma solidity ^0.8.20;
contract HelloWorld {
   function add(uint a, uint b) pure public returns(uint) {
       return a + b;
   }
}
"""
sol_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
SOL
,
 chunk_size
=
128
,
 chunk_overlap
=
0
)
sol_docs 
=
 sol_splitter
.
create_documents
(
[
SOL_CODE
]
)
sol_docs


[Document(page_content='pragma solidity ^0.8.20;'),
 Document(page_content='contract HelloWorld {\n   function add(uint a, uint b) pure public returns(uint) {\n       return a + b;\n   }\n}')]


C#
​


Here's an example using the C# text splitter:


C_CODE 
=
 
"""
using System;
class Program
{
    static void Main()
    {
        int age = 30; // Change the age value as needed
        // Categorize the age without any console output
        if (age < 18)
        {
            // Age is under 18
        }
        else if (age >= 18 && age < 65)
        {
            // Age is an adult
        }
        else
        {
            // Age is a senior citizen
        }
    }
}
"""
c_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
CSHARP
,
 chunk_size
=
128
,
 chunk_overlap
=
0
)
c_docs 
=
 c_splitter
.
create_documents
(
[
C_CODE
]
)
c_docs


[Document(page_content='using System;'),
 Document(page_content='class Program\n{\n    static void Main()\n    {\n        int age = 30; // Change the age value as needed'),
 Document(page_content='// Categorize the age without any console output\n        if (age < 18)\n        {\n            // Age is under 18'),
 Document(page_content='}\n        else if (age >= 18 && age < 65)\n        {\n            // Age is an adult\n        }\n        else\n        {'),
 Document(page_content='// Age is a senior citizen\n        }\n    }\n}')]


Haskell
​


Here's an example using the Haskell text splitter:


HASKELL_CODE 
=
 
"""
main :: IO ()
main = do
    putStrLn "Hello, World!"
-- Some sample functions
add :: Int -> Int -> Int
add x y = x + y
"""
haskell_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
HASKELL
,
 chunk_size
=
50
,
 chunk_overlap
=
0
)
haskell_docs 
=
 haskell_splitter
.
create_documents
(
[
HASKELL_CODE
]
)
haskell_docs


[Document(page_content='main :: IO ()'),
 Document(page_content='main = do\n    putStrLn "Hello, World!"\n-- Some'),
 Document(page_content='sample functions\nadd :: Int -> Int -> Int\nadd x y'),
 Document(page_content='= x + y')]


PHP
​


Here's an example using the PHP text splitter:


PHP_CODE 
=
 
"""<?php
namespace foo;
class Hello {
    public function __construct() { }
}
function hello() {
    echo "Hello World!";
}
interface Human {
    public function breath();
}
trait Foo { }
enum Color
{
    case Red;
    case Blue;
}"""
php_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
PHP
,
 chunk_size
=
50
,
 chunk_overlap
=
0
)
php_docs 
=
 php_splitter
.
create_documents
(
[
PHP_CODE
]
)
php_docs


[Document(page_content='<?php\nnamespace foo;'),
 Document(page_content='class Hello {'),
 Document(page_content='public function __construct() { }\n}'),
 Document(page_content='function hello() {\n    echo "Hello World!";\n}'),
 Document(page_content='interface Human {\n    public function breath();\n}'),
 Document(page_content='trait Foo { }\nenum Color\n{\n    case Red;'),
 Document(page_content='case Blue;\n}')]


PowerShell
​


Here's an example using the PowerShell text splitter:


POWERSHELL_CODE 
=
 
"""
$directoryPath = Get-Location
$items = Get-ChildItem -Path $directoryPath
$files = $items | Where-Object { -not $_.PSIsContainer }
$sortedFiles = $files | Sort-Object LastWriteTime
foreach ($file in $sortedFiles) {
    Write-Output ("Name: " + $file.Name + " | Last Write Time: " + $file.LastWriteTime)
}
"""
powershell_splitter 
=
 RecursiveCharacterTextSplitter
.
from_language
(
    language
=
Language
.
POWERSHELL
,
 chunk_size
=
100
,
 chunk_overlap
=
0
)
powershell_docs 
=
 powershell_splitter
.
create_documents
(
[
POWERSHELL_CODE
]
)
powershell_docs
Edit this page
Was this page helpful?
Previous
How to add tools to chatbots
Next
How to do retrieval with contextual compression
Python
JS
TS
Markdown
Latex
HTML
Solidity
C#
Haskell
PHP
PowerShell


--- docs_how_to_configure.txt ---

How-to guides
How to configure runtime chain internals
On this page
How to configure runtime chain internals


Prerequisites
This guide assumes familiarity with the following concepts:


The Runnable interface


Chaining runnables


Binding runtime arguments




Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things within your chains.
This can include tweaking parameters such as temperature or even swapping out one model for another.
In order to make this experience as easy as possible, we have defined two methods.




A 
configurable_fields
 method. This lets you configure particular fields of a runnable.




This is related to the 
.bind
 method on runnables, but allows you to specify parameters for a given step in a chain at runtime rather than specifying them beforehand.






A 
configurable_alternatives
 method. With this method, you can list out alternatives for any particular runnable that can be set during runtime, and swap them for those specified alternatives.




Configurable Fields
​


Let's walk through an example that configures chat model fields like temperature at runtime:


%
pip install 
-
-
upgrade 
-
-
quiet langchain langchain
-
openai
import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


Configuring fields on a chat model
​


If using 
init_chat_model
 to create a chat model, you can specify configurable fields in the constructor:


from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
    
"openai:gpt-4o-mini"
,
    configurable_fields
=
(
"temperature"
,
)
,
)
API Reference:
init_chat_model


You can then set the parameter at runtime using 
.with_config
:


response 
=
 llm
.
with_config
(
{
"temperature"
:
 
0
}
)
.
invoke
(
"Hello"
)
print
(
response
.
content
)


Hello! How can I assist you today?


tip
In addition to invocation parameters like temperature, configuring fields this way extends to clients and other attributes.


Use with tools
​


This method is applicable when 
binding tools
 as well:


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
get_weather
(
location
:
 
str
)
:
    
"""Get the weather."""
    
return
 
"It's sunny."
llm_with_tools 
=
 llm
.
bind_tools
(
[
get_weather
]
)
response 
=
 llm_with_tools
.
with_config
(
{
"temperature"
:
 
0
}
)
.
invoke
(
    
"What's the weather in SF?"
)
response
.
tool_calls
API Reference:
tool


[{'name': 'get_weather',
  'args': {'location': 'San Francisco'},
  'id': 'call_B93EttzlGyYUhzbIIiMcl3bE',
  'type': 'tool_call'}]


In addition to 
.with_config
, we can now include the parameter when passing a configuration directly. See example below, where we allow the underlying model temperature to be configurable inside of a 
langgraph agent
:


! pip install 
-
-
upgrade langgraph


from
 langgraph
.
prebuilt 
import
 create_react_agent
agent 
=
 create_react_agent
(
llm
,
 
[
get_weather
]
)
response 
=
 agent
.
invoke
(
    
{
"messages"
:
 
"What's the weather in Boston?"
}
,
    
{
"configurable"
:
 
{
"temperature"
:
 
0
}
}
,
)
API Reference:
create_react_agent


Configuring fields on arbitrary Runnables
​


You can also use the 
.configurable_fields
 method on arbitrary 
Runnables
, as shown below:


from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_core
.
runnables 
import
 ConfigurableField
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
temperature
=
0
)
.
configurable_fields
(
    temperature
=
ConfigurableField
(
        
id
=
"llm_temperature"
,
        name
=
"LLM Temperature"
,
        description
=
"The temperature of the LLM"
,
    
)
)
model
.
invoke
(
"pick a random number"
)
API Reference:
PromptTemplate
 | 
ConfigurableField
 | 
ChatOpenAI


AIMessage(content='17', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba26a0da-0a69-4533-ab7f-21178a73d303-0')


Above, we defined 
temperature
 as a 
ConfigurableField
 that we can set at runtime. To do so, we use the 
with_config
 method like this:


model
.
with_config
(
configurable
=
{
"llm_temperature"
:
 
0.9
}
)
.
invoke
(
"pick a random number"
)


AIMessage(content='12', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 11, 'total_tokens': 12}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ba8422ad-be77-4cb1-ac45-ad0aae74e3d9-0')


Note that the passed 
llm_temperature
 entry in the dict has the same key as the 
id
 of the 
ConfigurableField
.


We can also do this to affect just one step that's part of a chain:


prompt 
=
 PromptTemplate
.
from_template
(
"Pick a random number above {x}"
)
chain 
=
 prompt 
|
 model
chain
.
invoke
(
{
"x"
:
 
0
}
)


AIMessage(content='27', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-ecd4cadd-1b72-4f92-b9a0-15e08091f537-0')


chain
.
with_config
(
configurable
=
{
"llm_temperature"
:
 
0.9
}
)
.
invoke
(
{
"x"
:
 
0
}
)


AIMessage(content='35', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 14, 'total_tokens': 15}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-a916602b-3460-46d3-a4a8-7c926ec747c0-0')


Configurable Alternatives
​


The 
configurable_alternatives()
 method allows us to swap out steps in a chain with an alternative. Below, we swap out one chat model for another:


%
pip install 
-
-
upgrade 
-
-
quiet langchain
-
anthropic
import
 os
from
 getpass 
import
 getpass
if
 
"ANTHROPIC_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"ANTHROPIC_API_KEY"
]
 
=
 getpass
(
)


[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.
You should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m
[0mNote: you may need to restart the kernel to use updated packages.


from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_core
.
runnables 
import
 ConfigurableField
from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatAnthropic
(
    model
=
"claude-3-haiku-20240307"
,
 temperature
=
0
)
.
configurable_alternatives
(
    
# This gives this field an id
    
# When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField
(
id
=
"llm"
)
,
    
# This sets a default_key.
    
# If we specify this key, the default LLM (ChatAnthropic initialized above) will be used
    default_key
=
"anthropic"
,
    
# This adds a new option, with name `openai` that is equal to `ChatOpenAI()`
    openai
=
ChatOpenAI
(
)
,
    
# This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model="gpt-4")`
    gpt4
=
ChatOpenAI
(
model
=
"gpt-4"
)
,
    
# You can add more configuration options here
)
prompt 
=
 PromptTemplate
.
from_template
(
"Tell me a joke about {topic}"
)
chain 
=
 prompt 
|
 llm
# By default it will call Anthropic
chain
.
invoke
(
{
"topic"
:
 
"bears"
}
)
API Reference:
ChatAnthropic
 | 
PromptTemplate
 | 
ConfigurableField
 | 
ChatOpenAI


AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!\n\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!", response_metadata={'id': 'msg_018edUHh5fUbWdiimhrC3dZD', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-775bc58c-28d7-4e6b-a268-48fa6661f02f-0')


# We can use `.with_config(configurable={"llm": "openai"})` to specify an llm to use
chain
.
with_config
(
configurable
=
{
"llm"
:
 
"openai"
}
)
.
invoke
(
{
"topic"
:
 
"bears"
}
)


AIMessage(content="Why don't bears like fast food?\n\nBecause they can't catch it!", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-7bdaa992-19c9-4f0d-9a0c-1f326bc992d4-0')


# If we use the `default_key` then it uses the default
chain
.
with_config
(
configurable
=
{
"llm"
:
 
"anthropic"
}
)
.
invoke
(
{
"topic"
:
 
"bears"
}
)


AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!\n\nHow's that? I tried to come up with a simple, silly pun-based joke about bears. Puns and wordplay are a common way to create humorous bear jokes. Let me know if you'd like to hear another one!", response_metadata={'id': 'msg_01BZvbmnEPGBtcxRWETCHkct', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 80}}, id='run-59b6ee44-a1cd-41b8-a026-28ee67cdd718-0')


With Prompts
​


We can do a similar thing, but alternate between prompts


llm 
=
 ChatAnthropic
(
model
=
"claude-3-haiku-20240307"
,
 temperature
=
0
)
prompt 
=
 PromptTemplate
.
from_template
(
    
"Tell me a joke about {topic}"
)
.
configurable_alternatives
(
    
# This gives this field an id
    
# When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField
(
id
=
"prompt"
)
,
    
# This sets a default_key.
    
# If we specify this key, the default prompt (asking for a joke, as initialized above) will be used
    default_key
=
"joke"
,
    
# This adds a new option, with name `poem`
    poem
=
PromptTemplate
.
from_template
(
"Write a short poem about {topic}"
)
,
    
# You can add more configuration options here
)
chain 
=
 prompt 
|
 llm
# By default it will write a joke
chain
.
invoke
(
{
"topic"
:
 
"bears"
}
)


AIMessage(content="Here's a bear joke for you:\n\nWhy don't bears wear socks? \nBecause they have bear feet!", response_metadata={'id': 'msg_01DtM1cssjNFZYgeS3gMZ49H', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 28}}, id='run-8199af7d-ea31-443d-b064-483693f2e0a1-0')


# We can configure it write a poem
chain
.
with_config
(
configurable
=
{
"prompt"
:
 
"poem"
}
)
.
invoke
(
{
"topic"
:
 
"bears"
}
)


AIMessage(content="Here is a short poem about bears:\n\nMajestic bears, strong and true,\nRoaming the forests, wild and free.\nPowerful paws, fur soft and brown,\nCommanding respect, nature's crown.\n\nForaging for berries, fishing streams,\nProtecting their young, fierce and keen.\nMighty bears, a sight to behold,\nGuardians of the wilderness, untold.\n\nIn the wild they reign supreme,\nEmbodying nature's grand theme.\nBears, a symbol of strength and grace,\nCaptivating all who see their face.", response_metadata={'id': 'msg_01Wck3qPxrjURtutvtodaJFn', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 13, 'output_tokens': 134}}, id='run-69414a1e-51d7-4bec-a307-b34b7d61025e-0')


With Prompts and LLMs
​


We can also have multiple things configurable!
Here's an example doing that with both prompts and LLMs.


llm 
=
 ChatAnthropic
(
    model
=
"claude-3-haiku-20240307"
,
 temperature
=
0
)
.
configurable_alternatives
(
    
# This gives this field an id
    
# When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField
(
id
=
"llm"
)
,
    
# This sets a default_key.
    
# If we specify this key, the default LLM (ChatAnthropic initialized above) will be used
    default_key
=
"anthropic"
,
    
# This adds a new option, with name `openai` that is equal to `ChatOpenAI()`
    openai
=
ChatOpenAI
(
)
,
    
# This adds a new option, with name `gpt4` that is equal to `ChatOpenAI(model="gpt-4")`
    gpt4
=
ChatOpenAI
(
model
=
"gpt-4"
)
,
    
# You can add more configuration options here
)
prompt 
=
 PromptTemplate
.
from_template
(
    
"Tell me a joke about {topic}"
)
.
configurable_alternatives
(
    
# This gives this field an id
    
# When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField
(
id
=
"prompt"
)
,
    
# This sets a default_key.
    
# If we specify this key, the default prompt (asking for a joke, as initialized above) will be used
    default_key
=
"joke"
,
    
# This adds a new option, with name `poem`
    poem
=
PromptTemplate
.
from_template
(
"Write a short poem about {topic}"
)
,
    
# You can add more configuration options here
)
chain 
=
 prompt 
|
 llm
# We can configure it write a poem with OpenAI
chain
.
with_config
(
configurable
=
{
"prompt"
:
 
"poem"
,
 
"llm"
:
 
"openai"
}
)
.
invoke
(
    
{
"topic"
:
 
"bears"
}
)


AIMessage(content="In the forest deep and wide,\nBears roam with grace and pride.\nWith fur as dark as night,\nThey rule the land with all their might.\n\nIn winter's chill, they hibernate,\nIn spring they emerge, hungry and great.\nWith claws sharp and eyes so keen,\nThey hunt for food, fierce and lean.\n\nBut beneath their tough exterior,\nLies a gentle heart, warm and superior.\nThey love their cubs with all their might,\nProtecting them through day and night.\n\nSo let us admire these majestic creatures,\nIn awe of their strength and features.\nFor in the wild, they reign supreme,\nThe mighty bears, a timeless dream.", response_metadata={'token_usage': {'completion_tokens': 133, 'prompt_tokens': 13, 'total_tokens': 146}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-5eec0b96-d580-49fd-ac4e-e32a0803b49b-0')


# We can always just configure only one if we want
chain
.
with_config
(
configurable
=
{
"llm"
:
 
"openai"
}
)
.
invoke
(
{
"topic"
:
 
"bears"
}
)


AIMessage(content="Why don't bears wear shoes?\n\nBecause they have bear feet!", response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 26}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-c1b14c9c-4988-49b8-9363-15bfd479973a-0')


Saving configurations
​


We can also easily save configured chains as their own objects


openai_joke 
=
 chain
.
with_config
(
configurable
=
{
"llm"
:
 
"openai"
}
)
openai_joke
.
invoke
(
{
"topic"
:
 
"bears"
}
)


AIMessage(content="Why did the bear break up with his girlfriend? \nBecause he couldn't bear the relationship anymore!", response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-391ebd55-9137-458b-9a11-97acaff6a892-0')


Next steps
​


You now know how to configure a chain's internal steps at runtime.


To learn more, see the other how-to guides on runnables in this section, including:




Using 
.bind()
 as a simpler way to set a runnable's runtime parameters


Edit this page
Was this page helpful?
Previous
How to construct filters for query analysis
Next
How deal with high cardinality categoricals when doing query analysis
Configurable Fields
Configuring fields on a chat model
Configuring fields on arbitrary Runnables
Configurable Alternatives
With Prompts
With Prompts and LLMs
Saving configurations
Next steps


--- docs_how_to_contextual_compression.txt ---

How-to guides
How to do retrieval with contextual compression
On this page
How to do retrieval with contextual compression


One challenge with 
retrieval
 is that usually you don't know the specific queries your document storage system will face when you ingest data into the system. This means that the information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.


Contextual compression is meant to fix this. The idea is simple: instead of immediately returning retrieved documents as-is, you can compress them using the context of the given query, so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.


To use the Contextual Compression Retriever, you'll need:




a base 
retriever


a Document Compressor




The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether.


Get started
​


# Helper function for printing docs
def
 
pretty_print_docs
(
docs
)
:
    
print
(
        
f"\n
{
'-'
 
*
 
100
}
\n"
.
join
(
            
[
f"Document 
{
i
+
1
}
:\n\n"
 
+
 d
.
page_content 
for
 i
,
 d 
in
 
enumerate
(
docs
)
]
        
)
    
)


Using a vanilla vector store retriever
​


Let's start by initializing a simple vector store retriever and storing the 2023 State of the Union speech (in chunks). We can see that given an example question our retriever returns one or two relevant docs and a few irrelevant docs. And even the relevant docs have a lot of irrelevant information in them.


from
 langchain_community
.
document_loaders 
import
 TextLoader
from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 CharacterTextSplitter
documents 
=
 TextLoader
(
"state_of_the_union.txt"
)
.
load
(
)
text_splitter 
=
 CharacterTextSplitter
(
chunk_size
=
1000
,
 chunk_overlap
=
0
)
texts 
=
 text_splitter
.
split_documents
(
documents
)
retriever 
=
 FAISS
.
from_documents
(
texts
,
 OpenAIEmbeddings
(
)
)
.
as_retriever
(
)
docs 
=
 retriever
.
invoke
(
"What did the president say about Ketanji Brown Jackson"
)
pretty_print_docs
(
docs
)
API Reference:
TextLoader
 | 
FAISS
 | 
OpenAIEmbeddings
 | 
CharacterTextSplitter


Document 1:
Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 
Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 
One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 
And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.
----------------------------------------------------------------------------------------------------
Document 2:
A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. 
And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. 
We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  
We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  
We’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. 
We’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.
----------------------------------------------------------------------------------------------------
Document 3:
And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. 
As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. 
While it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. 
And soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. 
So tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  
First, beat the opioid epidemic.
----------------------------------------------------------------------------------------------------
Document 4:
Tonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. 
And as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  
That ends on my watch. 
Medicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. 
We’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. 
Let’s pass the Paycheck Fairness Act and paid leave.  
Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. 
Let’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.


Adding contextual compression with an 
LLMChainExtractor
​


Now let's wrap our base retriever with a 
ContextualCompressionRetriever
. We'll add an 
LLMChainExtractor
, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query.


from
 langchain
.
retrievers 
import
 ContextualCompressionRetriever
from
 langchain
.
retrievers
.
document_compressors 
import
 LLMChainExtractor
from
 langchain_openai 
import
 OpenAI
llm 
=
 OpenAI
(
temperature
=
0
)
compressor 
=
 LLMChainExtractor
.
from_llm
(
llm
)
compression_retriever 
=
 ContextualCompressionRetriever
(
    base_compressor
=
compressor
,
 base_retriever
=
retriever
)
compressed_docs 
=
 compression_retriever
.
invoke
(
    
"What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs
(
compressed_docs
)
API Reference:
ContextualCompressionRetriever
 | 
LLMChainExtractor
 | 
OpenAI


Document 1:
I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson.


More built-in compressors: filters
​


LLMChainFilter
​


The 
LLMChainFilter
 is slightly simpler but more robust compressor that uses an LLM chain to decide which of the initially retrieved documents to filter out and which ones to return, without manipulating the document contents.


from
 langchain
.
retrievers
.
document_compressors 
import
 LLMChainFilter
_filter 
=
 LLMChainFilter
.
from_llm
(
llm
)
compression_retriever 
=
 ContextualCompressionRetriever
(
    base_compressor
=
_filter
,
 base_retriever
=
retriever
)
compressed_docs 
=
 compression_retriever
.
invoke
(
    
"What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs
(
compressed_docs
)
API Reference:
LLMChainFilter


Document 1:
Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 
Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 
One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 
And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.


LLMListwiseRerank
​


LLMListwiseRerank
 uses 
zero-shot listwise document reranking
 and functions similarly to 
LLMChainFilter
 as a robust but more expensive option. It is recommended to use a more powerful LLM.


Note that 
LLMListwiseRerank
 requires a model with the 
with_structured_output
 method implemented.


from
 langchain
.
retrievers
.
document_compressors 
import
 LLMListwiseRerank
from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
_filter 
=
 LLMListwiseRerank
.
from_llm
(
llm
,
 top_n
=
1
)
compression_retriever 
=
 ContextualCompressionRetriever
(
    base_compressor
=
_filter
,
 base_retriever
=
retriever
)
compressed_docs 
=
 compression_retriever
.
invoke
(
    
"What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs
(
compressed_docs
)
API Reference:
LLMListwiseRerank
 | 
ChatOpenAI


Document 1:
Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 
Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 
One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 
And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.


EmbeddingsFilter
​


Making an extra LLM call over each retrieved document is expensive and slow. The 
EmbeddingsFilter
 provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query.


from
 langchain
.
retrievers
.
document_compressors 
import
 EmbeddingsFilter
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
)
embeddings_filter 
=
 EmbeddingsFilter
(
embeddings
=
embeddings
,
 similarity_threshold
=
0.76
)
compression_retriever 
=
 ContextualCompressionRetriever
(
    base_compressor
=
embeddings_filter
,
 base_retriever
=
retriever
)
compressed_docs 
=
 compression_retriever
.
invoke
(
    
"What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs
(
compressed_docs
)
API Reference:
EmbeddingsFilter
 | 
OpenAIEmbeddings


Document 1:
Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 
Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 
One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 
And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.
----------------------------------------------------------------------------------------------------
Document 2:
A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. 
And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. 
We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  
We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  
We’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. 
We’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.


Stringing compressors and document transformers together
​


Using the 
DocumentCompressorPipeline
 we can also easily combine multiple compressors in sequence. Along with compressors we can add 
BaseDocumentTransformer
s to our pipeline, which don't perform any contextual compression but simply perform some transformation on a set of documents. For example 
TextSplitter
s can be used as document transformers to split documents into smaller pieces, and the 
EmbeddingsRedundantFilter
 can be used to filter out redundant documents based on embedding similarity between documents.


Below we create a compressor pipeline by first splitting our docs into smaller chunks, then removing redundant documents, and then filtering based on relevance to the query.


from
 langchain
.
retrievers
.
document_compressors 
import
 DocumentCompressorPipeline
from
 langchain_community
.
document_transformers 
import
 EmbeddingsRedundantFilter
from
 langchain_text_splitters 
import
 CharacterTextSplitter
splitter 
=
 CharacterTextSplitter
(
chunk_size
=
300
,
 chunk_overlap
=
0
,
 separator
=
". "
)
redundant_filter 
=
 EmbeddingsRedundantFilter
(
embeddings
=
embeddings
)
relevant_filter 
=
 EmbeddingsFilter
(
embeddings
=
embeddings
,
 similarity_threshold
=
0.76
)
pipeline_compressor 
=
 DocumentCompressorPipeline
(
    transformers
=
[
splitter
,
 redundant_filter
,
 relevant_filter
]
)
API Reference:
DocumentCompressorPipeline
 | 
EmbeddingsRedundantFilter
 | 
CharacterTextSplitter


compression_retriever 
=
 ContextualCompressionRetriever
(
    base_compressor
=
pipeline_compressor
,
 base_retriever
=
retriever
)
compressed_docs 
=
 compression_retriever
.
invoke
(
    
"What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs
(
compressed_docs
)


Document 1:
One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 
And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson
----------------------------------------------------------------------------------------------------
Document 2:
As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. 
While it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year
----------------------------------------------------------------------------------------------------
Document 3:
A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder
----------------------------------------------------------------------------------------------------
Document 4:
Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. 
And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. 
We can do both
Edit this page
Was this page helpful?
Previous
How to split code
Next
How to convert Runnables to Tools
Get started
Using a vanilla vector store retriever
Adding contextual compression with an 
LLMChainExtractor
More built-in compressors: filters
LLMChainFilter
LLMListwiseRerank
EmbeddingsFilter
Stringing compressors and document transformers together


--- docs_how_to_convert_runnable_to_tool.txt ---

How-to guides
How to convert Runnables to Tools
On this page
How to convert Runnables to Tools


Prerequisites
This guide assumes familiarity with the following concepts:


Runnables


Tools


Agents




Here we will demonstrate how to convert a LangChain 
Runnable
 into a tool that can be used by agents, chains, or chat models.


Dependencies
​


Note
: this guide requires 
langchain-core
 >= 0.2.13. We will also use 
OpenAI
 for embeddings, but any LangChain embeddings should suffice. We will use a simple 
LangGraph
 agent for demonstration purposes.


%
%
capture 
-
-
no
-
stderr
%
pip install 
-
U langchain
-
core langchain
-
openai langgraph


LangChain 
tools
 are interfaces that an agent, chain, or chat model can use to interact with the world. See 
here
 for how-to guides covering tool-calling, built-in tools, custom tools, and more information.


LangChain tools-- instances of 
BaseTool
-- are 
Runnables
 with additional constraints that enable them to be invoked effectively by language models:




Their inputs are constrained to be serializable, specifically strings and Python 
dict
 objects;


They contain names and descriptions indicating how and when they should be used;


They may contain a detailed 
args_schema
 for their arguments. That is, while a tool (as a 
Runnable
) might accept a single 
dict
 input, the specific keys and type information needed to populate a dict should be specified in the 
args_schema
.




Runnables that accept string or 
dict
 input can be converted to tools using the 
as_tool
 method, which allows for the specification of names, descriptions, and additional schema information for arguments.


Basic usage
​


With typed 
dict
 input:


from
 typing 
import
 List
from
 langchain_core
.
runnables 
import
 RunnableLambda
from
 typing_extensions 
import
 TypedDict
class
 
Args
(
TypedDict
)
:
    a
:
 
int
    b
:
 List
[
int
]
def
 
f
(
x
:
 Args
)
 
-
>
 
str
:
    
return
 
str
(
x
[
"a"
]
 
*
 
max
(
x
[
"b"
]
)
)
runnable 
=
 RunnableLambda
(
f
)
as_tool 
=
 runnable
.
as_tool
(
    name
=
"My tool"
,
    description
=
"Explanation of when to use tool."
,
)
API Reference:
RunnableLambda


print
(
as_tool
.
description
)
as_tool
.
args_schema
.
schema
(
)


Explanation of when to use tool.


{'title': 'My tool',
 'type': 'object',
 'properties': {'a': {'title': 'A', 'type': 'integer'},
  'b': {'title': 'B', 'type': 'array', 'items': {'type': 'integer'}}},
 'required': ['a', 'b']}


as_tool
.
invoke
(
{
"a"
:
 
3
,
 
"b"
:
 
[
1
,
 
2
]
}
)


'6'


Without typing information, arg types can be specified via 
arg_types
:


from
 typing 
import
 Any
,
 Dict
def
 
g
(
x
:
 Dict
[
str
,
 Any
]
)
 
-
>
 
str
:
    
return
 
str
(
x
[
"a"
]
 
*
 
max
(
x
[
"b"
]
)
)
runnable 
=
 RunnableLambda
(
g
)
as_tool 
=
 runnable
.
as_tool
(
    name
=
"My tool"
,
    description
=
"Explanation of when to use tool."
,
    arg_types
=
{
"a"
:
 
int
,
 
"b"
:
 List
[
int
]
}
,
)


Alternatively, the schema can be fully specified by directly passing the desired 
args_schema
 for the tool:


from
 pydantic 
import
 BaseModel
,
 Field
class
 
GSchema
(
BaseModel
)
:
    
"""Apply a function to an integer and list of integers."""
    a
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"Integer"
)
    b
:
 List
[
int
]
 
=
 Field
(
.
.
.
,
 description
=
"List of ints"
)
runnable 
=
 RunnableLambda
(
g
)
as_tool 
=
 runnable
.
as_tool
(
GSchema
)


String input is also supported:


def
 
f
(
x
:
 
str
)
 
-
>
 
str
:
    
return
 x 
+
 
"a"
def
 
g
(
x
:
 
str
)
 
-
>
 
str
:
    
return
 x 
+
 
"z"
runnable 
=
 RunnableLambda
(
f
)
 
|
 g
as_tool 
=
 runnable
.
as_tool
(
)


as_tool
.
invoke
(
"b"
)


'baz'


In agents
​


Below we will incorporate LangChain Runnables as tools in an 
agent
 application. We will demonstrate with:




a document 
retriever
;


a simple 
RAG
 chain, allowing an agent to delegate relevant queries to it.




We first instantiate a chat model that supports 
tool calling
:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Following the 
RAG tutorial
, let's first construct a retriever:


from
 langchain_core
.
documents 
import
 Document
from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
from
 langchain_openai 
import
 OpenAIEmbeddings
documents 
=
 
[
    Document
(
        page_content
=
"Dogs are great companions, known for their loyalty and friendliness."
,
    
)
,
    Document
(
        page_content
=
"Cats are independent pets that often enjoy their own space."
,
    
)
,
]
vectorstore 
=
 InMemoryVectorStore
.
from_documents
(
    documents
,
 embedding
=
OpenAIEmbeddings
(
)
)
retriever 
=
 vectorstore
.
as_retriever
(
    search_type
=
"similarity"
,
    search_kwargs
=
{
"k"
:
 
1
}
,
)
API Reference:
Document
 | 
InMemoryVectorStore
 | 
OpenAIEmbeddings


We next create use a simple pre-built 
LangGraph agent
 and provide it the tool:


from
 langgraph
.
prebuilt 
import
 create_react_agent
tools 
=
 
[
    retriever
.
as_tool
(
        name
=
"pet_info_retriever"
,
        description
=
"Get information about pets."
,
    
)
]
agent 
=
 create_react_agent
(
llm
,
 tools
)
API Reference:
create_react_agent


for
 chunk 
in
 agent
.
stream
(
{
"messages"
:
 
[
(
"human"
,
 
"What are dogs known for?"
)
]
}
)
:
    
print
(
chunk
)
    
print
(
"----"
)


{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_W8cnfOjwqEn4cFcg19LN9mYD', 'function': {'arguments': '{"__arg1":"dogs"}', 'name': 'pet_info_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 60, 'total_tokens': 79}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d7f81de9-1fb7-4caf-81ed-16dcdb0b2ab4-0', tool_calls=[{'name': 'pet_info_retriever', 'args': {'__arg1': 'dogs'}, 'id': 'call_W8cnfOjwqEn4cFcg19LN9mYD'}], usage_metadata={'input_tokens': 60, 'output_tokens': 19, 'total_tokens': 79})]}}
----
{'tools': {'messages': [ToolMessage(content="[Document(id='86f835fe-4bbe-4ec6-aeb4-489a8b541707', page_content='Dogs are great companions, known for their loyalty and friendliness.')]", name='pet_info_retriever', tool_call_id='call_W8cnfOjwqEn4cFcg19LN9mYD')]}}
----
{'agent': {'messages': [AIMessage(content='Dogs are known for being great companions, known for their loyalty and friendliness.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 134, 'total_tokens': 152}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9ca5847a-a5eb-44c0-a774-84cc2c5bbc5b-0', usage_metadata={'input_tokens': 134, 'output_tokens': 18, 'total_tokens': 152})]}}
----


See 
LangSmith trace
 for the above run.


Going further, we can create a simple 
RAG
 chain that takes an additional parameter-- here, the "style" of the answer.


from
 operator 
import
 itemgetter
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
system_prompt 
=
 
"""
You are an assistant for question-answering tasks.
Use the below context to answer the question. If
you don't know the answer, say you don't know.
Use three sentences maximum and keep the answer
concise.
Answer in the style of {answer_style}.
Question: {question}
Context: {context}
"""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
[
(
"system"
,
 system_prompt
)
]
)
rag_chain 
=
 
(
    
{
        
"context"
:
 itemgetter
(
"question"
)
 
|
 retriever
,
        
"question"
:
 itemgetter
(
"question"
)
,
        
"answer_style"
:
 itemgetter
(
"answer_style"
)
,
    
}
    
|
 prompt
    
|
 llm
    
|
 StrOutputParser
(
)
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnablePassthrough


Note that the input schema for our chain contains the required arguments, so it converts to a tool without further specification:


rag_chain
.
input_schema
.
schema
(
)


{'title': 'RunnableParallel<context,question,answer_style>Input',
 'type': 'object',
 'properties': {'question': {'title': 'Question'},
  'answer_style': {'title': 'Answer Style'}}}


rag_tool 
=
 rag_chain
.
as_tool
(
    name
=
"pet_expert"
,
    description
=
"Get information about pets."
,
)


Below we again invoke the agent. Note that the agent populates the required parameters in its 
tool_calls
:


agent 
=
 create_react_agent
(
llm
,
 
[
rag_tool
]
)
for
 chunk 
in
 agent
.
stream
(
    
{
"messages"
:
 
[
(
"human"
,
 
"What would a pirate say dogs are known for?"
)
]
}
)
:
    
print
(
chunk
)
    
print
(
"----"
)


{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_17iLPWvOD23zqwd1QVQ00Y63', 'function': {'arguments': '{"question":"What are dogs known for according to pirates?","answer_style":"quote"}', 'name': 'pet_expert'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 59, 'total_tokens': 87}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7fef44f3-7bba-4e63-8c51-2ad9c5e65e2e-0', tool_calls=[{'name': 'pet_expert', 'args': {'question': 'What are dogs known for according to pirates?', 'answer_style': 'quote'}, 'id': 'call_17iLPWvOD23zqwd1QVQ00Y63'}], usage_metadata={'input_tokens': 59, 'output_tokens': 28, 'total_tokens': 87})]}}
----
{'tools': {'messages': [ToolMessage(content='"Dogs are known for their loyalty and friendliness, making them great companions for pirates on long sea voyages."', name='pet_expert', tool_call_id='call_17iLPWvOD23zqwd1QVQ00Y63')]}}
----
{'agent': {'messages': [AIMessage(content='According to pirates, dogs are known for their loyalty and friendliness, making them great companions for pirates on long sea voyages.', response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 119, 'total_tokens': 146}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a30edc3-7be0-4743-b980-ca2f8cad9b8d-0', usage_metadata={'input_tokens': 119, 'output_tokens': 27, 'total_tokens': 146})]}}
----


See 
LangSmith trace
 for the above run.
Edit this page
Was this page helpful?
Previous
How to do retrieval with contextual compression
Next
How to create custom callback handlers
Dependencies
Basic usage
In agents


--- docs_how_to_custom_callbacks.txt ---

How-to guides
How to create custom callback handlers
On this page
How to create custom callback handlers


Prerequisites
This guide assumes familiarity with the following concepts:


Callbacks




LangChain has some built-in callback handlers, but you will often want to create your own handlers with custom logic.


To create a custom callback handler, we need to determine the 
event(s)
 we want our callback handler to handle as well as what we want our callback handler to do when the event is triggered. Then all we need to do is attach the callback handler to the object, for example via 
the constructor
 or 
at runtime
.


In the example below, we'll implement streaming with a custom handler.


In our custom callback handler 
MyCustomHandler
, we implement the 
on_llm_new_token
 handler to print the token we have just received. We then attach our custom handler to the model object as a constructor callback.


from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_core
.
callbacks 
import
 BaseCallbackHandler
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
class
 
MyCustomHandler
(
BaseCallbackHandler
)
:
    
def
 
on_llm_new_token
(
self
,
 token
:
 
str
,
 
**
kwargs
)
 
-
>
 
None
:
        
print
(
f"My custom handler, token: 
{
token
}
"
)
prompt 
=
 ChatPromptTemplate
.
from_messages
(
[
"Tell me a joke about {animal}"
]
)
# To enable streaming, we pass in `streaming=True` to the ChatModel constructor
# Additionally, we pass in our custom handler as a list to the callbacks parameter
model 
=
 ChatAnthropic
(
    model
=
"claude-3-sonnet-20240229"
,
 streaming
=
True
,
 callbacks
=
[
MyCustomHandler
(
)
]
)
chain 
=
 prompt 
|
 model
response 
=
 chain
.
invoke
(
{
"animal"
:
 
"bears"
}
)
API Reference:
ChatAnthropic
 | 
BaseCallbackHandler
 | 
ChatPromptTemplate


My custom handler, token: Here
My custom handler, token: 's
My custom handler, token:  a
My custom handler, token:  bear
My custom handler, token:  joke
My custom handler, token:  for
My custom handler, token:  you
My custom handler, token: :
My custom handler, token: 
Why
My custom handler, token:  di
My custom handler, token: d the
My custom handler, token:  bear
My custom handler, token:  dissol
My custom handler, token: ve
My custom handler, token:  in
My custom handler, token:  water
My custom handler, token: ?
My custom handler, token: 
Because
My custom handler, token:  it
My custom handler, token:  was
My custom handler, token:  a
My custom handler, token:  polar
My custom handler, token:  bear
My custom handler, token: !


You can see 
this reference page
 for a list of events you can handle. Note that the 
handle_chain_*
 events run for most LCEL runnables.


Next steps
​


You've now learned how to create your own custom callback handlers.


Next, check out the other how-to guides in this section, such as 
how to attach callbacks to a runnable
.
Edit this page
Was this page helpful?
Previous
How to convert Runnables to Tools
Next
How to create a custom chat model class
Next steps


--- docs_how_to_custom_chat_model.txt ---

How-to guides
How to create a custom chat model class
On this page
How to create a custom chat model class


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models




In this guide, we'll learn how to create a custom 
chat model
 using LangChain abstractions.


Wrapping your LLM with the standard 
BaseChatModel
 interface allow you to use your LLM in existing LangChain programs with minimal code modifications!


As an bonus, your LLM will automatically become a LangChain 
Runnable
 and will benefit from some optimizations out of the box (e.g., batch via a threadpool), async support, the 
astream_events
 API, etc.


Inputs and outputs
​


First, we need to talk about 
messages
, which are the inputs and outputs of chat models.


Messages
​


Chat models take messages as inputs and return a message as output.


LangChain has a few 
built-in message types
:


Message Type
Description
SystemMessage
Used for priming AI behavior, usually passed in as the first of a sequence of input messages.
HumanMessage
Represents a message from a person interacting with the chat model.
AIMessage
Represents a message from the chat model. This can be either text or a request to invoke a tool.
FunctionMessage
 / 
ToolMessage
Message for passing the results of tool invocation back to the model.
AIMessageChunk
 / 
HumanMessageChunk
 / ...
Chunk variant of each type of message.


note
ToolMessage
 and 
FunctionMessage
 closely follow OpenAI's 
function
 and 
tool
 roles.
This is a rapidly developing field and as more models add function calling capabilities. Expect that there will be additions to this schema.


from
 langchain_core
.
messages 
import
 
(
    AIMessage
,
    BaseMessage
,
    FunctionMessage
,
    HumanMessage
,
    SystemMessage
,
    ToolMessage
,
)
API Reference:
AIMessage
 | 
BaseMessage
 | 
FunctionMessage
 | 
HumanMessage
 | 
SystemMessage
 | 
ToolMessage


Streaming Variant
​


All the chat messages have a streaming variant that contains 
Chunk
 in the name.


from
 langchain_core
.
messages 
import
 
(
    AIMessageChunk
,
    FunctionMessageChunk
,
    HumanMessageChunk
,
    SystemMessageChunk
,
    ToolMessageChunk
,
)
API Reference:
AIMessageChunk
 | 
FunctionMessageChunk
 | 
HumanMessageChunk
 | 
SystemMessageChunk
 | 
ToolMessageChunk


These chunks are used when streaming output from chat models, and they all define an additive property!


AIMessageChunk
(
content
=
"Hello"
)
 
+
 AIMessageChunk
(
content
=
" World!"
)


AIMessageChunk(content='Hello World!')


Base Chat Model
​


Let's implement a chat model that echoes back the first 
n
 characters of the last message in the prompt!


To do so, we will inherit from 
BaseChatModel
 and we'll need to implement the following:


Method/Property
Description
Required/Optional
_generate
Use to generate a chat result from a prompt
Required
_llm_type
 (property)
Used to uniquely identify the type of the model. Used for logging.
Required
_identifying_params
 (property)
Represent model parameterization for tracing purposes.
Optional
_stream
Use to implement streaming.
Optional
_agenerate
Use to implement a native async method.
Optional
_astream
Use to implement async version of 
_stream
.
Optional


tip
The 
_astream
 implementation uses 
run_in_executor
 to launch the sync 
_stream
 in a separate thread if 
_stream
 is implemented, otherwise it fallsback to use 
_agenerate
.
You can use this trick if you want to reuse the 
_stream
 implementation, but if you're able to implement code that's natively async that's a better solution since that code will run with less overhead.


Implementation
​


from
 typing 
import
 Any
,
 Dict
,
 Iterator
,
 List
,
 Optional
from
 langchain_core
.
callbacks 
import
 
(
    CallbackManagerForLLMRun
,
)
from
 langchain_core
.
language_models 
import
 BaseChatModel
from
 langchain_core
.
messages 
import
 
(
    AIMessage
,
    AIMessageChunk
,
    BaseMessage
,
)
from
 langchain_core
.
messages
.
ai 
import
 UsageMetadata
from
 langchain_core
.
outputs 
import
 ChatGeneration
,
 ChatGenerationChunk
,
 ChatResult
from
 pydantic 
import
 Field
class
 
ChatParrotLink
(
BaseChatModel
)
:
    
"""A custom chat model that echoes the first `parrot_buffer_length` characters
    of the input.
    When contributing an implementation to LangChain, carefully document
    the model including the initialization parameters, include
    an example of how to initialize the model and include any relevant
    links to the underlying models documentation or API.
    Example:
        .. code-block:: python
            model = ChatParrotLink(parrot_buffer_length=2, model="bird-brain-001")
            result = model.invoke([HumanMessage(content="hello")])
            result = model.batch([[HumanMessage(content="hello")],
                                 [HumanMessage(content="world")]])
    """
    model_name
:
 
str
 
=
 Field
(
alias
=
"model"
)
    
"""The name of the model"""
    parrot_buffer_length
:
 
int
    
"""The number of characters from the last message of the prompt to be echoed."""
    temperature
:
 Optional
[
float
]
 
=
 
None
    max_tokens
:
 Optional
[
int
]
 
=
 
None
    timeout
:
 Optional
[
int
]
 
=
 
None
    stop
:
 Optional
[
List
[
str
]
]
 
=
 
None
    max_retries
:
 
int
 
=
 
2
    
def
 
_generate
(
        self
,
        messages
:
 List
[
BaseMessage
]
,
        stop
:
 Optional
[
List
[
str
]
]
 
=
 
None
,
        run_manager
:
 Optional
[
CallbackManagerForLLMRun
]
 
=
 
None
,
        
**
kwargs
:
 Any
,
    
)
 
-
>
 ChatResult
:
        
"""Override the _generate method to implement the chat model logic.
        This can be a call to an API, a call to a local model, or any other
        implementation that generates a response to the input prompt.
        Args:
            messages: the prompt composed of a list of messages.
            stop: a list of strings on which the model should stop generating.
                  If generation stops due to a stop token, the stop token itself
                  SHOULD BE INCLUDED as part of the output. This is not enforced
                  across models right now, but it's a good practice to follow since
                  it makes it much easier to parse the output of the model
                  downstream and understand why generation stopped.
            run_manager: A run manager with callbacks for the LLM.
        """
        
# Replace this with actual logic to generate a response from a list
        
# of messages.
        last_message 
=
 messages
[
-
1
]
        tokens 
=
 last_message
.
content
[
:
 self
.
parrot_buffer_length
]
        ct_input_tokens 
=
 
sum
(
len
(
message
.
content
)
 
for
 message 
in
 messages
)
        ct_output_tokens 
=
 
len
(
tokens
)
        message 
=
 AIMessage
(
            content
=
tokens
,
            additional_kwargs
=
{
}
,
  
# Used to add additional payload to the message
            response_metadata
=
{
  
# Use for response metadata
                
"time_in_seconds"
:
 
3
,
                
"model_name"
:
 self
.
model_name
,
            
}
,
            usage_metadata
=
{
                
"input_tokens"
:
 ct_input_tokens
,
                
"output_tokens"
:
 ct_output_tokens
,
                
"total_tokens"
:
 ct_input_tokens 
+
 ct_output_tokens
,
            
}
,
        
)
        
##
        generation 
=
 ChatGeneration
(
message
=
message
)
        
return
 ChatResult
(
generations
=
[
generation
]
)
    
def
 
_stream
(
        self
,
        messages
:
 List
[
BaseMessage
]
,
        stop
:
 Optional
[
List
[
str
]
]
 
=
 
None
,
        run_manager
:
 Optional
[
CallbackManagerForLLMRun
]
 
=
 
None
,
        
**
kwargs
:
 Any
,
    
)
 
-
>
 Iterator
[
ChatGenerationChunk
]
:
        
"""Stream the output of the model.
        This method should be implemented if the model can generate output
        in a streaming fashion. If the model does not support streaming,
        do not implement it. In that case streaming requests will be automatically
        handled by the _generate method.
        Args:
            messages: the prompt composed of a list of messages.
            stop: a list of strings on which the model should stop generating.
                  If generation stops due to a stop token, the stop token itself
                  SHOULD BE INCLUDED as part of the output. This is not enforced
                  across models right now, but it's a good practice to follow since
                  it makes it much easier to parse the output of the model
                  downstream and understand why generation stopped.
            run_manager: A run manager with callbacks for the LLM.
        """
        last_message 
=
 messages
[
-
1
]
        tokens 
=
 
str
(
last_message
.
content
[
:
 self
.
parrot_buffer_length
]
)
        ct_input_tokens 
=
 
sum
(
len
(
message
.
content
)
 
for
 message 
in
 messages
)
        
for
 token 
in
 tokens
:
            usage_metadata 
=
 UsageMetadata
(
                
{
                    
"input_tokens"
:
 ct_input_tokens
,
                    
"output_tokens"
:
 
1
,
                    
"total_tokens"
:
 ct_input_tokens 
+
 
1
,
                
}
            
)
            ct_input_tokens 
=
 
0
            chunk 
=
 ChatGenerationChunk
(
                message
=
AIMessageChunk
(
content
=
token
,
 usage_metadata
=
usage_metadata
)
            
)
            
if
 run_manager
:
                
# This is optional in newer versions of LangChain
                
# The on_llm_new_token will be called automatically
                run_manager
.
on_llm_new_token
(
token
,
 chunk
=
chunk
)
            
yield
 chunk
        
# Let's add some other information (e.g., response metadata)
        chunk 
=
 ChatGenerationChunk
(
            message
=
AIMessageChunk
(
                content
=
""
,
                response_metadata
=
{
"time_in_sec"
:
 
3
,
 
"model_name"
:
 self
.
model_name
}
,
            
)
        
)
        
if
 run_manager
:
            
# This is optional in newer versions of LangChain
            
# The on_llm_new_token will be called automatically
            run_manager
.
on_llm_new_token
(
token
,
 chunk
=
chunk
)
        
yield
 chunk
    
@property
    
def
 
_llm_type
(
self
)
 
-
>
 
str
:
        
"""Get the type of language model used by this chat model."""
        
return
 
"echoing-chat-model-advanced"
    
@property
    
def
 
_identifying_params
(
self
)
 
-
>
 Dict
[
str
,
 Any
]
:
        
"""Return a dictionary of identifying parameters.
        This information is used by the LangChain callback system, which
        is used for tracing purposes make it possible to monitor LLMs.
        """
        
return
 
{
            
# The model name allows users to specify custom token counting
            
# rules in LLM monitoring applications (e.g., in LangSmith users
            
# can provide per token pricing for their model and monitor
            
# costs for the given LLM.)
            
"model_name"
:
 self
.
model_name
,
        
}
API Reference:
CallbackManagerForLLMRun
 | 
BaseChatModel
 | 
AIMessage
 | 
AIMessageChunk
 | 
BaseMessage
 | 
UsageMetadata
 | 
ChatGeneration
 | 
ChatGenerationChunk
 | 
ChatResult


Let's test it 🧪
​


The chat model will implement the standard 
Runnable
 interface of LangChain which many of the LangChain abstractions support!


model 
=
 ChatParrotLink
(
parrot_buffer_length
=
3
,
 model
=
"my_custom_model"
)
model
.
invoke
(
    
[
        HumanMessage
(
content
=
"hello!"
)
,
        AIMessage
(
content
=
"Hi there human!"
)
,
        HumanMessage
(
content
=
"Meow!"
)
,
    
]
)


AIMessage(content='Meo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-cf11aeb6-8ab6-43d7-8c68-c1ef89b6d78e-0', usage_metadata={'input_tokens': 26, 'output_tokens': 3, 'total_tokens': 29})


model
.
invoke
(
"hello"
)


AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-618e5ed4-d611-4083-8cf1-c270726be8d9-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8})


model
.
batch
(
[
"hello"
,
 
"goodbye"
]
)


[AIMessage(content='hel', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-eea4ed7d-d750-48dc-90c0-7acca1ff388f-0', usage_metadata={'input_tokens': 5, 'output_tokens': 3, 'total_tokens': 8}),
 AIMessage(content='goo', additional_kwargs={}, response_metadata={'time_in_seconds': 3}, id='run-07cfc5c1-3c62-485f-b1e0-3d46e1547287-0', usage_metadata={'input_tokens': 7, 'output_tokens': 3, 'total_tokens': 10})]


for
 chunk 
in
 model
.
stream
(
"cat"
)
:
    
print
(
chunk
.
content
,
 end
=
"|"
)


c|a|t||


Please see the implementation of 
_astream
 in the model! If you do not implement it, then no output will stream.!


async
 
for
 chunk 
in
 model
.
astream
(
"cat"
)
:
    
print
(
chunk
.
content
,
 end
=
"|"
)


c|a|t||


Let's try to use the astream events API which will also help double check that all the callbacks were implemented!


async
 
for
 event 
in
 model
.
astream_events
(
"cat"
,
 version
=
"v1"
)
:
    
print
(
event
)


{'event': 'on_chat_model_start', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'name': 'ChatParrotLink', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='c', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 1, 'total_tokens': 4})}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='a', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='t', additional_kwargs={}, response_metadata={}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 0, 'output_tokens': 1, 'total_tokens': 1})}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'name': 'ChatParrotLink', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a')}, 'parent_ids': []}
{'event': 'on_chat_model_end', 'name': 'ChatParrotLink', 'run_id': '3f0b5501-5c78-45b3-92fc-8322a6a5024a', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat', additional_kwargs={}, response_metadata={'time_in_sec': 3}, id='run-3f0b5501-5c78-45b3-92fc-8322a6a5024a', usage_metadata={'input_tokens': 3, 'output_tokens': 3, 'total_tokens': 6})}, 'parent_ids': []}


Contributing
​


We appreciate all chat model integration contributions.


Here's a checklist to help make sure your contribution gets added to LangChain:


Documentation:




The model contains doc-strings for all initialization arguments, as these will be surfaced in the 
API Reference
.


The class doc-string for the model contains a link to the model API if the model is powered by a service.




Tests:




 
Add unit or integration tests to the overridden methods. Verify that 
invoke
, 
ainvoke
, 
batch
, 
stream
 work if you've over-ridden the corresponding code.




Streaming (if you're implementing it):




 
Implement the _stream method to get streaming working




Stop Token Behavior:




 
Stop token should be respected


 
Stop token should be INCLUDED as part of the response




Secret API Keys:




 
If your model connects to an API it will likely accept API keys as part of its initialization. Use Pydantic's 
SecretStr
 type for secrets, so they don't get accidentally printed out when folks print the model.




Identifying Params:




 
Include a 
model_name
 in identifying params




Optimizations:


Consider providing native async support to reduce the overhead from the model!




 
Provided a native async of 
_agenerate
 (used by 
ainvoke
)


 
Provided a native async of 
_astream
 (used by 
astream
)




Next steps
​


You've now learned how to create your own custom chat models.


Next, check out the other how-to guides chat models in this section, like 
how to get a model to return structured output
 or 
how to track chat model token usage
.
Edit this page
Was this page helpful?
Previous
How to create custom callback handlers
Next
Custom Embeddings
Inputs and outputs
Messages
Streaming Variant
Base Chat Model
Implementation
Let's test it 🧪
Contributing
Next steps


--- docs_how_to_custom_embeddings.txt ---

How-to guides
Custom Embeddings
On this page
Custom Embeddings


LangChain is integrated with many 
3rd party embedding models
. In this guide we'll show you how to create a custom Embedding class, in case a built-in one does not already exist. Embeddings are critical in natural language processing applications as they convert text into a numerical form that algorithms can understand, thereby enabling a wide range of applications such as similarity search, text classification, and clustering.


Implementing embeddings using the standard 
Embeddings
 interface will allow your embeddings to be utilized in existing 
LangChain
 abstractions (e.g., as the embeddings powering a 
VectorStore
 or cached using 
CacheBackedEmbeddings
).


Interface
​


The current 
Embeddings
 abstraction in LangChain is designed to operate on text data. In this implementation, the inputs are either single strings or lists of strings, and the outputs are lists of numerical arrays (vectors), where each vector represents
an embedding of the input text into some n-dimensional space.


Your custom embedding must implement the following methods:


Method/Property
Description
Required/Optional
embed_documents(texts)
Generates embeddings for a list of strings.
Required
embed_query(text)
Generates an embedding for a single text query.
Required
aembed_documents(texts)
Asynchronously generates embeddings for a list of strings.
Optional
aembed_query(text)
Asynchronously generates an embedding for a single text query.
Optional


These methods ensure that your embedding model can be integrated seamlessly into the LangChain framework, providing both synchronous and asynchronous capabilities for scalability and performance optimization.


note
Embeddings
 do not currently implement the 
Runnable
 interface and are also 
not
 instances of pydantic 
BaseModel
.


Embedding queries vs documents
​


The 
embed_query
 and 
embed_documents
 methods are required. These methods both operate
on string inputs. The accessing of 
Document.page_content
 attributes is handled
by the vector store using the embedding model for legacy reasons.


embed_query
 takes in a single string and returns a single embedding as a list of floats.
If your model has different modes for embedding queries vs the underlying documents, you can
implement this method to handle that.


embed_documents
 takes in a list of strings and returns a list of embeddings as a list of lists of floats.


note
embed_documents
 takes in a list of plain text, not a list of LangChain 
Document
 objects. The name of this method
may change in future versions of LangChain.


Implementation
​


As an example, we'll implement a simple embeddings model that returns a constant vector. This model is for illustrative purposes only.


from
 typing 
import
 List
from
 langchain_core
.
embeddings 
import
 Embeddings
class
 
ParrotLinkEmbeddings
(
Embeddings
)
:
    
"""ParrotLink embedding model integration.
    # TODO: Populate with relevant params.
    Key init args — completion params:
        model: str
            Name of ParrotLink model to use.
    See full list of supported init args and their descriptions in the params section.
    # TODO: Replace with relevant init params.
    Instantiate:
        .. code-block:: python
            from langchain_parrot_link import ParrotLinkEmbeddings
            embed = ParrotLinkEmbeddings(
                model="...",
                # api_key="...",
                # other params...
            )
    Embed single text:
        .. code-block:: python
            input_text = "The meaning of life is 42"
            embed.embed_query(input_text)
        .. code-block:: python
            # TODO: Example output.
    # TODO: Delete if token-level streaming isn't supported.
    Embed multiple text:
        .. code-block:: python
             input_texts = ["Document 1...", "Document 2..."]
            embed.embed_documents(input_texts)
        .. code-block:: python
            # TODO: Example output.
    # TODO: Delete if native async isn't supported.
    Async:
        .. code-block:: python
            await embed.aembed_query(input_text)
            # multiple:
            # await embed.aembed_documents(input_texts)
        .. code-block:: python
            # TODO: Example output.
    """
    
def
 
__init__
(
self
,
 model
:
 
str
)
:
        self
.
model 
=
 model
    
def
 
embed_documents
(
self
,
 texts
:
 List
[
str
]
)
 
-
>
 List
[
List
[
float
]
]
:
        
"""Embed search docs."""
        
return
 
[
[
0.5
,
 
0.6
,
 
0.7
]
 
for
 _ 
in
 texts
]
    
def
 
embed_query
(
self
,
 text
:
 
str
)
 
-
>
 List
[
float
]
:
        
"""Embed query text."""
        
return
 self
.
embed_documents
(
[
text
]
)
[
0
]
    
# optional: add custom async implementations here
    
# you can also delete these, and the base class will
    
# use the default implementation, which calls the sync
    
# version in an async executor:
    
# async def aembed_documents(self, texts: List[str]) -> List[List[float]]:
    
#     """Asynchronous Embed search docs."""
    
#     ...
    
# async def aembed_query(self, text: str) -> List[float]:
    
#     """Asynchronous Embed query text."""
    
#     ...
API Reference:
Embeddings


Let's test it 🧪
​


embeddings 
=
 ParrotLinkEmbeddings
(
"test-model"
)
print
(
embeddings
.
embed_documents
(
[
"Hello"
,
 
"world"
]
)
)
print
(
embeddings
.
embed_query
(
"Hello"
)
)


[[0.5, 0.6, 0.7], [0.5, 0.6, 0.7]]
[0.5, 0.6, 0.7]


Contributing
​


We welcome contributions of Embedding models to the LangChain code base.


If you aim to contribute an embedding model for a new provider (e.g., with a new set of dependencies or SDK), we encourage you to publish your implementation in a separate 
langchain-*
 integration package. This will enable you to appropriately manage dependencies and version your package. Please refer to our 
contributing guide
 for a walkthrough of this process.
Edit this page
Was this page helpful?
Previous
How to create a custom chat model class
Next
How to create a custom LLM class
Interface
Embedding queries vs documents
Implementation
Let's test it 🧪
Contributing


--- docs_how_to_custom_llm.txt ---

How-to guides
How to create a custom LLM class
On this page
How to create a custom LLM class


This notebook goes over how to create a custom LLM wrapper, in case you want to use your own LLM or a different wrapper than one that is supported in LangChain.


Wrapping your LLM with the standard 
LLM
 interface allow you to use your LLM in existing LangChain programs with minimal code modifications.


As an bonus, your LLM will automatically become a LangChain 
Runnable
 and will benefit from some optimizations out of the box, async support, the 
astream_events
 API, etc.


caution
You are currently on a page documenting the use of 
text completion models
. Many of the latest and most popular models are 
chat completion models
.
Unless you are specifically using more advanced prompting techniques, you are probably looking for 
this page instead
.


Implementation
​


There are only two required things that a custom LLM needs to implement:


Method
Description
_call
Takes in a string and some optional stop words, and returns a string. Used by 
invoke
.
_llm_type
A property that returns a string, used for logging purposes only.


Optional implementations:


Method
Description
_identifying_params
Used to help with identifying the model and printing the LLM; should return a dictionary. This is a 
@property
.
_acall
Provides an async native implementation of 
_call
, used by 
ainvoke
.
_stream
Method to stream the output token by token.
_astream
Provides an async native implementation of 
_stream
; in newer LangChain versions, defaults to 
_stream
.


Let's implement a simple custom LLM that just returns the first n characters of the input.


from
 typing 
import
 Any
,
 Dict
,
 Iterator
,
 List
,
 Mapping
,
 Optional
from
 langchain_core
.
callbacks
.
manager 
import
 CallbackManagerForLLMRun
from
 langchain_core
.
language_models
.
llms 
import
 LLM
from
 langchain_core
.
outputs 
import
 GenerationChunk
class
 
CustomLLM
(
LLM
)
:
    
"""A custom chat model that echoes the first `n` characters of the input.
    When contributing an implementation to LangChain, carefully document
    the model including the initialization parameters, include
    an example of how to initialize the model and include any relevant
    links to the underlying models documentation or API.
    Example:
        .. code-block:: python
            model = CustomChatModel(n=2)
            result = model.invoke([HumanMessage(content="hello")])
            result = model.batch([[HumanMessage(content="hello")],
                                 [HumanMessage(content="world")]])
    """
    n
:
 
int
    
"""The number of characters from the last message of the prompt to be echoed."""
    
def
 
_call
(
        self
,
        prompt
:
 
str
,
        stop
:
 Optional
[
List
[
str
]
]
 
=
 
None
,
        run_manager
:
 Optional
[
CallbackManagerForLLMRun
]
 
=
 
None
,
        
**
kwargs
:
 Any
,
    
)
 
-
>
 
str
:
        
"""Run the LLM on the given input.
        Override this method to implement the LLM logic.
        Args:
            prompt: The prompt to generate from.
            stop: Stop words to use when generating. Model output is cut off at the
                first occurrence of any of the stop substrings.
                If stop tokens are not supported consider raising NotImplementedError.
            run_manager: Callback manager for the run.
            **kwargs: Arbitrary additional keyword arguments. These are usually passed
                to the model provider API call.
        Returns:
            The model output as a string. Actual completions SHOULD NOT include the prompt.
        """
        
if
 stop 
is
 
not
 
None
:
            
raise
 ValueError
(
"stop kwargs are not permitted."
)
        
return
 prompt
[
:
 self
.
n
]
    
def
 
_stream
(
        self
,
        prompt
:
 
str
,
        stop
:
 Optional
[
List
[
str
]
]
 
=
 
None
,
        run_manager
:
 Optional
[
CallbackManagerForLLMRun
]
 
=
 
None
,
        
**
kwargs
:
 Any
,
    
)
 
-
>
 Iterator
[
GenerationChunk
]
:
        
"""Stream the LLM on the given prompt.
        This method should be overridden by subclasses that support streaming.
        If not implemented, the default behavior of calls to stream will be to
        fallback to the non-streaming version of the model and return
        the output as a single chunk.
        Args:
            prompt: The prompt to generate from.
            stop: Stop words to use when generating. Model output is cut off at the
                first occurrence of any of these substrings.
            run_manager: Callback manager for the run.
            **kwargs: Arbitrary additional keyword arguments. These are usually passed
                to the model provider API call.
        Returns:
            An iterator of GenerationChunks.
        """
        
for
 char 
in
 prompt
[
:
 self
.
n
]
:
            chunk 
=
 GenerationChunk
(
text
=
char
)
            
if
 run_manager
:
                run_manager
.
on_llm_new_token
(
chunk
.
text
,
 chunk
=
chunk
)
            
yield
 chunk
    
@property
    
def
 
_identifying_params
(
self
)
 
-
>
 Dict
[
str
,
 Any
]
:
        
"""Return a dictionary of identifying parameters."""
        
return
 
{
            
# The model name allows users to specify custom token counting
            
# rules in LLM monitoring applications (e.g., in LangSmith users
            
# can provide per token pricing for their model and monitor
            
# costs for the given LLM.)
            
"model_name"
:
 
"CustomChatModel"
,
        
}
    
@property
    
def
 
_llm_type
(
self
)
 
-
>
 
str
:
        
"""Get the type of language model used by this chat model. Used for logging purposes only."""
        
return
 
"custom"
API Reference:
CallbackManagerForLLMRun
 | 
LLM
 | 
GenerationChunk


Let's test it 🧪
​


This LLM will implement the standard 
Runnable
 interface of LangChain which many of the LangChain abstractions support!


llm 
=
 CustomLLM
(
n
=
5
)
print
(
llm
)


[1mCustomLLM[0m
Params: {'model_name': 'CustomChatModel'}


llm
.
invoke
(
"This is a foobar thing"
)


'This '


await
 llm
.
ainvoke
(
"world"
)


'world'


llm
.
batch
(
[
"woof woof woof"
,
 
"meow meow meow"
]
)


['woof ', 'meow ']


await
 llm
.
abatch
(
[
"woof woof woof"
,
 
"meow meow meow"
]
)


['woof ', 'meow ']


async
 
for
 token 
in
 llm
.
astream
(
"hello"
)
:
    
print
(
token
,
 end
=
"|"
,
 flush
=
True
)


h|e|l|l|o|


Let's confirm that in integrates nicely with other 
LangChain
 APIs.


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
API Reference:
ChatPromptTemplate


prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 
"you are a bot"
)
,
 
(
"human"
,
 
"{input}"
)
]
)


llm 
=
 CustomLLM
(
n
=
7
)
chain 
=
 prompt 
|
 llm


idx 
=
 
0
async
 
for
 event 
in
 chain
.
astream_events
(
{
"input"
:
 
"hello there!"
}
,
 version
=
"v1"
)
:
    
print
(
event
)
    idx 
+=
 
1
    
if
 idx 
>
 
7
:
        
# Truncate
        
break


{'event': 'on_chain_start', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}
{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}}}
{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '7e996251-a926-4344-809e-c425a9846d21', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'input': 'hello there!'}, 'output': ChatPromptValue(messages=[SystemMessage(content='you are a bot'), HumanMessage(content='hello there!')])}}
{'event': 'on_llm_start', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'input': {'prompts': ['System: you are a bot\nHuman: hello there!']}}}
{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'S'}}
{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'S'}}
{'event': 'on_llm_stream', 'name': 'CustomLLM', 'run_id': 'a8766beb-10f4-41de-8750-3ea7cf0ca7e2', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': 'y'}}
{'event': 'on_chain_stream', 'run_id': '05f24b4f-7ea3-4fb6-8417-3aa21633462f', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'y'}}


Contributing
​


We appreciate all chat model integration contributions.


Here's a checklist to help make sure your contribution gets added to LangChain:


Documentation:




The model contains doc-strings for all initialization arguments, as these will be surfaced in the 
APIReference
.


The class doc-string for the model contains a link to the model API if the model is powered by a service.




Tests:




 
Add unit or integration tests to the overridden methods. Verify that 
invoke
, 
ainvoke
, 
batch
, 
stream
 work if you've over-ridden the corresponding code.




Streaming (if you're implementing it):




 
Make sure to invoke the 
on_llm_new_token
 callback


 
on_llm_new_token
 is invoked BEFORE yielding the chunk




Stop Token Behavior:




 
Stop token should be respected


 
Stop token should be INCLUDED as part of the response




Secret API Keys:




 
If your model connects to an API it will likely accept API keys as part of its initialization. Use Pydantic's 
SecretStr
 type for secrets, so they don't get accidentally printed out when folks print the model.


Edit this page
Was this page helpful?
Previous
Custom Embeddings
Next
Custom Retriever
Implementation
Let's test it 🧪
Contributing


--- docs_how_to_custom_retriever.txt ---

How-to guides
Custom Retriever
On this page
How to create a custom Retriever


Overview
​


Many LLM applications involve retrieving information from external data sources using a 
Retriever
.


A retriever is responsible for retrieving a list of relevant 
Documents
 to a given user 
query
.


The retrieved documents are often formatted into prompts that are fed into an LLM, allowing the LLM to use the information in the to generate an appropriate response (e.g., answering a user question based on a knowledge base).


Interface
​


To create your own retriever, you need to extend the 
BaseRetriever
 class and implement the following methods:


Method
Description
Required/Optional
_get_relevant_documents
Get documents relevant to a query.
Required
_aget_relevant_documents
Implement to provide async native support.
Optional


The logic inside of 
_get_relevant_documents
 can involve arbitrary calls to a database or to the web using requests.


tip
By inherting from 
BaseRetriever
, your retriever automatically becomes a LangChain 
Runnable
 and will gain the standard 
Runnable
 functionality out of the box!


info
You can use a 
RunnableLambda
 or 
RunnableGenerator
 to implement a retriever.
The main benefit of implementing a retriever as a 
BaseRetriever
 vs. a 
RunnableLambda
 (a custom 
runnable function
) is that a 
BaseRetriever
 is a well
known LangChain entity so some tooling for monitoring may implement specialized behavior for retrievers. Another difference
is that a 
BaseRetriever
 will behave slightly differently from 
RunnableLambda
 in some APIs; e.g., the 
start
 event
in 
astream_events
 API will be 
on_retriever_start
 instead of 
on_chain_start
.


Example
​


Let's implement a toy retriever that returns all documents whose text contains the text in the user query.


from
 typing 
import
 List
from
 langchain_core
.
callbacks 
import
 CallbackManagerForRetrieverRun
from
 langchain_core
.
documents 
import
 Document
from
 langchain_core
.
retrievers 
import
 BaseRetriever
class
 
ToyRetriever
(
BaseRetriever
)
:
    
"""A toy retriever that contains the top k documents that contain the user query.
    This retriever only implements the sync method _get_relevant_documents.
    If the retriever were to involve file access or network access, it could benefit
    from a native async implementation of `_aget_relevant_documents`.
    As usual, with Runnables, there's a default async implementation that's provided
    that delegates to the sync implementation running on another thread.
    """
    documents
:
 List
[
Document
]
    
"""List of documents to retrieve from."""
    k
:
 
int
    
"""Number of top results to return"""
    
def
 
_get_relevant_documents
(
        self
,
 query
:
 
str
,
 
*
,
 run_manager
:
 CallbackManagerForRetrieverRun
    
)
 
-
>
 List
[
Document
]
:
        
"""Sync implementations for retriever."""
        matching_documents 
=
 
[
]
        
for
 document 
in
 documents
:
            
if
 
len
(
matching_documents
)
 
>
 self
.
k
:
                
return
 matching_documents
            
if
 query
.
lower
(
)
 
in
 document
.
page_content
.
lower
(
)
:
                matching_documents
.
append
(
document
)
        
return
 matching_documents
    
# Optional: Provide a more efficient native implementation by overriding
    
# _aget_relevant_documents
    
# async def _aget_relevant_documents(
    
#     self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun
    
# ) -> List[Document]:
    
#     """Asynchronously get documents relevant to a query.
    
#     Args:
    
#         query: String to find relevant documents for
    
#         run_manager: The callbacks handler to use
    
#     Returns:
    
#         List of relevant documents
    
#     """
API Reference:
CallbackManagerForRetrieverRun
 | 
Document
 | 
BaseRetriever


Test it 🧪
​


documents 
=
 
[
    Document
(
        page_content
=
"Dogs are great companions, known for their loyalty and friendliness."
,
        metadata
=
{
"type"
:
 
"dog"
,
 
"trait"
:
 
"loyalty"
}
,
    
)
,
    Document
(
        page_content
=
"Cats are independent pets that often enjoy their own space."
,
        metadata
=
{
"type"
:
 
"cat"
,
 
"trait"
:
 
"independence"
}
,
    
)
,
    Document
(
        page_content
=
"Goldfish are popular pets for beginners, requiring relatively simple care."
,
        metadata
=
{
"type"
:
 
"fish"
,
 
"trait"
:
 
"low maintenance"
}
,
    
)
,
    Document
(
        page_content
=
"Parrots are intelligent birds capable of mimicking human speech."
,
        metadata
=
{
"type"
:
 
"bird"
,
 
"trait"
:
 
"intelligence"
}
,
    
)
,
    Document
(
        page_content
=
"Rabbits are social animals that need plenty of space to hop around."
,
        metadata
=
{
"type"
:
 
"rabbit"
,
 
"trait"
:
 
"social"
}
,
    
)
,
]
retriever 
=
 ToyRetriever
(
documents
=
documents
,
 k
=
3
)


retriever
.
invoke
(
"that"
)


[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'}),
 Document(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'type': 'rabbit', 'trait': 'social'})]


It's a 
runnable
 so it'll benefit from the standard Runnable Interface! 🤩


await
 retriever
.
ainvoke
(
"that"
)


[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'}),
 Document(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'type': 'rabbit', 'trait': 'social'})]


retriever
.
batch
(
[
"dog"
,
 
"cat"
]
)


[[Document(page_content='Dogs are great companions, known for their loyalty and friendliness.', metadata={'type': 'dog', 'trait': 'loyalty'})],
 [Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'})]]


async
 
for
 event 
in
 retriever
.
astream_events
(
"bar"
,
 version
=
"v1"
)
:
    
print
(
event
)


{'event': 'on_retriever_start', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'name': 'ToyRetriever', 'tags': [], 'metadata': {}, 'data': {'input': 'bar'}}
{'event': 'on_retriever_stream', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'tags': [], 'metadata': {}, 'name': 'ToyRetriever', 'data': {'chunk': []}}
{'event': 'on_retriever_end', 'name': 'ToyRetriever', 'run_id': 'f96f268d-8383-4921-b175-ca583924d9ff', 'tags': [], 'metadata': {}, 'data': {'output': []}}


Contributing
​


We appreciate contributions of interesting retrievers!


Here's a checklist to help make sure your contribution gets added to LangChain:


Documentation:




The retriever contains doc-strings for all initialization arguments, as these will be surfaced in the 
API Reference
.


The class doc-string for the model contains a link to any relevant APIs used for the retriever (e.g., if the retriever is retrieving from wikipedia, it'll be good to link to the wikipedia API!)




Tests:




 
Add unit or integration tests to verify that 
invoke
 and 
ainvoke
 work.




Optimizations:


If the retriever is connecting to external data sources (e.g., an API or a file), it'll almost certainly benefit from an async native optimization!




 
Provide a native async implementation of 
_aget_relevant_documents
 (used by 
ainvoke
)


Edit this page
Was this page helpful?
Previous
How to create a custom LLM class
Next
How to create tools
Overview
Interface
Example
Test it 🧪
Contributing


--- docs_how_to_custom_tools.txt ---

How-to guides
How to create tools
On this page
How to create tools


When constructing an 
agent
, you will need to provide it with a list of 
Tools
 that it can use. Besides the actual function that is called, the Tool consists of several components:


Attribute
Type
Description
name
str
Must be unique within a set of tools provided to an LLM or agent.
description
str
Describes what the tool does. Used as context by the LLM or agent.
args_schema
pydantic.BaseModel
Optional but recommended, and required if using callback handlers. It can be used to provide more information (e.g., few-shot examples) or validation for expected parameters.
return_direct
boolean
Only relevant for agents. When True, after invoking the given tool, the agent will stop and return the result direcly to the user.


LangChain supports the creation of tools from:




Functions;


LangChain 
Runnables
;


By sub-classing from 
BaseTool
 -- This is the most flexible method, it provides the largest degree of control, at the expense of more effort and code.




Creating tools from functions may be sufficient for most use cases, and can be done via a simple 
@tool decorator
. If more configuration is needed-- e.g., specification of both sync and async implementations-- one can also use the 
StructuredTool.from_function
 class method.


In this guide we provide an overview of these methods.


tip
Models will perform better if the tools have well chosen names, descriptions and JSON schemas.


Creating tools from functions
​


@tool decorator
​


This 
@tool
 decorator is the simplest way to define a custom tool. The decorator uses the function name as the tool name by default, but this can be overridden by passing a string as the first argument. Additionally, the decorator will use the function's docstring as the tool's description - so a docstring MUST be provided.


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two numbers."""
    
return
 a 
*
 b
# Let's inspect some of the attributes associated with the tool.
print
(
multiply
.
name
)
print
(
multiply
.
description
)
print
(
multiply
.
args
)
API Reference:
tool


multiply
Multiply two numbers.
{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}


Or create an 
async
 implementation, like this:


from
 langchain_core
.
tools 
import
 tool
@tool
async
 
def
 
amultiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two numbers."""
    
return
 a 
*
 b
API Reference:
tool


Note that 
@tool
 supports parsing of annotations, nested schemas, and other features:


from
 typing 
import
 Annotated
,
 List
@tool
def
 
multiply_by_max
(
    a
:
 Annotated
[
int
,
 
"scale factor"
]
,
    b
:
 Annotated
[
List
[
int
]
,
 
"list of ints over which to take maximum"
]
,
)
 
-
>
 
int
:
    
"""Multiply a by the maximum of b."""
    
return
 a 
*
 
max
(
b
)
print
(
multiply_by_max
.
args_schema
.
model_json_schema
(
)
)


{'description': 'Multiply a by the maximum of b.',
 'properties': {'a': {'description': 'scale factor',
   'title': 'A',
   'type': 'string'},
  'b': {'description': 'list of ints over which to take maximum',
   'items': {'type': 'integer'},
   'title': 'B',
   'type': 'array'}},
 'required': ['a', 'b'],
 'title': 'multiply_by_maxSchema',
 'type': 'object'}


You can also customize the tool name and JSON args by passing them into the tool decorator.


from
 pydantic 
import
 BaseModel
,
 Field
class
 
CalculatorInput
(
BaseModel
)
:
    a
:
 
int
 
=
 Field
(
description
=
"first number"
)
    b
:
 
int
 
=
 Field
(
description
=
"second number"
)
@tool
(
"multiplication-tool"
,
 args_schema
=
CalculatorInput
,
 return_direct
=
True
)
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two numbers."""
    
return
 a 
*
 b
# Let's inspect some of the attributes associated with the tool.
print
(
multiply
.
name
)
print
(
multiply
.
description
)
print
(
multiply
.
args
)
print
(
multiply
.
return_direct
)


multiplication-tool
Multiply two numbers.
{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}
True


Docstring parsing
​


@tool
 can optionally parse 
Google Style docstrings
 and associate the docstring components (such as arg descriptions) to the relevant parts of the tool schema. To toggle this behavior, specify 
parse_docstring
:


@tool
(
parse_docstring
=
True
)
def
 
foo
(
bar
:
 
str
,
 baz
:
 
int
)
 
-
>
 
str
:
    
"""The foo.
    Args:
        bar: The bar.
        baz: The baz.
    """
    
return
 bar
print
(
foo
.
args_schema
.
model_json_schema
(
)
)


{'description': 'The foo.',
 'properties': {'bar': {'description': 'The bar.',
   'title': 'Bar',
   'type': 'string'},
  'baz': {'description': 'The baz.', 'title': 'Baz', 'type': 'integer'}},
 'required': ['bar', 'baz'],
 'title': 'fooSchema',
 'type': 'object'}


caution
By default, 
@tool(parse_docstring=True)
 will raise 
ValueError
 if the docstring does not parse correctly. See 
API Reference
 for detail and examples.


StructuredTool
​


The 
StructuredTool.from_function
 class method provides a bit more configurability than the 
@tool
 decorator, without requiring much additional code.


from
 langchain_core
.
tools 
import
 StructuredTool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two numbers."""
    
return
 a 
*
 b
async
 
def
 
amultiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two numbers."""
    
return
 a 
*
 b
calculator 
=
 StructuredTool
.
from_function
(
func
=
multiply
,
 coroutine
=
amultiply
)
print
(
calculator
.
invoke
(
{
"a"
:
 
2
,
 
"b"
:
 
3
}
)
)
print
(
await
 calculator
.
ainvoke
(
{
"a"
:
 
2
,
 
"b"
:
 
5
}
)
)
API Reference:
StructuredTool


6
10


To configure it:


class
 
CalculatorInput
(
BaseModel
)
:
    a
:
 
int
 
=
 Field
(
description
=
"first number"
)
    b
:
 
int
 
=
 Field
(
description
=
"second number"
)
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two numbers."""
    
return
 a 
*
 b
calculator 
=
 StructuredTool
.
from_function
(
    func
=
multiply
,
    name
=
"Calculator"
,
    description
=
"multiply numbers"
,
    args_schema
=
CalculatorInput
,
    return_direct
=
True
,
    
# coroutine= ... <- you can specify an async method if desired as well
)
print
(
calculator
.
invoke
(
{
"a"
:
 
2
,
 
"b"
:
 
3
}
)
)
print
(
calculator
.
name
)
print
(
calculator
.
description
)
print
(
calculator
.
args
)


6
Calculator
multiply numbers
{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}


Creating tools from Runnables
​


LangChain 
Runnables
 that accept string or 
dict
 input can be converted to tools using the 
as_tool
 method, which allows for the specification of names, descriptions, and additional schema information for arguments.


Example usage:


from
 langchain_core
.
language_models 
import
 GenericFakeChatModel
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"human"
,
 
"Hello. Please respond in the style of {answer_style}."
)
]
)
# Placeholder LLM
llm 
=
 GenericFakeChatModel
(
messages
=
iter
(
[
"hello matey"
]
)
)
chain 
=
 prompt 
|
 llm 
|
 StrOutputParser
(
)
as_tool 
=
 chain
.
as_tool
(
    name
=
"Style responder"
,
 description
=
"Description of when to use tool."
)
as_tool
.
args
API Reference:
GenericFakeChatModel
 | 
StrOutputParser
 | 
ChatPromptTemplate


/var/folders/4j/2rz3865x6qg07tx43146py8h0000gn/T/ipykernel_95770/2548361071.py:14: LangChainBetaWarning: This API is in beta and may change in the future.
  as_tool = chain.as_tool(


{'answer_style': {'title': 'Answer Style', 'type': 'string'}}


See 
this guide
 for more detail.


Subclass BaseTool
​


You can define a custom tool by sub-classing from 
BaseTool
. This provides maximal control over the tool definition, but requires writing more code.


from
 typing 
import
 Optional
from
 langchain_core
.
callbacks 
import
 
(
    AsyncCallbackManagerForToolRun
,
    CallbackManagerForToolRun
,
)
from
 langchain_core
.
tools 
import
 BaseTool
from
 langchain_core
.
tools
.
base 
import
 ArgsSchema
from
 pydantic 
import
 BaseModel
,
 Field
class
 
CalculatorInput
(
BaseModel
)
:
    a
:
 
int
 
=
 Field
(
description
=
"first number"
)
    b
:
 
int
 
=
 Field
(
description
=
"second number"
)
# Note: It's important that every field has type hints. BaseTool is a
# Pydantic class and not having type hints can lead to unexpected behavior.
class
 
CustomCalculatorTool
(
BaseTool
)
:
    name
:
 
str
 
=
 
"Calculator"
    description
:
 
str
 
=
 
"useful for when you need to answer questions about math"
    args_schema
:
 Optional
[
ArgsSchema
]
 
=
 CalculatorInput
    return_direct
:
 
bool
 
=
 
True
    
def
 
_run
(
        self
,
 a
:
 
int
,
 b
:
 
int
,
 run_manager
:
 Optional
[
CallbackManagerForToolRun
]
 
=
 
None
    
)
 
-
>
 
int
:
        
"""Use the tool."""
        
return
 a 
*
 b
    
async
 
def
 
_arun
(
        self
,
        a
:
 
int
,
        b
:
 
int
,
        run_manager
:
 Optional
[
AsyncCallbackManagerForToolRun
]
 
=
 
None
,
    
)
 
-
>
 
int
:
        
"""Use the tool asynchronously."""
        
# If the calculation is cheap, you can just delegate to the sync implementation
        
# as shown below.
        
# If the sync calculation is expensive, you should delete the entire _arun method.
        
# LangChain will automatically provide a better implementation that will
        
# kick off the task in a thread to make sure it doesn't block other async code.
        
return
 self
.
_run
(
a
,
 b
,
 run_manager
=
run_manager
.
get_sync
(
)
)
API Reference:
AsyncCallbackManagerForToolRun
 | 
CallbackManagerForToolRun
 | 
BaseTool


multiply 
=
 CustomCalculatorTool
(
)
print
(
multiply
.
name
)
print
(
multiply
.
description
)
print
(
multiply
.
args
)
print
(
multiply
.
return_direct
)
print
(
multiply
.
invoke
(
{
"a"
:
 
2
,
 
"b"
:
 
3
}
)
)
print
(
await
 multiply
.
ainvoke
(
{
"a"
:
 
2
,
 
"b"
:
 
3
}
)
)


Calculator
useful for when you need to answer questions about math
{'a': {'description': 'first number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}
True
6
6


How to create async tools
​


LangChain Tools implement the 
Runnable interface 🏃
.


All Runnables expose the 
invoke
 and 
ainvoke
 methods (as well as other methods like 
batch
, 
abatch
, 
astream
 etc).


So even if you only provide an 
sync
 implementation of a tool, you could still use the 
ainvoke
 interface, but there
are some important things to know:




LangChain's by default provides an async implementation that assumes that the function is expensive to compute, so it'll delegate execution to another thread.


If you're working in an async codebase, you should create async tools rather than sync tools, to avoid incuring a small overhead due to that thread.


If you need both sync and async implementations, use 
StructuredTool.from_function
 or sub-class from 
BaseTool
.


If implementing both sync and async, and the sync code is fast to run, override the default LangChain async implementation and simply call the sync code.


You CANNOT and SHOULD NOT use the sync 
invoke
 with an 
async
 tool.




from
 langchain_core
.
tools 
import
 StructuredTool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two numbers."""
    
return
 a 
*
 b
calculator 
=
 StructuredTool
.
from_function
(
func
=
multiply
)
print
(
calculator
.
invoke
(
{
"a"
:
 
2
,
 
"b"
:
 
3
}
)
)
print
(
    
await
 calculator
.
ainvoke
(
{
"a"
:
 
2
,
 
"b"
:
 
5
}
)
)
  
# Uses default LangChain async implementation incurs small overhead
API Reference:
StructuredTool


6
10


from
 langchain_core
.
tools 
import
 StructuredTool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two numbers."""
    
return
 a 
*
 b
async
 
def
 
amultiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two numbers."""
    
return
 a 
*
 b
calculator 
=
 StructuredTool
.
from_function
(
func
=
multiply
,
 coroutine
=
amultiply
)
print
(
calculator
.
invoke
(
{
"a"
:
 
2
,
 
"b"
:
 
3
}
)
)
print
(
    
await
 calculator
.
ainvoke
(
{
"a"
:
 
2
,
 
"b"
:
 
5
}
)
)
  
# Uses use provided amultiply without additional overhead
API Reference:
StructuredTool


6
10


You should not and cannot use 
.invoke
 when providing only an async definition.


@tool
async
 
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two numbers."""
    
return
 a 
*
 b
try
:
    multiply
.
invoke
(
{
"a"
:
 
2
,
 
"b"
:
 
3
}
)
except
 NotImplementedError
:
    
print
(
"Raised not implemented error. You should not be doing this."
)


Raised not implemented error. You should not be doing this.


Handling Tool Errors
​


If you're using tools with agents, you will likely need an error handling strategy, so the agent can recover from the error and continue execution.


A simple strategy is to throw a 
ToolException
 from inside the tool and specify an error handler using 
handle_tool_error
.


When the error handler is specified, the exception will be caught and the error handler will decide which output to return from the tool.


You can set 
handle_tool_error
 to 
True
, a string value, or a function. If it's a function, the function should take a 
ToolException
 as a parameter and return a value.


Please note that only raising a 
ToolException
 won't be effective. You need to first set the 
handle_tool_error
 of the tool because its default value is 
False
.


from
 langchain_core
.
tools 
import
 ToolException
def
 
get_weather
(
city
:
 
str
)
 
-
>
 
int
:
    
"""Get weather for the given city."""
    
raise
 ToolException
(
f"Error: There is no city by the name of 
{
city
}
."
)
API Reference:
ToolException


Here's an example with the default 
handle_tool_error=True
 behavior.


get_weather_tool 
=
 StructuredTool
.
from_function
(
    func
=
get_weather
,
    handle_tool_error
=
True
,
)
get_weather_tool
.
invoke
(
{
"city"
:
 
"foobar"
}
)


'Error: There is no city by the name of foobar.'


We can set 
handle_tool_error
 to a string that will always be returned.


get_weather_tool 
=
 StructuredTool
.
from_function
(
    func
=
get_weather
,
    handle_tool_error
=
"There is no such city, but it's probably above 0K there!"
,
)
get_weather_tool
.
invoke
(
{
"city"
:
 
"foobar"
}
)


"There is no such city, but it's probably above 0K there!"


Handling the error using a function:


def
 
_handle_error
(
error
:
 ToolException
)
 
-
>
 
str
:
    
return
 
f"The following errors occurred during tool execution: `
{
error
.
args
[
0
]
}
`"
get_weather_tool 
=
 StructuredTool
.
from_function
(
    func
=
get_weather
,
    handle_tool_error
=
_handle_error
,
)
get_weather_tool
.
invoke
(
{
"city"
:
 
"foobar"
}
)


'The following errors occurred during tool execution: `Error: There is no city by the name of foobar.`'


Returning artifacts of Tool execution
​


Sometimes there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns custom objects like Documents, we may want to pass some view or metadata about this output to the model without passing the raw output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.


The Tool and 
ToolMessage
 interfaces make it possible to distinguish between the parts of the tool output meant for the model (this is the ToolMessage.content) and those parts which are meant for use outside the model (ToolMessage.artifact).


Requires 
langchain-core >= 0.2.19
This functionality was added in 
langchain-core == 0.2.19
. Please make sure your package is up to date.


If we want our tool to distinguish between message content and other artifacts, we need to specify 
response_format="content_and_artifact"
 when defining our tool and make sure that we return a tuple of (content, artifact):


import
 random
from
 typing 
import
 List
,
 Tuple
from
 langchain_core
.
tools 
import
 tool
@tool
(
response_format
=
"content_and_artifact"
)
def
 
generate_random_ints
(
min
:
 
int
,
 
max
:
 
int
,
 size
:
 
int
)
 
-
>
 Tuple
[
str
,
 List
[
int
]
]
:
    
"""Generate size random ints in the range [min, max]."""
    array 
=
 
[
random
.
randint
(
min
,
 
max
)
 
for
 _ 
in
 
range
(
size
)
]
    content 
=
 
f"Successfully generated array of 
{
size
}
 random ints in [
{
min
}
, 
{
max
}
]."
    
return
 content
,
 array
API Reference:
tool


If we invoke our tool directly with the tool arguments, we'll get back just the content part of the output:


generate_random_ints
.
invoke
(
{
"min"
:
 
0
,
 
"max"
:
 
9
,
 
"size"
:
 
10
}
)


'Successfully generated array of 10 random ints in [0, 9].'


If we invoke our tool with a ToolCall (like the ones generated by tool-calling models), we'll get back a ToolMessage that contains both the content and artifact generated by the Tool:


generate_random_ints
.
invoke
(
    
{
        
"name"
:
 
"generate_random_ints"
,
        
"args"
:
 
{
"min"
:
 
0
,
 
"max"
:
 
9
,
 
"size"
:
 
10
}
,
        
"id"
:
 
"123"
,
  
# required
        
"type"
:
 
"tool_call"
,
  
# required
    
}
)


ToolMessage(content='Successfully generated array of 10 random ints in [0, 9].', name='generate_random_ints', tool_call_id='123', artifact=[4, 8, 2, 4, 1, 0, 9, 5, 8, 1])


We can do the same when subclassing BaseTool:


from
 langchain_core
.
tools 
import
 BaseTool
class
 
GenerateRandomFloats
(
BaseTool
)
:
    name
:
 
str
 
=
 
"generate_random_floats"
    description
:
 
str
 
=
 
"Generate size random floats in the range [min, max]."
    response_format
:
 
str
 
=
 
"content_and_artifact"
    ndigits
:
 
int
 
=
 
2
    
def
 
_run
(
self
,
 
min
:
 
float
,
 
max
:
 
float
,
 size
:
 
int
)
 
-
>
 Tuple
[
str
,
 List
[
float
]
]
:
        range_ 
=
 
max
 
-
 
min
        array 
=
 
[
            
round
(
min
 
+
 
(
range_ 
*
 random
.
random
(
)
)
,
 ndigits
=
self
.
ndigits
)
            
for
 _ 
in
 
range
(
size
)
        
]
        content 
=
 
f"Generated 
{
size
}
 floats in [
{
min
}
, 
{
max
}
], rounded to 
{
self
.
ndigits
}
 decimals."
        
return
 content
,
 array
    
# Optionally define an equivalent async method
    
# async def _arun(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:
    
#     ...
API Reference:
BaseTool


rand_gen 
=
 GenerateRandomFloats
(
ndigits
=
4
)
rand_gen
.
invoke
(
    
{
        
"name"
:
 
"generate_random_floats"
,
        
"args"
:
 
{
"min"
:
 
0.1
,
 
"max"
:
 
3.3333
,
 
"size"
:
 
3
}
,
        
"id"
:
 
"123"
,
        
"type"
:
 
"tool_call"
,
    
}
)


ToolMessage(content='Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.', name='generate_random_floats', tool_call_id='123', artifact=[1.5566, 0.5134, 2.7914])
Edit this page
Was this page helpful?
Previous
Custom Retriever
Next
How to debug your LLM apps
Creating tools from functions
@tool decorator
StructuredTool
Creating tools from Runnables
Subclass BaseTool
How to create async tools
Handling Tool Errors
Returning artifacts of Tool execution


--- docs_how_to_debugging.txt ---

How-to guides
How to debug your LLM apps
On this page
How to debug your LLM apps


Like building any type of software, at some point you'll need to debug when building with LLMs. A model call will fail, or model output will be misformatted, or there will be some nested model calls and it won't be clear where along the way an incorrect output was created.


There are three main methods for debugging:




Verbose Mode: This adds print statements for "important" events in your chain.


Debug Mode: This add logging statements for ALL events in your chain.


LangSmith Tracing: This logs events to 
LangSmith
 to allow for visualization there.




Verbose Mode
Debug Mode
LangSmith Tracing
Free
✅
✅
✅
UI
❌
❌
✅
Persisted
❌
❌
✅
See all events
❌
✅
✅
See "important" events
✅
❌
✅
Runs Locally
✅
✅
❌


Tracing
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with 
LangSmith
.


After you sign up at the link above, make sure to set your environment variables to start logging traces:


export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."


Or, if in a notebook, you can set them with:


import
 getpass
import
 os
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Let's suppose we have an agent, and want to visualize the actions it takes and tool outputs it receives. Without any debugging, here's what we see:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


from
 langchain
.
agents 
import
 AgentExecutor
,
 create_tool_calling_agent
from
 langchain_community
.
tools
.
tavily_search 
import
 TavilySearchResults
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
tools 
=
 
[
TavilySearchResults
(
max_results
=
1
)
]
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"You are a helpful assistant."
,
        
)
,
        
(
"placeholder"
,
 
"{chat_history}"
)
,
        
(
"human"
,
 
"{input}"
)
,
        
(
"placeholder"
,
 
"{agent_scratchpad}"
)
,
    
]
)
# Construct the Tools agent
agent 
=
 create_tool_calling_agent
(
llm
,
 tools
,
 prompt
)
# Create an agent executor by passing in the agent and tools
agent_executor 
=
 AgentExecutor
(
agent
=
agent
,
 tools
=
tools
)
agent_executor
.
invoke
(
    
{
"input"
:
 
"Who directed the 2023 film Oppenheimer and what is their age in days?"
}
)
API Reference:
AgentExecutor
 | 
create_tool_calling_agent
 | 
TavilySearchResults
 | 
ChatPromptTemplate


{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?',
 'output': 'The 2023 film "Oppenheimer" was directed by Christopher Nolan.\n\nTo calculate Christopher Nolan\'s age in days, we first need his birthdate, which is July 30, 1970. Let\'s calculate his age in days from his birthdate to today\'s date, December 7, 2023.\n\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\n2. Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\n3. From July 30, 2023, to December 7, 2023, is 130 days.\n\nNow, calculate the total days:\n- 53 years = 53 x 365 = 19,345 days\n- Adding leap years from 1970 to 2023: There are 13 leap years (1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020). So, add 13 days.\n- Total days from years and leap years = 19,345 + 13 = 19,358 days\n- Add the days from July 30, 2023, to December 7, 2023 = 130 days\n\nTotal age in days = 19,358 + 130 = 19,488 days\n\nChristopher Nolan is 19,488 days old as of December 7, 2023.'}


We don't get much output, but since we set up LangSmith we can easily see what happened under the hood:


https://smith.langchain.com/public/a89ff88f-9ddc-4757-a395-3a1b365655bf/r


set_debug
 and 
set_verbose
​


If you're prototyping in Jupyter Notebooks or running Python scripts, it can be helpful to print out the intermediate steps of a chain run.


There are a number of ways to enable printing at varying degrees of verbosity.


Note: These still work even with LangSmith enabled, so you can have both turned on and running at the same time


set_verbose(True)
​


Setting the 
verbose
 flag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic.


from
 langchain
.
globals
 
import
 set_verbose
set_verbose
(
True
)
agent_executor 
=
 AgentExecutor
(
agent
=
agent
,
 tools
=
tools
)
agent_executor
.
invoke
(
    
{
"input"
:
 
"Who directed the 2023 film Oppenheimer and what is their age in days?"
}
)
API Reference:
set_verbose


[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': 'director of the 2023 film Oppenheimer'}`
[0m[36;1m[1;3m[{'url': 'https://m.imdb.com/title/tt15398776/', 'content': 'Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.'}][0m[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': 'birth date of Christopher Nolan'}`
[0m[36;1m[1;3m[{'url': 'https://m.imdb.com/name/nm0634240/bio/', 'content': 'Christopher Nolan. Writer: Tenet. Best known for his cerebral, often nonlinear, storytelling, acclaimed Academy Award winner writer/director/producer Sir Christopher Nolan CBE was born in London, England. Over the course of more than 25 years of filmmaking, Nolan has gone from low-budget independent films to working on some of the biggest blockbusters ever made and became one of the most ...'}][0m[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': 'Christopher Nolan birth date'}`
responded: The 2023 film **Oppenheimer** was directed by **Christopher Nolan**.
To calculate Christopher Nolan's age in days, I need his exact birth date. Let me find that information for you.
[0m[36;1m[1;3m[{'url': 'https://m.imdb.com/name/nm0634240/bio/', 'content': 'Christopher Nolan. Writer: Tenet. Best known for his cerebral, often nonlinear, storytelling, acclaimed Academy Award winner writer/director/producer Sir Christopher Nolan CBE was born in London, England. Over the course of more than 25 years of filmmaking, Nolan has gone from low-budget independent films to working on some of the biggest blockbusters ever made and became one of the most ...'}][0m[32;1m[1;3m
Invoking: `tavily_search_results_json` with `{'query': 'Christopher Nolan date of birth'}`
responded: It appears that I need to refine my search to get the exact birth date of Christopher Nolan. Let me try again to find that specific information.
[0m[36;1m[1;3m[{'url': 'https://m.imdb.com/name/nm0634240/bio/', 'content': 'Christopher Nolan. Writer: Tenet. Best known for his cerebral, often nonlinear, storytelling, acclaimed Academy Award winner writer/director/producer Sir Christopher Nolan CBE was born in London, England. Over the course of more than 25 years of filmmaking, Nolan has gone from low-budget independent films to working on some of the biggest blockbusters ever made and became one of the most ...'}][0m[32;1m[1;3mI am currently unable to retrieve the exact birth date of Christopher Nolan from the sources available. However, it is widely known that he was born on July 30, 1970. Using this date, I can calculate his age in days as of today.
Let's calculate:
- Christopher Nolan's birth date: July 30, 1970.
- Today's date: December 7, 2023.
The number of days between these two dates can be calculated as follows:
1. From July 30, 1970, to July 30, 2023, is 53 years.
2. From July 30, 2023, to December 7, 2023, is 130 days.
Calculating the total days for 53 years (considering leap years):
- 53 years × 365 days/year = 19,345 days
- Adding leap years (1972, 1976, ..., 2020, 2024 - 13 leap years): 13 days
Total days from birth until July 30, 2023: 19,345 + 13 = 19,358 days
Adding the days from July 30, 2023, to December 7, 2023: 130 days
Total age in days as of December 7, 2023: 19,358 + 130 = 19,488 days.
Therefore, Christopher Nolan is 19,488 days old as of December 7, 2023.[0m
[1m> Finished chain.[0m


{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?',
 'output': "I am currently unable to retrieve the exact birth date of Christopher Nolan from the sources available. However, it is widely known that he was born on July 30, 1970. Using this date, I can calculate his age in days as of today.\n\nLet's calculate:\n\n- Christopher Nolan's birth date: July 30, 1970.\n- Today's date: December 7, 2023.\n\nThe number of days between these two dates can be calculated as follows:\n\n1. From July 30, 1970, to July 30, 2023, is 53 years.\n2. From July 30, 2023, to December 7, 2023, is 130 days.\n\nCalculating the total days for 53 years (considering leap years):\n- 53 years × 365 days/year = 19,345 days\n- Adding leap years (1972, 1976, ..., 2020, 2024 - 13 leap years): 13 days\n\nTotal days from birth until July 30, 2023: 19,345 + 13 = 19,358 days\nAdding the days from July 30, 2023, to December 7, 2023: 130 days\n\nTotal age in days as of December 7, 2023: 19,358 + 130 = 19,488 days.\n\nTherefore, Christopher Nolan is 19,488 days old as of December 7, 2023."}


set_debug(True)
​


Setting the global 
debug
 flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate. This is the most verbose setting and will fully log raw inputs and outputs.


from
 langchain
.
globals
 
import
 set_debug
set_debug
(
True
)
set_verbose
(
False
)
agent_executor 
=
 AgentExecutor
(
agent
=
agent
,
 tools
=
tools
)
agent_executor
.
invoke
(
    
{
"input"
:
 
"Who directed the 2023 film Oppenheimer and what is their age in days?"
}
)
API Reference:
set_debug


[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor] Entering Chain run with input:
[0m{
  "input": "Who directed the 2023 film Oppenheimer and what is their age in days?"
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad> > 5:chain:RunnableLambda] Entering Chain run with input:
[0m{
  "input": ""
}
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad> > 5:chain:RunnableLambda] [1ms] Exiting Chain run with output:
[0m{
  "output": []
}
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad> > 4:chain:RunnableParallel<agent_scratchpad>] [2ms] Exiting Chain run with output:
[0m{
  "agent_scratchpad": []
}
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 3:chain:RunnableAssign<agent_scratchpad>] [5ms] Exiting Chain run with output:
[0m{
  "input": "Who directed the 2023 film Oppenheimer and what is their age in days?",
  "intermediate_steps": [],
  "agent_scratchpad": []
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] Entering Prompt run with input:
[0m{
  "input": "Who directed the 2023 film Oppenheimer and what is their age in days?",
  "intermediate_steps": [],
  "agent_scratchpad": []
}
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:
[0m[outputs]
[32;1m[1;3m[llm/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 7:llm:ChatOpenAI] Entering LLM run with input:
[0m{
  "prompts": [
    "System: You are a helpful assistant.\nHuman: Who directed the 2023 film Oppenheimer and what is their age in days?"
  ]
}
[36;1m[1;3m[llm/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 7:llm:ChatOpenAI] [3.17s] Exiting LLM run with output:
[0m{
  "generations": [
    [
      {
        "text": "",
        "generation_info": {
          "finish_reason": "tool_calls"
        },
        "type": "ChatGenerationChunk",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": "",
            "example": false,
            "additional_kwargs": {
              "tool_calls": [
                {
                  "index": 0,
                  "id": "call_fnfq6GjSQED4iF6lo4rxkUup",
                  "function": {
                    "arguments": "{\"query\": \"director of the 2023 film Oppenheimer\"}",
                    "name": "tavily_search_results_json"
                  },
                  "type": "function"
                },
                {
                  "index": 1,
                  "id": "call_mwhVi6pk49f4OIo5rOWrr4TD",
                  "function": {
                    "arguments": "{\"query\": \"birth date of Christopher Nolan\"}",
                    "name": "tavily_search_results_json"
                  },
                  "type": "function"
                }
              ]
            },
            "tool_call_chunks": [
              {
                "name": "tavily_search_results_json",
                "args": "{\"query\": \"director of the 2023 film Oppenheimer\"}",
                "id": "call_fnfq6GjSQED4iF6lo4rxkUup",
                "index": 0
              },
              {
                "name": "tavily_search_results_json",
                "args": "{\"query\": \"birth date of Christopher Nolan\"}",
                "id": "call_mwhVi6pk49f4OIo5rOWrr4TD",
                "index": 1
              }
            ],
            "response_metadata": {
              "finish_reason": "tool_calls"
            },
            "id": "run-6e160323-15f9-491d-aadf-b5d337e9e2a1",
            "tool_calls": [
              {
                "name": "tavily_search_results_json",
                "args": {
                  "query": "director of the 2023 film Oppenheimer"
                },
                "id": "call_fnfq6GjSQED4iF6lo4rxkUup"
              },
              {
                "name": "tavily_search_results_json",
                "args": {
                  "query": "birth date of Christopher Nolan"
                },
                "id": "call_mwhVi6pk49f4OIo5rOWrr4TD"
              }
            ],
            "invalid_tool_calls": []
          }
        }
      }
    ]
  ],
  "llm_output": null,
  "run": null
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 8:parser:ToolsAgentOutputParser] Entering Parser run with input:
[0m[inputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence > 8:parser:ToolsAgentOutputParser] [1ms] Exiting Parser run with output:
[0m[outputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 2:chain:RunnableSequence] [3.18s] Exiting Chain run with output:
[0m[outputs]
[32;1m[1;3m[tool/start][0m [1m[1:chain:AgentExecutor > 9:tool:tavily_search_results_json] Entering Tool run with input:
[0m"{'query': 'director of the 2023 film Oppenheimer'}"
``````output
Error in ConsoleCallbackHandler.on_tool_end callback: AttributeError("'list' object has no attribute 'strip'")
``````output
[32;1m[1;3m[tool/start][0m [1m[1:chain:AgentExecutor > 10:tool:tavily_search_results_json] Entering Tool run with input:
[0m"{'query': 'birth date of Christopher Nolan'}"
``````output
Error in ConsoleCallbackHandler.on_tool_end callback: AttributeError("'list' object has no attribute 'strip'")
``````output
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad> > 13:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:
[0m{
  "input": ""
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad> > 13:chain:RunnableParallel<agent_scratchpad> > 14:chain:RunnableLambda] Entering Chain run with input:
[0m{
  "input": ""
}
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad> > 13:chain:RunnableParallel<agent_scratchpad> > 14:chain:RunnableLambda] [1ms] Exiting Chain run with output:
[0m[outputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad> > 13:chain:RunnableParallel<agent_scratchpad>] [4ms] Exiting Chain run with output:
[0m[outputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 12:chain:RunnableAssign<agent_scratchpad>] [8ms] Exiting Chain run with output:
[0m[outputs]
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 15:prompt:ChatPromptTemplate] Entering Prompt run with input:
[0m[inputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 15:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:
[0m[outputs]
[32;1m[1;3m[llm/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 16:llm:ChatOpenAI] Entering LLM run with input:
[0m{
  "prompts": [
    "System: You are a helpful assistant.\nHuman: Who directed the 2023 film Oppenheimer and what is their age in days?\nAI: \nTool: [{\"url\": \"https://m.imdb.com/title/tt15398776/fullcredits/\", \"content\": \"Oppenheimer (2023) cast and crew credits, including actors, actresses, directors, writers and more. Menu. ... director of photography: behind-the-scenes Jason Gary ... best boy grip ... film loader Luc Poullain ... aerial coordinator\"}]\nTool: [{\"url\": \"https://en.wikipedia.org/wiki/Christopher_Nolan\", \"content\": \"In early 2003, Nolan approached Warner Bros. with the idea of making a new Batman film, based on the character's origin story.[58] Nolan was fascinated by the notion of grounding it in a more realistic world than a comic-book fantasy.[59] He relied heavily on traditional stunts and miniature effects during filming, with minimal use of computer-generated imagery (CGI).[60] Batman Begins (2005), the biggest project Nolan had undertaken to that point,[61] was released to critical acclaim and commercial success.[62][63] Starring Christian Bale as Bruce Wayne / Batman—along with Michael Caine, Gary Oldman, Morgan Freeman and Liam Neeson—Batman Begins revived the franchise.[64][65] Batman Begins was 2005's ninth-highest-grossing film and was praised for its psychological depth and contemporary relevance;[63][66] it is cited as one of the most influential films of the 2000s.[67] Film author Ian Nathan wrote that within five years of his career, Nolan \\\"[went] from unknown to indie darling to gaining creative control over one of the biggest properties in Hollywood, and (perhaps unwittingly) fomenting the genre that would redefine the entire industry\\\".[68]\\nNolan directed, co-wrote and produced The Prestige (2006), an adaptation of the Christopher Priest novel about two rival 19th-century magicians.[69] He directed, wrote and edited the short film Larceny (1996),[19] which was filmed over a weekend in black and white with limited equipment and a small cast and crew.[12][20] Funded by Nolan and shot with the UCL Union Film society's equipment, it appeared at the Cambridge Film Festival in 1996 and is considered one of UCL's best shorts.[21] For unknown reasons, the film has since been removed from public view.[19] Nolan filmed a third short, Doodlebug (1997), about a man seemingly chasing an insect with his shoe, only to discover that it is a miniature of himself.[14][22] Nolan and Thomas first attempted to make a feature in the mid-1990s with Larry Mahoney, which they scrapped.[23] During this period in his career, Nolan had little to no success getting his projects off the ground, facing several rejections; he added, \\\"[T]here's a very limited pool of finance in the UK. Philosophy professor David Kyle Johnson wrote that \\\"Inception became a classic almost as soon as it was projected on silver screens\\\", praising its exploration of philosophical ideas, including leap of faith and allegory of the cave.[97] The film grossed over $836 million worldwide.[98] Nominated for eight Academy Awards—including Best Picture and Best Original Screenplay—it won Best Cinematography, Best Sound Mixing, Best Sound Editing and Best Visual Effects.[99] Nolan was nominated for a BAFTA Award and a Golden Globe Award for Best Director, among other accolades.[40]\\nAround the release of The Dark Knight Rises (2012), Nolan's third and final Batman film, Joseph Bevan of the British Film Institute wrote a profile on him: \\\"In the space of just over a decade, Christopher Nolan has shot from promising British indie director to undisputed master of a new brand of intelligent escapism. He further wrote that Nolan's body of work reflect \\\"a heterogeneity of conditions of products\\\" extending from low-budget films to lucrative blockbusters, \\\"a wide range of genres and settings\\\" and \\\"a diversity of styles that trumpet his versatility\\\".[193]\\nDavid Bordwell, a film theorist, wrote that Nolan has been able to blend his \\\"experimental impulses\\\" with the demands of mainstream entertainment, describing his oeuvre as \\\"experiments with cinematic time by means of techniques of subjective viewpoint and crosscutting\\\".[194] Nolan's use of practical, in-camera effects, miniatures and models, as well as shooting on celluloid film, has been highly influential in early 21st century cinema.[195][196] IndieWire wrote in 2019 that, Nolan \\\"kept a viable alternate model of big-budget filmmaking alive\\\", in an era where blockbuster filmmaking has become \\\"a largely computer-generated art form\\\".[196] Initially reluctant to make a sequel, he agreed after Warner Bros. repeatedly insisted.[78] Nolan wanted to expand on the noir quality of the first film by broadening the canvas and taking on \\\"the dynamic of a story of the city, a large crime story ... where you're looking at the police, the justice system, the vigilante, the poor people, the rich people, the criminals\\\".[79] Continuing to minimalise the use of CGI, Nolan employed high-resolution IMAX cameras, making it the first major motion picture to use this technology.[80][81]\"}]"
  ]
}
[36;1m[1;3m[llm/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 16:llm:ChatOpenAI] [20.22s] Exiting LLM run with output:
[0m{
  "generations": [
    [
      {
        "text": "The 2023 film \"Oppenheimer\" was directed by Christopher Nolan.\n\nTo calculate Christopher Nolan's age in days, we first need his birth date, which is July 30, 1970. Let's calculate his age in days from his birth date to today's date, December 7, 2023.\n\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\n2. Christopher Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\n3. From July 30, 2023, to December 7, 2023, is 130 days.\n\nNow, calculate the total days for 53 years:\n- Each year has 365 days, so 53 years × 365 days/year = 19,345 days.\n- Adding the leap years from 1970 to 2023: 1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, and 2024 (up to February). This gives us 14 leap years.\n- Total days from leap years: 14 days.\n\nAdding all together:\n- Total days = 19,345 days (from years) + 14 days (from leap years) + 130 days (from July 30, 2023, to December 7, 2023) = 19,489 days.\n\nTherefore, as of December 7, 2023, Christopher Nolan is 19,489 days old.",
        "generation_info": {
          "finish_reason": "stop"
        },
        "type": "ChatGenerationChunk",
        "message": {
          "lc": 1,
          "type": "constructor",
          "id": [
            "langchain",
            "schema",
            "messages",
            "AIMessageChunk"
          ],
          "kwargs": {
            "content": "The 2023 film \"Oppenheimer\" was directed by Christopher Nolan.\n\nTo calculate Christopher Nolan's age in days, we first need his birth date, which is July 30, 1970. Let's calculate his age in days from his birth date to today's date, December 7, 2023.\n\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\n2. Christopher Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\n3. From July 30, 2023, to December 7, 2023, is 130 days.\n\nNow, calculate the total days for 53 years:\n- Each year has 365 days, so 53 years × 365 days/year = 19,345 days.\n- Adding the leap years from 1970 to 2023: 1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, and 2024 (up to February). This gives us 14 leap years.\n- Total days from leap years: 14 days.\n\nAdding all together:\n- Total days = 19,345 days (from years) + 14 days (from leap years) + 130 days (from July 30, 2023, to December 7, 2023) = 19,489 days.\n\nTherefore, as of December 7, 2023, Christopher Nolan is 19,489 days old.",
            "example": false,
            "additional_kwargs": {},
            "tool_call_chunks": [],
            "response_metadata": {
              "finish_reason": "stop"
            },
            "id": "run-1c08a44f-db70-4836-935b-417caaf422a5",
            "tool_calls": [],
            "invalid_tool_calls": []
          }
        }
      }
    ]
  ],
  "llm_output": null,
  "run": null
}
[32;1m[1;3m[chain/start][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 17:parser:ToolsAgentOutputParser] Entering Parser run with input:
[0m[inputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence > 17:parser:ToolsAgentOutputParser] [2ms] Exiting Parser run with output:
[0m[outputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor > 11:chain:RunnableSequence] [20.27s] Exiting Chain run with output:
[0m[outputs]
[36;1m[1;3m[chain/end][0m [1m[1:chain:AgentExecutor] [26.37s] Exiting Chain run with output:
[0m{
  "output": "The 2023 film \"Oppenheimer\" was directed by Christopher Nolan.\n\nTo calculate Christopher Nolan's age in days, we first need his birth date, which is July 30, 1970. Let's calculate his age in days from his birth date to today's date, December 7, 2023.\n\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\n2. Christopher Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\n3. From July 30, 2023, to December 7, 2023, is 130 days.\n\nNow, calculate the total days for 53 years:\n- Each year has 365 days, so 53 years × 365 days/year = 19,345 days.\n- Adding the leap years from 1970 to 2023: 1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, and 2024 (up to February). This gives us 14 leap years.\n- Total days from leap years: 14 days.\n\nAdding all together:\n- Total days = 19,345 days (from years) + 14 days (from leap years) + 130 days (from July 30, 2023, to December 7, 2023) = 19,489 days.\n\nTherefore, as of December 7, 2023, Christopher Nolan is 19,489 days old."
}


{'input': 'Who directed the 2023 film Oppenheimer and what is their age in days?',
 'output': 'The 2023 film "Oppenheimer" was directed by Christopher Nolan.\n\nTo calculate Christopher Nolan\'s age in days, we first need his birth date, which is July 30, 1970. Let\'s calculate his age in days from his birth date to today\'s date, December 7, 2023.\n\n1. Calculate the total number of days from July 30, 1970, to December 7, 2023.\n2. Christopher Nolan was born on July 30, 1970. From July 30, 1970, to July 30, 2023, is 53 years.\n3. From July 30, 2023, to December 7, 2023, is 130 days.\n\nNow, calculate the total days for 53 years:\n- Each year has 365 days, so 53 years × 365 days/year = 19,345 days.\n- Adding the leap years from 1970 to 2023: 1972, 1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020, and 2024 (up to February). This gives us 14 leap years.\n- Total days from leap years: 14 days.\n\nAdding all together:\n- Total days = 19,345 days (from years) + 14 days (from leap years) + 130 days (from July 30, 2023, to December 7, 2023) = 19,489 days.\n\nTherefore, as of December 7, 2023, Christopher Nolan is 19,489 days old.'}
Edit this page
Was this page helpful?
Previous
How to create tools
Next
How to load CSVs
Tracing
set_debug
 and 
set_verbose
set_verbose(True)
set_debug(True)


--- docs_how_to_document_loader_csv.txt ---

How-to guides
How to load CSVs
On this page
How to load CSVs


A 
comma-separated values (CSV)
 file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.


LangChain implements a 
CSV Loader
 that will load CSV files into a sequence of 
Document
 objects. Each row of the CSV file is translated to one document.


from
 langchain_community
.
document_loaders
.
csv_loader 
import
 CSVLoader
file_path 
=
 
"../integrations/document_loaders/example_data/mlb_teams_2012.csv"
loader 
=
 CSVLoader
(
file_path
=
file_path
)
data 
=
 loader
.
load
(
)
for
 record 
in
 data
[
:
2
]
:
    
print
(
record
)
API Reference:
CSVLoader


page_content='Team: Nationals\n"Payroll (millions)": 81.34\n"Wins": 98' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 0}
page_content='Team: Reds\n"Payroll (millions)": 82.20\n"Wins": 97' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 1}


Customizing the CSV parsing and loading
​


CSVLoader
 will accept a 
csv_args
 kwarg that supports customization of arguments passed to Python's 
csv.DictReader
. See the 
csv module
 documentation for more information of what csv args are supported.


loader 
=
 CSVLoader
(
    file_path
=
file_path
,
    csv_args
=
{
        
"delimiter"
:
 
","
,
        
"quotechar"
:
 
'"'
,
        
"fieldnames"
:
 
[
"MLB Team"
,
 
"Payroll in millions"
,
 
"Wins"
]
,
    
}
,
)
data 
=
 loader
.
load
(
)
for
 record 
in
 data
[
:
2
]
:
    
print
(
record
)


page_content='MLB Team: Team\nPayroll in millions: "Payroll (millions)"\nWins: "Wins"' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 0}
page_content='MLB Team: Nationals\nPayroll in millions: 81.34\nWins: 98' metadata={'source': '../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv', 'row': 1}


Specify a column to identify the document source
​


The 
"source"
 key on 
Document
 metadata can be set using a column of the CSV. Use the 
source_column
 argument to specify a source for the document created from each row. Otherwise 
file_path
 will be used as the source for all documents created from the CSV file.


This is useful when using documents loaded from CSV files for chains that answer questions using sources.


loader 
=
 CSVLoader
(
file_path
=
file_path
,
 source_column
=
"Team"
)
data 
=
 loader
.
load
(
)
for
 record 
in
 data
[
:
2
]
:
    
print
(
record
)


page_content='Team: Nationals\n"Payroll (millions)": 81.34\n"Wins": 98' metadata={'source': 'Nationals', 'row': 0}
page_content='Team: Reds\n"Payroll (millions)": 82.20\n"Wins": 97' metadata={'source': 'Reds', 'row': 1}


Load from a string
​


Python's 
tempfile
 can be used when working with CSV strings directly.


import
 tempfile
from
 io 
import
 StringIO
string_data 
=
 
"""
"Team", "Payroll (millions)", "Wins"
"Nationals",     81.34, 98
"Reds",          82.20, 97
"Yankees",      197.96, 95
"Giants",       117.62, 94
"""
.
strip
(
)
with
 tempfile
.
NamedTemporaryFile
(
delete
=
False
,
 mode
=
"w+"
)
 
as
 temp_file
:
    temp_file
.
write
(
string_data
)
    temp_file_path 
=
 temp_file
.
name
loader 
=
 CSVLoader
(
file_path
=
temp_file_path
)
data 
=
 loader
.
load
(
)
for
 record 
in
 data
[
:
2
]
:
    
print
(
record
)


page_content='Team: Nationals\n"Payroll (millions)": 81.34\n"Wins": 98' metadata={'source': 'Nationals', 'row': 0}
page_content='Team: Reds\n"Payroll (millions)": 82.20\n"Wins": 97' metadata={'source': 'Reds', 'row': 1}
Edit this page
Was this page helpful?
Previous
How to debug your LLM apps
Next
How to load documents from a directory
Customizing the CSV parsing and loading
Specify a column to identify the document source
Load from a string


--- docs_how_to_document_loader_custom.txt ---

How-to guides
Custom Document Loader
On this page
How to create a custom Document Loader


Overview
​


Applications based on LLMs frequently entail extracting data from databases or files, like PDFs, and converting it into a format that LLMs can utilize. In LangChain, this usually involves creating Document objects, which encapsulate the extracted text (
page_content
) along with metadata—a dictionary containing details about the document, such as the author's name or the date of publication.


Document
 objects are often formatted into prompts that are fed into an LLM, allowing the LLM to use the information in the 
Document
 to generate a desired response (e.g., summarizing the document).

Documents
 can be either used immediately or indexed into a vectorstore for future retrieval and use.


The main abstractions for 
Document Loading
 are:


Component
Description
Document
Contains 
text
 and 
metadata
BaseLoader
Use to convert raw data into 
Documents
Blob
A representation of binary data that's located either in a file or in memory
BaseBlobParser
Logic to parse a 
Blob
 to yield 
Document
 objects


This guide will demonstrate how to write custom document loading and file parsing logic; specifically, we'll see how to:




Create a standard document Loader by sub-classing from 
BaseLoader
.


Create a parser using  
BaseBlobParser
 and use it in conjunction with 
Blob
 and 
BlobLoaders
. This is useful primarily when working with files.




Standard Document Loader
​


A document loader can be implemented by sub-classing from a 
BaseLoader
 which provides a standard interface for loading documents.


Interface
​


Method Name
Explanation
lazy_load
Used to load documents one by one 
lazily
. Use for production code.
alazy_load
Async variant of 
lazy_load
load
Used to load all the documents into memory 
eagerly
. Use for prototyping or interactive work.
aload
Used to load all the documents into memory 
eagerly
. Use for prototyping or interactive work. 
Added in 2024-04 to LangChain.




The 
load
 methods is a convenience method meant solely for prototyping work -- it just invokes 
list(self.lazy_load())
.


The 
alazy_load
 has a default implementation that will delegate to 
lazy_load
. If you're using async, we recommend overriding the default implementation and providing a native async implementation.




important
When implementing a document loader do 
NOT
 provide parameters via the 
lazy_load
 or 
alazy_load
 methods.
All configuration is expected to be passed through the initializer (
init
). This was a design choice made by LangChain to make sure that once a document loader has been instantiated it has all the information needed to load documents.


Implementation
​


Let's create an example of a standard document loader that loads a file and creates a document from each line in the file.


from
 typing 
import
 AsyncIterator
,
 Iterator
from
 langchain_core
.
document_loaders 
import
 BaseLoader
from
 langchain_core
.
documents 
import
 Document
class
 
CustomDocumentLoader
(
BaseLoader
)
:
    
"""An example document loader that reads a file line by line."""
    
def
 
__init__
(
self
,
 file_path
:
 
str
)
 
-
>
 
None
:
        
"""Initialize the loader with a file path.
        Args:
            file_path: The path to the file to load.
        """
        self
.
file_path 
=
 file_path
    
def
 
lazy_load
(
self
)
 
-
>
 Iterator
[
Document
]
:
  
# <-- Does not take any arguments
        
"""A lazy loader that reads a file line by line.
        When you're implementing lazy load methods, you should use a generator
        to yield documents one by one.
        """
        
with
 
open
(
self
.
file_path
,
 encoding
=
"utf-8"
)
 
as
 f
:
            line_number 
=
 
0
            
for
 line 
in
 f
:
                
yield
 Document
(
                    page_content
=
line
,
                    metadata
=
{
"line_number"
:
 line_number
,
 
"source"
:
 self
.
file_path
}
,
                
)
                line_number 
+=
 
1
    
# alazy_load is OPTIONAL.
    
# If you leave out the implementation, a default implementation which delegates to lazy_load will be used!
    
async
 
def
 
alazy_load
(
        self
,
    
)
 
-
>
 AsyncIterator
[
Document
]
:
  
# <-- Does not take any arguments
        
"""An async lazy loader that reads a file line by line."""
        
# Requires aiofiles (install with pip)
        
# https://github.com/Tinche/aiofiles
        
import
 aiofiles
        
async
 
with
 aiofiles
.
open
(
self
.
file_path
,
 encoding
=
"utf-8"
)
 
as
 f
:
            line_number 
=
 
0
            
async
 
for
 line 
in
 f
:
                
yield
 Document
(
                    page_content
=
line
,
                    metadata
=
{
"line_number"
:
 line_number
,
 
"source"
:
 self
.
file_path
}
,
                
)
                line_number 
+=
 
1
API Reference:
BaseLoader
 | 
Document


Test 🧪
​


To test out the document loader, we need a file with some quality content.


with
 
open
(
"./meow.txt"
,
 
"w"
,
 encoding
=
"utf-8"
)
 
as
 f
:
    quality_content 
=
 
"meow meow🐱 \n meow meow🐱 \n meow😻😻"
    f
.
write
(
quality_content
)
loader 
=
 CustomDocumentLoader
(
"./meow.txt"
)


## Test out the lazy load interface
for
 doc 
in
 loader
.
lazy_load
(
)
:
    
print
(
)
    
print
(
type
(
doc
)
)
    
print
(
doc
)


<class 'langchain_core.documents.base.Document'>
page_content='meow meow🐱 \n' metadata={'line_number': 0, 'source': './meow.txt'}
<class 'langchain_core.documents.base.Document'>
page_content=' meow meow🐱 \n' metadata={'line_number': 1, 'source': './meow.txt'}
<class 'langchain_core.documents.base.Document'>
page_content=' meow😻😻' metadata={'line_number': 2, 'source': './meow.txt'}


## Test out the async implementation
async
 
for
 doc 
in
 loader
.
alazy_load
(
)
:
    
print
(
)
    
print
(
type
(
doc
)
)
    
print
(
doc
)


<class 'langchain_core.documents.base.Document'>
page_content='meow meow🐱 \n' metadata={'line_number': 0, 'source': './meow.txt'}
<class 'langchain_core.documents.base.Document'>
page_content=' meow meow🐱 \n' metadata={'line_number': 1, 'source': './meow.txt'}
<class 'langchain_core.documents.base.Document'>
page_content=' meow😻😻' metadata={'line_number': 2, 'source': './meow.txt'}


tip
load()
 can be helpful in an interactive environment such as a jupyter notebook.
Avoid using it for production code since eager loading assumes that all the content
can fit into memory, which is not always the case, especially for enterprise data.


loader
.
load
(
)


[Document(page_content='meow meow🐱 \n', metadata={'line_number': 0, 'source': './meow.txt'}),
 Document(page_content=' meow meow🐱 \n', metadata={'line_number': 1, 'source': './meow.txt'}),
 Document(page_content=' meow😻😻', metadata={'line_number': 2, 'source': './meow.txt'})]


Working with Files
​


Many document loaders involve parsing files. The difference between such loaders usually stems from how the file is parsed, rather than how the file is loaded. For example, you can use 
open
 to read the binary content of either a PDF or a markdown file, but you need different parsing logic to convert that binary data into text.


As a result, it can be helpful to decouple the parsing logic from the loading logic, which makes it easier to re-use a given parser regardless of how the data was loaded.


BaseBlobParser
​


A 
BaseBlobParser
 is an interface that accepts a 
blob
 and outputs a list of 
Document
 objects. A 
blob
 is a representation of data that lives either in memory or in a file. LangChain python has a 
Blob
 primitive which is inspired by the 
Blob WebAPI spec
.


from
 langchain_core
.
document_loaders 
import
 BaseBlobParser
,
 Blob
class
 
MyParser
(
BaseBlobParser
)
:
    
"""A simple parser that creates a document from each line."""
    
def
 
lazy_parse
(
self
,
 blob
:
 Blob
)
 
-
>
 Iterator
[
Document
]
:
        
"""Parse a blob into a document line by line."""
        line_number 
=
 
0
        
with
 blob
.
as_bytes_io
(
)
 
as
 f
:
            
for
 line 
in
 f
:
                line_number 
+=
 
1
                
yield
 Document
(
                    page_content
=
line
,
                    metadata
=
{
"line_number"
:
 line_number
,
 
"source"
:
 blob
.
source
}
,
                
)
API Reference:
BaseBlobParser
 | 
Blob


blob 
=
 Blob
.
from_path
(
"./meow.txt"
)
parser 
=
 MyParser
(
)


list
(
parser
.
lazy_parse
(
blob
)
)


[Document(page_content='meow meow🐱 \n', metadata={'line_number': 1, 'source': './meow.txt'}),
 Document(page_content=' meow meow🐱 \n', metadata={'line_number': 2, 'source': './meow.txt'}),
 Document(page_content=' meow😻😻', metadata={'line_number': 3, 'source': './meow.txt'})]


Using the 
blob
 API also allows one to load content directly from memory without having to read it from a file!


blob 
=
 Blob
(
data
=
b"some data from memory\nmeow"
)
list
(
parser
.
lazy_parse
(
blob
)
)


[Document(page_content='some data from memory\n', metadata={'line_number': 1, 'source': None}),
 Document(page_content='meow', metadata={'line_number': 2, 'source': None})]


Blob
​


Let's take a quick look through some of the Blob API.


blob 
=
 Blob
.
from_path
(
"./meow.txt"
,
 metadata
=
{
"foo"
:
 
"bar"
}
)


blob
.
encoding


'utf-8'


blob
.
as_bytes
(
)


b'meow meow\xf0\x9f\x90\xb1 \n meow meow\xf0\x9f\x90\xb1 \n meow\xf0\x9f\x98\xbb\xf0\x9f\x98\xbb'


blob
.
as_string
(
)


'meow meow🐱 \n meow meow🐱 \n meow😻😻'


blob
.
as_bytes_io
(
)


<contextlib._GeneratorContextManager at 0x743f34324450>


blob
.
metadata


{'foo': 'bar'}


blob
.
source


'./meow.txt'


Blob Loaders
​


While a parser encapsulates the logic needed to parse binary data into documents, 
blob loaders
 encapsulate the logic that's necessary to load blobs from a given storage location.


At the moment, 
LangChain
 only supports 
FileSystemBlobLoader
.


You can use the 
FileSystemBlobLoader
 to load blobs and then use the parser to parse them.


from
 langchain_community
.
document_loaders
.
blob_loaders 
import
 FileSystemBlobLoader
blob_loader 
=
 FileSystemBlobLoader
(
path
=
"."
,
 glob
=
"*.mdx"
,
 show_progress
=
True
)
API Reference:
FileSystemBlobLoader


parser 
=
 MyParser
(
)
for
 blob 
in
 blob_loader
.
yield_blobs
(
)
:
    
for
 doc 
in
 parser
.
lazy_parse
(
blob
)
:
        
print
(
doc
)
        
break


  0%|          | 0/8 [00:00<?, ?it/s]


page_content='# Microsoft Office\n' metadata={'line_number': 1, 'source': 'office_file.mdx'}
page_content='# Markdown\n' metadata={'line_number': 1, 'source': 'markdown.mdx'}
page_content='# JSON\n' metadata={'line_number': 1, 'source': 'json.mdx'}
page_content='---\n' metadata={'line_number': 1, 'source': 'pdf.mdx'}
page_content='---\n' metadata={'line_number': 1, 'source': 'index.mdx'}
page_content='# File Directory\n' metadata={'line_number': 1, 'source': 'file_directory.mdx'}
page_content='# CSV\n' metadata={'line_number': 1, 'source': 'csv.mdx'}
page_content='# HTML\n' metadata={'line_number': 1, 'source': 'html.mdx'}


Generic Loader
​


LangChain has a 
GenericLoader
 abstraction which composes a 
BlobLoader
 with a 
BaseBlobParser
.


GenericLoader
 is meant to provide standardized classmethods that make it easy to use existing 
BlobLoader
 implementations. At the moment, only the 
FileSystemBlobLoader
 is supported.


from
 langchain_community
.
document_loaders
.
generic 
import
 GenericLoader
loader 
=
 GenericLoader
.
from_filesystem
(
    path
=
"."
,
 glob
=
"*.mdx"
,
 show_progress
=
True
,
 parser
=
MyParser
(
)
)
for
 idx
,
 doc 
in
 
enumerate
(
loader
.
lazy_load
(
)
)
:
    
if
 idx 
<
 
5
:
        
print
(
doc
)
print
(
"... output truncated for demo purposes"
)
API Reference:
GenericLoader


  0%|          | 0/8 [00:00<?, ?it/s]


page_content='# Microsoft Office\n' metadata={'line_number': 1, 'source': 'office_file.mdx'}
page_content='\n' metadata={'line_number': 2, 'source': 'office_file.mdx'}
page_content='>[The Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\n' metadata={'line_number': 3, 'source': 'office_file.mdx'}
page_content='\n' metadata={'line_number': 4, 'source': 'office_file.mdx'}
page_content='This covers how to load commonly used file formats including `DOCX`, `XLSX` and `PPTX` documents into a document format that we can use downstream.\n' metadata={'line_number': 5, 'source': 'office_file.mdx'}
... output truncated for demo purposes


Custom Generic Loader
​


If you really like creating classes, you can sub-class and create a class to encapsulate the logic together.


You can sub-class from this class to load content using an existing loader.


from
 typing 
import
 Any
class
 
MyCustomLoader
(
GenericLoader
)
:
    
@staticmethod
    
def
 
get_parser
(
**
kwargs
:
 Any
)
 
-
>
 BaseBlobParser
:
        
"""Override this method to associate a default parser with the class."""
        
return
 MyParser
(
)


loader 
=
 MyCustomLoader
.
from_filesystem
(
path
=
"."
,
 glob
=
"*.mdx"
,
 show_progress
=
True
)
for
 idx
,
 doc 
in
 
enumerate
(
loader
.
lazy_load
(
)
)
:
    
if
 idx 
<
 
5
:
        
print
(
doc
)
print
(
"... output truncated for demo purposes"
)


  0%|          | 0/8 [00:00<?, ?it/s]


page_content='# Microsoft Office\n' metadata={'line_number': 1, 'source': 'office_file.mdx'}
page_content='\n' metadata={'line_number': 2, 'source': 'office_file.mdx'}
page_content='>[The Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\n' metadata={'line_number': 3, 'source': 'office_file.mdx'}
page_content='\n' metadata={'line_number': 4, 'source': 'office_file.mdx'}
page_content='This covers how to load commonly used file formats including `DOCX`, `XLSX` and `PPTX` documents into a document format that we can use downstream.\n' metadata={'line_number': 5, 'source': 'office_file.mdx'}
... output truncated for demo purposes
Edit this page
Was this page helpful?
Previous
How deal with high cardinality categoricals when doing query analysis
Next
How to use the MultiQueryRetriever
Overview
Standard Document Loader
Interface
Implementation
Test 🧪
Working with Files
BaseBlobParser
Blob
Blob Loaders
Generic Loader


--- docs_how_to_document_loader_directory.txt ---

How-to guides
How to load documents from a directory
On this page
How to load documents from a directory


LangChain's 
DirectoryLoader
 implements functionality for reading files from disk into LangChain 
Document
 objects. Here we demonstrate:




How to load from a filesystem, including use of wildcard patterns;


How to use multithreading for file I/O;


How to use custom loader classes to parse specific file types (e.g., code);


How to handle errors, such as those due to decoding.




from
 langchain_community
.
document_loaders 
import
 DirectoryLoader
API Reference:
DirectoryLoader


DirectoryLoader
 accepts a 
loader_cls
 kwarg, which defaults to 
UnstructuredLoader
. 
Unstructured
 supports parsing for a number of formats, such as PDF and HTML. Here we use it to read in a markdown (.md) file.


We can use the 
glob
 parameter to control which files to load. Note that here it doesn't load the 
.rst
 file or the 
.html
 files.


loader 
=
 DirectoryLoader
(
"../"
,
 glob
=
"**/*.md"
)
docs 
=
 loader
.
load
(
)
len
(
docs
)


20


print
(
docs
[
0
]
.
page_content
[
:
100
]
)


Security
LangChain has a large ecosystem of integrations with various external resources like local


Show a progress bar
​


By default a progress bar will not be shown. To show a progress bar, install the 
tqdm
 library (e.g. 
pip install tqdm
), and set the 
show_progress
 parameter to 
True
.


loader 
=
 DirectoryLoader
(
"../"
,
 glob
=
"**/*.md"
,
 show_progress
=
True
)
docs 
=
 loader
.
load
(
)


100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 54.56it/s]


Use multithreading
​


By default the loading happens in one thread. In order to utilize several threads set the 
use_multithreading
 flag to true.


loader 
=
 DirectoryLoader
(
"../"
,
 glob
=
"**/*.md"
,
 use_multithreading
=
True
)
docs 
=
 loader
.
load
(
)


Change loader class
​


By default this uses the 
UnstructuredLoader
 class. To customize the loader, specify the loader class in the 
loader_cls
 kwarg. Below we show an example using 
TextLoader
:


from
 langchain_community
.
document_loaders 
import
 TextLoader
loader 
=
 DirectoryLoader
(
"../"
,
 glob
=
"**/*.md"
,
 loader_cls
=
TextLoader
)
docs 
=
 loader
.
load
(
)
API Reference:
TextLoader


print
(
docs
[
0
]
.
page_content
[
:
100
]
)


# Security
LangChain has a large ecosystem of integrations with various external resources like loc


Notice that while the 
UnstructuredLoader
 parses Markdown headers, 
TextLoader
 does not.


If you need to load Python source code files, use the 
PythonLoader
:


from
 langchain_community
.
document_loaders 
import
 PythonLoader
loader 
=
 DirectoryLoader
(
"../../../../../"
,
 glob
=
"**/*.py"
,
 loader_cls
=
PythonLoader
)
API Reference:
PythonLoader


Auto-detect file encodings with TextLoader
​


DirectoryLoader
 can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.


path 
=
 
"../../../libs/langchain/tests/unit_tests/examples/"
loader 
=
 DirectoryLoader
(
path
,
 glob
=
"**/*.txt"
,
 loader_cls
=
TextLoader
)


A. Default Behavior
​


By default we raise an error:


loader
.
load
(
)


Error loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt


---------------------------------------------------------------------------
``````output
UnicodeDecodeError                        Traceback (most recent call last)
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)
     42     with open(self.file_path, encoding=self.encoding) as f:
---> 43         text = f.read()
     44 except UnicodeDecodeError as e:
``````output
File ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)
    321 data = self.buffer + input
--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)
    323 # keep undecoded input until the next call
``````output
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xca in position 0: invalid continuation byte
``````output
The above exception was the direct cause of the following exception:
``````output
RuntimeError                              Traceback (most recent call last)
``````output
Cell In[10], line 1
----> 1 loader.load()
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)
    115 def load(self) -> List[Document]:
    116     """Load documents."""
--> 117     return list(self.lazy_load())
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)
    180 else:
    181     for i in items:
--> 182         yield from self._lazy_load_file(i, p, pbar)
    184 if pbar:
    185     pbar.close()
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)
    218     else:
    219         logger.error(f"Error loading file {str(item)}")
--> 220         raise e
    221 finally:
    222     if pbar:
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)
    208 loader = self.loader_cls(str(item), **self.loader_kwargs)
    209 try:
--> 210     for subdoc in loader.lazy_load():
    211         yield subdoc
    212 except NotImplementedError:
``````output
File ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)
     54                 continue
     55     else:
---> 56         raise RuntimeError(f"Error loading {self.file_path}") from e
     57 except Exception as e:
     58     raise RuntimeError(f"Error loading {self.file_path}") from e
``````output
RuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt


The file 
example-non-utf8.txt
 uses a different encoding, so the 
load()
 function fails with a helpful message indicating which file failed decoding.


With the default behavior of 
TextLoader
 any failure to load any of the documents will fail the whole loading process and no documents are loaded.


B. Silent fail
​


We can pass the parameter 
silent_errors
 to the 
DirectoryLoader
 to skip the files which could not be loaded and continue the load process.


loader 
=
 DirectoryLoader
(
    path
,
 glob
=
"**/*.txt"
,
 loader_cls
=
TextLoader
,
 silent_errors
=
True
)
docs 
=
 loader
.
load
(
)


Error loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt


doc_sources 
=
 
[
doc
.
metadata
[
"source"
]
 
for
 doc 
in
 docs
]
doc_sources


['../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt']


C. Auto detect encodings
​


We can also ask 
TextLoader
 to auto detect the file encoding before failing, by passing the 
autodetect_encoding
 to the loader class.


text_loader_kwargs 
=
 
{
"autodetect_encoding"
:
 
True
}
loader 
=
 DirectoryLoader
(
    path
,
 glob
=
"**/*.txt"
,
 loader_cls
=
TextLoader
,
 loader_kwargs
=
text_loader_kwargs
)
docs 
=
 loader
.
load
(
)


doc_sources 
=
 
[
doc
.
metadata
[
"source"
]
 
for
 doc 
in
 docs
]
doc_sources


['../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt',
 '../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt']
Edit this page
Was this page helpful?
Previous
How to load CSVs
Next
How to load HTML
Show a progress bar
Use multithreading
Change loader class
Auto-detect file encodings with TextLoader
A. Default Behavior
B. Silent fail
C. Auto detect encodings


--- docs_how_to_document_loader_html.txt ---

How-to guides
How to load HTML
On this page
How to load HTML


The HyperText Markup Language or 
HTML
 is the standard markup language for documents designed to be displayed in a web browser.


This covers how to load 
HTML
 documents into a LangChain 
Document
 objects that we can use downstream.


Parsing HTML files often requires specialized tools. Here we demonstrate parsing via 
Unstructured
 and 
BeautifulSoup4
, which can be installed via pip. Head over to the integrations page to find integrations with additional services, such as 
Azure AI Document Intelligence
 or 
FireCrawl
.


Loading HTML with Unstructured
​


%
pip install unstructured


from
 langchain_community
.
document_loaders 
import
 UnstructuredHTMLLoader
file_path 
=
 
"../../docs/integrations/document_loaders/example_data/fake-content.html"
loader 
=
 UnstructuredHTMLLoader
(
file_path
)
data 
=
 loader
.
load
(
)
print
(
data
)
API Reference:
UnstructuredHTMLLoader


[Document(page_content='My First Heading\n\nMy first paragraph.', metadata={'source': '../../docs/integrations/document_loaders/example_data/fake-content.html'})]


Loading HTML with BeautifulSoup4
​


We can also use 
BeautifulSoup4
 to load HTML documents using the 
BSHTMLLoader
.  This will extract the text from the HTML into 
page_content
, and the page title as 
title
 into 
metadata
.


%
pip install bs4


from
 langchain_community
.
document_loaders 
import
 BSHTMLLoader
loader 
=
 BSHTMLLoader
(
file_path
)
data 
=
 loader
.
load
(
)
print
(
data
)
API Reference:
BSHTMLLoader


[Document(page_content='\nTest Title\n\n\nMy First Heading\nMy first paragraph.\n\n\n', metadata={'source': '../../docs/integrations/document_loaders/example_data/fake-content.html', 'title': 'Test Title'})]
Edit this page
Was this page helpful?
Previous
How to load documents from a directory
Next
How to load JSON
Loading HTML with Unstructured
Loading HTML with BeautifulSoup4


--- docs_how_to_document_loader_json.txt ---

How-to guides
How to load JSON
On this page
How to load JSON


JSON (JavaScript Object Notation)
 is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).


JSON Lines
 is a file format where each line is a valid JSON value.


LangChain implements a 
JSONLoader

to convert JSON and JSONL data into LangChain 
Document

objects. It uses a specified 
jq schema
 to parse the JSON files, allowing for the extraction of specific fields into the content
and metadata of the LangChain Document.


It uses the 
jq
 python package. Check out this 
manual
 for a detailed documentation of the 
jq
 syntax.


Here we will demonstrate:




How to load JSON and JSONL data into the content of a LangChain 
Document
;


How to load JSON and JSONL data into metadata associated with a 
Document
.




#!pip install jq


from
 langchain_community
.
document_loaders 
import
 JSONLoader
API Reference:
JSONLoader


import
 json
from
 pathlib 
import
 Path
from
 pprint 
import
 pprint
file_path
=
'./example_data/facebook_chat.json'
data 
=
 json
.
loads
(
Path
(
file_path
)
.
read_text
(
)
)


pprint
(
data
)


    {'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},
     'is_still_participant': True,
     'joinable_mode': {'link': '', 'mode': 1},
     'magic_words': [],
     'messages': [{'content': 'Bye!',
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675597571851},
                  {'content': 'Oh no worries! Bye',
                   'sender_name': 'User 1',
                   'timestamp_ms': 1675597435669},
                  {'content': 'No Im sorry it was my mistake, the blue one is not '
                              'for sale',
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675596277579},
                  {'content': 'I thought you were selling the blue one!',
                   'sender_name': 'User 1',
                   'timestamp_ms': 1675595140251},
                  {'content': 'Im not interested in this bag. Im interested in the '
                              'blue one!',
                   'sender_name': 'User 1',
                   'timestamp_ms': 1675595109305},
                  {'content': 'Here is $129',
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675595068468},
                  {'photos': [{'creation_timestamp': 1675595059,
                               'uri': 'url_of_some_picture.jpg'}],
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675595060730},
                  {'content': 'Online is at least $100',
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675595045152},
                  {'content': 'How much do you want?',
                   'sender_name': 'User 1',
                   'timestamp_ms': 1675594799696},
                  {'content': 'Goodmorning! $50 is too low.',
                   'sender_name': 'User 2',
                   'timestamp_ms': 1675577876645},
                  {'content': 'Hi! Im interested in your bag. Im offering $50. Let '
                              'me know if you are interested. Thanks!',
                   'sender_name': 'User 1',
                   'timestamp_ms': 1675549022673}],
     'participants': [{'name': 'User 1'}, {'name': 'User 2'}],
     'thread_path': 'inbox/User 1 and User 2 chat',
     'title': 'User 1 and User 2 chat'}


Using 
JSONLoader
​


Suppose we are interested in extracting the values under the 
content
 field within the 
messages
 key of the JSON data. This can easily be done through the 
JSONLoader
 as shown below.


JSON file
​


loader 
=
 JSONLoader
(
    file_path
=
'./example_data/facebook_chat.json'
,
    jq_schema
=
'.messages[].content'
,
    text_content
=
False
)
data 
=
 loader
.
load
(
)


pprint
(
data
)


    [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1}),
     Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2}),
     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3}),
     Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4}),
     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5}),
     Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6}),
     Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7}),
     Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8}),
     Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9}),
     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10}),
     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11})]


JSON Lines file
​


If you want to load documents from a JSON Lines file, you pass 
json_lines=True

and specify 
jq_schema
 to extract 
page_content
 from a single JSON object.


file_path 
=
 
'./example_data/facebook_chat_messages.jsonl'
pprint
(
Path
(
file_path
)
.
read_text
(
)
)


    ('{"sender_name": "User 2", "timestamp_ms": 1675597571851, "content": "Bye!"}\n'
     '{"sender_name": "User 1", "timestamp_ms": 1675597435669, "content": "Oh no '
     'worries! Bye"}\n'
     '{"sender_name": "User 2", "timestamp_ms": 1675596277579, "content": "No Im '
     'sorry it was my mistake, the blue one is not for sale"}\n')


loader 
=
 JSONLoader
(
    file_path
=
'./example_data/facebook_chat_messages.jsonl'
,
    jq_schema
=
'.content'
,
    text_content
=
False
,
    json_lines
=
True
)
data 
=
 loader
.
load
(
)


pprint
(
data
)


    [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}),
     Document(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}),
     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})]


Another option is to set 
jq_schema='.'
 and provide 
content_key
:


loader 
=
 JSONLoader
(
    file_path
=
'./example_data/facebook_chat_messages.jsonl'
,
    jq_schema
=
'.'
,
    content_key
=
'sender_name'
,
    json_lines
=
True
)
data 
=
 loader
.
load
(
)


pprint
(
data
)


    [Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 1}),
     Document(page_content='User 1', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 2}),
     Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl', 'seq_num': 3})]


JSON file with jq schema 
content_key
​


To load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True.
Ensure that content_key is compatible and can be parsed using the jq schema.


file_path 
=
 
'./sample.json'
pprint
(
Path
(
file_path
)
.
read_text
(
)
)


    {"data": [
        {"attributes": {
            "message": "message1",
            "tags": [
            "tag1"]},
        "id": "1"},
        {"attributes": {
            "message": "message2",
            "tags": [
            "tag2"]},
        "id": "2"}]}


loader 
=
 JSONLoader
(
    file_path
=
file_path
,
    jq_schema
=
".data[]"
,
    content_key
=
".attributes.message"
,
    is_content_key_jq_parsable
=
True
,
)
data 
=
 loader
.
load
(
)


pprint
(
data
)


    [Document(page_content='message1', metadata={'source': '/path/to/sample.json', 'seq_num': 1}),
     Document(page_content='message2', metadata={'source': '/path/to/sample.json', 'seq_num': 2})]


Extracting metadata
​


Generally, we want to include metadata available in the JSON file into the documents that we create from the content.


The following demonstrates how metadata can be extracted using the 
JSONLoader
.


There are some key changes to be noted. In the previous example where we didn't collect the metadata, we managed to directly specify in the schema where the value for the 
page_content
 can be extracted from.


.messages[].content


In the current example, we have to tell the loader to iterate over the records in the 
messages
 field. The jq_schema then has to be:


.messages[]


This allows us to pass the records (dict) into the 
metadata_func
 that has to be implemented. The 
metadata_func
 is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final 
Document
 object.


Additionally, we now have to explicitly specify in the loader, via the 
content_key
 argument, the key from the record where the value for the 
page_content
 needs to be extracted from.


# Define the metadata extraction function.
def
 
metadata_func
(
record
:
 
dict
,
 metadata
:
 
dict
)
 
-
>
 
dict
:
    metadata
[
"sender_name"
]
 
=
 record
.
get
(
"sender_name"
)
    metadata
[
"timestamp_ms"
]
 
=
 record
.
get
(
"timestamp_ms"
)
    
return
 metadata
loader 
=
 JSONLoader
(
    file_path
=
'./example_data/facebook_chat.json'
,
    jq_schema
=
'.messages[]'
,
    content_key
=
"content"
,
    metadata_func
=
metadata_func
)
data 
=
 loader
.
load
(
)


pprint
(
data
)


    [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),
     Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),
     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),
     Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),
     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),
     Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),
     Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),
     Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),
     Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),
     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),
     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]


Now, you will see that the documents contain the metadata associated with the content we extracted.


The 
metadata_func
​


As shown above, the 
metadata_func
 accepts the default metadata generated by the 
JSONLoader
. This allows full control to the user with respect to how the metadata is formatted.


For example, the default metadata contains the 
source
 and the 
seq_num
 keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the 
metadata_func
 to rename the default keys and use the ones from the JSON data.


The example below shows how we can modify the 
source
 to only contain information of the file source relative to the 
langchain
 directory.


# Define the metadata extraction function.
def
 
metadata_func
(
record
:
 
dict
,
 metadata
:
 
dict
)
 
-
>
 
dict
:
    metadata
[
"sender_name"
]
 
=
 record
.
get
(
"sender_name"
)
    metadata
[
"timestamp_ms"
]
 
=
 record
.
get
(
"timestamp_ms"
)
    
if
 
"source"
 
in
 metadata
:
        source 
=
 metadata
[
"source"
]
.
split
(
"/"
)
        source 
=
 source
[
source
.
index
(
"langchain"
)
:
]
        metadata
[
"source"
]
 
=
 
"/"
.
join
(
source
)
    
return
 metadata
loader 
=
 JSONLoader
(
    file_path
=
'./example_data/facebook_chat.json'
,
    jq_schema
=
'.messages[]'
,
    content_key
=
"content"
,
    metadata_func
=
metadata_func
)
data 
=
 loader
.
load
(
)


pprint
(
data
)


    [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),
     Document(page_content='Oh no worries! Bye', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),
     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),
     Document(page_content='I thought you were selling the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),
     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),
     Document(page_content='Here is $129', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),
     Document(page_content='', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),
     Document(page_content='Online is at least $100', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),
     Document(page_content='How much do you want?', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),
     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),
     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]


Common JSON structures with jq schema
​


The list below provides a reference to the possible 
jq_schema
 the user can use to extract content from the JSON data depending on the structure.


JSON        -> [{"text": ...}, {"text": ...}, {"text": ...}]
jq_schema   -> ".[].text"
JSON        -> {"key": [{"text": ...}, {"text": ...}, {"text": ...}]}
jq_schema   -> ".key[].text"
JSON        -> ["...", "...", "..."]
jq_schema   -> ".[]"
Edit this page
Was this page helpful?
Previous
How to load HTML
Next
How to load Markdown
Using 
JSONLoader
JSON file
JSON Lines file
JSON file with jq schema 
content_key
Extracting metadata
The 
metadata_func
Common JSON structures with jq schema


--- docs_how_to_document_loader_markdown.txt ---

How-to guides
How to load Markdown
On this page
How to load Markdown


Markdown
 is a lightweight markup language for creating formatted text using a plain-text editor.


Here we cover how to load 
Markdown
 documents into LangChain 
Document
 objects that we can use downstream.


We will cover:




Basic usage;


Parsing of Markdown into elements such as titles, list items, and text.




LangChain implements an 
UnstructuredMarkdownLoader
 object which requires the 
Unstructured
 package. First we install it:


%
pip install 
"unstructured[md]"
 nltk


Basic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain's readme:


from
 langchain_community
.
document_loaders 
import
 UnstructuredMarkdownLoader
from
 langchain_core
.
documents 
import
 Document
markdown_path 
=
 
"../../../README.md"
loader 
=
 UnstructuredMarkdownLoader
(
markdown_path
)
data 
=
 loader
.
load
(
)
assert
 
len
(
data
)
 
==
 
1
assert
 
isinstance
(
data
[
0
]
,
 Document
)
readme_content 
=
 data
[
0
]
.
page_content
print
(
readme_content
[
:
250
]
)
API Reference:
UnstructuredMarkdownLoader
 | 
Document


🦜️🔗 LangChain
⚡ Build context-aware reasoning applications ⚡
Looking for the JS/TS library? Check out LangChain.js.
To help you ship LangChain apps to production faster, check out LangSmith. 
LangSmith is a unified developer platform for building,


Retain Elements
​


Under the hood, Unstructured creates different "elements" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying 
mode="elements"
.


loader 
=
 UnstructuredMarkdownLoader
(
markdown_path
,
 mode
=
"elements"
)
data 
=
 loader
.
load
(
)
print
(
f"Number of documents: 
{
len
(
data
)
}
\n"
)
for
 document 
in
 data
[
:
2
]
:
    
print
(
f"
{
document
}
\n"
)


Number of documents: 66
page_content='🦜️🔗 LangChain' metadata={'source': '../../../README.md', 'category_depth': 0, 'last_modified': '2024-06-28T15:20:01', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../..', 'filename': 'README.md', 'category': 'Title'}
page_content='⚡ Build context-aware reasoning applications ⚡' metadata={'source': '../../../README.md', 'last_modified': '2024-06-28T15:20:01', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '../../..', 'filename': 'README.md', 'category': 'NarrativeText'}


Note that in this case we recover three distinct element types:


print
(
set
(
document
.
metadata
[
"category"
]
 
for
 document 
in
 data
)
)


{'ListItem', 'NarrativeText', 'Title'}
Edit this page
Was this page helpful?
Previous
How to load JSON
Next
How to load Microsoft Office files
Retain Elements


--- docs_how_to_document_loader_office_file.txt ---

How-to guides
How to load Microsoft Office files
On this page
How to load Microsoft Office files


The 
Microsoft Office
 suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.


This covers how to load commonly used file formats including 
DOCX
, 
XLSX
 and 
PPTX
 documents into a LangChain

Document

object that we can use downstream.


Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader
​


Azure AI Document Intelligence
 (formerly known as 
Azure Form Recognizer
) is machine-learning
based service that extracts texts (including handwriting), tables, document structures (e.g., titles, section headings, etc.) and key-value-pairs from
digital or scanned PDFs, images, Office and HTML files. Document Intelligence supports 
PDF
, 
JPEG/JPG
, 
PNG
, 
BMP
, 
TIFF
, 
HEIF
, 
DOCX
, 
XLSX
, 
PPTX
 and 
HTML
.


This 
current implementation
 of a loader using 
Document Intelligence
 can incorporate content page-wise and turn it into LangChain documents. The default output format is markdown, which can be easily chained with 
MarkdownHeaderTextSplitter
 for semantic document chunking. You can also use 
mode="single"
 or 
mode="page"
 to return pure texts in a single page or document split by page.


Prerequisite
​


An Azure AI Document Intelligence resource in one of the 3 preview regions: 
East US
, 
West US2
, 
West Europe
 - follow 
this document
 to create one if you don't have. You will be passing 
<endpoint>
 and 
<key>
 as parameters to the loader.


%
pip install 
-
-
upgrade 
-
-
quiet  langchain langchain
-
community azure
-
ai
-
documentintelligence
from
 langchain_community
.
document_loaders 
import
 AzureAIDocumentIntelligenceLoader
file_path 
=
 
"<filepath>"
endpoint 
=
 
"<endpoint>"
key 
=
 
"<key>"
loader 
=
 AzureAIDocumentIntelligenceLoader
(
    api_endpoint
=
endpoint
,
 api_key
=
key
,
 file_path
=
file_path
,
 api_model
=
"prebuilt-layout"
)
documents 
=
 loader
.
load
(
)
API Reference:
AzureAIDocumentIntelligenceLoader
Edit this page
Was this page helpful?
Previous
How to load Markdown
Next
How to load PDFs
Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader
Prerequisite


--- docs_how_to_document_loader_pdf.txt ---

How-to guides
How to load PDFs
On this page
How to load PDFs


Portable Document Format (PDF)
, standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.


This guide covers how to 
load
 
PDF
 documents into the LangChain 
Document
 format that we use downstream.


Text in PDFs is typically represented via text boxes. They may also contain images. A PDF parser might do some combination of the following:




Agglomerate text boxes into lines, paragraphs, and other structures via heuristics or ML inference;


Run 
OCR
 on images to detect text therein;


Classify text as belonging to paragraphs, lists, tables, or other structures;


Structure text into table rows and columns, or key-value pairs.




LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your needs. Below we enumerate the possibilities.


We will demonstrate these approaches on a 
sample file
:


file_path 
=
 
(
    
"../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf"
)


A note on multimodal models
Many modern LLMs support inference over multimodal inputs (e.g., images). In some applications -- such as question-answering over PDFs with complex layouts, diagrams, or scans -- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. We demonstrate an example of this in the 
Use of multimodal models
 section below.


Simple and fast text extraction
​


If you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of 
Document
 objects-- one per page-- containing a single string of the page's text in the Document's 
page_content
 attribute. It will not parse text in images or scanned PDF pages. Under the hood it uses the 
pypdf
 Python library.


LangChain 
document loaders
 implement 
lazy_load
 and its async variant, 
alazy_load
, which return iterators of 
Document
 objects. We will use these below.


%
pip install 
-
qU pypdf


from
 langchain_community
.
document_loaders 
import
 PyPDFLoader
loader 
=
 PyPDFLoader
(
file_path
)
pages 
=
 
[
]
async
 
for
 page 
in
 loader
.
alazy_load
(
)
:
    pages
.
append
(
page
)
API Reference:
PyPDFLoader


print
(
f"
{
pages
[
0
]
.
metadata
}
\n"
)
print
(
pages
[
0
]
.
page_content
)


{'source': '../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf', 'page': 0}
LayoutParser : A Uniﬁed Toolkit for Deep
Learning Based Document Image Analysis
Zejiang Shen1( �), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain
Lee4, Jacob Carlson3, and Weining Li5
1Allen Institute for AI
shannons@allenai.org
2Brown University
ruochen zhang@brown.edu
3Harvard University
{melissadell,jacob carlson }@fas.harvard.edu
4University of Washington
bcgl@cs.washington.edu
5University of Waterloo
w422li@uwaterloo.ca
Abstract. Recent advances in document image analysis (DIA) have been
primarily driven by the application of neural networks. Ideally, research
outcomes could be easily deployed in production and extended for further
investigation. However, various factors like loosely organized codebases
and sophisticated model conﬁgurations complicate the easy reuse of im-
portant innovations by a wide audience. Though there have been on-going
eﬀorts to improve reusability and simplify deep learning (DL) model
development in disciplines like natural language processing and computer
vision, none of them are optimized for challenges in the domain of DIA.
This represents a major gap in the existing toolkit, as DIA is central to
academic research across a wide range of disciplines in the social sciences
and humanities. This paper introduces LayoutParser , an open-source
library for streamlining the usage of DL in DIA research and applica-
tions. The core LayoutParser library comes with a set of simple and
intuitive interfaces for applying and customizing DL models for layout de-
tection, character recognition, and many other document processing tasks.
To promote extensibility, LayoutParser also incorporates a community
platform for sharing both pre-trained models and full document digiti-
zation pipelines. We demonstrate that LayoutParser is helpful for both
lightweight and large-scale digitization pipelines in real-word use cases.
The library is publicly available at https://layout-parser.github.io .
Keywords: Document Image Analysis ·Deep Learning ·Layout Analysis
·Character Recognition ·Open Source library ·Toolkit.
1 Introduction
Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of
document image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021


Note that the metadata of each document stores the corresponding page number.


Vector search over PDFs
​


Once we have loaded PDFs into LangChain 
Document
 objects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain 
embeddings
 model will suffice.


%
pip install 
-
qU langchain
-
openai


import
 getpass
import
 os
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"OpenAI API Key:"
)


from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
from
 langchain_openai 
import
 OpenAIEmbeddings
vector_store 
=
 InMemoryVectorStore
.
from_documents
(
pages
,
 OpenAIEmbeddings
(
)
)
docs 
=
 vector_store
.
similarity_search
(
"What is LayoutParser?"
,
 k
=
2
)
for
 doc 
in
 docs
:
    
print
(
f'Page 
{
doc
.
metadata
[
"page"
]
}
: 
{
doc
.
page_content
[
:
300]
}
\n'
)
API Reference:
InMemoryVectorStore
 | 
OpenAIEmbeddings


Page 13: 14 Z. Shen et al.
6 Conclusion
LayoutParser provides a comprehensive toolkit for deep learning-based document
image analysis. The oﬀ-the-shelf library is easy to install, and can be used to
build ﬂexible and accurate pipelines for processing documents with complicated
structures. It also supports hi
Page 0: LayoutParser : A Uniﬁed Toolkit for Deep
Learning Based Document Image Analysis
Zejiang Shen1( �), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain
Lee4, Jacob Carlson3, and Weining Li5
1Allen Institute for AI
shannons@allenai.org
2Brown University
ruochen zhang@brown.edu
3Harvard University


Layout analysis and extraction of text from images
​


If you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of 
Document
 objects, where each object represents a structure on the page. The Document's metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).


Under the hood it uses the 
langchain-unstructured
 library. See the 
integration docs
 for more information about using 
Unstructured
 with LangChain.


Unstructured supports multiple parameters for PDF parsing:




strategy
 (e.g., 
"fast"
 or 
"hi-res"
)


API or local processing. You will need an API key to use the API.




The 
hi-res
 strategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See 
local parsing
 section below for considerations when running locally.


%
pip install 
-
qU langchain
-
unstructured


import
 getpass
import
 os
if
 
"UNSTRUCTURED_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"UNSTRUCTURED_API_KEY"
]
 
=
 getpass
.
getpass
(
"Unstructured API Key:"
)


Unstructured API Key: ········


As before, we initialize a loader and load documents lazily:


from
 langchain_unstructured 
import
 UnstructuredLoader
loader 
=
 UnstructuredLoader
(
    file_path
=
file_path
,
    strategy
=
"hi_res"
,
    partition_via_api
=
True
,
    coordinates
=
True
,
)
docs 
=
 
[
]
for
 doc 
in
 loader
.
lazy_load
(
)
:
    docs
.
append
(
doc
)
API Reference:
UnstructuredLoader


INFO: Preparing to split document for partition.
INFO: Starting page number set to 1
INFO: Allow failed set to 0
INFO: Concurrency level set to 5
INFO: Splitting pages 1 to 16 (16 total)
INFO: Determined optimal split size of 4 pages.
INFO: Partitioning 4 files with 4 page(s) each.
INFO: Partitioning set #1 (pages 1-4).
INFO: Partitioning set #2 (pages 5-8).
INFO: Partitioning set #3 (pages 9-12).
INFO: Partitioning set #4 (pages 13-16).
INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general "HTTP/1.1 200 OK"
INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general "HTTP/1.1 200 OK"
INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general "HTTP/1.1 200 OK"
INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general "HTTP/1.1 200 OK"
INFO: Successfully partitioned set #1, elements added to the final result.
INFO: Successfully partitioned set #2, elements added to the final result.
INFO: Successfully partitioned set #3, elements added to the final result.
INFO: Successfully partitioned set #4, elements added to the final result.


Here we recover 171 distinct structures over the 16 page document:


print
(
len
(
docs
)
)


171


We can use the document metadata to recover content from a single page:


first_page_docs 
=
 
[
doc 
for
 doc 
in
 docs 
if
 doc
.
metadata
.
get
(
"page_number"
)
 
==
 
1
]
for
 doc 
in
 first_page_docs
:
    
print
(
doc
.
page_content
)


LayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis
1 2 0 2 n u J 1 2 ] V C . s c [ 2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a
Zejiang Shen® (<), Ruochen Zhang?, Melissa Dell®, Benjamin Charles Germain Lee?, Jacob Carlson®, and Weining Li®
1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca
Abstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.
Keywords: Document Image Analysis · Deep Learning · Layout Analysis · Character Recognition · Open Source library · Toolkit.
1 Introduction
Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classiﬁcation [11,


Extracting tables and other structures
​


Each 
Document
 we load represents a structure, like a title, paragraph, or table.


Some structures may be of special interest for indexing or question-answering tasks. These structures may be:




Classified for easy identification;


Parsed into a more structured representation.




Below, we identify and extract a table:


Click to expand code for rendering pages
%pip install -qU matplotlib PyMuPDF pillow
import
 fitz
import
 matplotlib
.
patches 
as
 patches
import
 matplotlib
.
pyplot 
as
 plt
from
 PIL 
import
 Image
def
 
plot_pdf_with_boxes
(
pdf_page
,
 segments
)
:
    pix 
=
 pdf_page
.
get_pixmap
(
)
    pil_image 
=
 Image
.
frombytes
(
"RGB"
,
 
[
pix
.
width
,
 pix
.
height
]
,
 pix
.
samples
)
    fig
,
 ax 
=
 plt
.
subplots
(
1
,
 figsize
=
(
10
,
 
10
)
)
    ax
.
imshow
(
pil_image
)
    categories 
=
 
set
(
)
    category_to_color 
=
 
{
        
"Title"
:
 
"orchid"
,
        
"Image"
:
 
"forestgreen"
,
        
"Table"
:
 
"tomato"
,
    
}
    
for
 segment 
in
 segments
:
        points 
=
 segment
[
"coordinates"
]
[
"points"
]
        layout_width 
=
 segment
[
"coordinates"
]
[
"layout_width"
]
        layout_height 
=
 segment
[
"coordinates"
]
[
"layout_height"
]
        scaled_points 
=
 
[
            
(
x 
*
 pix
.
width 
/
 layout_width
,
 y 
*
 pix
.
height 
/
 layout_height
)
            
for
 x
,
 y 
in
 points
        
]
        box_color 
=
 category_to_color
.
get
(
segment
[
"category"
]
,
 
"deepskyblue"
)
        categories
.
add
(
segment
[
"category"
]
)
        rect 
=
 patches
.
Polygon
(
            scaled_points
,
 linewidth
=
1
,
 edgecolor
=
box_color
,
 facecolor
=
"none"
        
)
        ax
.
add_patch
(
rect
)
    
# Make legend
    legend_handles 
=
 
[
patches
.
Patch
(
color
=
"deepskyblue"
,
 label
=
"Text"
)
]
    
for
 category 
in
 
[
"Title"
,
 
"Image"
,
 
"Table"
]
:
        
if
 category 
in
 categories
:
            legend_handles
.
append
(
                patches
.
Patch
(
color
=
category_to_color
[
category
]
,
 label
=
category
)
            
)
    ax
.
axis
(
"off"
)
    ax
.
legend
(
handles
=
legend_handles
,
 loc
=
"upper right"
)
    plt
.
tight_layout
(
)
    plt
.
show
(
)
def
 
render_page
(
doc_list
:
 
list
,
 page_number
:
 
int
,
 print_text
=
True
)
 
-
>
 
None
:
    pdf_page 
=
 fitz
.
open
(
file_path
)
.
load_page
(
page_number 
-
 
1
)
    page_docs 
=
 
[
        doc 
for
 doc 
in
 doc_list 
if
 doc
.
metadata
.
get
(
"page_number"
)
 
==
 page_number
    
]
    segments 
=
 
[
doc
.
metadata 
for
 doc 
in
 page_docs
]
    plot_pdf_with_boxes
(
pdf_page
,
 segments
)
    
if
 print_text
:
        
for
 doc 
in
 page_docs
:
            
print
(
f"
{
doc
.
page_content
}
\n"
)


render_page
(
docs
,
 
5
)




LayoutParser: A Uniﬁed Toolkit for DL-Based DIA
5
Table 1: Current layout detection models in the LayoutParser model zoo
Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents
1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.
layout data structures, which are optimized for eﬃciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniﬁed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.
3.1 Layout Detection Models
In LayoutParser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Diﬀerent from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CNN [28] and Mask R-CNN [12] are used. This yields prediction results of high accuracy and makes it possible to build a concise, generalized interface for layout detection. LayoutParser, built upon Detectron2 [35], provides a minimal API that can perform layout detection with only four lines of code in Python:
1 import layoutparser as lp 2 image = cv2 . imread ( " image_file " ) # load images 3 model = lp . De t e c tro n2 Lay outM odel ( " lp :// PubLayNet / f as t er _ r c nn _ R _ 50 _ F P N_ 3 x / config " ) 4 5 layout = model . detect ( image )
LayoutParser provides a wealth of pre-trained model weights using various datasets covering diﬀerent languages, time periods, and document types. Due to domain shift [7], the prediction performance can notably drop when models are ap- plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As document structures and layouts vary greatly in diﬀerent domains, it is important to select models trained on a dataset similar to the test samples. A semantic syntax is used for initializing the model weights in LayoutParser, using both the dataset name and model name lp://<dataset-name>/<model-architecture-name>.


Note that although the table text is collapsed into a single string in the document's content, the metadata contains a representation of its rows and columns:


from
 IPython
.
display 
import
 HTML
,
 display
segments 
=
 
[
    doc
.
metadata
    
for
 doc 
in
 docs
    
if
 doc
.
metadata
.
get
(
"page_number"
)
 
==
 
5
 
and
 doc
.
metadata
.
get
(
"category"
)
 
==
 
"Table"
]
display
(
HTML
(
segments
[
0
]
[
"text_as_html"
]
)
)


able 1. LUllclll 1ayoul actCCLloll 1110AdCs 111 L1C LayoOulralsel 1110U4cl 200
Dataset
| Base Model'|
Notes
PubLayNet [38]
F/M
Layouts of modern scientific documents
PRImA
M
Layouts of scanned modern magazines and scientific reports
Newspaper
F
Layouts of scanned US newspapers from the 20th century
TableBank [18]
F
Table region on modern scientific and business document
HJDataset
F/M
Layouts of history Japanese documents


Extracting text from specific sections
​


Structures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding 
Document
 objects.


Below, we extract all text associated with the document's "Conclusion" section:


render_page
(
docs
,
 
14
,
 print_text
=
False
)




conclusion_docs 
=
 
[
]
parent_id 
=
 
-
1
for
 doc 
in
 docs
:
    
if
 doc
.
metadata
[
"category"
]
 
==
 
"Title"
 
and
 
"Conclusion"
 
in
 doc
.
page_content
:
        parent_id 
=
 doc
.
metadata
[
"element_id"
]
    
if
 doc
.
metadata
.
get
(
"parent_id"
)
 
==
 parent_id
:
        conclusion_docs
.
append
(
doc
)
for
 doc 
in
 conclusion_docs
:
    
print
(
doc
.
page_content
)


LayoutParser provides a comprehensive toolkit for deep learning-based document image analysis. The oﬀ-the-shelf library is easy to install, and can be used to build ﬂexible and accurate pipelines for processing documents with complicated structures. It also supports high-level customization and enables easy labeling and training of DL models on unique document image datasets. The LayoutParser community platform facilitates sharing DL models and DIA pipelines, inviting discussion and promoting code reproducibility and reusability. The LayoutParser team is committed to keeping the library updated continuously and bringing the most recent advances in DL-based DIA, such as multi-modal document modeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.
Acknowledgements We thank the anonymous reviewers for their comments and suggestions. This project is supported in part by NSF Grant OIA-2033558 and funding from the Harvard Data Science Initiative and Harvard Catalyst. Zejiang Shen thanks Doug Downey for suggestions.


Extracting text from images
​


OCR is run on images, enabling the extraction of text therein:


render_page
(
docs
,
 
11
)




LayoutParser: A Uniﬁed Toolkit for DL-Based DIA
focuses on precision, eﬃciency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and ﬂexibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.
11
5.1 A Comprehensive Historical Document Digitization Pipeline
The digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, ob- taining a structured representation of historical document scans is often extremely complicated. In this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to gener- ate high-quality structured data from historical Japanese ﬁrm ﬁnancial ta- bles with complicated layouts. The pipeline applies two layout models to identify diﬀerent levels of document structures and two customized OCR engines for optimized character recog- nition accuracy.
‘Active Learning Layout Annotate Layout Dataset | +—— Annotation Toolkit A4 Deep Learning Layout Layout Detection Model Training & Inference, A Post-processing — Handy Data Structures & \ Lo orajport 7 ) Al Pls for Layout Data A4 Default and Customized Text Recognition 0CR Models ¥ Visualization & Export Layout Structure Visualization & Storage The Japanese Document Helpful LayoutParser Modules Digitization Pipeline
As shown in Figure 4 (a), the document contains columns of text written vertically 15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have vari- able widths, and hence cannot be eas- ily identiﬁed via rule-based methods. Within each column, words are sepa- rated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.
Fig. 5: Illustration of how LayoutParser helps with the historical document digi- tization pipeline.
15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.


Note that the text from the figure on the right is extracted and incorporated into the content of the 
Document
.


Local parsing
​


Parsing locally requires the installation of additional dependencies.


Poppler
 (PDF analysis)




Linux: 
apt-get install poppler-utils


Mac: 
brew install poppler


Windows: 
https://github.com/oschwartz10612/poppler-windows




Tesseract
 (OCR)




Linux: 
apt-get install tesseract-ocr


Mac: 
brew install tesseract


Windows: 
https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows




We will also need to install the 
unstructured
 PDF extras:


%
pip install 
-
qU 
"unstructured[pdf]"


We can then use the 
UnstructuredLoader
 much the same way, forgoing the API key and 
partition_via_api
 setting:


loader_local 
=
 UnstructuredLoader
(
    file_path
=
file_path
,
    strategy
=
"hi_res"
,
)
docs_local 
=
 
[
]
for
 doc 
in
 loader_local
.
lazy_load
(
)
:
    docs_local
.
append
(
doc
)


WARNING: This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name
INFO: Reading PDF for file: /Users/chestercurme/repos/langchain/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Detecting page elements ...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: padding image by 20 for structure detection
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: padding image by 20 for structure detection
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...
INFO: Processing entire page OCR with tesseract...


The list of documents can then be processed similarly to those obtained from the API.


Use of multimodal models
​


Many modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. This allows a model to reason over the two dimensional content on the page, instead of a "one-dimensional" string representation.


In principle we can use any LangChain 
chat model
 that supports multimodal inputs. A list of these models is documented 
here
. Below we use OpenAI's 
gpt-4o-mini
.


First we define a short utility function to convert a PDF page to a base64-encoded image:


%
pip install 
-
qU PyMuPDF pillow langchain
-
openai


import
 base64
import
 io
import
 fitz
from
 PIL 
import
 Image
def
 
pdf_page_to_base64
(
pdf_path
:
 
str
,
 page_number
:
 
int
)
:
    pdf_document 
=
 fitz
.
open
(
pdf_path
)
    page 
=
 pdf_document
.
load_page
(
page_number 
-
 
1
)
  
# input is one-indexed
    pix 
=
 page
.
get_pixmap
(
)
    img 
=
 Image
.
frombytes
(
"RGB"
,
 
[
pix
.
width
,
 pix
.
height
]
,
 pix
.
samples
)
    
buffer
 
=
 io
.
BytesIO
(
)
    img
.
save
(
buffer
,
 
format
=
"PNG"
)
    
return
 base64
.
b64encode
(
buffer
.
getvalue
(
)
)
.
decode
(
"utf-8"
)


from
 IPython
.
display 
import
 Image 
as
 IPImage
from
 IPython
.
display 
import
 display
base64_image 
=
 pdf_page_to_base64
(
file_path
,
 
11
)
display
(
IPImage
(
data
=
base64
.
b64decode
(
base64_image
)
)
)




We can then query the model in the 
usual way
. Below we ask it a question on related to the diagram on the page.


from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
API Reference:
ChatOpenAI


from
 langchain_core
.
messages 
import
 HumanMessage
query 
=
 
"What is the name of the first step in the pipeline?"
message 
=
 HumanMessage
(
    content
=
[
        
{
"type"
:
 
"text"
,
 
"text"
:
 query
}
,
        
{
            
"type"
:
 
"image_url"
,
            
"image_url"
:
 
{
"url"
:
 
f"data:image/jpeg;base64,
{
base64_image
}
"
}
,
        
}
,
    
]
,
)
response 
=
 llm
.
invoke
(
[
message
]
)
print
(
response
.
content
)
API Reference:
HumanMessage


INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
``````output
The first step in the pipeline is "Annotate Layout Dataset."


Other PDF loaders
​


For a list of available LangChain PDF loaders, please see 
this table
.
Edit this page
Was this page helpful?
Previous
How to load Microsoft Office files
Next
How to load web pages
Simple and fast text extraction
Vector search over PDFs
Layout analysis and extraction of text from images
Extracting tables and other structures
Extracting text from specific sections
Extracting text from images
Local parsing
Use of multimodal models
Other PDF loaders


--- docs_how_to_document_loader_web.txt ---

How-to guides
How to load web pages
On this page
How to load web pages


This guide covers how to 
load
 web pages into the LangChain 
Document
 format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.


LangChain integrates with a host of parsers that are appropriate for web pages. The right parser will depend on your needs. Below we demonstrate two possibilities:




Simple and fast
 parsing, in which we recover one 
Document
 per web page with its content represented as a "flattened" string;


Advanced
 parsing, in which we recover multiple 
Document
 objects per page, allowing one to identify and traverse sections, links, tables, and other structures.




Setup
​


For the "simple and fast" parsing, we will need 
langchain-community
 and the 
beautifulsoup4
 library:


%
pip install 
-
qU langchain
-
community beautifulsoup4


For advanced parsing, we will use 
langchain-unstructured
:


%
pip install 
-
qU langchain
-
unstructured


Simple and fast text extraction
​


If you are looking for a simple string representation of text that is embedded in a web page, the method below is appropriate. It will return a list of 
Document
 objects -- one per page -- containing a single string of the page's text. Under the hood it uses the 
beautifulsoup4
 Python library.


LangChain document loaders implement 
lazy_load
 and its async variant, 
alazy_load
, which return iterators of 
Document objects
. We will use these below.


import
 bs4
from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
page_url 
=
 
"https://python.langchain.com/docs/how_to/chatbots_memory/"
loader 
=
 WebBaseLoader
(
web_paths
=
[
page_url
]
)
docs 
=
 
[
]
async
 
for
 doc 
in
 loader
.
alazy_load
(
)
:
    docs
.
append
(
doc
)
assert
 
len
(
docs
)
 
==
 
1
doc 
=
 docs
[
0
]
API Reference:
WebBaseLoader


USER_AGENT environment variable not set, consider setting it to identify your requests.


print
(
f"
{
doc
.
metadata
}
\n"
)
print
(
doc
.
page_content
[
:
500
]
.
strip
(
)
)


{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/', 'title': 'How to add memory to chatbots | \uf8ffü¶úÔ∏è\uf8ffüîó LangChain', 'description': 'A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:', 'language': 'en'}
How to add memory to chatbots | ü¶úÔ∏èüîó LangChain
Skip to main contentShare your thoughts on AI agents. Take the 3-min survey.IntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingd


This is essentially a dump of the text from the page's HTML. It may contain extraneous information like headings and navigation bars. If you are familiar with the expected HTML, you can specify desired 
<div>
 classes and other parameters via BeautifulSoup. Below we parse only the body text of the article:


loader 
=
 WebBaseLoader
(
    web_paths
=
[
page_url
]
,
    bs_kwargs
=
{
        
"parse_only"
:
 bs4
.
SoupStrainer
(
class_
=
"theme-doc-markdown markdown"
)
,
    
}
,
    bs_get_text_kwargs
=
{
"separator"
:
 
" | "
,
 
"strip"
:
 
True
}
,
)
docs 
=
 
[
]
async
 
for
 doc 
in
 loader
.
alazy_load
(
)
:
    docs
.
append
(
doc
)
assert
 
len
(
docs
)
 
==
 
1
doc 
=
 docs
[
0
]


print
(
f"
{
doc
.
metadata
}
\n"
)
print
(
doc
.
page_content
[
:
500
]
)


{'source': 'https://python.langchain.com/docs/how_to/chatbots_memory/'}
How to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We'll go into more detail on a few techniq


print
(
doc
.
page_content
[
-
500
:
]
)


a greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.'), | HumanMessage(content='What did I say my name was?'), | AIMessage(content='You introduced yourself as Nemo. How can I assist you today, Nemo?')] | Note that invoking the chain again will generate another summary generated from the initial summary plus new messages and so on. You could also design a hybrid approach where a certain number of messages are retained in chat history while others are summarized.


Note that this required advance technical knowledge of how the body text is represented in the underlying HTML.


We can parameterize 
WebBaseLoader
 with a variety of settings, allowing for specification of request headers, rate limits, and parsers and other kwargs for BeautifulSoup. See its 
API reference
 for detail.


Advanced parsing
​


This method is appropriate if we want more granular control or processing of the page content. Below, instead of generating one 
Document
 per page and controlling its content via BeautifulSoup, we generate multiple 
Document
 objects representing distinct structures on a page. These structures can include section titles and their corresponding body texts, lists or enumerations, tables, and more.


Under the hood it uses the 
langchain-unstructured
 library. See the 
integration docs
 for more information about using 
Unstructured
 with LangChain.


from
 langchain_unstructured 
import
 UnstructuredLoader
page_url 
=
 
"https://python.langchain.com/docs/how_to/chatbots_memory/"
loader 
=
 UnstructuredLoader
(
web_url
=
page_url
)
docs 
=
 
[
]
async
 
for
 doc 
in
 loader
.
alazy_load
(
)
:
    docs
.
append
(
doc
)
API Reference:
UnstructuredLoader


INFO: Note: NumExpr detected 12 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: NumExpr defaulting to 8 threads.


Note that with no advance knowledge of the page HTML structure, we recover a natural organization of the body text:


for
 doc 
in
 docs
[
:
5
]
:
    
print
(
doc
.
page_content
)


How to add memory to chatbots
A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:
Simply stuffing previous messages into a chat model prompt.
The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.
More complex modifications like synthesizing summaries for long running conversations.
ERROR! Session/line number was not unique in database. History logging moved to new session 2747


Extracting content from specific sections
​


Each 
Document
 object represents an element of the page. Its metadata contains useful information, such as its category:


for
 doc 
in
 docs
[
:
5
]
:
    
print
(
f'
{
doc
.
metadata
[
"category"
]
}
: 
{
doc
.
page_content
}
'
)


Title: How to add memory to chatbots
NarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:
ListItem: Simply stuffing previous messages into a chat model prompt.
ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.
ListItem: More complex modifications like synthesizing summaries for long running conversations.


Elements may also have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding 
Document
 objects.


As an example, below we load the content of the "Setup" sections for two web pages:


from
 typing 
import
 List
from
 langchain_core
.
documents 
import
 Document
async
 
def
 
_get_setup_docs_from_url
(
url
:
 
str
)
 
-
>
 List
[
Document
]
:
    loader 
=
 UnstructuredLoader
(
web_url
=
url
)
    setup_docs 
=
 
[
]
    parent_id 
=
 
-
1
    
async
 
for
 doc 
in
 loader
.
alazy_load
(
)
:
        
if
 doc
.
metadata
[
"category"
]
 
==
 
"Title"
 
and
 doc
.
page_content
.
startswith
(
"Setup"
)
:
            parent_id 
=
 doc
.
metadata
[
"element_id"
]
        
if
 doc
.
metadata
.
get
(
"parent_id"
)
 
==
 parent_id
:
            setup_docs
.
append
(
doc
)
    
return
 setup_docs
page_urls 
=
 
[
    
"https://python.langchain.com/docs/how_to/chatbots_memory/"
,
    
"https://python.langchain.com/docs/how_to/chatbots_tools/"
,
]
setup_docs 
=
 
[
]
for
 url 
in
 page_urls
:
    page_setup_docs 
=
 
await
 _get_setup_docs_from_url
(
url
)
    setup_docs
.
extend
(
page_setup_docs
)
API Reference:
Document


from
 collections 
import
 defaultdict
setup_text 
=
 defaultdict
(
str
)
for
 doc 
in
 setup_docs
:
    url 
=
 doc
.
metadata
[
"url"
]
    setup_text
[
url
]
 
+=
 
f"
{
doc
.
page_content
}
\n"
dict
(
setup_text
)


{'https://python.langchain.com/docs/how_to/chatbots_memory/': "You'll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\n%pip install --upgrade --quiet langchain langchain-openai\n\n# Set env var OPENAI_API_KEY or load from a .env file:\nimport dotenv\n\ndotenv.load_dotenv()\n[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\nYou should consider upgrading via the '/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip' command.[0m[33m\n[0mNote: you may need to restart the kernel to use updated packages.\n",
 'https://python.langchain.com/docs/how_to/chatbots_tools/': "For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.\nYou'll need to sign up for an account on the Tavily website, and install the following packages:\n%pip install --upgrade --quiet langchain-community langchain-openai tavily-python\n\n# Set env var OPENAI_API_KEY or load from a .env file:\nimport dotenv\n\ndotenv.load_dotenv()\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.\n"}


Vector search over page content
​


Once we have loaded the page contents into LangChain 
Document
 objects, we can index them (e.g., for a RAG application) in the usual way. Below we use OpenAI 
embeddings
, although any LangChain embeddings model will suffice.


%
pip install 
-
qU langchain
-
openai


import
 getpass
import
 os
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"OpenAI API Key:"
)


from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
from
 langchain_openai 
import
 OpenAIEmbeddings
vector_store 
=
 InMemoryVectorStore
.
from_documents
(
setup_docs
,
 OpenAIEmbeddings
(
)
)
retrieved_docs 
=
 vector_store
.
similarity_search
(
"Install Tavily"
,
 k
=
2
)
for
 doc 
in
 retrieved_docs
:
    
print
(
f'Page 
{
doc
.
metadata
[
"url"
]
}
: 
{
doc
.
page_content
[
:
300]
}
\n'
)
API Reference:
InMemoryVectorStore
 | 
OpenAIEmbeddings


INFO: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
INFO: HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
``````output
Page https://python.langchain.com/docs/how_to/chatbots_tools/: You'll need to sign up for an account on the Tavily website, and install the following packages:
Page https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we'll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you're using Tavily.


Other web page loaders
​


For a list of available LangChain web page loaders, please see 
this table
.
Edit this page
Was this page helpful?
Previous
How to load PDFs
Next
How to create a dynamic (self-constructing) chain
Setup
Simple and fast text extraction
Advanced parsing
Extracting content from specific sections
Vector search over page content
Other web page loaders


--- docs_how_to_dynamic_chain.txt ---

How-to guides
How to create a dynamic (self-constructing) chain
How to create a dynamic (self-constructing) chain


Prerequisites
This guide assumes familiarity with the following:


LangChain Expression Language (LCEL)


How to turn any function into a runnable




Sometimes we want to construct parts of a chain at runtime, depending on the chain inputs (
routing
 is the most common example of this). We can create dynamic chains like this using a very useful property of RunnableLambda's, which is that if a RunnableLambda returns a Runnable, that Runnable is itself invoked. Let's see an example.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


# | echo: false
from
 langchain_anthropic 
import
 ChatAnthropic
llm 
=
 ChatAnthropic
(
model
=
"claude-3-sonnet-20240229"
)
API Reference:
ChatAnthropic


from
 operator 
import
 itemgetter
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 Runnable
,
 RunnablePassthrough
,
 chain
contextualize_instructions 
=
 
"""Convert the latest user question into a standalone question given the chat history. Don't answer the question, return the question and nothing else (no descriptive text)."""
contextualize_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 contextualize_instructions
)
,
        
(
"placeholder"
,
 
"{chat_history}"
)
,
        
(
"human"
,
 
"{question}"
)
,
    
]
)
contextualize_question 
=
 contextualize_prompt 
|
 llm 
|
 StrOutputParser
(
)
qa_instructions 
=
 
(
    
"""Answer the user question given the following context:\n\n{context}."""
)
qa_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 qa_instructions
)
,
 
(
"human"
,
 
"{question}"
)
]
)
@chain
def
 
contextualize_if_needed
(
input_
:
 
dict
)
 
-
>
 Runnable
:
    
if
 input_
.
get
(
"chat_history"
)
:
        
# NOTE: This is returning another Runnable, not an actual output.
        
return
 contextualize_question
    
else
:
        
return
 RunnablePassthrough
(
)
 
|
 itemgetter
(
"question"
)
@chain
def
 
fake_retriever
(
input_
:
 
dict
)
 
-
>
 
str
:
    
return
 
"egypt's population in 2024 is about 111 million"
full_chain 
=
 
(
    RunnablePassthrough
.
assign
(
question
=
contextualize_if_needed
)
.
assign
(
        context
=
fake_retriever
    
)
    
|
 qa_prompt
    
|
 llm
    
|
 StrOutputParser
(
)
)
full_chain
.
invoke
(
    
{
        
"question"
:
 
"what about egypt"
,
        
"chat_history"
:
 
[
            
(
"human"
,
 
"what's the population of indonesia"
)
,
            
(
"ai"
,
 
"about 276 million"
)
,
        
]
,
    
}
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
Runnable
 | 
RunnablePassthrough
 | 
chain


"According to the context provided, Egypt's population in 2024 is estimated to be about 111 million."


The key here is that 
contextualize_if_needed
 returns another Runnable and not an actual output. This returned Runnable is itself run when the full chain is executed.


Looking at the trace we can see that, since we passed in chat_history, we executed the contextualize_question chain as part of the full chain: 
https://smith.langchain.com/public/9e0ae34c-4082-4f3f-beed-34a2a2f4c991/r


Note that the streaming, batching, etc. capabilities of the returned Runnable are all preserved


for
 chunk 
in
 contextualize_if_needed
.
stream
(
    
{
        
"question"
:
 
"what about egypt"
,
        
"chat_history"
:
 
[
            
(
"human"
,
 
"what's the population of indonesia"
)
,
            
(
"ai"
,
 
"about 276 million"
)
,
        
]
,
    
}
)
:
    
print
(
chunk
)


What
 is
 the
 population
 of
 Egypt
?
Edit this page
Was this page helpful?
Previous
How to load web pages
Next
Text embedding models


--- docs_how_to_embed_text.txt ---

How-to guides
Text embedding models
On this page
Text embedding models


info
Head to 
Integrations
 for documentation on built-in integrations with text embedding model providers.


The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.


Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.


The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former, 
.embed_documents
, takes as input multiple texts, while the latter, 
.embed_query
, takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).

.embed_query
 will return a list of floats, whereas 
.embed_documents
 returns a list of lists of floats.


Get started
​


Setup
​




Select 
embeddings model
:
OpenAI
▾
OpenAI
Azure
Google
AWS
HuggingFace
Ollama
Cohere
MistralAI
Nomic
NVIDIA
Voyage AI
IBM watsonx
Fake
pip install -qU langchain-openai
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings_model 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-large"
)


embed_documents
​


Embed list of texts
​


Use 
.embed_documents
 to embed a list of strings, recovering a list of embeddings:


embeddings 
=
 embeddings_model
.
embed_documents
(
    
[
        
"Hi there!"
,
        
"Oh, hello!"
,
        
"What's your name?"
,
        
"My friends call me World"
,
        
"Hello World!"
    
]
)
len
(
embeddings
)
,
 
len
(
embeddings
[
0
]
)


(5, 1536)


embed_query
​


Embed single query
​


Use 
.embed_query
 to embed a single piece of text (e.g., for the purpose of comparing to other embedded pieces of texts).


embedded_query 
=
 embeddings_model
.
embed_query
(
"What was the name mentioned in the conversation?"
)
embedded_query
[
:
5
]


[0.0053587136790156364,
 -0.0004999046213924885,
 0.038883671164512634,
 -0.003001077566295862,
 -0.00900818221271038]
Edit this page
Was this page helpful?
Previous
How to create a dynamic (self-constructing) chain
Next
How to combine results from multiple retrievers
Get started
Setup
embed_documents
embed_query


--- docs_how_to_ensemble_retriever.txt ---

How-to guides
How to combine results from multiple retrievers
On this page
How to combine results from multiple retrievers


The 
EnsembleRetriever
 supports ensembling of results from multiple 
retrievers
. It is initialized with a list of 
BaseRetriever
 objects. EnsembleRetrievers rerank the results of the constituent retrievers based on the 
Reciprocal Rank Fusion
 algorithm.


By leveraging the strengths of different algorithms, the 
EnsembleRetriever
 can achieve better performance than any single algorithm.


The most common pattern is to combine a sparse retriever (like BM25) with a dense retriever (like embedding similarity), because their strengths are complementary. It is also known as "hybrid search". The sparse retriever is good at finding relevant documents based on keywords, while the dense retriever is good at finding relevant documents based on semantic similarity.


Basic usage
​


Below we demonstrate ensembling of a 
BM25Retriever
 with a retriever derived from the 
FAISS vector store
.


%
pip install 
-
-
upgrade 
-
-
quiet  rank_bm25 
>
 
/
dev
/
null


from
 langchain
.
retrievers 
import
 EnsembleRetriever
from
 langchain_community
.
retrievers 
import
 BM25Retriever
from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_openai 
import
 OpenAIEmbeddings
doc_list_1 
=
 
[
    
"I like apples"
,
    
"I like oranges"
,
    
"Apples and oranges are fruits"
,
]
# initialize the bm25 retriever and faiss retriever
bm25_retriever 
=
 BM25Retriever
.
from_texts
(
    doc_list_1
,
 metadatas
=
[
{
"source"
:
 
1
}
]
 
*
 
len
(
doc_list_1
)
)
bm25_retriever
.
k 
=
 
2
doc_list_2 
=
 
[
    
"You like apples"
,
    
"You like oranges"
,
]
embedding 
=
 OpenAIEmbeddings
(
)
faiss_vectorstore 
=
 FAISS
.
from_texts
(
    doc_list_2
,
 embedding
,
 metadatas
=
[
{
"source"
:
 
2
}
]
 
*
 
len
(
doc_list_2
)
)
faiss_retriever 
=
 faiss_vectorstore
.
as_retriever
(
search_kwargs
=
{
"k"
:
 
2
}
)
# initialize the ensemble retriever
ensemble_retriever 
=
 EnsembleRetriever
(
    retrievers
=
[
bm25_retriever
,
 faiss_retriever
]
,
 weights
=
[
0.5
,
 
0.5
]
)
API Reference:
EnsembleRetriever
 | 
BM25Retriever
 | 
FAISS
 | 
OpenAIEmbeddings


docs 
=
 ensemble_retriever
.
invoke
(
"apples"
)
docs


[Document(page_content='I like apples', metadata={'source': 1}),
 Document(page_content='You like apples', metadata={'source': 2}),
 Document(page_content='Apples and oranges are fruits', metadata={'source': 1}),
 Document(page_content='You like oranges', metadata={'source': 2})]


Runtime Configuration
​


We can also configure the individual retrievers at runtime using 
configurable fields
. Below we update the "top-k" parameter for the FAISS retriever specifically:


from
 langchain_core
.
runnables 
import
 ConfigurableField
faiss_retriever 
=
 faiss_vectorstore
.
as_retriever
(
    search_kwargs
=
{
"k"
:
 
2
}
)
.
configurable_fields
(
    search_kwargs
=
ConfigurableField
(
        
id
=
"search_kwargs_faiss"
,
        name
=
"Search Kwargs"
,
        description
=
"The search kwargs to use"
,
    
)
)
ensemble_retriever 
=
 EnsembleRetriever
(
    retrievers
=
[
bm25_retriever
,
 faiss_retriever
]
,
 weights
=
[
0.5
,
 
0.5
]
)
API Reference:
ConfigurableField


config 
=
 
{
"configurable"
:
 
{
"search_kwargs_faiss"
:
 
{
"k"
:
 
1
}
}
}
docs 
=
 ensemble_retriever
.
invoke
(
"apples"
,
 config
=
config
)
docs


[Document(page_content='I like apples', metadata={'source': 1}),
 Document(page_content='You like apples', metadata={'source': 2}),
 Document(page_content='Apples and oranges are fruits', metadata={'source': 1})]


Notice that this only returns one source from the FAISS retriever, because we pass in the relevant configuration at run time
Edit this page
Was this page helpful?
Previous
Text embedding models
Next
How to select examples from a LangSmith dataset
Basic usage
Runtime Configuration


--- docs_how_to_example_selectors.txt ---

How-to guides
How to use example selectors
On this page
How to use example selectors


If you have a large number of examples, you may need to select which ones to include in the prompt. The 
Example Selector
 is the class responsible for doing so.


The base interface is defined as below:


class
 
BaseExampleSelector
(
ABC
)
:
    
"""Interface for selecting examples to include in prompts."""
    
@abstractmethod
    
def
 
select_examples
(
self
,
 input_variables
:
 Dict
[
str
,
 
str
]
)
 
-
>
 List
[
dict
]
:
        
"""Select which examples to use based on the inputs."""
        
    
@abstractmethod
    
def
 
add_example
(
self
,
 example
:
 Dict
[
str
,
 
str
]
)
 
-
>
 Any
:
        
"""Add new example to store."""


The only method it needs to define is a 
select_examples
 method. This takes in the input variables and then returns a list of examples. It is up to each specific implementation as to how those examples are selected.


LangChain has a few different types of example selectors. For an overview of all these types, see the 
below table
.


In this guide, we will walk through creating a custom example selector.


Examples
​


In order to use an example selector, we need to create a list of examples. These should generally be example inputs and outputs. For this demo purpose, let's imagine we are selecting examples of how to translate English to Italian.


examples 
=
 
[
    
{
"input"
:
 
"hi"
,
 
"output"
:
 
"ciao"
}
,
    
{
"input"
:
 
"bye"
,
 
"output"
:
 
"arrivederci"
}
,
    
{
"input"
:
 
"soccer"
,
 
"output"
:
 
"calcio"
}
,
]


Custom Example Selector
​


Let's write an example selector that chooses what example to pick based on the length of the word.


from
 langchain_core
.
example_selectors
.
base 
import
 BaseExampleSelector
class
 
CustomExampleSelector
(
BaseExampleSelector
)
:
    
def
 
__init__
(
self
,
 examples
)
:
        self
.
examples 
=
 examples
    
def
 
add_example
(
self
,
 example
)
:
        self
.
examples
.
append
(
example
)
    
def
 
select_examples
(
self
,
 input_variables
)
:
        
# This assumes knowledge that part of the input will be a 'text' key
        new_word 
=
 input_variables
[
"input"
]
        new_word_length 
=
 
len
(
new_word
)
        
# Initialize variables to store the best match and its length difference
        best_match 
=
 
None
        smallest_diff 
=
 
float
(
"inf"
)
        
# Iterate through each example
        
for
 example 
in
 self
.
examples
:
            
# Calculate the length difference with the first word of the example
            current_diff 
=
 
abs
(
len
(
example
[
"input"
]
)
 
-
 new_word_length
)
            
# Update the best match if the current one is closer in length
            
if
 current_diff 
<
 smallest_diff
:
                smallest_diff 
=
 current_diff
                best_match 
=
 example
        
return
 
[
best_match
]
API Reference:
BaseExampleSelector


example_selector 
=
 CustomExampleSelector
(
examples
)


example_selector
.
select_examples
(
{
"input"
:
 
"okay"
}
)


[{'input': 'bye', 'output': 'arrivederci'}]


example_selector
.
add_example
(
{
"input"
:
 
"hand"
,
 
"output"
:
 
"mano"
}
)


example_selector
.
select_examples
(
{
"input"
:
 
"okay"
}
)


[{'input': 'hand', 'output': 'mano'}]


Use in a Prompt
​


We can now use this example selector in a prompt


from
 langchain_core
.
prompts
.
few_shot 
import
 FewShotPromptTemplate
from
 langchain_core
.
prompts
.
prompt 
import
 PromptTemplate
example_prompt 
=
 PromptTemplate
.
from_template
(
"Input: {input} -> Output: {output}"
)
API Reference:
FewShotPromptTemplate
 | 
PromptTemplate


prompt 
=
 FewShotPromptTemplate
(
    example_selector
=
example_selector
,
    example_prompt
=
example_prompt
,
    suffix
=
"Input: {input} -> Output:"
,
    prefix
=
"Translate the following words from English to Italian:"
,
    input_variables
=
[
"input"
]
,
)
print
(
prompt
.
format
(
input
=
"word"
)
)


Translate the following words from English to Italian:
Input: hand -> Output: mano
Input: word -> Output:


Example Selector Types
​


Name
Description
Similarity
Uses semantic similarity between inputs and examples to decide which examples to choose.
MMR
Uses Max Marginal Relevance between inputs and examples to decide which examples to choose.
Length
Selects examples based on how many can fit within a certain length
Ngram
Uses ngram overlap between inputs and examples to decide which examples to choose.
Edit this page
Was this page helpful?
Previous
How to add memory to chatbots
Next
How to add a semantic layer over graph database
Examples
Custom Example Selector
Use in a Prompt
Example Selector Types


--- docs_how_to_example_selectors_langsmith.txt ---

How-to guides
How to select examples from a LangSmith dataset
On this page
How to select examples from a LangSmith dataset




📚
Prerequisites
Chat models
Few-shot-prompting
LangSmith


📦
Compatibility
The code in this guide requires
 
langsmith>=0.1.101
, 
langchain-core>=0.2.34
. Please ensure you have the correct packages installed.


LangSmith
 datasets have built-in support for similarity search, making them a great tool for building and querying few-shot examples.


In this guide we'll see how to use an indexed LangSmith dataset as a few-shot example selector.


Setup
​


Before getting started make sure you've 
created a LangSmith account
 and set your credentials:


import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"LANGSMITH_API_KEY"
)
:
    os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
"Set LangSmith API key:\n\n"
)
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"


Set LangSmith API key:
········


We'll need to install the 
langsmith
 SDK. In this example we'll also make use of 
langchain
, 
langchain-openai
, and 
langchain-benchmarks
:


%
pip install 
-
qU 
"langsmith>=0.1.101"
 
"langchain-core>=0.2.34"
 langchain langchain
-
openai langchain
-
benchmarks


Now we'll clone a public dataset and turn on indexing for the dataset. We can also turn on indexing via the 
LangSmith UI
.


We'll clone the 
Multiverse math few shot example dataset
.


This enables searching over the dataset and will make sure that anytime we update/add examples they are also indexed.


from
 langsmith 
import
 Client 
as
 LangSmith
ls_client 
=
 LangSmith
(
)
dataset_name 
=
 
"multiverse-math-few-shot-examples-v2"
dataset_public_url 
=
 
(
    
"https://smith.langchain.com/public/620596ee-570b-4d2b-8c8f-f828adbe5242/d"
)
ls_client
.
clone_public_dataset
(
dataset_public_url
)
dataset_id 
=
 ls_client
.
read_dataset
(
dataset_name
=
dataset_name
)
.
id
ls_client
.
index_dataset
(
dataset_id
=
dataset_id
)


Querying dataset
​


Indexing can take a few seconds. Once the dataset is indexed, we can search for similar examples. Note that the input to the 
similar_examples
 method must have the same schema as the examples inputs. In this case our example inputs are a dictionary with a "question" key:


examples 
=
 ls_client
.
similar_examples
(
    
{
"question"
:
 
"whats the negation of the negation of the negation of 3"
}
,
    limit
=
3
,
    dataset_id
=
dataset_id
,
)
len
(
examples
)


3


examples
[
0
]
.
inputs
[
"question"
]


'evaluate the negation of -100'


For this dataset, the outputs are the conversation that followed the question in OpenAI message format:


examples
[
0
]
.
outputs
[
"conversation"
]


[{'role': 'assistant',
  'content': None,
  'tool_calls': [{'id': 'toolu_01HTpq4cYNUac6F7omUc2Wz3',
    'type': 'function',
    'function': {'name': 'negate', 'arguments': '{"a": -100}'}}]},
 {'role': 'tool',
  'content': '-100.0',
  'tool_call_id': 'toolu_01HTpq4cYNUac6F7omUc2Wz3'},
 {'role': 'assistant', 'content': 'So the answer is 100.'},
 {'role': 'user',
  'content': '100 is incorrect. Please refer to the output of your tool call.'},
 {'role': 'assistant',
  'content': [{'text': "You're right, my previous answer was incorrect. Let me re-evaluate using the tool output:",
    'type': 'text'}],
  'tool_calls': [{'id': 'toolu_01XsJQboYghGDygQpPjJkeRq',
    'type': 'function',
    'function': {'name': 'negate', 'arguments': '{"a": -100}'}}]},
 {'role': 'tool',
  'content': '-100.0',
  'tool_call_id': 'toolu_01XsJQboYghGDygQpPjJkeRq'},
 {'role': 'assistant', 'content': 'The answer is -100.0'},
 {'role': 'user',
  'content': 'You have the correct numerical answer but are returning additional text. Please only respond with the numerical answer.'},
 {'role': 'assistant', 'content': '-100.0'}]


Creating dynamic few-shot prompts
​


The search returns the examples whose inputs are most similar to the query input. We can use this for few-shot prompting a model like so:


from
 langchain
.
chat_models 
import
 init_chat_model
from
 langchain_benchmarks
.
tool_usage
.
tasks
.
multiverse_math 
import
 
(
    add
,
    cos
,
    divide
,
    log
,
    multiply
,
    negate
,
    pi
,
    power
,
    sin
,
    subtract
,
)
from
 langchain_core
.
runnables 
import
 RunnableLambda
from
 langsmith 
import
 AsyncClient 
as
 AsyncLangSmith
async_ls_client 
=
 AsyncLangSmith
(
)
def
 
similar_examples
(
input_
:
 
dict
)
 
-
>
 
dict
:
    examples 
=
 ls_client
.
similar_examples
(
input_
,
 limit
=
5
,
 dataset_id
=
dataset_id
)
    
return
 
{
**
input_
,
 
"examples"
:
 examples
}
async
 
def
 
asimilar_examples
(
input_
:
 
dict
)
 
-
>
 
dict
:
    examples 
=
 
await
 async_ls_client
.
similar_examples
(
        input_
,
 limit
=
5
,
 dataset_id
=
dataset_id
    
)
    
return
 
{
**
input_
,
 
"examples"
:
 examples
}
def
 
construct_prompt
(
input_
:
 
dict
)
 
-
>
 
list
:
    instructions 
=
 
"""You are great at using mathematical tools."""
    examples 
=
 
[
]
    
for
 ex 
in
 input_
[
"examples"
]
:
        examples
.
append
(
{
"role"
:
 
"user"
,
 
"content"
:
 ex
.
inputs
[
"question"
]
}
)
        
for
 msg 
in
 ex
.
outputs
[
"conversation"
]
:
            
if
 msg
[
"role"
]
 
==
 
"assistant"
:
                msg
[
"name"
]
 
=
 
"example_assistant"
            
if
 msg
[
"role"
]
 
==
 
"user"
:
                msg
[
"name"
]
 
=
 
"example_user"
            examples
.
append
(
msg
)
    
return
 
[
        
{
"role"
:
 
"system"
,
 
"content"
:
 instructions
}
,
        
*
examples
,
        
{
"role"
:
 
"user"
,
 
"content"
:
 input_
[
"question"
]
}
,
    
]
tools 
=
 
[
add
,
 cos
,
 divide
,
 log
,
 multiply
,
 negate
,
 pi
,
 power
,
 sin
,
 subtract
]
llm 
=
 init_chat_model
(
"gpt-4o-2024-08-06"
)
llm_with_tools 
=
 llm
.
bind_tools
(
tools
)
example_selector 
=
 RunnableLambda
(
func
=
similar_examples
,
 afunc
=
asimilar_examples
)
chain 
=
 example_selector 
|
 construct_prompt 
|
 llm_with_tools
API Reference:
init_chat_model
 | 
RunnableLambda


ai_msg 
=
 
await
 chain
.
ainvoke
(
{
"question"
:
 
"whats the negation of the negation of 3"
}
)
ai_msg
.
tool_calls


[{'name': 'negate',
  'args': {'a': 3},
  'id': 'call_uMSdoTl6ehfHh5a6JQUb2NoZ',
  'type': 'tool_call'}]


Looking at the LangSmith trace, we can see that relevant examples were pulled in in the 
similar_examples
 step and passed as messages to ChatOpenAI: 
https://smith.langchain.com/public/9585e30f-765a-4ed9-b964-2211420cd2f8/r/fdea98d6-e90f-49d4-ac22-dfd012e9e0d9
.
Edit this page
Was this page helpful?
Previous
How to combine results from multiple retrievers
Next
How to select examples by length
Setup
Querying dataset
Creating dynamic few-shot prompts


--- docs_how_to_example_selectors_length_based.txt ---

How-to guides
How to select examples by length
How to select examples by length


This 
example selector
 selects which examples to use based on length. This is useful when you are worried about constructing a prompt that will go over the length of the context window. For longer inputs, it will select fewer examples to include, while for shorter inputs it will select more.


from
 langchain_core
.
example_selectors 
import
 LengthBasedExampleSelector
from
 langchain_core
.
prompts 
import
 FewShotPromptTemplate
,
 PromptTemplate
# Examples of a pretend task of creating antonyms.
examples 
=
 
[
    
{
"input"
:
 
"happy"
,
 
"output"
:
 
"sad"
}
,
    
{
"input"
:
 
"tall"
,
 
"output"
:
 
"short"
}
,
    
{
"input"
:
 
"energetic"
,
 
"output"
:
 
"lethargic"
}
,
    
{
"input"
:
 
"sunny"
,
 
"output"
:
 
"gloomy"
}
,
    
{
"input"
:
 
"windy"
,
 
"output"
:
 
"calm"
}
,
]
example_prompt 
=
 PromptTemplate
(
    input_variables
=
[
"input"
,
 
"output"
]
,
    template
=
"Input: {input}\nOutput: {output}"
,
)
example_selector 
=
 LengthBasedExampleSelector
(
    
# The examples it has available to choose from.
    examples
=
examples
,
    
# The PromptTemplate being used to format the examples.
    example_prompt
=
example_prompt
,
    
# The maximum length that the formatted examples should be.
    
# Length is measured by the get_text_length function below.
    max_length
=
25
,
    
# The function used to get the length of a string, which is used
    
# to determine which examples to include. It is commented out because
    
# it is provided as a default value if none is specified.
    
# get_text_length: Callable[[str], int] = lambda x: len(re.split("\n| ", x))
)
dynamic_prompt 
=
 FewShotPromptTemplate
(
    
# We provide an ExampleSelector instead of examples.
    example_selector
=
example_selector
,
    example_prompt
=
example_prompt
,
    prefix
=
"Give the antonym of every input"
,
    suffix
=
"Input: {adjective}\nOutput:"
,
    input_variables
=
[
"adjective"
]
,
)
API Reference:
LengthBasedExampleSelector
 | 
FewShotPromptTemplate
 | 
PromptTemplate


# An example with small input, so it selects all examples.
print
(
dynamic_prompt
.
format
(
adjective
=
"big"
)
)


Give the antonym of every input
Input: happy
Output: sad
Input: tall
Output: short
Input: energetic
Output: lethargic
Input: sunny
Output: gloomy
Input: windy
Output: calm
Input: big
Output:


# An example with long input, so it selects only one example.
long_string 
=
 
"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else"
print
(
dynamic_prompt
.
format
(
adjective
=
long_string
)
)


Give the antonym of every input
Input: happy
Output: sad
Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else
Output:


# You can add an example to an example selector as well.
new_example 
=
 
{
"input"
:
 
"big"
,
 
"output"
:
 
"small"
}
dynamic_prompt
.
example_selector
.
add_example
(
new_example
)
print
(
dynamic_prompt
.
format
(
adjective
=
"enthusiastic"
)
)


Give the antonym of every input
Input: happy
Output: sad
Input: tall
Output: short
Input: energetic
Output: lethargic
Input: sunny
Output: gloomy
Input: windy
Output: calm
Input: big
Output: small
Input: enthusiastic
Output:
Edit this page
Was this page helpful?
Previous
How to select examples from a LangSmith dataset
Next
How to select examples by maximal marginal relevance (MMR)


--- docs_how_to_example_selectors_mmr.txt ---

How-to guides
How to select examples by maximal marginal relevance (MMR)
How to select examples by maximal marginal relevance (MMR)


The 
MaxMarginalRelevanceExampleSelector
 selects 
examples
 based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.


from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_core
.
example_selectors 
import
 
(
    MaxMarginalRelevanceExampleSelector
,
    SemanticSimilarityExampleSelector
,
)
from
 langchain_core
.
prompts 
import
 FewShotPromptTemplate
,
 PromptTemplate
from
 langchain_openai 
import
 OpenAIEmbeddings
example_prompt 
=
 PromptTemplate
(
    input_variables
=
[
"input"
,
 
"output"
]
,
    template
=
"Input: {input}\nOutput: {output}"
,
)
# Examples of a pretend task of creating antonyms.
examples 
=
 
[
    
{
"input"
:
 
"happy"
,
 
"output"
:
 
"sad"
}
,
    
{
"input"
:
 
"tall"
,
 
"output"
:
 
"short"
}
,
    
{
"input"
:
 
"energetic"
,
 
"output"
:
 
"lethargic"
}
,
    
{
"input"
:
 
"sunny"
,
 
"output"
:
 
"gloomy"
}
,
    
{
"input"
:
 
"windy"
,
 
"output"
:
 
"calm"
}
,
]
API Reference:
FAISS
 | 
MaxMarginalRelevanceExampleSelector
 | 
SemanticSimilarityExampleSelector
 | 
FewShotPromptTemplate
 | 
PromptTemplate
 | 
OpenAIEmbeddings


example_selector 
=
 MaxMarginalRelevanceExampleSelector
.
from_examples
(
    
# The list of examples available to select from.
    examples
,
    
# The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings
(
)
,
    
# The VectorStore class that is used to store the embeddings and do a similarity search over.
    FAISS
,
    
# The number of examples to produce.
    k
=
2
,
)
mmr_prompt 
=
 FewShotPromptTemplate
(
    
# We provide an ExampleSelector instead of examples.
    example_selector
=
example_selector
,
    example_prompt
=
example_prompt
,
    prefix
=
"Give the antonym of every input"
,
    suffix
=
"Input: {adjective}\nOutput:"
,
    input_variables
=
[
"adjective"
]
,
)


# Input is a feeling, so should select the happy/sad example as the first one
print
(
mmr_prompt
.
format
(
adjective
=
"worried"
)
)


Give the antonym of every input
Input: happy
Output: sad
Input: windy
Output: calm
Input: worried
Output:


# Let's compare this to what we would just get if we went solely off of similarity,
# by using SemanticSimilarityExampleSelector instead of MaxMarginalRelevanceExampleSelector.
example_selector 
=
 SemanticSimilarityExampleSelector
.
from_examples
(
    
# The list of examples available to select from.
    examples
,
    
# The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings
(
)
,
    
# The VectorStore class that is used to store the embeddings and do a similarity search over.
    FAISS
,
    
# The number of examples to produce.
    k
=
2
,
)
similar_prompt 
=
 FewShotPromptTemplate
(
    
# We provide an ExampleSelector instead of examples.
    example_selector
=
example_selector
,
    example_prompt
=
example_prompt
,
    prefix
=
"Give the antonym of every input"
,
    suffix
=
"Input: {adjective}\nOutput:"
,
    input_variables
=
[
"adjective"
]
,
)
print
(
similar_prompt
.
format
(
adjective
=
"worried"
)
)


Give the antonym of every input
Input: happy
Output: sad
Input: sunny
Output: gloomy
Input: worried
Output:
Edit this page
Was this page helpful?
Previous
How to select examples by length
Next
How to select examples by n-gram overlap


--- docs_how_to_example_selectors_ngram.txt ---

How-to guides
How to select examples by n-gram overlap
How to select examples by n-gram overlap


The 
NGramOverlapExampleSelector
 selects and orders examples based on which examples are most similar to the input, according to an ngram overlap score. The ngram overlap score is a float between 0.0 and 1.0, inclusive.


The 
selector
 allows for a threshold score to be set. Examples with an ngram overlap score less than or equal to the threshold are excluded. The threshold is set to -1.0, by default, so will not exclude any examples, only reorder them. Setting the threshold to 0.0 will exclude examples that have no ngram overlaps with the input.


from
 langchain_community
.
example_selectors 
import
 NGramOverlapExampleSelector
from
 langchain_core
.
prompts 
import
 FewShotPromptTemplate
,
 PromptTemplate
example_prompt 
=
 PromptTemplate
(
    input_variables
=
[
"input"
,
 
"output"
]
,
    template
=
"Input: {input}\nOutput: {output}"
,
)
# Examples of a fictional translation task.
examples 
=
 
[
    
{
"input"
:
 
"See Spot run."
,
 
"output"
:
 
"Ver correr a Spot."
}
,
    
{
"input"
:
 
"My dog barks."
,
 
"output"
:
 
"Mi perro ladra."
}
,
    
{
"input"
:
 
"Spot can run."
,
 
"output"
:
 
"Spot puede correr."
}
,
]
API Reference:
NGramOverlapExampleSelector
 | 
FewShotPromptTemplate
 | 
PromptTemplate


example_selector 
=
 NGramOverlapExampleSelector
(
    
# The examples it has available to choose from.
    examples
=
examples
,
    
# The PromptTemplate being used to format the examples.
    example_prompt
=
example_prompt
,
    
# The threshold, at which selector stops.
    
# It is set to -1.0 by default.
    threshold
=
-
1.0
,
    
# For negative threshold:
    
# Selector sorts examples by ngram overlap score, and excludes none.
    
# For threshold greater than 1.0:
    
# Selector excludes all examples, and returns an empty list.
    
# For threshold equal to 0.0:
    
# Selector sorts examples by ngram overlap score,
    
# and excludes those with no ngram overlap with input.
)
dynamic_prompt 
=
 FewShotPromptTemplate
(
    
# We provide an ExampleSelector instead of examples.
    example_selector
=
example_selector
,
    example_prompt
=
example_prompt
,
    prefix
=
"Give the Spanish translation of every input"
,
    suffix
=
"Input: {sentence}\nOutput:"
,
    input_variables
=
[
"sentence"
]
,
)


# An example input with large ngram overlap with "Spot can run."
# and no overlap with "My dog barks."
print
(
dynamic_prompt
.
format
(
sentence
=
"Spot can run fast."
)
)


Give the Spanish translation of every input
Input: Spot can run.
Output: Spot puede correr.
Input: See Spot run.
Output: Ver correr a Spot.
Input: My dog barks.
Output: Mi perro ladra.
Input: Spot can run fast.
Output:


# You can add examples to NGramOverlapExampleSelector as well.
new_example 
=
 
{
"input"
:
 
"Spot plays fetch."
,
 
"output"
:
 
"Spot juega a buscar."
}
example_selector
.
add_example
(
new_example
)
print
(
dynamic_prompt
.
format
(
sentence
=
"Spot can run fast."
)
)


Give the Spanish translation of every input
Input: Spot can run.
Output: Spot puede correr.
Input: See Spot run.
Output: Ver correr a Spot.
Input: Spot plays fetch.
Output: Spot juega a buscar.
Input: My dog barks.
Output: Mi perro ladra.
Input: Spot can run fast.
Output:


# You can set a threshold at which examples are excluded.
# For example, setting threshold equal to 0.0
# excludes examples with no ngram overlaps with input.
# Since "My dog barks." has no ngram overlaps with "Spot can run fast."
# it is excluded.
example_selector
.
threshold 
=
 
0.0
print
(
dynamic_prompt
.
format
(
sentence
=
"Spot can run fast."
)
)


Give the Spanish translation of every input
Input: Spot can run.
Output: Spot puede correr.
Input: See Spot run.
Output: Ver correr a Spot.
Input: Spot plays fetch.
Output: Spot juega a buscar.
Input: Spot can run fast.
Output:


# Setting small nonzero threshold
example_selector
.
threshold 
=
 
0.09
print
(
dynamic_prompt
.
format
(
sentence
=
"Spot can play fetch."
)
)


Give the Spanish translation of every input
Input: Spot can run.
Output: Spot puede correr.
Input: Spot plays fetch.
Output: Spot juega a buscar.
Input: Spot can play fetch.
Output:


# Setting threshold greater than 1.0
example_selector
.
threshold 
=
 
1.0
 
+
 
1e-9
print
(
dynamic_prompt
.
format
(
sentence
=
"Spot can play fetch."
)
)


Give the Spanish translation of every input
Input: Spot can play fetch.
Output:
Edit this page
Was this page helpful?
Previous
How to select examples by maximal marginal relevance (MMR)
Next
How to select examples by similarity


--- docs_how_to_example_selectors_similarity.txt ---

How-to guides
How to select examples by similarity
How to select examples by similarity


This object selects 
examples
 based on similarity to the inputs. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs.


from
 langchain_chroma 
import
 Chroma
from
 langchain_core
.
example_selectors 
import
 SemanticSimilarityExampleSelector
from
 langchain_core
.
prompts 
import
 FewShotPromptTemplate
,
 PromptTemplate
from
 langchain_openai 
import
 OpenAIEmbeddings
example_prompt 
=
 PromptTemplate
(
    input_variables
=
[
"input"
,
 
"output"
]
,
    template
=
"Input: {input}\nOutput: {output}"
,
)
# Examples of a pretend task of creating antonyms.
examples 
=
 
[
    
{
"input"
:
 
"happy"
,
 
"output"
:
 
"sad"
}
,
    
{
"input"
:
 
"tall"
,
 
"output"
:
 
"short"
}
,
    
{
"input"
:
 
"energetic"
,
 
"output"
:
 
"lethargic"
}
,
    
{
"input"
:
 
"sunny"
,
 
"output"
:
 
"gloomy"
}
,
    
{
"input"
:
 
"windy"
,
 
"output"
:
 
"calm"
}
,
]
API Reference:
SemanticSimilarityExampleSelector
 | 
FewShotPromptTemplate
 | 
PromptTemplate
 | 
OpenAIEmbeddings


example_selector 
=
 SemanticSimilarityExampleSelector
.
from_examples
(
    
# The list of examples available to select from.
    examples
,
    
# The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings
(
)
,
    
# The VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma
,
    
# The number of examples to produce.
    k
=
1
,
)
similar_prompt 
=
 FewShotPromptTemplate
(
    
# We provide an ExampleSelector instead of examples.
    example_selector
=
example_selector
,
    example_prompt
=
example_prompt
,
    prefix
=
"Give the antonym of every input"
,
    suffix
=
"Input: {adjective}\nOutput:"
,
    input_variables
=
[
"adjective"
]
,
)


# Input is a feeling, so should select the happy/sad example
print
(
similar_prompt
.
format
(
adjective
=
"worried"
)
)


Give the antonym of every input
Input: happy
Output: sad
Input: worried
Output:


# Input is a measurement, so should select the tall/short example
print
(
similar_prompt
.
format
(
adjective
=
"large"
)
)


Give the antonym of every input
Input: tall
Output: short
Input: large
Output:


# You can add new examples to the SemanticSimilarityExampleSelector as well
similar_prompt
.
example_selector
.
add_example
(
    
{
"input"
:
 
"enthusiastic"
,
 
"output"
:
 
"apathetic"
}
)
print
(
similar_prompt
.
format
(
adjective
=
"passionate"
)
)


Give the antonym of every input
Input: enthusiastic
Output: apathetic
Input: passionate
Output:
Edit this page
Was this page helpful?
Previous
How to select examples by n-gram overlap
Next
How to use reference examples when doing extraction


--- docs_how_to_extraction_examples.txt ---

How-to guides
How to use reference examples when doing extraction
On this page
How to use reference examples when doing extraction


The quality of extractions can often be improved by providing reference examples to the LLM.


Data extraction attempts to generate 
structured representations
 of information found in text and other unstructured or semi-structured formats. 
Tool-calling
 LLM features are often used in this context. This guide demonstrates how to build few-shot examples of tool calls to help steer the behavior of extraction and similar applications.


tip
While this guide focuses how to use examples with a tool calling model, this technique is generally applicable, and will work
also with JSON more or prompt based techniques.


LangChain implements a 
tool-call attribute
 on messages from LLMs that include tool calls. See our 
how-to guide on tool calling
 for more detail. To build reference examples for data extraction, we build a chat history containing a sequence of:




HumanMessage
 containing example inputs;


AIMessage
 containing example tool calls;


ToolMessage
 containing example tool outputs.




LangChain adopts this convention for structuring tool calls into conversation across LLM model providers.


First we build a prompt template that includes a placeholder for these messages:


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 MessagesPlaceholder
# Define a custom prompt to provide instructions and any additional context.
# 1) You can add examples into the prompt template to improve extraction quality
# 2) Introduce additional parameters to take context into account (e.g., include metadata
#    about the document from which the text was extracted.)
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"You are an expert extraction algorithm. "
            
"Only extract relevant information from the text. "
            
"If you do not know the value of an attribute asked "
            
"to extract, return null for the attribute's value."
,
        
)
,
        
# ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓
        MessagesPlaceholder
(
"examples"
)
,
  
# <-- EXAMPLES!
        
# ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑
        
(
"human"
,
 
"{text}"
)
,
    
]
)
API Reference:
ChatPromptTemplate
 | 
MessagesPlaceholder


Test out the template:


from
 langchain_core
.
messages 
import
 
(
    HumanMessage
,
)
prompt
.
invoke
(
    
{
"text"
:
 
"this is some text"
,
 
"examples"
:
 
[
HumanMessage
(
content
=
"testing 1 2 3"
)
]
}
)
API Reference:
HumanMessage


ChatPromptValue(messages=[SystemMessage(content="You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.", additional_kwargs={}, response_metadata={}), HumanMessage(content='testing 1 2 3', additional_kwargs={}, response_metadata={}), HumanMessage(content='this is some text', additional_kwargs={}, response_metadata={})])


Define the schema
​


Let's re-use the person schema from the 
extraction tutorial
.


from
 typing 
import
 List
,
 Optional
from
 langchain_openai 
import
 ChatOpenAI
from
 pydantic 
import
 BaseModel
,
 Field
class
 
Person
(
BaseModel
)
:
    
"""Information about a person."""
    
# ^ Doc-string for the entity Person.
    
# This doc-string is sent to the LLM as the description of the schema Person,
    
# and it can help to improve extraction results.
    
# Note that:
    
# 1. Each field is an `optional` -- this allows the model to decline to extract it!
    
# 2. Each field has a `description` -- this description is used by the LLM.
    
# Having a good description can help improve extraction results.
    name
:
 Optional
[
str
]
 
=
 Field
(
.
.
.
,
 description
=
"The name of the person"
)
    hair_color
:
 Optional
[
str
]
 
=
 Field
(
        
.
.
.
,
 description
=
"The color of the person's hair if known"
    
)
    height_in_meters
:
 Optional
[
str
]
 
=
 Field
(
.
.
.
,
 description
=
"Height in METERs"
)
class
 
Data
(
BaseModel
)
:
    
"""Extracted data about people."""
    
# Creates a model so that we can extract multiple entities.
    people
:
 List
[
Person
]
API Reference:
ChatOpenAI


Define reference examples
​


Examples can be defined as a list of input-output pairs.


Each example contains an example 
input
 text and an example 
output
 showing what should be extracted from the text.


important
This is a bit in the weeds, so feel free to skip.
The format of the example needs to match the API used (e.g., tool calling or JSON mode etc.).
Here, the formatted examples will match the format expected for the tool calling API since that's what we're using.


import
 uuid
from
 typing 
import
 Dict
,
 List
,
 TypedDict
from
 langchain_core
.
messages 
import
 
(
    AIMessage
,
    BaseMessage
,
    HumanMessage
,
    SystemMessage
,
    ToolMessage
,
)
from
 pydantic 
import
 BaseModel
,
 Field
class
 
Example
(
TypedDict
)
:
    
"""A representation of an example consisting of text input and expected tool calls.
    For extraction, the tool calls are represented as instances of pydantic model.
    """
    
input
:
 
str
  
# This is the example text
    tool_calls
:
 List
[
BaseModel
]
  
# Instances of pydantic model that should be extracted
def
 
tool_example_to_messages
(
example
:
 Example
)
 
-
>
 List
[
BaseMessage
]
:
    
"""Convert an example into a list of messages that can be fed into an LLM.
    This code is an adapter that converts our example to a list of messages
    that can be fed into a chat model.
    The list of messages per example corresponds to:
    1) HumanMessage: contains the content from which content should be extracted.
    2) AIMessage: contains the extracted information from the model
    3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.
    The ToolMessage is required because some of the chat models are hyper-optimized for agents
    rather than for an extraction use case.
    """
    messages
:
 List
[
BaseMessage
]
 
=
 
[
HumanMessage
(
content
=
example
[
"input"
]
)
]
    tool_calls 
=
 
[
]
    
for
 tool_call 
in
 example
[
"tool_calls"
]
:
        tool_calls
.
append
(
            
{
                
"id"
:
 
str
(
uuid
.
uuid4
(
)
)
,
                
"args"
:
 tool_call
.
dict
(
)
,
                
# The name of the function right now corresponds
                
# to the name of the pydantic model
                
# This is implicit in the API right now,
                
# and will be improved over time.
                
"name"
:
 tool_call
.
__class__
.
__name__
,
            
}
,
        
)
    messages
.
append
(
AIMessage
(
content
=
""
,
 tool_calls
=
tool_calls
)
)
    tool_outputs 
=
 example
.
get
(
"tool_outputs"
)
 
or
 
[
        
"You have correctly called this tool."
    
]
 
*
 
len
(
tool_calls
)
    
for
 output
,
 tool_call 
in
 
zip
(
tool_outputs
,
 tool_calls
)
:
        messages
.
append
(
ToolMessage
(
content
=
output
,
 tool_call_id
=
tool_call
[
"id"
]
)
)
    
return
 messages
API Reference:
AIMessage
 | 
BaseMessage
 | 
HumanMessage
 | 
SystemMessage
 | 
ToolMessage


Next let's define our examples and then convert them into message format.


examples 
=
 
[
    
(
        
"The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it."
,
        Data
(
people
=
[
]
)
,
    
)
,
    
(
        
"Fiona traveled far from France to Spain."
,
        Data
(
people
=
[
Person
(
name
=
"Fiona"
,
 height_in_meters
=
None
,
 hair_color
=
None
)
]
)
,
    
)
,
]
messages 
=
 
[
]
for
 text
,
 tool_call 
in
 examples
:
    messages
.
extend
(
        tool_example_to_messages
(
{
"input"
:
 text
,
 
"tool_calls"
:
 
[
tool_call
]
}
)
    
)


Let's test out the prompt


example_prompt 
=
 prompt
.
invoke
(
{
"text"
:
 
"this is some text"
,
 
"examples"
:
 messages
}
)
for
 message 
in
 example_prompt
.
messages
:
    
print
(
f"
{
message
.
type
}
: 
{
message
}
"
)


system: content="You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value." additional_kwargs={} response_metadata={}
human: content="The ocean is vast and blue. It's more than 20,000 feet deep. There are many fish in it." additional_kwargs={} response_metadata={}
ai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'Data', 'args': {'people': []}, 'id': '240159b1-1405-4107-a07c-3c6b91b3d5b7', 'type': 'tool_call'}]
tool: content='You have correctly called this tool.' tool_call_id='240159b1-1405-4107-a07c-3c6b91b3d5b7'
human: content='Fiona traveled far from France to Spain.' additional_kwargs={} response_metadata={}
ai: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'Data', 'args': {'people': [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]}, 'id': '3fc521e4-d1d2-4c20-bf40-e3d72f1068da', 'type': 'tool_call'}]
tool: content='You have correctly called this tool.' tool_call_id='3fc521e4-d1d2-4c20-bf40-e3d72f1068da'
human: content='this is some text' additional_kwargs={} response_metadata={}


Create an extractor
​


Let's select an LLM. Because we are using tool-calling, we will need a model that supports a tool-calling feature. See 
this table
 for available LLMs.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4-0125-preview"
,
 model_provider
=
"openai"
,
 temperature
=
0
)


Following the 
extraction tutorial
, we use the 
.with_structured_output
 method to structure model outputs according to the desired schema:


runnable 
=
 prompt 
|
 llm
.
with_structured_output
(
    schema
=
Data
,
    method
=
"function_calling"
,
    include_raw
=
False
,
)


Without examples 😿
​


Notice that even capable models can fail with a 
very simple
 test case!


for
 _ 
in
 
range
(
5
)
:
    text 
=
 
"The solar system is large, but earth has only 1 moon."
    
print
(
runnable
.
invoke
(
{
"text"
:
 text
,
 
"examples"
:
 
[
]
}
)
)


people=[Person(name='earth', hair_color='null', height_in_meters='null')]
``````output
people=[Person(name='earth', hair_color='null', height_in_meters='null')]
``````output
people=[]
``````output
people=[Person(name='earth', hair_color='null', height_in_meters='null')]
``````output
people=[]


With examples 😻
​


Reference examples helps to fix the failure!


for
 _ 
in
 
range
(
5
)
:
    text 
=
 
"The solar system is large, but earth has only 1 moon."
    
print
(
runnable
.
invoke
(
{
"text"
:
 text
,
 
"examples"
:
 messages
}
)
)


people=[]
``````output
people=[]
``````output
people=[]
``````output
people=[]
``````output
people=[]


Note that we can see the few-shot examples as tool-calls in the 
Langsmith trace
.


And we retain performance on a positive sample:


runnable
.
invoke
(
    
{
        
"text"
:
 
"My name is Harrison. My hair is black."
,
        
"examples"
:
 messages
,
    
}
)


Data(people=[Person(name='Harrison', hair_color='black', height_in_meters=None)])
Edit this page
Was this page helpful?
Previous
How to select examples by similarity
Next
How to handle long text when doing extraction
Define the schema
Define reference examples
Create an extractor
Without examples 😿
With examples 😻


--- docs_how_to_extraction_long_text.txt ---

How-to guides
How to handle long text when doing extraction
On this page
How to handle long text when doing extraction


When working with files, like PDFs, you're likely to encounter text that exceeds your language model's context window. To process this text, consider these strategies:




Change LLM
 Choose a different LLM that supports a larger context window.


Brute Force
 Chunk the document, and extract content from each chunk.


RAG
 Chunk the document, index the chunks, and only extract content from a subset of chunks that look "relevant".




Keep in mind that these strategies have different trade off and the best strategy likely depends on the application that you're designing!


This guide demonstrates how to implement strategies 2 and 3.


Setup
​


First we'll install the dependencies needed for this guide:


%
pip install 
-
qU langchain
-
community lxml faiss
-
cpu langchain
-
openai


Note: you may need to restart the kernel to use updated packages.


Now we need some example data! Let's download an article about 
cars from wikipedia
 and load it as a LangChain 
Document
.


import
 re
import
 requests
from
 langchain_community
.
document_loaders 
import
 BSHTMLLoader
# Download the content
response 
=
 requests
.
get
(
"https://en.wikipedia.org/wiki/Car"
)
# Write it to a file
with
 
open
(
"car.html"
,
 
"w"
,
 encoding
=
"utf-8"
)
 
as
 f
:
    f
.
write
(
response
.
text
)
# Load it with an HTML parser
loader 
=
 BSHTMLLoader
(
"car.html"
)
document 
=
 loader
.
load
(
)
[
0
]
# Clean up code
# Replace consecutive new lines with a single new line
document
.
page_content 
=
 re
.
sub
(
"\n\n+"
,
 
"\n"
,
 document
.
page_content
)
API Reference:
BSHTMLLoader


print
(
len
(
document
.
page_content
)
)


78865


Define the schema
​


Following the 
extraction tutorial
, we will use Pydantic to define the schema of information we wish to extract. In this case, we will extract a list of "key developments" (e.g., important historical events) that include a year and description.


Note that we also include an 
evidence
 key and instruct the model to provide in verbatim the relevant sentences of text from the article. This allows us to compare the extraction results to (the model's reconstruction of) text from the original document.


from
 typing 
import
 List
,
 Optional
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 MessagesPlaceholder
from
 pydantic 
import
 BaseModel
,
 Field
class
 
KeyDevelopment
(
BaseModel
)
:
    
"""Information about a development in the history of cars."""
    year
:
 
int
 
=
 Field
(
        
.
.
.
,
 description
=
"The year when there was an important historic development."
    
)
    description
:
 
str
 
=
 Field
(
        
.
.
.
,
 description
=
"What happened in this year? What was the development?"
    
)
    evidence
:
 
str
 
=
 Field
(
        
.
.
.
,
        description
=
"Repeat in verbatim the sentence(s) from which the year and description information were extracted"
,
    
)
class
 
ExtractionData
(
BaseModel
)
:
    
"""Extracted information about key developments in the history of cars."""
    key_developments
:
 List
[
KeyDevelopment
]
# Define a custom prompt to provide instructions and any additional context.
# 1) You can add examples into the prompt template to improve extraction quality
# 2) Introduce additional parameters to take context into account (e.g., include metadata
#    about the document from which the text was extracted.)
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"You are an expert at identifying key historic development in text. "
            
"Only extract important historic developments. Extract nothing if no important information can be found in the text."
,
        
)
,
        
(
"human"
,
 
"{text}"
)
,
    
]
)
API Reference:
ChatPromptTemplate
 | 
MessagesPlaceholder


Create an extractor
​


Let's select an LLM. Because we are using tool-calling, we will need a model that supports a tool-calling feature. See 
this table
 for available LLMs.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o"
,
 model_provider
=
"openai"
,
 temperature
=
0
)


extractor 
=
 prompt 
|
 llm
.
with_structured_output
(
    schema
=
ExtractionData
,
    include_raw
=
False
,
)


Brute force approach
​


Split the documents into chunks such that each chunk fits into the context window of the LLMs.


from
 langchain_text_splitters 
import
 TokenTextSplitter
text_splitter 
=
 TokenTextSplitter
(
    
# Controls the size of each chunk
    chunk_size
=
2000
,
    
# Controls overlap between chunks
    chunk_overlap
=
20
,
)
texts 
=
 text_splitter
.
split_text
(
document
.
page_content
)
API Reference:
TokenTextSplitter


Use 
batch
 functionality to run the extraction in 
parallel
 across each chunk!


tip
You can often use .batch() to parallelize the extractions! 
.batch
 uses a threadpool under the hood to help you parallelize workloads.
If your model is exposed via an API, this will likely speed up your extraction flow!


# Limit just to the first 3 chunks
# so the code can be re-run quickly
first_few 
=
 texts
[
:
3
]
extractions 
=
 extractor
.
batch
(
    
[
{
"text"
:
 text
}
 
for
 text 
in
 first_few
]
,
    
{
"max_concurrency"
:
 
5
}
,
  
# limit the concurrency by passing max concurrency!
)


Merge results
​


After extracting data from across the chunks, we'll want to merge the extractions together.


key_developments 
=
 
[
]
for
 extraction 
in
 extractions
:
    key_developments
.
extend
(
extraction
.
key_developments
)
key_developments
[
:
10
]


[KeyDevelopment(year=1769, description='Nicolas-Joseph Cugnot built the first steam-powered road vehicle.', evidence='The French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769, while the Swiss inventor François Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.'),
 KeyDevelopment(year=1808, description='François Isaac de Rivaz designed and constructed the first internal combustion-powered automobile.', evidence='The French inventor Nicolas-Joseph Cugnot built the first steam-powered road vehicle in 1769, while the Swiss inventor François Isaac de Rivaz designed and constructed the first internal combustion-powered automobile in 1808.'),
 KeyDevelopment(year=1886, description='Carl Benz invented the modern car, a practical, marketable automobile for everyday use, and patented his Benz Patent-Motorwagen.', evidence='The modern car—a practical, marketable automobile for everyday use—was invented in 1886, when the German inventor Carl Benz patented his Benz Patent-Motorwagen.'),
 KeyDevelopment(year=1901, description='The Oldsmobile Curved Dash became the first mass-produced car.', evidence='The 1901 Oldsmobile Curved Dash and the 1908 Ford Model T, both American cars, are widely considered the first mass-produced[3][4] and mass-affordable[5][6][7] cars, respectively.'),
 KeyDevelopment(year=1908, description='The Ford Model T became the first mass-affordable car.', evidence='The 1901 Oldsmobile Curved Dash and the 1908 Ford Model T, both American cars, are widely considered the first mass-produced[3][4] and mass-affordable[5][6][7] cars, respectively.'),
 KeyDevelopment(year=1885, description='Carl Benz built the original Benz Patent-Motorwagen, the first modern car.', evidence='The original Benz Patent-Motorwagen, the first modern car, built in 1885 and awarded the patent for the concept'),
 KeyDevelopment(year=1881, description='Gustave Trouvé demonstrated a three-wheeled car powered by electricity.', evidence='In November 1881, French inventor Gustave Trouvé demonstrated a three-wheeled car powered by electricity at the International Exposition of Electricity.'),
 KeyDevelopment(year=1888, description="Bertha Benz undertook the first road trip by car to prove the road-worthiness of her husband's invention.", evidence="In August 1888, Bertha Benz, the wife and business partner of Carl Benz, undertook the first road trip by car, to prove the road-worthiness of her husband's invention."),
 KeyDevelopment(year=1896, description='Benz designed and patented the first internal-combustion flat engine, called boxermotor.', evidence='In 1896, Benz designed and patented the first internal-combustion flat engine, called boxermotor.'),
 KeyDevelopment(year=1897, description='The first motor car in central Europe and one of the first factory-made cars in the world was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra), the Präsident automobil.', evidence='The first motor car in central Europe and one of the first factory-made cars in the world, was produced by Czech company Nesselsdorfer Wagenbau (later renamed to Tatra) in 1897, the Präsident automobil.')]


RAG based approach
​


Another simple idea is to chunk up the text, but instead of extracting information from every chunk, just focus on the the most relevant chunks.


caution
It can be difficult to identify which chunks are relevant.
For example, in the 
car
 article we're using here, most of the article contains key development information. So by using

RAG
, we'll likely be throwing out a lot of relevant information.
We suggest experimenting with your use case and determining whether this approach works or not.


To implement the RAG based approach:




Chunk up your document(s) and index them (e.g., in a vectorstore);


Prepend the 
extractor
 chain with a retrieval step using the vectorstore.




Here's a simple example that relies on the 
FAISS
 vectorstore.


from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_core
.
documents 
import
 Document
from
 langchain_core
.
runnables 
import
 RunnableLambda
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 CharacterTextSplitter
texts 
=
 text_splitter
.
split_text
(
document
.
page_content
)
vectorstore 
=
 FAISS
.
from_texts
(
texts
,
 embedding
=
OpenAIEmbeddings
(
)
)
retriever 
=
 vectorstore
.
as_retriever
(
    search_kwargs
=
{
"k"
:
 
1
}
)
  
# Only extract from first document
API Reference:
FAISS
 | 
Document
 | 
RunnableLambda
 | 
OpenAIEmbeddings
 | 
CharacterTextSplitter


In this case the RAG extractor is only looking at the top document.


rag_extractor 
=
 
{
    
"text"
:
 retriever 
|
 
(
lambda
 docs
:
 docs
[
0
]
.
page_content
)
  
# fetch content of top doc
}
 
|
 extractor


results 
=
 rag_extractor
.
invoke
(
"Key developments associated with cars"
)


for
 key_development 
in
 results
.
key_developments
:
    
print
(
key_development
)


year=2006 description='Car-sharing services in the US experienced double-digit growth in revenue and membership.' evidence='in the US, some car-sharing services have experienced double-digit growth in revenue and membership growth between 2006 and 2007.'
year=2020 description='56 million cars were manufactured worldwide, with China producing the most.' evidence='In 2020, there were 56 million cars manufactured worldwide, down from 67 million the previous year. The automotive industry in China produces by far the most (20 million in 2020).'


Common issues
​


Different methods have their own pros and cons related to cost, speed, and accuracy.


Watch out for these issues:




Chunking content means that the LLM can fail to extract information if the information is spread across multiple chunks.


Large chunk overlap may cause the same information to be extracted twice, so be prepared to de-duplicate!


LLMs can make up data. If looking for a single fact across a large text and using a brute force approach, you may end up getting more made up data.


Edit this page
Was this page helpful?
Previous
How to use reference examples when doing extraction
Next
How to use prompting alone (no tool calling) to do extraction
Setup
Define the schema
Create an extractor
Brute force approach
Merge results
RAG based approach
Common issues


--- docs_how_to_extraction_parse.txt ---

How-to guides
How to use prompting alone (no tool calling) to do extraction
On this page
How to use prompting alone (no tool calling) to do extraction


Tool calling
 features are not required for generating structured output from LLMs. LLMs that are able to follow prompt instructions well can be tasked with outputting information in a given format.


This approach relies on designing good prompts and then parsing the output of the LLMs to make them extract information well.


To extract data without tool-calling features:




Instruct the LLM to generate text following an expected format (e.g., JSON with a certain schema);


Use 
output parsers
 to structure the model response into a desired Python object.




First we select a LLM:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


tip
This tutorial is meant to be simple, but generally should really include reference examples to squeeze out performance!


Using PydanticOutputParser
​


The following example uses the built-in 
PydanticOutputParser
 to parse the output of a chat model.


from
 typing 
import
 List
,
 Optional
from
 langchain_core
.
output_parsers 
import
 PydanticOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 pydantic 
import
 BaseModel
,
 Field
,
 validator
class
 
Person
(
BaseModel
)
:
    
"""Information about a person."""
    name
:
 
str
 
=
 Field
(
.
.
.
,
 description
=
"The name of the person"
)
    height_in_meters
:
 
float
 
=
 Field
(
        
.
.
.
,
 description
=
"The height of the person expressed in meters."
    
)
class
 
People
(
BaseModel
)
:
    
"""Identifying information about all people in a text."""
    people
:
 List
[
Person
]
# Set up a parser
parser 
=
 PydanticOutputParser
(
pydantic_object
=
People
)
# Prompt
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"Answer the user query. Wrap the output in `json` tags\n{format_instructions}"
,
        
)
,
        
(
"human"
,
 
"{query}"
)
,
    
]
)
.
partial
(
format_instructions
=
parser
.
get_format_instructions
(
)
)
API Reference:
PydanticOutputParser
 | 
ChatPromptTemplate


Let's take a look at what information is sent to the model


query 
=
 
"Anna is 23 years old and she is 6 feet tall"


print
(
prompt
.
format_prompt
(
query
=
query
)
.
to_string
(
)
)


System: Answer the user query. Wrap the output in `json` tags
The output should be formatted as a JSON instance that conforms to the JSON schema below.
As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.
Here is the output schema:
\`\`\`
{"$defs": {"Person": {"description": "Information about a person.", "properties": {"name": {"description": "The name of the person", "title": "Name", "type": "string"}, "height_in_meters": {"description": "The height of the person expressed in meters.", "title": "Height In Meters", "type": "number"}}, "required": ["name", "height_in_meters"], "title": "Person", "type": "object"}}, "description": "Identifying information about all people in a text.", "properties": {"people": {"items": {"$ref": "#/$defs/Person"}, "title": "People", "type": "array"}}, "required": ["people"]}
\`\`\`
Human: Anna is 23 years old and she is 6 feet tall


Having defined our prompt, we simply chain together the prompt, model and output parser:


chain 
=
 prompt 
|
 model 
|
 parser
chain
.
invoke
(
{
"query"
:
 query
}
)


People(people=[Person(name='Anna', height_in_meters=1.83)])


Check out the associated 
Langsmith trace
.


Note that the schema shows up in two places:




In the prompt, via 
parser.get_format_instructions()
;


In the chain, to receive the formatted output and structure it into a Python object (in this case, the Pydantic object 
People
).




Custom Parsing
​


If desired, it's easy to create a custom prompt and parser with 
LangChain
 and 
LCEL
.


To create a custom parser, define a function to parse the output from the model (typically an 
AIMessage
) into an object of your choice.


See below for a simple implementation of a JSON parser.


import
 json
import
 re
from
 typing 
import
 List
,
 Optional
from
 langchain_anthropic
.
chat_models 
import
 ChatAnthropic
from
 langchain_core
.
messages 
import
 AIMessage
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 pydantic 
import
 BaseModel
,
 Field
,
 validator
class
 
Person
(
BaseModel
)
:
    
"""Information about a person."""
    name
:
 
str
 
=
 Field
(
.
.
.
,
 description
=
"The name of the person"
)
    height_in_meters
:
 
float
 
=
 Field
(
        
.
.
.
,
 description
=
"The height of the person expressed in meters."
    
)
class
 
People
(
BaseModel
)
:
    
"""Identifying information about all people in a text."""
    people
:
 List
[
Person
]
# Prompt
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"Answer the user query. Output your answer as JSON that  "
            
"matches the given schema: \`\`\`json\n{schema}\n\`\`\`. "
            
"Make sure to wrap the answer in \`\`\`json and \`\`\` tags"
,
        
)
,
        
(
"human"
,
 
"{query}"
)
,
    
]
)
.
partial
(
schema
=
People
.
schema
(
)
)
# Custom parser
def
 
extract_json
(
message
:
 AIMessage
)
 
-
>
 List
[
dict
]
:
    
"""Extracts JSON content from a string where JSON is embedded between \`\`\`json and \`\`\` tags.
    Parameters:
        text (str): The text containing the JSON content.
    Returns:
        list: A list of extracted JSON strings.
    """
    text 
=
 message
.
content
    
# Define the regular expression pattern to match JSON blocks
    pattern 
=
 
r"\`\`\`json(.*?)\`\`\`"
    
# Find all non-overlapping matches of the pattern in the string
    matches 
=
 re
.
findall
(
pattern
,
 text
,
 re
.
DOTALL
)
    
# Return the list of matched JSON strings, stripping any leading or trailing whitespace
    
try
:
        
return
 
[
json
.
loads
(
match
.
strip
(
)
)
 
for
 
match
 
in
 matches
]
    
except
 Exception
:
        
raise
 ValueError
(
f"Failed to parse: 
{
message
}
"
)
API Reference:
ChatAnthropic
 | 
AIMessage
 | 
ChatPromptTemplate


query 
=
 
"Anna is 23 years old and she is 6 feet tall"
print
(
prompt
.
format_prompt
(
query
=
query
)
.
to_string
(
)
)


System: Answer the user query. Output your answer as JSON that  matches the given schema: \`\`\`json
{'$defs': {'Person': {'description': 'Information about a person.', 'properties': {'name': {'description': 'The name of the person', 'title': 'Name', 'type': 'string'}, 'height_in_meters': {'description': 'The height of the person expressed in meters.', 'title': 'Height In Meters', 'type': 'number'}}, 'required': ['name', 'height_in_meters'], 'title': 'Person', 'type': 'object'}}, 'description': 'Identifying information about all people in a text.', 'properties': {'people': {'items': {'$ref': '#/$defs/Person'}, 'title': 'People', 'type': 'array'}}, 'required': ['people'], 'title': 'People', 'type': 'object'}
\`\`\`. Make sure to wrap the answer in \`\`\`json and \`\`\` tags
Human: Anna is 23 years old and she is 6 feet tall


chain 
=
 prompt 
|
 model 
|
 extract_json
chain
.
invoke
(
{
"query"
:
 query
}
)


/Users/bagatur/langchain/.venv/lib/python3.11/site-packages/pydantic/_internal/_fields.py:201: UserWarning: Field name "schema" in "PromptInput" shadows an attribute in parent "BaseModel"
  warnings.warn(


[{'people': [{'name': 'Anna', 'height_in_meters': 1.83}]}]


Other Libraries
​


If you're looking at extracting using a parsing approach, check out the 
Kor
 library. It's written by one of the 
LangChain
 maintainers and it
helps to craft a prompt that takes examples into account, allows controlling formats (e.g., JSON or CSV) and expresses the schema in TypeScript. It seems to work pretty!
Edit this page
Was this page helpful?
Previous
How to handle long text when doing extraction
Next
How to add fallbacks to a runnable
Using PydanticOutputParser
Custom Parsing
Other Libraries


--- docs_how_to_fallbacks.txt ---

How-to guides
How to add fallbacks to a runnable
On this page
How to add fallbacks to a runnable


When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That's why we've introduced the concept of fallbacks.


A 
fallback
 is an alternative plan that may be used in an emergency.


Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don't just want to send the same prompt to Anthropic - you probably want to use a different prompt template and send a different version there.


Fallback for LLM API Errors
​


This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things.


IMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing.


%
pip install 
-
-
upgrade 
-
-
quiet  langchain langchain
-
openai


from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_openai 
import
 ChatOpenAI
API Reference:
ChatAnthropic
 | 
ChatOpenAI


First, let's mock out what happens if we hit a RateLimitError from OpenAI


from
 unittest
.
mock 
import
 patch
import
 httpx
from
 openai 
import
 RateLimitError
request 
=
 httpx
.
Request
(
"GET"
,
 
"/"
)
response 
=
 httpx
.
Response
(
200
,
 request
=
request
)
error 
=
 RateLimitError
(
"rate limit"
,
 response
=
response
,
 body
=
""
)


# Note that we set max_retries = 0 to avoid retrying on RateLimits, etc
openai_llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 max_retries
=
0
)
anthropic_llm 
=
 ChatAnthropic
(
model
=
"claude-3-haiku-20240307"
)
llm 
=
 openai_llm
.
with_fallbacks
(
[
anthropic_llm
]
)


# Let's use just the OpenAI LLm first, to show that we run into an error
with
 patch
(
"openai.resources.chat.completions.Completions.create"
,
 side_effect
=
error
)
:
    
try
:
        
print
(
openai_llm
.
invoke
(
"Why did the chicken cross the road?"
)
)
    
except
 RateLimitError
:
        
print
(
"Hit error"
)


Hit error


# Now let's try with fallbacks to Anthropic
with
 patch
(
"openai.resources.chat.completions.Completions.create"
,
 side_effect
=
error
)
:
    
try
:
        
print
(
llm
.
invoke
(
"Why did the chicken cross the road?"
)
)
    
except
 RateLimitError
:
        
print
(
"Hit error"
)


content=' I don\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\n\n- To get to the other side!\n\n- It was too chicken to just stand there. \n\n- It wanted a change of scenery.\n\n- It wanted to show the possum it could be done.\n\n- It was on its way to a poultry farmers\' convention.\n\nThe joke plays on the double meaning of "the other side" - literally crossing the road to the other side, or the "other side" meaning the afterlife. So it\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=False


We can use our "LLM with Fallbacks" as we would a normal LLM.


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"You're a nice assistant who always includes a compliment in your response"
,
        
)
,
        
(
"human"
,
 
"Why did the {animal} cross the road"
)
,
    
]
)
chain 
=
 prompt 
|
 llm
with
 patch
(
"openai.resources.chat.completions.Completions.create"
,
 side_effect
=
error
)
:
    
try
:
        
print
(
chain
.
invoke
(
{
"animal"
:
 
"kangaroo"
}
)
)
    
except
 RateLimitError
:
        
print
(
"Hit error"
)
API Reference:
ChatPromptTemplate


content=" I don't actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\n\n- To get to the other side (the classic joke answer!)\n\n- It was trying to find some food or water \n\n- It was trying to find a mate during mating season\n\n- It was fleeing from a predator or perceived threat\n\n- It was disoriented and crossed accidentally \n\n- It was following a herd of other kangaroos who were crossing\n\n- It wanted a change of scenery or environment \n\n- It was trying to reach a new habitat or territory\n\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher." additional_kwargs={} example=False


Fallback for Sequences
​


We can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt.


# First let's create a chain with a ChatModel
# We add in a string output parser here so the outputs between the two are the same type
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
chat_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"You're a nice assistant who always includes a compliment in your response"
,
        
)
,
        
(
"human"
,
 
"Why did the {animal} cross the road"
)
,
    
]
)
# Here we're going to use a bad model name to easily create a chain that will error
chat_model 
=
 ChatOpenAI
(
model
=
"gpt-fake"
)
bad_chain 
=
 chat_prompt 
|
 chat_model 
|
 StrOutputParser
(
)
API Reference:
StrOutputParser


# Now lets create a chain with the normal OpenAI model
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_openai 
import
 OpenAI
prompt_template 
=
 
"""Instructions: You should always include a compliment in your response.
Question: Why did the {animal} cross the road?"""
prompt 
=
 PromptTemplate
.
from_template
(
prompt_template
)
llm 
=
 OpenAI
(
)
good_chain 
=
 prompt 
|
 llm
API Reference:
PromptTemplate
 | 
OpenAI


# We can now create a final chain which combines the two
chain 
=
 bad_chain
.
with_fallbacks
(
[
good_chain
]
)
chain
.
invoke
(
{
"animal"
:
 
"turtle"
}
)


'\n\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.'


Fallback for Long Inputs
​


One of the big limiting factors of LLMs is their context window. Usually, you can count and track the length of prompts before sending them to an LLM, but in situations where that is hard/complicated, you can fallback to a model with a longer context length.


short_llm 
=
 ChatOpenAI
(
)
long_llm 
=
 ChatOpenAI
(
model
=
"gpt-3.5-turbo-16k"
)
llm 
=
 short_llm
.
with_fallbacks
(
[
long_llm
]
)


inputs 
=
 
"What is the next number: "
 
+
 
", "
.
join
(
[
"one"
,
 
"two"
]
 
*
 
3000
)


try
:
    
print
(
short_llm
.
invoke
(
inputs
)
)
except
 Exception 
as
 e
:
    
print
(
e
)


This model's maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages.


try
:
    
print
(
llm
.
invoke
(
inputs
)
)
except
 Exception 
as
 e
:
    
print
(
e
)


content='The next number in the sequence is two.' additional_kwargs={} example=False


Fallback to Better Model
​


Often times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle. This naturally points to fallbacks - we can try with GPT-3.5 (faster, cheaper), but then if parsing fails we can use GPT-4.


from
 langchain
.
output_parsers 
import
 DatetimeOutputParser
API Reference:
DatetimeOutputParser


prompt 
=
 ChatPromptTemplate
.
from_template
(
    
"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)"
)


# In this case we are going to do the fallbacks on the LLM + output parser level
# Because the error will get raised in the OutputParser
openai_35 
=
 ChatOpenAI
(
)
 
|
 DatetimeOutputParser
(
)
openai_4 
=
 ChatOpenAI
(
model
=
"gpt-4"
)
 
|
 DatetimeOutputParser
(
)


only_35 
=
 prompt 
|
 openai_35
fallback_4 
=
 prompt 
|
 openai_35
.
with_fallbacks
(
[
openai_4
]
)


try
:
    
print
(
only_35
.
invoke
(
{
"event"
:
 
"the superbowl in 1994"
}
)
)
except
 Exception 
as
 e
:
    
print
(
f"Error: 
{
e
}
"
)


Error: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z


try
:
    
print
(
fallback_4
.
invoke
(
{
"event"
:
 
"the superbowl in 1994"
}
)
)
except
 Exception 
as
 e
:
    
print
(
f"Error: 
{
e
}
"
)


1994-01-30 15:30:00
Edit this page
Was this page helpful?
Previous
How to use prompting alone (no tool calling) to do extraction
Next
How to filter messages
Fallback for LLM API Errors
Fallback for Sequences
Fallback for Long Inputs
Fallback to Better Model


--- docs_how_to_few_shot_examples.txt ---

How-to guides
How to use few shot examples
On this page
How to use few shot examples


Prerequisites
This guide assumes familiarity with the following concepts:


Prompt templates


Example selectors


LLMs


Vectorstores




In this guide, we'll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called 
few-shotting
, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.


A few-shot prompt template can be constructed from either a set of examples, or from an 
Example Selector
 class responsible for choosing a subset of examples from the defined set.


This guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see 
here
.


Create a formatter for the few-shot examples
​


Configure a formatter that will format the few-shot examples into a string. This formatter should be a 
PromptTemplate
 object.


from
 langchain_core
.
prompts 
import
 PromptTemplate
example_prompt 
=
 PromptTemplate
.
from_template
(
"Question: {question}\n{answer}"
)
API Reference:
PromptTemplate


Creating the example set
​


Next, we'll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.


examples 
=
 
[
    
{
        
"question"
:
 
"Who lived longer, Muhammad Ali or Alan Turing?"
,
        
"answer"
:
 
"""
Are follow up questions needed here: Yes.
Follow up: How old was Muhammad Ali when he died?
Intermediate answer: Muhammad Ali was 74 years old when he died.
Follow up: How old was Alan Turing when he died?
Intermediate answer: Alan Turing was 41 years old when he died.
So the final answer is: Muhammad Ali
"""
,
    
}
,
    
{
        
"question"
:
 
"When was the founder of craigslist born?"
,
        
"answer"
:
 
"""
Are follow up questions needed here: Yes.
Follow up: Who was the founder of craigslist?
Intermediate answer: Craigslist was founded by Craig Newmark.
Follow up: When was Craig Newmark born?
Intermediate answer: Craig Newmark was born on December 6, 1952.
So the final answer is: December 6, 1952
"""
,
    
}
,
    
{
        
"question"
:
 
"Who was the maternal grandfather of George Washington?"
,
        
"answer"
:
 
"""
Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball
"""
,
    
}
,
    
{
        
"question"
:
 
"Are both the directors of Jaws and Casino Royale from the same country?"
,
        
"answer"
:
 
"""
Are follow up questions needed here: Yes.
Follow up: Who is the director of Jaws?
Intermediate Answer: The director of Jaws is Steven Spielberg.
Follow up: Where is Steven Spielberg from?
Intermediate Answer: The United States.
Follow up: Who is the director of Casino Royale?
Intermediate Answer: The director of Casino Royale is Martin Campbell.
Follow up: Where is Martin Campbell from?
Intermediate Answer: New Zealand.
So the final answer is: No
"""
,
    
}
,
]


Let's test the formatting prompt with one of our examples:


print
(
example_prompt
.
invoke
(
examples
[
0
]
)
.
to_string
(
)
)


Question: Who lived longer, Muhammad Ali or Alan Turing?
Are follow up questions needed here: Yes.
Follow up: How old was Muhammad Ali when he died?
Intermediate answer: Muhammad Ali was 74 years old when he died.
Follow up: How old was Alan Turing when he died?
Intermediate answer: Alan Turing was 41 years old when he died.
So the final answer is: Muhammad Ali


Pass the examples and formatter to 
FewShotPromptTemplate
​


Finally, create a 
FewShotPromptTemplate
 object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this 
FewShotPromptTemplate
 is formatted, it formats the passed examples using the 
example_prompt
, then and adds them to the final prompt before 
suffix
:


from
 langchain_core
.
prompts 
import
 FewShotPromptTemplate
prompt 
=
 FewShotPromptTemplate
(
    examples
=
examples
,
    example_prompt
=
example_prompt
,
    suffix
=
"Question: {input}"
,
    input_variables
=
[
"input"
]
,
)
print
(
    prompt
.
invoke
(
{
"input"
:
 
"Who was the father of Mary Ball Washington?"
}
)
.
to_string
(
)
)
API Reference:
FewShotPromptTemplate


Question: Who lived longer, Muhammad Ali or Alan Turing?
Are follow up questions needed here: Yes.
Follow up: How old was Muhammad Ali when he died?
Intermediate answer: Muhammad Ali was 74 years old when he died.
Follow up: How old was Alan Turing when he died?
Intermediate answer: Alan Turing was 41 years old when he died.
So the final answer is: Muhammad Ali
Question: When was the founder of craigslist born?
Are follow up questions needed here: Yes.
Follow up: Who was the founder of craigslist?
Intermediate answer: Craigslist was founded by Craig Newmark.
Follow up: When was Craig Newmark born?
Intermediate answer: Craig Newmark was born on December 6, 1952.
So the final answer is: December 6, 1952
Question: Who was the maternal grandfather of George Washington?
Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball
Question: Are both the directors of Jaws and Casino Royale from the same country?
Are follow up questions needed here: Yes.
Follow up: Who is the director of Jaws?
Intermediate Answer: The director of Jaws is Steven Spielberg.
Follow up: Where is Steven Spielberg from?
Intermediate Answer: The United States.
Follow up: Who is the director of Casino Royale?
Intermediate Answer: The director of Casino Royale is Martin Campbell.
Follow up: Where is Martin Campbell from?
Intermediate Answer: New Zealand.
So the final answer is: No
Question: Who was the father of Mary Ball Washington?


By providing the model with examples like this, we can guide the model to a better response.


Using an example selector
​


We will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the 
FewShotPromptTemplate
 object, we will feed them into an implementation of 
ExampleSelector
 called 
SemanticSimilarityExampleSelector
 instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.


To show what it looks like, let's initialize an instance and call it in isolation:


from
 langchain_chroma 
import
 Chroma
from
 langchain_core
.
example_selectors 
import
 SemanticSimilarityExampleSelector
from
 langchain_openai 
import
 OpenAIEmbeddings
example_selector 
=
 SemanticSimilarityExampleSelector
.
from_examples
(
    
# This is the list of examples available to select from.
    examples
,
    
# This is the embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings
(
)
,
    
# This is the VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma
,
    
# This is the number of examples to produce.
    k
=
1
,
)
# Select the most similar example to the input.
question 
=
 
"Who was the father of Mary Ball Washington?"
selected_examples 
=
 example_selector
.
select_examples
(
{
"question"
:
 question
}
)
print
(
f"Examples most similar to the input: 
{
question
}
"
)
for
 example 
in
 selected_examples
:
    
print
(
"\n"
)
    
for
 k
,
 v 
in
 example
.
items
(
)
:
        
print
(
f"
{
k
}
: 
{
v
}
"
)
API Reference:
SemanticSimilarityExampleSelector
 | 
OpenAIEmbeddings


Examples most similar to the input: Who was the father of Mary Ball Washington?
answer: 
Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball
question: Who was the maternal grandfather of George Washington?


Now, let's create a 
FewShotPromptTemplate
 object. This object takes in the example selector and the formatter prompt for the few-shot examples.


prompt 
=
 FewShotPromptTemplate
(
    example_selector
=
example_selector
,
    example_prompt
=
example_prompt
,
    suffix
=
"Question: {input}"
,
    input_variables
=
[
"input"
]
,
)
print
(
    prompt
.
invoke
(
{
"input"
:
 
"Who was the father of Mary Ball Washington?"
}
)
.
to_string
(
)
)


Question: Who was the maternal grandfather of George Washington?
Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball
Question: Who was the father of Mary Ball Washington?


Next steps
​


You've now learned how to add few-shot examples to your prompts.


Next, check out the other how-to guides on prompt templates in this section, the related how-to guide on 
few shotting with chat models
, or the other 
example selector how-to guides
.
Edit this page
Was this page helpful?
Previous
How to add examples to the prompt for query analysis
Next
How to run custom functions
Create a formatter for the few-shot examples
Creating the example set
Pass the examples and formatter to 
FewShotPromptTemplate
Using an example selector
Next steps


--- docs_how_to_few_shot_examples_chat.txt ---

How-to guides
How to use few shot examples in chat models
On this page
How to use few shot examples in chat models


Prerequisites
This guide assumes familiarity with the following concepts:


Prompt templates


Example selectors


Chat models


Vectorstores




This guide covers how to prompt a chat model with example inputs and outputs. Providing the model with a few such examples is called 
few-shotting
, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.


There does not appear to be solid consensus on how best to do few-shot prompting, and the optimal prompt compilation will likely vary by model. Because of this, we provide few-shot prompt templates like the 
FewShotChatMessagePromptTemplate
 as a flexible starting point, and you can modify or replace them as you see fit.


The goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model.


Note:
 The following code examples are for chat models only, since 
FewShotChatMessagePromptTemplates
 are designed to output formatted 
chat messages
 rather than pure strings. For similar few-shot prompt examples for pure string templates compatible with completion models (LLMs), see the 
few-shot prompt templates
 guide.


Fixed Examples
​


The most basic (and common) few-shot prompting technique is to use fixed prompt examples. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production.


The basic components of the template are:




examples
: A list of dictionary examples to include in the final prompt.


example_prompt
: converts each example into 1 or more messages through its 
format_messages
 method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.




Below is a simple demonstration. First, define the examples you'd like to include. Let's give the LLM an unfamiliar mathematical operator, denoted by the "🦜" emoji:


%
pip install 
-
qU langchain langchain
-
openai langchain
-
chroma
import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


If we try to ask the model what the result of this expression is, it will fail:


from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0.0
)
model
.
invoke
(
"What is 2 🦜 9?"
)
API Reference:
ChatOpenAI


AIMessage(content='The expression "2 🦜 9" is not a standard mathematical operation or equation. It appears to be a combination of the number 2 and the parrot emoji 🦜 followed by the number 9. It does not have a specific mathematical meaning.', response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 17, 'total_tokens': 71}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aad12dda-5c47-4a1e-9949-6fe94e03242a-0', usage_metadata={'input_tokens': 17, 'output_tokens': 54, 'total_tokens': 71})


Now let's see what happens if we give the LLM some examples to work with. We'll define some below:


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 FewShotChatMessagePromptTemplate
examples 
=
 
[
    
{
"input"
:
 
"2 🦜 2"
,
 
"output"
:
 
"4"
}
,
    
{
"input"
:
 
"2 🦜 3"
,
 
"output"
:
 
"5"
}
,
]
API Reference:
ChatPromptTemplate
 | 
FewShotChatMessagePromptTemplate


Next, assemble them into the few-shot prompt template.


# This is a prompt template used to format each individual example.
example_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"human"
,
 
"{input}"
)
,
        
(
"ai"
,
 
"{output}"
)
,
    
]
)
few_shot_prompt 
=
 FewShotChatMessagePromptTemplate
(
    example_prompt
=
example_prompt
,
    examples
=
examples
,
)
print
(
few_shot_prompt
.
invoke
(
{
}
)
.
to_messages
(
)
)


[HumanMessage(content='2 🦜 2'), AIMessage(content='4'), HumanMessage(content='2 🦜 3'), AIMessage(content='5')]


Finally, we assemble the final prompt as shown below, passing 
few_shot_prompt
 directly into the 
from_messages
 factory method, and use it with a model:


final_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a wondrous wizard of math."
)
,
        few_shot_prompt
,
        
(
"human"
,
 
"{input}"
)
,
    
]
)


And now let's ask the model the initial question and see how it does:


from
 langchain_openai 
import
 ChatOpenAI
chain 
=
 final_prompt 
|
 model
chain
.
invoke
(
{
"input"
:
 
"What is 2 🦜 9?"
}
)
API Reference:
ChatOpenAI


AIMessage(content='11', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 60, 'total_tokens': 61}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5ec4e051-262f-408e-ad00-3f2ebeb561c3-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1, 'total_tokens': 61})


And we can see that the model has now inferred that the parrot emoji means addition from the given few-shot examples!


Dynamic few-shot prompting
​


Sometimes you may want to select only a few examples from your overall set to show based on the input. For this, you can replace the 
examples
 passed into 
FewShotChatMessagePromptTemplate
 with an 
example_selector
. The other components remain the same as above! Our dynamic few-shot prompt template would look like:




example_selector
: responsible for selecting few-shot examples (and the order in which they are returned) for a given input. These implement the 
BaseExampleSelector
 interface. A common example is the vectorstore-backed 
SemanticSimilarityExampleSelector


example_prompt
: convert each example into 1 or more messages through its 
format_messages
 method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message.




These once again can be composed with other messages and chat templates to assemble your final prompt.


Let's walk through an example with the 
SemanticSimilarityExampleSelector
. Since this implementation uses a vectorstore to select examples based on semantic similarity, we will want to first populate the store. Since the basic idea here is that we want to search for and return examples most similar to the text input, we embed the 
values
 of our prompt examples rather than considering the keys:


from
 langchain_chroma 
import
 Chroma
from
 langchain_core
.
example_selectors 
import
 SemanticSimilarityExampleSelector
from
 langchain_openai 
import
 OpenAIEmbeddings
examples 
=
 
[
    
{
"input"
:
 
"2 🦜 2"
,
 
"output"
:
 
"4"
}
,
    
{
"input"
:
 
"2 🦜 3"
,
 
"output"
:
 
"5"
}
,
    
{
"input"
:
 
"2 🦜 4"
,
 
"output"
:
 
"6"
}
,
    
{
"input"
:
 
"What did the cow say to the moon?"
,
 
"output"
:
 
"nothing at all"
}
,
    
{
        
"input"
:
 
"Write me a poem about the moon"
,
        
"output"
:
 
"One for the moon, and one for me, who are we to talk about the moon?"
,
    
}
,
]
to_vectorize 
=
 
[
" "
.
join
(
example
.
values
(
)
)
 
for
 example 
in
 examples
]
embeddings 
=
 OpenAIEmbeddings
(
)
vectorstore 
=
 Chroma
.
from_texts
(
to_vectorize
,
 embeddings
,
 metadatas
=
examples
)
API Reference:
SemanticSimilarityExampleSelector
 | 
OpenAIEmbeddings


Create the 
example_selector
​


With a vectorstore created, we can create the 
example_selector
. Here we will call it in isolation, and set 
k
 on it to only fetch the two example closest to the input.


example_selector 
=
 SemanticSimilarityExampleSelector
(
    vectorstore
=
vectorstore
,
    k
=
2
,
)
# The prompt template will load examples by passing the input do the `select_examples` method
example_selector
.
select_examples
(
{
"input"
:
 
"horse"
}
)


[{'input': 'What did the cow say to the moon?', 'output': 'nothing at all'},
 {'input': '2 🦜 4', 'output': '6'}]


Create prompt template
​


We now assemble the prompt template, using the 
example_selector
 created above.


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 FewShotChatMessagePromptTemplate
# Define the few-shot prompt.
few_shot_prompt 
=
 FewShotChatMessagePromptTemplate
(
    
# The input variables select the values to pass to the example_selector
    input_variables
=
[
"input"
]
,
    example_selector
=
example_selector
,
    
# Define how each example will be formatted.
    
# In this case, each example will become 2 messages:
    
# 1 human, and 1 AI
    example_prompt
=
ChatPromptTemplate
.
from_messages
(
        
[
(
"human"
,
 
"{input}"
)
,
 
(
"ai"
,
 
"{output}"
)
]
    
)
,
)
print
(
few_shot_prompt
.
invoke
(
input
=
"What's 3 🦜 3?"
)
.
to_messages
(
)
)
API Reference:
ChatPromptTemplate
 | 
FewShotChatMessagePromptTemplate


[HumanMessage(content='2 🦜 3'), AIMessage(content='5'), HumanMessage(content='2 🦜 4'), AIMessage(content='6')]


And we can pass this few-shot chat message prompt template into another chat prompt template:


final_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a wondrous wizard of math."
)
,
        few_shot_prompt
,
        
(
"human"
,
 
"{input}"
)
,
    
]
)
print
(
few_shot_prompt
.
invoke
(
input
=
"What's 3 🦜 3?"
)
)


messages=[HumanMessage(content='2 🦜 3'), AIMessage(content='5'), HumanMessage(content='2 🦜 4'), AIMessage(content='6')]


Use with an chat model
​


Finally, you can connect your model to the few-shot prompt.


chain 
=
 final_prompt 
|
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0.0
)
chain
.
invoke
(
{
"input"
:
 
"What's 3 🦜 3?"
}
)


AIMessage(content='6', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 60, 'total_tokens': 61}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d1863e5e-17cd-4e9d-bf7a-b9f118747a65-0', usage_metadata={'input_tokens': 60, 'output_tokens': 1, 'total_tokens': 61})


Next steps
​


You've now learned how to add few-shot examples to your chat prompts.


Next, check out the other how-to guides on prompt templates in this section, the related how-to guide on 
few shotting with text completion models
, or the other 
example selector how-to guides
.
Edit this page
Was this page helpful?
Previous
How to add retrieval to chatbots
Next
How to do tool/function calling
Fixed Examples
Dynamic few-shot prompting
Create the 
example_selector
Create prompt template
Use with an chat model
Next steps


--- docs_how_to_filter_messages.txt ---

How-to guides
How to filter messages
On this page
How to filter messages


In more complex chains and agents we might track state with a list of 
messages
. This list can start to accumulate messages from multiple different models, speakers, sub-chains, etc., and we may only want to pass subsets of this full list of messages to each model call in the chain/agent.


The 
filter_messages
 utility makes it easy to filter messages by type, id, or name.


Basic usage
​


from
 langchain_core
.
messages 
import
 
(
    AIMessage
,
    HumanMessage
,
    SystemMessage
,
    filter_messages
,
)
messages 
=
 
[
    SystemMessage
(
"you are a good assistant"
,
 
id
=
"1"
)
,
    HumanMessage
(
"example input"
,
 
id
=
"2"
,
 name
=
"example_user"
)
,
    AIMessage
(
"example output"
,
 
id
=
"3"
,
 name
=
"example_assistant"
)
,
    HumanMessage
(
"real input"
,
 
id
=
"4"
,
 name
=
"bob"
)
,
    AIMessage
(
"real output"
,
 
id
=
"5"
,
 name
=
"alice"
)
,
]
filter_messages
(
messages
,
 include_types
=
"human"
)
API Reference:
AIMessage
 | 
HumanMessage
 | 
SystemMessage
 | 
filter_messages


[HumanMessage(content='example input', name='example_user', id='2'),
 HumanMessage(content='real input', name='bob', id='4')]


filter_messages
(
messages
,
 exclude_names
=
[
"example_user"
,
 
"example_assistant"
]
)


[SystemMessage(content='you are a good assistant', id='1'),
 HumanMessage(content='real input', name='bob', id='4'),
 AIMessage(content='real output', name='alice', id='5')]


filter_messages
(
messages
,
 include_types
=
[
HumanMessage
,
 AIMessage
]
,
 exclude_ids
=
[
"3"
]
)


[HumanMessage(content='example input', name='example_user', id='2'),
 HumanMessage(content='real input', name='bob', id='4'),
 AIMessage(content='real output', name='alice', id='5')]


Chaining
​


filter_messages
 can be used in an imperatively (like above) or declaratively, making it easy to compose with other components in a chain:


%
pip install 
-
qU langchain
-
anthropic


from
 langchain_anthropic 
import
 ChatAnthropic
llm 
=
 ChatAnthropic
(
model
=
"claude-3-sonnet-20240229"
,
 temperature
=
0
)
# Notice we don't pass in messages. This creates
# a RunnableLambda that takes messages as input
filter_ 
=
 filter_messages
(
exclude_names
=
[
"example_user"
,
 
"example_assistant"
]
)
chain 
=
 filter_ 
|
 llm
chain
.
invoke
(
messages
)
API Reference:
ChatAnthropic


AIMessage(content=[], response_metadata={'id': 'msg_01Wz7gBHahAwkZ1KCBNtXmwA', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 16, 'output_tokens': 3}}, id='run-b5d8a3fe-004f-4502-a071-a6c025031827-0', usage_metadata={'input_tokens': 16, 'output_tokens': 3, 'total_tokens': 19})


Looking at the LangSmith trace we can see that before the messages are passed to the model they are filtered: 
https://smith.langchain.com/public/f808a724-e072-438e-9991-657cc9e7e253/r


Looking at just the filter_, we can see that it's a Runnable object that can be invoked like all Runnables:


filter_
.
invoke
(
messages
)


[HumanMessage(content='real input', name='bob', id='4'),
 AIMessage(content='real output', name='alice', id='5')]


API reference
​


For a complete description of all arguments head to the API reference: 
https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.filter_messages.html
Edit this page
Was this page helpful?
Previous
How to add fallbacks to a runnable
Next
Hybrid Search
Basic usage
Chaining
API reference


--- docs_how_to_function_calling.txt ---

How-to guides
How to do tool/function calling
On this page
How to do tool/function calling


info
We use the term tool calling interchangeably with function calling. Although
function calling is sometimes meant to refer to invocations of a single function,
we treat all models as though they can return multiple tool or function calls in
each message.


Tool calling allows a model to respond to a given prompt by generating output that
matches a user-defined schema. While the name implies that the model is performing
some action, this is actually not the case! The model is coming up with the
arguments to a tool, and actually running the tool (or not) is up to the user -
for example, if you want to 
extract output matching some schema

from unstructured text, you could give the model an "extraction" tool that takes
parameters matching the desired schema, then treat the generated output as your final
result.


A tool call includes a name, arguments dict, and an optional identifier. The
arguments dict is structured 
{argument_name: argument_value}
.


Many LLM providers, including 
Anthropic
,

Cohere
, 
Google
,

Mistral
, 
OpenAI
, and others,
support variants of a tool calling feature. These features typically allow requests
to the LLM to include available tools and their schemas, and for responses to include
calls to these tools. For instance, given a search engine tool, an LLM might handle a
query by first issuing a call to the search engine. The system calling the LLM can
receive the tool call, execute it, and return the output to the LLM to inform its
response. LangChain includes a suite of 
built-in tools

and supports several methods for defining your own 
custom tools
.
Tool-calling is extremely useful for building 
tool-using chains and agents
,
and for getting structured outputs from models more generally.


Providers adopt different conventions for formatting tool schemas and tool calls.
For instance, Anthropic returns tool calls as parsed structures within a larger content block:


[
  
{
    
"text"
:
 
"<thinking>\nI should use a tool.\n</thinking>"
,
    
"type"
:
 
"text"
  
}
,
  
{
    
"id"
:
 
"id_value"
,
    
"input"
:
 
{
"arg_name"
:
 
"arg_value"
}
,
    
"name"
:
 
"tool_name"
,
    
"type"
:
 
"tool_use"
  
}
]


whereas OpenAI separates tool calls into a distinct parameter, with arguments as JSON strings:


{
  
"tool_calls"
:
 
[
    
{
      
"id"
:
 
"id_value"
,
      
"function"
:
 
{
        
"arguments"
:
 
'{"arg_name": "arg_value"}'
,
        
"name"
:
 
"tool_name"
      
}
,
      
"type"
:
 
"function"
    
}
  
]
}


LangChain implements standard interfaces for defining tools, passing them to LLMs,
and representing tool calls.


Passing tools to LLMs
​


Chat models supporting tool calling features implement a 
.bind_tools
 method, which
receives a list of LangChain 
tool objects

and binds them to the chat model in its expected format. Subsequent invocations of the
chat model will include tool schemas in its calls to the LLM.


For example, we can define the schema for custom tools using the 
@tool
 decorator
on Python functions:


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
add
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Adds a and b."""
    
return
 a 
+
 b
@tool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiplies a and b."""
    
return
 a 
*
 b
tools 
=
 
[
add
,
 multiply
]
API Reference:
tool


Or below, we define the schema using Pydantic:


from
 pydantic 
import
 BaseModel
,
 Field
# Note that the docstrings here are crucial, as they will be passed along
# to the model along with the class name.
class
 
Add
(
BaseModel
)
:
    
"""Add two integers together."""
    a
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"First integer"
)
    b
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"Second integer"
)
class
 
Multiply
(
BaseModel
)
:
    
"""Multiply two integers together."""
    a
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"First integer"
)
    b
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"Second integer"
)
tools 
=
 
[
Add
,
 Multiply
]


We can bind them to chat models as follows:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


We can use the 
bind_tools()
 method to handle converting

Multiply
 to a "tool" and binding it to the model (i.e.,
passing it in each time the model is invoked).


llm_with_tools 
=
 llm
.
bind_tools
(
tools
)


Tool calls
​


If tool calls are included in a LLM response, they are attached to the corresponding

message

or 
message chunk

as a list of 
tool call

objects in the 
.tool_calls
 attribute. A 
ToolCall
 is a typed dict that includes a
tool name, dict of argument values, and (optionally) an identifier. Messages with no
tool calls default to an empty list for this attribute.


Example:


query 
=
 
"What is 3 * 12? Also, what is 11 + 49?"
llm_with_tools
.
invoke
(
query
)
.
tool_calls


[{'name': 'Multiply',
  'args': {'a': 3, 'b': 12},
  'id': 'call_1Tdp5wUXbYQzpkBoagGXqUTo'},
 {'name': 'Add',
  'args': {'a': 11, 'b': 49},
  'id': 'call_k9v09vYioS3X0Qg35zESuUKI'}]


The 
.tool_calls
 attribute should contain valid tool calls. Note that on occasion,
model providers may output malformed tool calls (e.g., arguments that are not
valid JSON). When parsing fails in these cases, instances
of 
InvalidToolCall

are populated in the 
.invalid_tool_calls
 attribute. An 
InvalidToolCall
 can have
a name, string arguments, identifier, and error message.


If desired, 
output parsers
 can further
process the output. For example, we can convert back to the original Pydantic class:


from
 langchain_core
.
output_parsers
.
openai_tools 
import
 PydanticToolsParser
chain 
=
 llm_with_tools 
|
 PydanticToolsParser
(
tools
=
[
Multiply
,
 Add
]
)
chain
.
invoke
(
query
)
API Reference:
PydanticToolsParser


[Multiply(a=3, b=12), Add(a=11, b=49)]


Streaming
​


When tools are called in a streaming context,

message chunks

will be populated with 
tool call chunk

objects in a list via the 
.tool_call_chunks
 attribute. A 
ToolCallChunk
 includes
optional string fields for the tool 
name
, 
args
, and 
id
, and includes an optional
integer field 
index
 that can be used to join chunks together. Fields are optional
because portions of a tool call may be streamed across different chunks (e.g., a chunk
that includes a substring of the arguments may have null values for the tool name and id).


Because message chunks inherit from their parent message class, an

AIMessageChunk

with tool call chunks will also include 
.tool_calls
 and 
.invalid_tool_calls
 fields.
These fields are parsed best-effort from the message's tool call chunks.


Note that not all providers currently support streaming for tool calls.


Example:


async
 
for
 chunk 
in
 llm_with_tools
.
astream
(
query
)
:
    
print
(
chunk
.
tool_call_chunks
)


[]
[{'name': 'Multiply', 'args': '', 'id': 'call_d39MsxKM5cmeGJOoYKdGBgzc', 'index': 0}]
[{'name': None, 'args': '{"a"', 'id': None, 'index': 0}]
[{'name': None, 'args': ': 3, ', 'id': None, 'index': 0}]
[{'name': None, 'args': '"b": 1', 'id': None, 'index': 0}]
[{'name': None, 'args': '2}', 'id': None, 'index': 0}]
[{'name': 'Add', 'args': '', 'id': 'call_QJpdxD9AehKbdXzMHxgDMMhs', 'index': 1}]
[{'name': None, 'args': '{"a"', 'id': None, 'index': 1}]
[{'name': None, 'args': ': 11,', 'id': None, 'index': 1}]
[{'name': None, 'args': ' "b": ', 'id': None, 'index': 1}]
[{'name': None, 'args': '49}', 'id': None, 'index': 1}]
[]


Note that adding message chunks will merge their corresponding tool call chunks. This is the principle by which LangChain's various 
tool output parsers
 support streaming.


For example, below we accumulate tool call chunks:


first 
=
 
True
async
 
for
 chunk 
in
 llm_with_tools
.
astream
(
query
)
:
    
if
 first
:
        gathered 
=
 chunk
        first 
=
 
False
    
else
:
        gathered 
=
 gathered 
+
 chunk
    
print
(
gathered
.
tool_call_chunks
)


[]
[{'name': 'Multiply', 'args': '', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a"', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, ', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 1', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{"a"', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{"a": 11,', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": ', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": 49}', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_erKtz8z3e681cmxYKbRof0NS', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": 49}', 'id': 'call_tYHYdEV2YBvzDcSCiFCExNvw', 'index': 1}]


print
(
type
(
gathered
.
tool_call_chunks
[
0
]
[
"args"
]
)
)


<class 'str'>


And below we accumulate tool calls to demonstrate partial parsing:


first 
=
 
True
async
 
for
 chunk 
in
 llm_with_tools
.
astream
(
query
)
:
    
if
 first
:
        gathered 
=
 chunk
        first 
=
 
False
    
else
:
        gathered 
=
 gathered 
+
 chunk
    
print
(
gathered
.
tool_calls
)


[]
[]
[{'name': 'Multiply', 'args': {}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]
[{'name': 'Multiply', 'args': {'a': 3}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 1}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_BXqUtt6jYCwR1DguqpS2ehP0'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_UjSHJKROSAw2BDc8cp9cSv4i'}]


print
(
type
(
gathered
.
tool_calls
[
0
]
[
"args"
]
)
)


<class 'dict'>


Passing tool outputs to model
​


If we're using the model-generated tool invocations to actually call tools and want to pass the tool results back to the model, we can do so using 
ToolMessage
s.


from
 langchain_core
.
messages 
import
 HumanMessage
,
 ToolMessage
messages 
=
 
[
HumanMessage
(
query
)
]
ai_msg 
=
 llm_with_tools
.
invoke
(
messages
)
messages
.
append
(
ai_msg
)
for
 tool_call 
in
 ai_msg
.
tool_calls
:
    selected_tool 
=
 
{
"add"
:
 add
,
 
"multiply"
:
 multiply
}
[
tool_call
[
"name"
]
.
lower
(
)
]
    tool_output 
=
 selected_tool
.
invoke
(
tool_call
[
"args"
]
)
    messages
.
append
(
ToolMessage
(
tool_output
,
 tool_call_id
=
tool_call
[
"id"
]
)
)
messages
API Reference:
HumanMessage
 | 
ToolMessage


[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?'),
 AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_K5DsWEmgt6D08EI9AFu9NaL1', 'function': {'arguments': '{"a": 3, "b": 12}', 'name': 'Multiply'}, 'type': 'function'}, {'id': 'call_qywVrsplg0ZMv7LHYYMjyG81', 'function': {'arguments': '{"a": 11, "b": 49}', 'name': 'Add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 105, 'total_tokens': 155}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1a0b8cdd-9221-4d94-b2ed-5701f67ce9fe-0', tool_calls=[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_K5DsWEmgt6D08EI9AFu9NaL1'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_qywVrsplg0ZMv7LHYYMjyG81'}]),
 ToolMessage(content='36', tool_call_id='call_K5DsWEmgt6D08EI9AFu9NaL1'),
 ToolMessage(content='60', tool_call_id='call_qywVrsplg0ZMv7LHYYMjyG81')]


llm_with_tools
.
invoke
(
messages
)


AIMessage(content='3 * 12 is 36 and 11 + 49 is 60.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 171, 'total_tokens': 189}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-a6c8093c-b16a-4c92-8308-7c9ac998118c-0')


Few-shot prompting
​


For more complex tool use it's very useful to add few-shot examples to the prompt. We can do this by adding 
AIMessage
s with 
ToolCall
s and corresponding 
ToolMessage
s to our prompt.


For example, even with some special instructions our model can get tripped up by order of operations:


llm_with_tools
.
invoke
(
    
"Whats 119 times 8 minus 20. Don't do any math yourself, only use tools for math. Respect order of operations"
)
.
tool_calls


[{'name': 'Multiply',
  'args': {'a': 119, 'b': 8},
  'id': 'call_Dl3FXRVkQCFW4sUNYOe4rFr7'},
 {'name': 'Add',
  'args': {'a': 952, 'b': -20},
  'id': 'call_n03l4hmka7VZTCiP387Wud2C'}]


The model shouldn't be trying to add anything yet, since it technically can't know the results of 119 * 8 yet.


By adding a prompt with some examples we can correct this behavior:


from
 langchain_core
.
messages 
import
 AIMessage
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
examples 
=
 
[
    HumanMessage
(
        
"What's the product of 317253 and 128472 plus four"
,
 name
=
"example_user"
    
)
,
    AIMessage
(
        
""
,
        name
=
"example_assistant"
,
        tool_calls
=
[
            
{
"name"
:
 
"Multiply"
,
 
"args"
:
 
{
"x"
:
 
317253
,
 
"y"
:
 
128472
}
,
 
"id"
:
 
"1"
}
        
]
,
    
)
,
    ToolMessage
(
"16505054784"
,
 tool_call_id
=
"1"
)
,
    AIMessage
(
        
""
,
        name
=
"example_assistant"
,
        tool_calls
=
[
{
"name"
:
 
"Add"
,
 
"args"
:
 
{
"x"
:
 
16505054784
,
 
"y"
:
 
4
}
,
 
"id"
:
 
"2"
}
]
,
    
)
,
    ToolMessage
(
"16505054788"
,
 tool_call_id
=
"2"
)
,
    AIMessage
(
        
"The product of 317253 and 128472 plus four is 16505054788"
,
        name
=
"example_assistant"
,
    
)
,
]
system 
=
 
"""You are bad at math but are an expert at using a calculator. 
Use past tool usage as an example of how to correctly use the tools."""
few_shot_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        
*
examples
,
        
(
"human"
,
 
"{query}"
)
,
    
]
)
chain 
=
 
{
"query"
:
 RunnablePassthrough
(
)
}
 
|
 few_shot_prompt 
|
 llm_with_tools
chain
.
invoke
(
"Whats 119 times 8 minus 20"
)
.
tool_calls
API Reference:
AIMessage
 | 
ChatPromptTemplate
 | 
RunnablePassthrough


[{'name': 'Multiply',
  'args': {'a': 119, 'b': 8},
  'id': 'call_MoSgwzIhPxhclfygkYaKIsGZ'}]


Seems like we get the correct output this time.


Here's what the 
LangSmith trace
 looks like.


Next steps
​




Output parsing
: See 
OpenAI Tools output
parsers

to learn about extracting the function calling API responses into
various formats.


Structured output chains
: 
Some models have constructors
 that
handle creating a structured output chain for you.


Tool use
: See how to construct chains and agents that
call the invoked tools in 
these
guides
.


Edit this page
Was this page helpful?
Previous
How to use few shot examples in chat models
Next
How to install LangChain packages
Passing tools to LLMs
Tool calls
Streaming
Passing tool outputs to model
Few-shot prompting
Next steps


--- docs_how_to_functions.txt ---

How-to guides
How to run custom functions
On this page
How to run custom functions


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Expression Language (LCEL)


Chaining runnables




You can use arbitrary functions as 
Runnables
. This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called 
RunnableLambdas
.


Note that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple arguments.


This guide will cover:




How to explicitly create a runnable from a custom function using the 
RunnableLambda
 constructor and the convenience 
@chain
 decorator


Coercion of custom functions into runnables when used in chains


How to accept and use run metadata in your custom function


How to stream with custom functions by having them return generators




Using the constructor
​


Below, we explicitly wrap our custom logic using the 
RunnableLambda
 constructor:


%
pip install 
-
qU langchain langchain_openai
import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


from
 operator 
import
 itemgetter
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnableLambda
from
 langchain_openai 
import
 ChatOpenAI
def
 
length_function
(
text
)
:
    
return
 
len
(
text
)
def
 
_multiple_length_function
(
text1
,
 text2
)
:
    
return
 
len
(
text1
)
 
*
 
len
(
text2
)
def
 
multiple_length_function
(
_dict
)
:
    
return
 _multiple_length_function
(
_dict
[
"text1"
]
,
 _dict
[
"text2"
]
)
model 
=
 ChatOpenAI
(
)
prompt 
=
 ChatPromptTemplate
.
from_template
(
"what is {a} + {b}"
)
chain 
=
 
(
    
{
        
"a"
:
 itemgetter
(
"foo"
)
 
|
 RunnableLambda
(
length_function
)
,
        
"b"
:
 
{
"text1"
:
 itemgetter
(
"foo"
)
,
 
"text2"
:
 itemgetter
(
"bar"
)
}
        
|
 RunnableLambda
(
multiple_length_function
)
,
    
}
    
|
 prompt
    
|
 model
)
chain
.
invoke
(
{
"foo"
:
 
"bar"
,
 
"bar"
:
 
"gah"
}
)
API Reference:
ChatPromptTemplate
 | 
RunnableLambda
 | 
ChatOpenAI


AIMessage(content='3 + 9 equals 12.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-73728de3-e483-49e3-ad54-51bd9570e71a-0')


The convenience 
@chain
 decorator
​


You can also turn an arbitrary function into a chain by adding a 
@chain
 decorator. This is functionally equivalent to wrapping the function in a 
RunnableLambda
 constructor as shown above. Here's an example:


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
runnables 
import
 chain
prompt1 
=
 ChatPromptTemplate
.
from_template
(
"Tell me a joke about {topic}"
)
prompt2 
=
 ChatPromptTemplate
.
from_template
(
"What is the subject of this joke: {joke}"
)
@chain
def
 
custom_chain
(
text
)
:
    prompt_val1 
=
 prompt1
.
invoke
(
{
"topic"
:
 text
}
)
    output1 
=
 ChatOpenAI
(
)
.
invoke
(
prompt_val1
)
    parsed_output1 
=
 StrOutputParser
(
)
.
invoke
(
output1
)
    chain2 
=
 prompt2 
|
 ChatOpenAI
(
)
 
|
 StrOutputParser
(
)
    
return
 chain2
.
invoke
(
{
"joke"
:
 parsed_output1
}
)
custom_chain
.
invoke
(
"bears"
)
API Reference:
StrOutputParser
 | 
chain


'The subject of the joke is the bear and his girlfriend.'


Above, the 
@chain
 decorator is used to convert 
custom_chain
 into a runnable, which we invoke with the 
.invoke()
 method.


If you are using a tracing with 
LangSmith
, you should see a 
custom_chain
 trace in there, with the calls to OpenAI nested underneath.


Automatic coercion in chains
​


When using custom functions in chains with the pipe operator (
|
), you can omit the 
RunnableLambda
 or 
@chain
 constructor and rely on coercion. Here's a simple example with a function that takes the output from the model and returns the first five letters of it:


prompt 
=
 ChatPromptTemplate
.
from_template
(
"tell me a story about {topic}"
)
model 
=
 ChatOpenAI
(
)
chain_with_coerced_function 
=
 prompt 
|
 model 
|
 
(
lambda
 x
:
 x
.
content
[
:
5
]
)
chain_with_coerced_function
.
invoke
(
{
"topic"
:
 
"bears"
}
)


'Once '


Note that we didn't need to wrap the custom function 
(lambda x: x.content[:5])
 in a 
RunnableLambda
 constructor because the 
model
 on the left of the pipe operator is already a Runnable. The custom function is 
coerced
 into a runnable. See 
this section
 for more information.


Passing run metadata
​


Runnable lambdas can optionally accept a 
RunnableConfig
 parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.


import
 json
from
 langchain_core
.
runnables 
import
 RunnableConfig
def
 
parse_or_fix
(
text
:
 
str
,
 config
:
 RunnableConfig
)
:
    fixing_chain 
=
 
(
        ChatPromptTemplate
.
from_template
(
            
"Fix the following text:\n\n\`\`\`text\n{input}\n\`\`\`\nError: {error}"
            
" Don't narrate, just respond with the fixed data."
        
)
        
|
 model
        
|
 StrOutputParser
(
)
    
)
    
for
 _ 
in
 
range
(
3
)
:
        
try
:
            
return
 json
.
loads
(
text
)
        
except
 Exception 
as
 e
:
            text 
=
 fixing_chain
.
invoke
(
{
"input"
:
 text
,
 
"error"
:
 e
}
,
 config
)
    
return
 
"Failed to parse"
from
 langchain_community
.
callbacks 
import
 get_openai_callback
with
 get_openai_callback
(
)
 
as
 cb
:
    output 
=
 RunnableLambda
(
parse_or_fix
)
.
invoke
(
        
"{foo: bar}"
,
 
{
"tags"
:
 
[
"my-tag"
]
,
 
"callbacks"
:
 
[
cb
]
}
    
)
    
print
(
output
)
    
print
(
cb
)
API Reference:
RunnableConfig
 | 
get_openai_callback


{'foo': 'bar'}
Tokens Used: 62
	Prompt Tokens: 56
	Completion Tokens: 6
Successful Requests: 1
Total Cost (USD): $9.6e-05


from
 langchain_community
.
callbacks 
import
 get_openai_callback
with
 get_openai_callback
(
)
 
as
 cb
:
    output 
=
 RunnableLambda
(
parse_or_fix
)
.
invoke
(
        
"{foo: bar}"
,
 
{
"tags"
:
 
[
"my-tag"
]
,
 
"callbacks"
:
 
[
cb
]
}
    
)
    
print
(
output
)
    
print
(
cb
)
API Reference:
get_openai_callback


{'foo': 'bar'}
Tokens Used: 62
	Prompt Tokens: 56
	Completion Tokens: 6
Successful Requests: 1
Total Cost (USD): $9.6e-05


Streaming
​


note
RunnableLambda
 is best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use 
RunnableGenerator
 instead as in the example below.


You can use generator functions (ie. functions that use the 
yield
 keyword, and behave like iterators) in a chain.


The signature of these generators should be 
Iterator[Input] -> Iterator[Output]
. Or for async generators: 
AsyncIterator[Input] -> AsyncIterator[Output]
.


These are useful for:




implementing a custom output parser


modifying the output of a previous step, while preserving streaming capabilities




Here's an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:


from
 typing 
import
 Iterator
,
 List
prompt 
=
 ChatPromptTemplate
.
from_template
(
    
"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers"
)
str_chain 
=
 prompt 
|
 model 
|
 StrOutputParser
(
)
for
 chunk 
in
 str_chain
.
stream
(
{
"animal"
:
 
"bear"
}
)
:
    
print
(
chunk
,
 end
=
""
,
 flush
=
True
)


lion, tiger, wolf, gorilla, panda


Next, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:


# This is a custom parser that splits an iterator of llm tokens
# into a list of strings separated by commas
def
 
split_into_list
(
input
:
 Iterator
[
str
]
)
 
-
>
 Iterator
[
List
[
str
]
]
:
    
# hold partial input until we get a comma
    
buffer
 
=
 
""
    
for
 chunk 
in
 
input
:
        
# add current chunk to buffer
        
buffer
 
+=
 chunk
        
# while there are commas in the buffer
        
while
 
","
 
in
 
buffer
:
            
# split buffer on comma
            comma_index 
=
 
buffer
.
index
(
","
)
            
# yield everything before the comma
            
yield
 
[
buffer
[
:
comma_index
]
.
strip
(
)
]
            
# save the rest for the next iteration
            
buffer
 
=
 
buffer
[
comma_index 
+
 
1
 
:
]
    
# yield the last chunk
    
yield
 
[
buffer
.
strip
(
)
]
list_chain 
=
 str_chain 
|
 split_into_list
for
 chunk 
in
 list_chain
.
stream
(
{
"animal"
:
 
"bear"
}
)
:
    
print
(
chunk
,
 flush
=
True
)


['lion']
['tiger']
['wolf']
['gorilla']
['raccoon']


Invoking it gives a full array of values:


list_chain
.
invoke
(
{
"animal"
:
 
"bear"
}
)


['lion', 'tiger', 'wolf', 'gorilla', 'raccoon']


Async version
​


If you are working in an 
async
 environment, here is an 
async
 version of the above example:


from
 typing 
import
 AsyncIterator
async
 
def
 
asplit_into_list
(
    
input
:
 AsyncIterator
[
str
]
,
)
 
-
>
 AsyncIterator
[
List
[
str
]
]
:
  
# async def
    
buffer
 
=
 
""
    
async
 
for
 
(
        chunk
    
)
 
in
 
input
:
  
# `input` is a `async_generator` object, so use `async for`
        
buffer
 
+=
 chunk
        
while
 
","
 
in
 
buffer
:
            comma_index 
=
 
buffer
.
index
(
","
)
            
yield
 
[
buffer
[
:
comma_index
]
.
strip
(
)
]
            
buffer
 
=
 
buffer
[
comma_index 
+
 
1
 
:
]
    
yield
 
[
buffer
.
strip
(
)
]
list_chain 
=
 str_chain 
|
 asplit_into_list
async
 
for
 chunk 
in
 list_chain
.
astream
(
{
"animal"
:
 
"bear"
}
)
:
    
print
(
chunk
,
 flush
=
True
)


['lion']
['tiger']
['wolf']
['gorilla']
['panda']


await
 list_chain
.
ainvoke
(
{
"animal"
:
 
"bear"
}
)


['lion', 'tiger', 'wolf', 'gorilla', 'panda']


Next steps
​


Now you've learned a few different ways to use custom logic within your chains, and how to implement streaming.


To learn more, see the other how-to guides on runnables in this section.
Edit this page
Was this page helpful?
Previous
How to use few shot examples
Next
How to use output parsers to parse an LLM response into structured format
Using the constructor
The convenience 
@chain
 decorator
Automatic coercion in chains
Passing run metadata
Streaming
Async version
Next steps


--- docs_how_to_graph_constructing.txt ---

How-to guides
How to construct knowledge graphs
On this page
How to construct knowledge graphs


In this guide we'll go over the basic ways of constructing a knowledge graph based on unstructured text. The constructured graph can then be used as knowledge base in a 
RAG
 application.


⚠️ Security note ⚠️
​


Constructing knowledge graphs requires executing write access to the database. There are inherent risks in doing this. Make sure that you verify and validate data before importing it. For more on general security best practices, 
see here
.


Architecture
​


At a high-level, the steps of constructing a knowledge graph from text are:




Extracting structured information from text
: Model is used to extract structured graph information from text.


Storing into graph database
: Storing the extracted structured graph information into a graph database enables downstream RAG applications




Setup
​


First, get required packages and set environment variables.
In this example, we will be using Neo4j graph database.


%
pip install 
-
-
upgrade 
-
-
quiet  langchain langchain
-
neo4j langchain
-
openai langchain
-
experimental neo4j


[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m A new release of pip is available: [0m[31;49m24.0[0m[39;49m -> [0m[32;49m24.3.1[0m
[1m[[0m[34;49mnotice[0m[1;39;49m][0m[39;49m To update, run: [0m[32;49mpip install --upgrade pip[0m
Note: you may need to restart the kernel to use updated packages.


We default to OpenAI models in this guide.


import
 getpass
import
 os
os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
)
# Uncomment the below to use LangSmith. Not required.
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"


 ········


Next, we need to define Neo4j credentials and connection.
Follow 
these installation steps
 to set up a Neo4j database.


import
 os
from
 langchain_neo4j 
import
 Neo4jGraph
os
.
environ
[
"NEO4J_URI"
]
 
=
 
"bolt://localhost:7687"
os
.
environ
[
"NEO4J_USERNAME"
]
 
=
 
"neo4j"
os
.
environ
[
"NEO4J_PASSWORD"
]
 
=
 
"password"
graph 
=
 Neo4jGraph
(
refresh_schema
=
False
)
API Reference:
Neo4jGraph


LLM Graph Transformer
​


Extracting graph data from text enables the transformation of unstructured information into structured formats, facilitating deeper insights and more efficient navigation through complex relationships and patterns. The 
LLMGraphTransformer
 converts text documents into structured graph documents by leveraging a LLM to parse and categorize entities and their relationships. The selection of the LLM model significantly influences the output by determining the accuracy and nuance of the extracted graph data.


import
 os
from
 langchain_experimental
.
graph_transformers 
import
 LLMGraphTransformer
from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatOpenAI
(
temperature
=
0
,
 model_name
=
"gpt-4-turbo"
)
llm_transformer 
=
 LLMGraphTransformer
(
llm
=
llm
)
API Reference:
LLMGraphTransformer
 | 
ChatOpenAI


Now we can pass in example text and examine the results.


from
 langchain_core
.
documents 
import
 Document
text 
=
 
"""
Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.
She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.
Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.
She was, in 1906, the first woman to become a professor at the University of Paris.
"""
documents 
=
 
[
Document
(
page_content
=
text
)
]
graph_documents 
=
 
await
 llm_transformer
.
aconvert_to_graph_documents
(
documents
)
print
(
f"Nodes:
{
graph_documents
[
0
]
.
nodes
}
"
)
print
(
f"Relationships:
{
graph_documents
[
0
]
.
relationships
}
"
)
API Reference:
Document


Nodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]
Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='MARRIED', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='PROFESSOR', properties={})]


Examine the following image to better grasp the structure of the generated knowledge graph.




Note that the graph construction process is non-deterministic since we are using LLM. Therefore, you might get slightly different results on each execution.


Additionally, you have the flexibility to define specific types of nodes and relationships for extraction according to your requirements.


llm_transformer_filtered 
=
 LLMGraphTransformer
(
    llm
=
llm
,
    allowed_nodes
=
[
"Person"
,
 
"Country"
,
 
"Organization"
]
,
    allowed_relationships
=
[
"NATIONALITY"
,
 
"LOCATED_IN"
,
 
"WORKED_AT"
,
 
"SPOUSE"
]
,
)
graph_documents_filtered 
=
 
await
 llm_transformer_filtered
.
aconvert_to_graph_documents
(
    documents
)
print
(
f"Nodes:
{
graph_documents_filtered
[
0
]
.
nodes
}
"
)
print
(
f"Relationships:
{
graph_documents_filtered
[
0
]
.
relationships
}
"
)


Nodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]
Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]


To define the graph schema more precisely, consider using a three-tuple approach for relationships. In this approach, each tuple consists of three elements: the source node, the relationship type, and the target node.


allowed_relationships 
=
 
[
    
(
"Person"
,
 
"SPOUSE"
,
 
"Person"
)
,
    
(
"Person"
,
 
"NATIONALITY"
,
 
"Country"
)
,
    
(
"Person"
,
 
"WORKED_AT"
,
 
"Organization"
)
,
]
llm_transformer_tuple 
=
 LLMGraphTransformer
(
    llm
=
llm
,
    allowed_nodes
=
[
"Person"
,
 
"Country"
,
 
"Organization"
]
,
    allowed_relationships
=
allowed_relationships
,
)
graph_documents_filtered 
=
 
await
 llm_transformer_tuple
.
aconvert_to_graph_documents
(
    documents
)
print
(
f"Nodes:
{
graph_documents_filtered
[
0
]
.
nodes
}
"
)
print
(
f"Relationships:
{
graph_documents_filtered
[
0
]
.
relationships
}
"
)


Nodes:[Node(id='Marie Curie', type='Person', properties={}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={})]
Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]


For a better understanding of the generated graph, we can again visualize it.




The 
node_properties
 parameter enables the extraction of node properties, allowing the creation of a more detailed graph.
When set to 
True
, LLM autonomously identifies and extracts relevant node properties.
Conversely, if 
node_properties
 is defined as a list of strings, the LLM selectively retrieves only the specified properties from the text.


llm_transformer_props 
=
 LLMGraphTransformer
(
    llm
=
llm
,
    allowed_nodes
=
[
"Person"
,
 
"Country"
,
 
"Organization"
]
,
    allowed_relationships
=
[
"NATIONALITY"
,
 
"LOCATED_IN"
,
 
"WORKED_AT"
,
 
"SPOUSE"
]
,
    node_properties
=
[
"born_year"
]
,
)
graph_documents_props 
=
 
await
 llm_transformer_props
.
aconvert_to_graph_documents
(
    documents
)
print
(
f"Nodes:
{
graph_documents_props
[
0
]
.
nodes
}
"
)
print
(
f"Relationships:
{
graph_documents_props
[
0
]
.
relationships
}
"
)


Nodes:[Node(id='Marie Curie', type='Person', properties={'born_year': '1867'}), Node(id='Pierre Curie', type='Person', properties={}), Node(id='University Of Paris', type='Organization', properties={}), Node(id='Poland', type='Country', properties={}), Node(id='France', type='Country', properties={})]
Relationships:[Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Poland', type='Country', properties={}), type='NATIONALITY', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='France', type='Country', properties={}), type='NATIONALITY', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='Pierre Curie', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Marie Curie', type='Person', properties={}), target=Node(id='University Of Paris', type='Organization', properties={}), type='WORKED_AT', properties={})]


Storing to graph database
​


The generated graph documents can be stored to a graph database using the 
add_graph_documents
 method.


graph
.
add_graph_documents
(
graph_documents_props
)


Most graph databases support indexes to optimize data import and retrieval. Since we might not know all the node labels in advance, we can handle this by adding a secondary base label to each node using the 
baseEntityLabel
 parameter.


graph
.
add_graph_documents
(
graph_documents
,
 baseEntityLabel
=
True
)


Results will look like:




The final option is to also import the source documents for the extracted nodes and relationships. This approach lets us track which documents each entity appeared in.


graph
.
add_graph_documents
(
graph_documents
,
 include_source
=
True
)


Graph will have the following structure:




In this visualization, the source document is highlighted in blue, with all entities extracted from it connected by 
MENTIONS
 relationships.
Edit this page
Was this page helpful?
Previous
Build an Agent with AgentExecutor (Legacy)
Next
How to partially format prompt templates
⚠️ Security note ⚠️
Architecture
Setup
LLM Graph Transformer
Storing to graph database


--- docs_how_to_graph_semantic.txt ---

How-to guides
How to add a semantic layer over graph database
On this page
How to add a semantic layer over graph database


You can use database queries to retrieve information from a graph database like Neo4j.
One option is to use LLMs to generate Cypher statements.
While that option provides excellent flexibility, the solution could be brittle and not consistently generating precise Cypher statements.
Instead of generating Cypher statements, we can implement Cypher templates as tools in a semantic layer that an LLM agent can interact with.




Setup
​


First, get required packages and set environment variables:


%
pip install 
-
-
upgrade 
-
-
quiet  langchain langchain
-
neo4j langchain
-
openai


We default to OpenAI models in this guide, but you can swap them out for the model provider of your choice.


import
 getpass
import
 os
os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
)
# Uncomment the below to use LangSmith. Not required.
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"


 ········


Next, we need to define Neo4j credentials.
Follow 
these installation steps
 to set up a Neo4j database.


os
.
environ
[
"NEO4J_URI"
]
 
=
 
"bolt://localhost:7687"
os
.
environ
[
"NEO4J_USERNAME"
]
 
=
 
"neo4j"
os
.
environ
[
"NEO4J_PASSWORD"
]
 
=
 
"password"


The below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.


from
 langchain_neo4j 
import
 Neo4jGraph
graph 
=
 Neo4jGraph
(
refresh_schema
=
False
)
# Import movie information
movies_query 
=
 
"""
LOAD CSV WITH HEADERS FROM 
'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'
AS row
MERGE (m:Movie {id:row.movieId})
SET m.released = date(row.released),
    m.title = row.title,
    m.imdbRating = toFloat(row.imdbRating)
FOREACH (director in split(row.director, '|') | 
    MERGE (p:Person {name:trim(director)})
    MERGE (p)-[:DIRECTED]->(m))
FOREACH (actor in split(row.actors, '|') | 
    MERGE (p:Person {name:trim(actor)})
    MERGE (p)-[:ACTED_IN]->(m))
FOREACH (genre in split(row.genres, '|') | 
    MERGE (g:Genre {name:trim(genre)})
    MERGE (m)-[:IN_GENRE]->(g))
"""
graph
.
query
(
movies_query
)
API Reference:
Neo4jGraph


[]


Custom tools with Cypher templates
​


A semantic layer consists of various tools exposed to an LLM that it can use to interact with a knowledge graph.
They can be of various complexity. You can think of each tool in a semantic layer as a function.


The function we will implement is to retrieve information about movies or their cast.


description_query 
=
 
"""
MATCH (m:Movie|Person)
WHERE m.title CONTAINS $candidate OR m.name CONTAINS $candidate
MATCH (m)-[r:ACTED_IN|IN_GENRE]-(t)
WITH m, type(r) as type, collect(coalesce(t.name, t.title)) as names
WITH m, type+": "+reduce(s="", n IN names | s + n + ", ") as types
WITH m, collect(types) as contexts
WITH m, "type:" + labels(m)[0] + "\ntitle: "+ coalesce(m.title, m.name) 
       + "\nyear: "+coalesce(m.released,"") +"\n" +
       reduce(s="", c in contexts | s + substring(c, 0, size(c)-2) +"\n") as context
RETURN context LIMIT 1
"""
def
 
get_information
(
entity
:
 
str
)
 
-
>
 
str
:
    
try
:
        data 
=
 graph
.
query
(
description_query
,
 params
=
{
"candidate"
:
 entity
}
)
        
return
 data
[
0
]
[
"context"
]
    
except
 IndexError
:
        
return
 
"No information was found"


You can observe that we have defined the Cypher statement used to retrieve information.
Therefore, we can avoid generating Cypher statements and use the LLM agent to only populate the input parameters.
To provide additional information to an LLM agent about when to use the tool and their input parameters, we wrap the function as a tool.


from
 typing 
import
 Optional
,
 Type
from
 langchain_core
.
tools 
import
 BaseTool
from
 pydantic 
import
 BaseModel
,
 Field
class
 
InformationInput
(
BaseModel
)
:
    entity
:
 
str
 
=
 Field
(
description
=
"movie or a person mentioned in the question"
)
class
 
InformationTool
(
BaseTool
)
:
    name
:
 
str
 
=
 
"Information"
    description
:
 
str
 
=
 
(
        
"useful for when you need to answer questions about various actors or movies"
    
)
    args_schema
:
 Type
[
BaseModel
]
 
=
 InformationInput
    
def
 
_run
(
        self
,
        entity
:
 
str
,
    
)
 
-
>
 
str
:
        
"""Use the tool."""
        
return
 get_information
(
entity
)
    
async
 
def
 
_arun
(
        self
,
        entity
:
 
str
,
    
)
 
-
>
 
str
:
        
"""Use the tool asynchronously."""
        
return
 get_information
(
entity
)
API Reference:
BaseTool


LangGraph Agent
​


We will implement a straightforward ReAct agent using LangGraph.


The agent consists of an LLM and tools step. As we interact with the agent, we will first call the LLM to decide if we should use tools. Then we will run a loop:


If the agent said to take an action (i.e. call tool), we’ll run the tools and pass the results back to the agent.
If the agent did not ask to run tools, we will finish (respond to the user).


The code implementation is as straightforward as it gets. First we bind the tools to the LLM and define the assistant step.


from
 langchain_core
.
messages 
import
 HumanMessage
,
 SystemMessage
from
 langchain_openai 
import
 ChatOpenAI
from
 langgraph
.
graph 
import
 MessagesState
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o"
)
tools 
=
 
[
InformationTool
(
)
]
llm_with_tools 
=
 llm
.
bind_tools
(
tools
)
# System message
sys_msg 
=
 SystemMessage
(
    content
=
"You are a helpful assistant tasked with finding and explaining relevant information about movies."
)
# Node
def
 
assistant
(
state
:
 MessagesState
)
:
    
return
 
{
"messages"
:
 
[
llm_with_tools
.
invoke
(
[
sys_msg
]
 
+
 state
[
"messages"
]
)
]
}
API Reference:
HumanMessage
 | 
SystemMessage
 | 
ChatOpenAI


Next we define the LangGraph flow.


from
 IPython
.
display 
import
 Image
,
 display
from
 langgraph
.
graph 
import
 END
,
 START
,
 StateGraph
from
 langgraph
.
prebuilt 
import
 ToolNode
,
 tools_condition
# Graph
builder 
=
 StateGraph
(
MessagesState
)
# Define nodes: these do the work
builder
.
add_node
(
"assistant"
,
 assistant
)
builder
.
add_node
(
"tools"
,
 ToolNode
(
tools
)
)
# Define edges: these determine how the control flow moves
builder
.
add_edge
(
START
,
 
"assistant"
)
builder
.
add_conditional_edges
(
    
"assistant"
,
    
# If the latest message (result) from assistant is a tool call -> tools_condition routes to tools
    
# If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END
    tools_condition
,
)
builder
.
add_edge
(
"tools"
,
 
"assistant"
)
react_graph 
=
 builder
.
compile
(
)
# Show
display
(
Image
(
react_graph
.
get_graph
(
xray
=
True
)
.
draw_mermaid_png
(
)
)
)
API Reference:
StateGraph
 | 
ToolNode
 | 
tools_condition




Let's test the workflow now with an example question.


input_messages 
=
 
[
HumanMessage
(
content
=
"Who played in the Casino?"
)
]
messages 
=
 react_graph
.
invoke
(
{
"messages"
:
 input_messages
}
)
for
 m 
in
 messages
[
"messages"
]
:
    m
.
pretty_print
(
)


================================[1m Human Message [0m=================================
Who played in the Casino?
==================================[1m Ai Message [0m==================================
Tool Calls:
  Information (call_j4usgFStGtBM16fuguRaeoGc)
 Call ID: call_j4usgFStGtBM16fuguRaeoGc
  Args:
    entity: Casino
=================================[1m Tool Message [0m=================================
Name: Information
type:Movie
title: Casino
year: 1995-11-22
ACTED_IN: Robert De Niro, Joe Pesci, Sharon Stone, James Woods
IN_GENRE: Drama, Crime
==================================[1m Ai Message [0m==================================
The movie "Casino," released in 1995, features the following actors:
- Robert De Niro
- Joe Pesci
- Sharon Stone
- James Woods
The film is in the Drama and Crime genres.
Edit this page
Was this page helpful?
Previous
How to use example selectors
Next
How to invoke runnables in parallel
Setup
Custom tools with Cypher templates
LangGraph Agent


--- docs_how_to_hybrid.txt ---

How-to guides
Hybrid Search
On this page
Hybrid Search


The standard search in LangChain is done by vector similarity. However, a number of 
vector store
 implementations (Astra DB, ElasticSearch, Neo4J, AzureSearch, Qdrant...) also support more advanced search combining vector similarity search and other search techniques (full-text, BM25, and so on). This is generally referred to as "Hybrid" search.


Step 1: Make sure the vectorstore you are using supports hybrid search


At the moment, there is no unified way to perform hybrid search in LangChain. Each vectorstore may have their own way to do it. This is generally exposed as a keyword argument that is passed in during 
similarity_search
.


By reading the documentation or source code, figure out whether the vectorstore you are using supports hybrid search, and, if so, how to use it.


Step 2: Add that parameter as a configurable field for the chain


This will let you easily call the chain and configure any relevant flags at runtime. See 
this documentation
 for more information on configuration.


Step 3: Call the chain with that configurable field


Now, at runtime you can call this chain with configurable field.


Code Example
​


Let's see a concrete example of what this looks like in code. We will use the Cassandra/CQL interface of Astra DB for this example.


Install the following Python package:


!pip install 
"cassio>=0.1.7"


Get the 
connection secrets
.


Initialize cassio:


import
 cassio
cassio
.
init
(
    database_id
=
"Your database ID"
,
    token
=
"Your application token"
,
    keyspace
=
"Your key space"
,
)


Create the Cassandra VectorStore with a standard 
index analyzer
. The index analyzer is needed to enable term matching.


from
 cassio
.
table
.
cql 
import
 STANDARD_ANALYZER
from
 langchain_community
.
vectorstores 
import
 Cassandra
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
)
vectorstore 
=
 Cassandra
(
    embedding
=
embeddings
,
    table_name
=
"test_hybrid"
,
    body_index_options
=
[
STANDARD_ANALYZER
]
,
    session
=
None
,
    keyspace
=
None
,
)
vectorstore
.
add_texts
(
    
[
        
"In 2023, I visited Paris"
,
        
"In 2022, I visited New York"
,
        
"In 2021, I visited New Orleans"
,
    
]
)
API Reference:
Cassandra
 | 
OpenAIEmbeddings


If we do a standard similarity search, we get all the documents:


vectorstore
.
as_retriever
(
)
.
invoke
(
"What city did I visit last?"
)


[Document(page_content='In 2022, I visited New York'),
Document(page_content='In 2023, I visited Paris'),
Document(page_content='In 2021, I visited New Orleans')]


The Astra DB vectorstore 
body_search
 argument can be used to filter the search on the term 
new
.


vectorstore
.
as_retriever
(
search_kwargs
=
{
"body_search"
:
 
"new"
}
)
.
invoke
(
    
"What city did I visit last?"
)


[Document(page_content='In 2022, I visited New York'),
Document(page_content='In 2021, I visited New Orleans')]


We can now create the chain that we will use to do question-answering over


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 
(
    ConfigurableField
,
    RunnablePassthrough
,
)
from
 langchain_openai 
import
 ChatOpenAI
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
ConfigurableField
 | 
RunnablePassthrough
 | 
ChatOpenAI


This is basic question-answering chain set up.


template 
=
 
"""Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt 
=
 ChatPromptTemplate
.
from_template
(
template
)
model 
=
 ChatOpenAI
(
)
retriever 
=
 vectorstore
.
as_retriever
(
)


Here we mark the retriever as having a configurable field. All vectorstore retrievers have 
search_kwargs
 as a field. This is just a dictionary, with vectorstore specific fields


configurable_retriever 
=
 retriever
.
configurable_fields
(
    search_kwargs
=
ConfigurableField
(
        
id
=
"search_kwargs"
,
        name
=
"Search Kwargs"
,
        description
=
"The search kwargs to use"
,
    
)
)


We can now create the chain using our configurable retriever


chain 
=
 
(
    
{
"context"
:
 configurable_retriever
,
 
"question"
:
 RunnablePassthrough
(
)
}
    
|
 prompt
    
|
 model
    
|
 StrOutputParser
(
)
)


chain
.
invoke
(
"What city did I visit last?"
)


Paris


We can now invoke the chain with configurable options. 
search_kwargs
 is the id of the configurable field. The value is the search kwargs to use for Astra DB.


chain
.
invoke
(
    
"What city did I visit last?"
,
    config
=
{
"configurable"
:
 
{
"search_kwargs"
:
 
{
"body_search"
:
 
"new"
}
}
}
,
)


New York
Edit this page
Was this page helpful?
Previous
How to filter messages
Next
How to use the LangChain indexing API
Code Example


--- docs_how_to_indexing.txt ---

How-to guides
How to use the LangChain indexing API
On this page
How to use the LangChain indexing API


Here, we will look at a basic indexing workflow using the LangChain indexing API.


The indexing API lets you load and keep in sync documents from any source into a 
vector store
. Specifically, it helps:




Avoid writing duplicated content into the vector store


Avoid re-writing unchanged content


Avoid re-computing embeddings over unchanged content




All of which should save you time and money, as well as improve your vector search results.


Crucially, the indexing API will work even with documents that have gone through several
transformation steps (e.g., via text chunking) with respect to the original source documents.


How it works
​


LangChain indexing makes use of a record manager (
RecordManager
) that keeps track of document writes into the vector store.


When indexing content, hashes are computed for each document, and the following information is stored in the record manager:




the document hash (hash of both page content and metadata)


write time


the source id -- each document should include information in its metadata to allow us to determine the ultimate source of this document




Deletion modes
​


When indexing documents into a vector store, it's possible that some existing documents in the vector store should be deleted. In certain situations you may want to remove any existing documents that are derived from the same sources as the new documents being indexed. In others you may want to delete all existing documents wholesale. The indexing API deletion modes let you pick the behavior you want:


Cleanup Mode
De-Duplicates Content
Parallelizable
Cleans Up Deleted Source Docs
Cleans Up Mutations of Source Docs and/or Derived Docs
Clean Up Timing
None
✅
✅
❌
❌
-
Incremental
✅
✅
❌
✅
Continuously
Full
✅
❌
✅
✅
At end of indexing
Scoped_Full
✅
✅
❌
✅
At end of indexing


None
 does not do any automatic clean up, allowing the user to manually do clean up of old content.


incremental
, 
full
 and 
scoped_full
 offer the following automated clean up:




If the content of the source document or derived documents has 
changed
, all 3 modes will clean up (delete) previous versions of the content.


If the source document has been 
deleted
 (meaning it is not included in the documents currently being indexed), the 
full
 cleanup mode will delete it from the vector store correctly, but the 
incremental
 and 
scoped_full
 mode will not.




When content is mutated (e.g., the source PDF file was revised) there will be a period of time during indexing when both the new and old versions may be returned to the user. This happens after the new content was written, but before the old version was deleted.




incremental
 indexing minimizes this period of time as it is able to do clean up continuously, as it writes.


full
 and 
scoped_full
 mode does the clean up after all batches have been written.




Requirements
​




Do not use with a store that has been pre-populated with content independently of the indexing API, as the record manager will not know that records have been inserted previously.


Only works with LangChain 
vectorstore
's that support:




document addition by id (
add_documents
 method with 
ids
 argument)


delete by id (
delete
 method with 
ids
 argument)








Compatible Vectorstores: 
Aerospike
, 
AnalyticDB
, 
AstraDB
, 
AwaDB
, 
AzureCosmosDBNoSqlVectorSearch
, 
AzureCosmosDBVectorSearch
, 
AzureSearch
, 
Bagel
, 
Cassandra
, 
Chroma
, 
CouchbaseVectorStore
, 
DashVector
, 
DatabricksVectorSearch
, 
DeepLake
, 
Dingo
, 
ElasticVectorSearch
, 
ElasticsearchStore
, 
FAISS
, 
HanaDB
, 
Milvus
, 
MongoDBAtlasVectorSearch
, 
MyScale
, 
OpenSearchVectorSearch
, 
PGVector
, 
Pinecone
, 
Qdrant
, 
Redis
, 
Rockset
, 
ScaNN
, 
SingleStoreDB
, 
SupabaseVectorStore
, 
SurrealDBStore
, 
TimescaleVector
, 
Vald
, 
VDMS
, 
Vearch
, 
VespaStore
, 
Weaviate
, 
Yellowbrick
, 
ZepVectorStore
, 
TencentVectorDB
, 
OpenSearchVectorSearch
.


Caution
​


The record manager relies on a time-based mechanism to determine what content can be cleaned up (when using 
full
 or 
incremental
 or 
scoped_full
 cleanup modes).


If two tasks run back-to-back, and the first task finishes before the clock time changes, then the second task may not be able to clean up content.


This is unlikely to be an issue in actual settings for the following reasons:




The RecordManager uses higher resolution timestamps.


The data would need to change between the first and the second tasks runs, which becomes unlikely if the time interval between the tasks is small.


Indexing tasks typically take more than a few ms.




Quickstart
​


from
 langchain
.
indexes 
import
 SQLRecordManager
,
 index
from
 langchain_core
.
documents 
import
 Document
from
 langchain_elasticsearch 
import
 ElasticsearchStore
from
 langchain_openai 
import
 OpenAIEmbeddings
API Reference:
SQLRecordManager
 | 
index
 | 
Document
 | 
ElasticsearchStore
 | 
OpenAIEmbeddings


Initialize a vector store and set up the embeddings:


collection_name 
=
 
"test_index"
embedding 
=
 OpenAIEmbeddings
(
)
vectorstore 
=
 ElasticsearchStore
(
    es_url
=
"http://localhost:9200"
,
 index_name
=
"test_index"
,
 embedding
=
embedding
)


Initialize a record manager with an appropriate namespace.


Suggestion:
 Use a namespace that takes into account both the vector store and the collection name in the vector store; e.g., 'redis/my_docs', 'chromadb/my_docs' or 'postgres/my_docs'.


namespace 
=
 
f"elasticsearch/
{
collection_name
}
"
record_manager 
=
 SQLRecordManager
(
    namespace
,
 db_url
=
"sqlite:///record_manager_cache.sql"
)


Create a schema before using the record manager.


record_manager
.
create_schema
(
)


Let's index some test documents:


doc1 
=
 Document
(
page_content
=
"kitty"
,
 metadata
=
{
"source"
:
 
"kitty.txt"
}
)
doc2 
=
 Document
(
page_content
=
"doggy"
,
 metadata
=
{
"source"
:
 
"doggy.txt"
}
)


Indexing into an empty vector store:


def
 
_clear
(
)
:
    
"""Hacky helper method to clear content. See the `full` mode section to to understand why it works."""
    index
(
[
]
,
 record_manager
,
 vectorstore
,
 cleanup
=
"full"
,
 source_id_key
=
"source"
)


None
 deletion mode
​


This mode does not do automatic clean up of old versions of content; however, it still takes care of content de-duplication.


_clear
(
)


index
(
    
[
doc1
,
 doc1
,
 doc1
,
 doc1
,
 doc1
]
,
    record_manager
,
    vectorstore
,
    cleanup
=
None
,
    source_id_key
=
"source"
,
)


{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}


_clear
(
)


index
(
[
doc1
,
 doc2
]
,
 record_manager
,
 vectorstore
,
 cleanup
=
None
,
 source_id_key
=
"source"
)


{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}


Second time around all content will be skipped:


index
(
[
doc1
,
 doc2
]
,
 record_manager
,
 vectorstore
,
 cleanup
=
None
,
 source_id_key
=
"source"
)


{'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}


"incremental"
 deletion mode
​


_clear
(
)


index
(
    
[
doc1
,
 doc2
]
,
    record_manager
,
    vectorstore
,
    cleanup
=
"incremental"
,
    source_id_key
=
"source"
,
)


{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}


Indexing again should result in both documents getting 
skipped
 -- also skipping the embedding operation!


index
(
    
[
doc1
,
 doc2
]
,
    record_manager
,
    vectorstore
,
    cleanup
=
"incremental"
,
    source_id_key
=
"source"
,
)


{'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}


If we provide no documents with incremental indexing mode, nothing will change.


index
(
[
]
,
 record_manager
,
 vectorstore
,
 cleanup
=
"incremental"
,
 source_id_key
=
"source"
)


{'num_added': 0, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}


If we mutate a document, the new version will be written and all old versions sharing the same source will be deleted.


changed_doc_2 
=
 Document
(
page_content
=
"puppy"
,
 metadata
=
{
"source"
:
 
"doggy.txt"
}
)


index
(
    
[
changed_doc_2
]
,
    record_manager
,
    vectorstore
,
    cleanup
=
"incremental"
,
    source_id_key
=
"source"
,
)


{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 1}


"full"
 deletion mode
​


In 
full
 mode the user should pass the 
full
 universe of content that should be indexed into the indexing function.


Any documents that are not passed into the indexing function and are present in the vectorstore will be deleted!


This behavior is useful to handle deletions of source documents.


_clear
(
)


all_docs 
=
 
[
doc1
,
 doc2
]


index
(
all_docs
,
 record_manager
,
 vectorstore
,
 cleanup
=
"full"
,
 source_id_key
=
"source"
)


{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}


Say someone deleted the first doc:


del
 all_docs
[
0
]


all_docs


[Document(page_content='doggy', metadata={'source': 'doggy.txt'})]


Using full mode will clean up the deleted content as well.


index
(
all_docs
,
 record_manager
,
 vectorstore
,
 cleanup
=
"full"
,
 source_id_key
=
"source"
)


{'num_added': 0, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}


Source
​


The metadata attribute contains a field called 
source
. This source should be pointing at the 
ultimate
 provenance associated with the given document.


For example, if these documents are representing chunks of some parent document, the 
source
 for both documents should be the same and reference the parent document.


In general, 
source
 should always be specified. Only use a 
None
, if you 
never
 intend to use 
incremental
 mode, and for some reason can't specify the 
source
 field correctly.


from
 langchain_text_splitters 
import
 CharacterTextSplitter
API Reference:
CharacterTextSplitter


doc1 
=
 Document
(
    page_content
=
"kitty kitty kitty kitty kitty"
,
 metadata
=
{
"source"
:
 
"kitty.txt"
}
)
doc2 
=
 Document
(
page_content
=
"doggy doggy the doggy"
,
 metadata
=
{
"source"
:
 
"doggy.txt"
}
)


new_docs 
=
 CharacterTextSplitter
(
    separator
=
"t"
,
 keep_separator
=
True
,
 chunk_size
=
12
,
 chunk_overlap
=
2
)
.
split_documents
(
[
doc1
,
 doc2
]
)
new_docs


[Document(page_content='kitty kit', metadata={'source': 'kitty.txt'}),
 Document(page_content='tty kitty ki', metadata={'source': 'kitty.txt'}),
 Document(page_content='tty kitty', metadata={'source': 'kitty.txt'}),
 Document(page_content='doggy doggy', metadata={'source': 'doggy.txt'}),
 Document(page_content='the doggy', metadata={'source': 'doggy.txt'})]


_clear
(
)


index
(
    new_docs
,
    record_manager
,
    vectorstore
,
    cleanup
=
"incremental"
,
    source_id_key
=
"source"
,
)


{'num_added': 5, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}


changed_doggy_docs 
=
 
[
    Document
(
page_content
=
"woof woof"
,
 metadata
=
{
"source"
:
 
"doggy.txt"
}
)
,
    Document
(
page_content
=
"woof woof woof"
,
 metadata
=
{
"source"
:
 
"doggy.txt"
}
)
,
]


This should delete the old versions of documents associated with 
doggy.txt
 source and replace them with the new versions.


index
(
    changed_doggy_docs
,
    record_manager
,
    vectorstore
,
    cleanup
=
"incremental"
,
    source_id_key
=
"source"
,
)


{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 2}


vectorstore
.
similarity_search
(
"dog"
,
 k
=
30
)


[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}),
 Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'}),
 Document(page_content='tty kitty', metadata={'source': 'kitty.txt'}),
 Document(page_content='tty kitty ki', metadata={'source': 'kitty.txt'}),
 Document(page_content='kitty kit', metadata={'source': 'kitty.txt'})]


Using with loaders
​


Indexing can accept either an iterable of documents or else any loader.


Attention:
 The loader 
must
 set source keys correctly.


from
 langchain_core
.
document_loaders 
import
 BaseLoader
class
 
MyCustomLoader
(
BaseLoader
)
:
    
def
 
lazy_load
(
self
)
:
        text_splitter 
=
 CharacterTextSplitter
(
            separator
=
"t"
,
 keep_separator
=
True
,
 chunk_size
=
12
,
 chunk_overlap
=
2
        
)
        docs 
=
 
[
            Document
(
page_content
=
"woof woof"
,
 metadata
=
{
"source"
:
 
"doggy.txt"
}
)
,
            Document
(
page_content
=
"woof woof woof"
,
 metadata
=
{
"source"
:
 
"doggy.txt"
}
)
,
        
]
        
yield
 
from
 text_splitter
.
split_documents
(
docs
)
    
def
 
load
(
self
)
:
        
return
 
list
(
self
.
lazy_load
(
)
)
API Reference:
BaseLoader


_clear
(
)


loader 
=
 MyCustomLoader
(
)


loader
.
load
(
)


[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}),
 Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'})]


index
(
loader
,
 record_manager
,
 vectorstore
,
 cleanup
=
"full"
,
 source_id_key
=
"source"
)


{'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}


vectorstore
.
similarity_search
(
"dog"
,
 k
=
30
)


[Document(page_content='woof woof', metadata={'source': 'doggy.txt'}),
 Document(page_content='woof woof woof', metadata={'source': 'doggy.txt'})]
Edit this page
Was this page helpful?
Previous
Hybrid Search
Next
How to inspect runnables
How it works
Deletion modes
Requirements
Caution
Quickstart
None
 deletion mode
"incremental"
 deletion mode
"full"
 deletion mode
Source
Using with loaders


--- docs_how_to_inspect.txt ---

How-to guides
How to inspect runnables
On this page
How to inspect runnables


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Expression Language (LCEL)


Chaining runnables




Once you create a runnable with 
LangChain Expression Language
, you may often want to inspect it to get a better sense for what is going on. This notebook covers some methods for doing so.


This guide shows some ways you can programmatically introspect the internal steps of chains. If you are instead interested in debugging issues in your chain, see 
this section
 instead.


First, let's create an example chain. We will create one that does retrieval:


%
pip install 
-
qU langchain langchain
-
openai faiss
-
cpu tiktoken


from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
,
 OpenAIEmbeddings
vectorstore 
=
 FAISS
.
from_texts
(
    
[
"harrison worked at kensho"
]
,
 embedding
=
OpenAIEmbeddings
(
)
)
retriever 
=
 vectorstore
.
as_retriever
(
)
template 
=
 
"""Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt 
=
 ChatPromptTemplate
.
from_template
(
template
)
model 
=
 ChatOpenAI
(
)
chain 
=
 
(
    
{
"context"
:
 retriever
,
 
"question"
:
 RunnablePassthrough
(
)
}
    
|
 prompt
    
|
 model
    
|
 StrOutputParser
(
)
)
API Reference:
FAISS
 | 
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI
 | 
OpenAIEmbeddings


Get a graph
​


You can use the 
get_graph()
 method to get a graph representation of the runnable:


chain
.
get_graph
(
)


Print a graph
​


While that is not super legible, you can use the 
print_ascii()
 method to show that graph in a way that's easier to understand:


chain
.
get_graph
(
)
.
print_ascii
(
)


           +---------------------------------+         
           | Parallel<context,question>Input |         
           +---------------------------------+         
                    **               **                
                 ***                   ***             
               **                         **           
+----------------------+              +-------------+  
| VectorStoreRetriever |              | Passthrough |  
+----------------------+              +-------------+  
                    **               **                
                      ***         ***                  
                         **     **                     
           +----------------------------------+        
           | Parallel<context,question>Output |        
           +----------------------------------+        
                             *                         
                             *                         
                             *                         
                  +--------------------+               
                  | ChatPromptTemplate |               
                  +--------------------+               
                             *                         
                             *                         
                             *                         
                      +------------+                   
                      | ChatOpenAI |                   
                      +------------+                   
                             *                         
                             *                         
                             *                         
                   +-----------------+                 
                   | StrOutputParser |                 
                   +-----------------+                 
                             *                         
                             *                         
                             *                         
                +-----------------------+              
                | StrOutputParserOutput |              
                +-----------------------+


Get the prompts
​


You may want to see just the prompts that are used in a chain with the 
get_prompts()
 method:


chain
.
get_prompts
(
)


[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n'))])]


Next steps
​


You've now learned how to introspect your composed LCEL chains.


Next, check out the other how-to guides on runnables in this section, or the related how-to guide on 
debugging your chains
.
Edit this page
Was this page helpful?
Previous
How to use the LangChain indexing API
Next
LangChain Expression Language Cheatsheet
Get a graph
Print a graph
Get the prompts
Next steps


--- docs_how_to_installation.txt ---

How-to guides
How to install LangChain packages
On this page
How to install LangChain packages


The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of
functionality to install.


Official release
​


To install the main 
langchain
 package, run:




Pip
Conda
pip install langchain
conda install langchain -c conda-forge


While this package acts as a sane starting point to using LangChain,
much of the value of LangChain comes when integrating it with various model providers, datastores, etc.
By default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.


Ecosystem packages
​


With the exception of the 
langsmith
 SDK, all packages in the LangChain ecosystem depend on 
langchain-core
, which contains base
classes and abstractions that other packages use. The dependency graph below shows how the different packages are related.
A directed arrow indicates that the source package depends on the target package:




When installing a package, you do not need to explicitly install that package's explicit dependencies (such as 
langchain-core
).
However, you may choose to if you are using a feature only available in a certain version of that dependency.
If you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.


LangChain core
​


The 
langchain-core
 package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by 
langchain
, but can also be used separately. Install with:


pip install langchain-core


Integration packages
​


Certain integrations like OpenAI and Anthropic have their own packages.
Any integrations that require their own package will be documented as such in the 
Integration docs
.
You can see a list of all integration packages in the 
API reference
 under the "Partner libs" dropdown.
To install one of these run:


pip install langchain-openai


Any integrations that haven't been split out into their own packages will live in the 
langchain-community
 package. Install with:


pip install langchain-community


LangChain experimental
​


The 
langchain-experimental
 package holds experimental LangChain code, intended for research and experimental uses.
Install with:


pip install langchain-experimental


LangGraph
​


langgraph
 is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.
Install with:


pip install langgraph


LangServe
​


LangServe helps developers deploy LangChain runnables and chains as a REST API.
LangServe is automatically installed by LangChain CLI.
If not using LangChain CLI, install with:


pip install "langserve[all]"


for both client and server dependencies. Or 
pip install "langserve[client]"
 for client code, and 
pip install "langserve[server]"
 for server code.


LangChain CLI
​


The LangChain CLI is useful for working with LangChain templates and other LangServe projects.
Install with:


pip install langchain-cli


LangSmith SDK
​


The LangSmith SDK is automatically installed by LangChain. However, it does not depend on

langchain-core
, and can be installed and used independently if desired.
If you are not using LangChain, you can install it with:


pip install langsmith


From source
​


If you want to install a package from source, you can do so by cloning the 
main LangChain repo
, enter the directory of the package you want to install 
PATH/TO/REPO/langchain/libs/{package}
, and run:


pip install -e .


LangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see 
all repos here
.
Edit this page
Was this page helpful?
Previous
How to do tool/function calling
Next
How to add examples to the prompt for query analysis
Official release
Ecosystem packages
LangChain core
Integration packages
LangChain experimental
LangGraph
LangServe
LangChain CLI
LangSmith SDK
From source


--- docs_how_to_lcel_cheatsheet.txt ---

How-to guides
LangChain Expression Language Cheatsheet
On this page
LangChain Expression Language Cheatsheet


This is a quick reference for all the most important 
LCEL
 primitives. For more advanced usage see the 
LCEL how-to guides
 and the 
full API reference
.


Invoke a runnable
​


Runnable.invoke()
 / 
Runnable.ainvoke()
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
runnable 
=
 RunnableLambda
(
lambda
 x
:
 
str
(
x
)
)
runnable
.
invoke
(
5
)
# Async variant:
# await runnable.ainvoke(5)
API Reference:
RunnableLambda


'5'


Batch a runnable
​


Runnable.batch()
 / 
Runnable.abatch()
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
runnable 
=
 RunnableLambda
(
lambda
 x
:
 
str
(
x
)
)
runnable
.
batch
(
[
7
,
 
8
,
 
9
]
)
# Async variant:
# await runnable.abatch([7, 8, 9])
API Reference:
RunnableLambda


['7', '8', '9']


Stream a runnable
​


Runnable.stream()
 / 
Runnable.astream()
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
def
 
func
(
x
)
:
    
for
 y 
in
 x
:
        
yield
 
str
(
y
)
runnable 
=
 RunnableLambda
(
func
)
for
 chunk 
in
 runnable
.
stream
(
range
(
5
)
)
:
    
print
(
chunk
)
# Async variant:
# async for chunk in await runnable.astream(range(5)):
#     print(chunk)
API Reference:
RunnableLambda


0
1
2
3
4


Compose runnables
​


Pipe operator 
|
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 
{
"foo"
:
 x
}
)
runnable2 
=
 RunnableLambda
(
lambda
 x
:
 
[
x
]
 
*
 
2
)
chain 
=
 runnable1 
|
 runnable2
chain
.
invoke
(
2
)
API Reference:
RunnableLambda


[{'foo': 2}, {'foo': 2}]


Invoke runnables in parallel
​


RunnableParallel
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnableParallel
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 
{
"foo"
:
 x
}
)
runnable2 
=
 RunnableLambda
(
lambda
 x
:
 
[
x
]
 
*
 
2
)
chain 
=
 RunnableParallel
(
first
=
runnable1
,
 second
=
runnable2
)
chain
.
invoke
(
2
)
API Reference:
RunnableLambda
 | 
RunnableParallel


{'first': {'foo': 2}, 'second': [2, 2]}


Turn any function into a runnable
​


RunnableLambda
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
def
 
func
(
x
)
:
    
return
 x 
+
 
5
runnable 
=
 RunnableLambda
(
func
)
runnable
.
invoke
(
2
)
API Reference:
RunnableLambda


7


Merge input and output dicts
​


RunnablePassthrough.assign
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnablePassthrough
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 x
[
"foo"
]
 
+
 
7
)
chain 
=
 RunnablePassthrough
.
assign
(
bar
=
runnable1
)
chain
.
invoke
(
{
"foo"
:
 
10
}
)
API Reference:
RunnableLambda
 | 
RunnablePassthrough


{'foo': 10, 'bar': 17}


Include input dict in output dict
​


RunnablePassthrough
​


from
 langchain_core
.
runnables 
import
 
(
    RunnableLambda
,
    RunnableParallel
,
    RunnablePassthrough
,
)
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 x
[
"foo"
]
 
+
 
7
)
chain 
=
 RunnableParallel
(
bar
=
runnable1
,
 baz
=
RunnablePassthrough
(
)
)
chain
.
invoke
(
{
"foo"
:
 
10
}
)
API Reference:
RunnableLambda
 | 
RunnableParallel
 | 
RunnablePassthrough


{'bar': 17, 'baz': {'foo': 10}}


Add default invocation args
​


Runnable.bind
​


from
 typing 
import
 Optional
from
 langchain_core
.
runnables 
import
 RunnableLambda
def
 
func
(
main_arg
:
 
dict
,
 other_arg
:
 Optional
[
str
]
 
=
 
None
)
 
-
>
 
dict
:
    
if
 other_arg
:
        
return
 
{
**
main_arg
,
 
**
{
"foo"
:
 other_arg
}
}
    
return
 main_arg
runnable1 
=
 RunnableLambda
(
func
)
bound_runnable1 
=
 runnable1
.
bind
(
other_arg
=
"bye"
)
bound_runnable1
.
invoke
(
{
"bar"
:
 
"hello"
}
)
API Reference:
RunnableLambda


{'bar': 'hello', 'foo': 'bye'}


Add fallbacks
​


Runnable.with_fallbacks
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 x 
+
 
"foo"
)
runnable2 
=
 RunnableLambda
(
lambda
 x
:
 
str
(
x
)
 
+
 
"foo"
)
chain 
=
 runnable1
.
with_fallbacks
(
[
runnable2
]
)
chain
.
invoke
(
5
)
API Reference:
RunnableLambda


'5foo'


Add retries
​


Runnable.with_retry
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
counter 
=
 
-
1
def
 
func
(
x
)
:
    
global
 counter
    counter 
+=
 
1
    
print
(
f"attempt with 
{
counter
=
}
"
)
    
return
 x 
/
 counter
chain 
=
 RunnableLambda
(
func
)
.
with_retry
(
stop_after_attempt
=
2
)
chain
.
invoke
(
2
)
API Reference:
RunnableLambda


attempt with counter=0
attempt with counter=1


2.0


Configure runnable execution
​


RunnableConfig
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnableParallel
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 
{
"foo"
:
 x
}
)
runnable2 
=
 RunnableLambda
(
lambda
 x
:
 
[
x
]
 
*
 
2
)
runnable3 
=
 RunnableLambda
(
lambda
 x
:
 
str
(
x
)
)
chain 
=
 RunnableParallel
(
first
=
runnable1
,
 second
=
runnable2
,
 third
=
runnable3
)
chain
.
invoke
(
7
,
 config
=
{
"max_concurrency"
:
 
2
}
)
API Reference:
RunnableLambda
 | 
RunnableParallel


{'first': {'foo': 7}, 'second': [7, 7], 'third': '7'}


Add default config to runnable
​


Runnable.with_config
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnableParallel
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 
{
"foo"
:
 x
}
)
runnable2 
=
 RunnableLambda
(
lambda
 x
:
 
[
x
]
 
*
 
2
)
runnable3 
=
 RunnableLambda
(
lambda
 x
:
 
str
(
x
)
)
chain 
=
 RunnableParallel
(
first
=
runnable1
,
 second
=
runnable2
,
 third
=
runnable3
)
configured_chain 
=
 chain
.
with_config
(
max_concurrency
=
2
)
chain
.
invoke
(
7
)
API Reference:
RunnableLambda
 | 
RunnableParallel


{'first': {'foo': 7}, 'second': [7, 7], 'third': '7'}


Make runnable attributes configurable
​


Runnable.with_configurable_fields
​


from
 typing 
import
 Any
,
 Optional
from
 langchain_core
.
runnables 
import
 
(
    ConfigurableField
,
    RunnableConfig
,
    RunnableSerializable
,
)
class
 
FooRunnable
(
RunnableSerializable
[
dict
,
 
dict
]
)
:
    output_key
:
 
str
    
def
 
invoke
(
        self
,
 
input
:
 Any
,
 config
:
 Optional
[
RunnableConfig
]
 
=
 
None
,
 
**
kwargs
:
 Any
    
)
 
-
>
 
list
:
        
return
 self
.
_call_with_config
(
self
.
subtract_seven
,
 
input
,
 config
,
 
**
kwargs
)
    
def
 
subtract_seven
(
self
,
 
input
:
 
dict
)
 
-
>
 
dict
:
        
return
 
{
self
.
output_key
:
 
input
[
"foo"
]
 
-
 
7
}
runnable1 
=
 FooRunnable
(
output_key
=
"bar"
)
configurable_runnable1 
=
 runnable1
.
configurable_fields
(
    output_key
=
ConfigurableField
(
id
=
"output_key"
)
)
configurable_runnable1
.
invoke
(
    
{
"foo"
:
 
10
}
,
 config
=
{
"configurable"
:
 
{
"output_key"
:
 
"not bar"
}
}
)
API Reference:
ConfigurableField
 | 
RunnableConfig
 | 
RunnableSerializable


{'not bar': 3}


configurable_runnable1
.
invoke
(
{
"foo"
:
 
10
}
)


{'bar': 3}


Make chain components configurable
​


Runnable.with_configurable_alternatives
​


from
 typing 
import
 Any
,
 Optional
from
 langchain_core
.
runnables 
import
 RunnableConfig
,
 RunnableLambda
,
 RunnableParallel
class
 
ListRunnable
(
RunnableSerializable
[
Any
,
 
list
]
)
:
    
def
 
invoke
(
        self
,
 
input
:
 Any
,
 config
:
 Optional
[
RunnableConfig
]
 
=
 
None
,
 
**
kwargs
:
 Any
    
)
 
-
>
 
list
:
        
return
 self
.
_call_with_config
(
self
.
listify
,
 
input
,
 config
,
 
**
kwargs
)
    
def
 
listify
(
self
,
 
input
:
 Any
)
 
-
>
 
list
:
        
return
 
[
input
]
class
 
StrRunnable
(
RunnableSerializable
[
Any
,
 
str
]
)
:
    
def
 
invoke
(
        self
,
 
input
:
 Any
,
 config
:
 Optional
[
RunnableConfig
]
 
=
 
None
,
 
**
kwargs
:
 Any
    
)
 
-
>
 
list
:
        
return
 self
.
_call_with_config
(
self
.
strify
,
 
input
,
 config
,
 
**
kwargs
)
    
def
 
strify
(
self
,
 
input
:
 Any
)
 
-
>
 
str
:
        
return
 
str
(
input
)
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 
{
"foo"
:
 x
}
)
configurable_runnable 
=
 ListRunnable
(
)
.
configurable_alternatives
(
    ConfigurableField
(
id
=
"second_step"
)
,
 default_key
=
"list"
,
 string
=
StrRunnable
(
)
)
chain 
=
 runnable1 
|
 configurable_runnable
chain
.
invoke
(
7
,
 config
=
{
"configurable"
:
 
{
"second_step"
:
 
"string"
}
}
)
API Reference:
RunnableConfig
 | 
RunnableLambda
 | 
RunnableParallel


"{'foo': 7}"


chain
.
invoke
(
7
)


[{'foo': 7}]


Build a chain dynamically based on input
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnableParallel
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 
{
"foo"
:
 x
}
)
runnable2 
=
 RunnableLambda
(
lambda
 x
:
 
[
x
]
 
*
 
2
)
chain 
=
 RunnableLambda
(
lambda
 x
:
 runnable1 
if
 x 
>
 
6
 
else
 runnable2
)
chain
.
invoke
(
7
)
API Reference:
RunnableLambda
 | 
RunnableParallel


{'foo': 7}


chain
.
invoke
(
5
)


[5, 5]


Generate a stream of events
​


Runnable.astream_events
​


# | echo: false
import
 nest_asyncio
nest_asyncio
.
apply
(
)


from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnableParallel
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 
{
"foo"
:
 x
}
,
 name
=
"first"
)
async
 
def
 
func
(
x
)
:
    
for
 _ 
in
 
range
(
5
)
:
        
yield
 x
runnable2 
=
 RunnableLambda
(
func
,
 name
=
"second"
)
chain 
=
 runnable1 
|
 runnable2
async
 
for
 event 
in
 chain
.
astream_events
(
"bar"
,
 version
=
"v2"
)
:
    
print
(
f"event=
{
event
[
'event'
]
}
 | name=
{
event
[
'name'
]
}
 | data=
{
event
[
'data'
]
}
"
)
API Reference:
RunnableLambda
 | 
RunnableParallel


event=on_chain_start | name=RunnableSequence | data={'input': 'bar'}
event=on_chain_start | name=first | data={}
event=on_chain_stream | name=first | data={'chunk': {'foo': 'bar'}}
event=on_chain_start | name=second | data={}
event=on_chain_end | name=first | data={'output': {'foo': 'bar'}, 'input': 'bar'}
event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}
event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}
event=on_chain_end | name=second | data={'output': {'foo': 'bar'}, 'input': {'foo': 'bar'}}
event=on_chain_end | name=RunnableSequence | data={'output': {'foo': 'bar'}}


Yield batched outputs as they complete
​


Runnable.batch_as_completed
 / 
Runnable.abatch_as_completed
​


import
 time
from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnableParallel
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 time
.
sleep
(
x
)
 
or
 
print
(
f"slept 
{
x
}
"
)
)
for
 idx
,
 result 
in
 runnable1
.
batch_as_completed
(
[
5
,
 
1
]
)
:
    
print
(
idx
,
 result
)
# Async variant:
# async for idx, result in runnable1.abatch_as_completed([5, 1]):
#     print(idx, result)
API Reference:
RunnableLambda
 | 
RunnableParallel


slept 1
1 None
slept 5
0 None


Return subset of output dict
​


Runnable.pick
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnablePassthrough
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 x
[
"baz"
]
 
+
 
5
)
chain 
=
 RunnablePassthrough
.
assign
(
foo
=
runnable1
)
.
pick
(
[
"foo"
,
 
"bar"
]
)
chain
.
invoke
(
{
"bar"
:
 
"hi"
,
 
"baz"
:
 
2
}
)
API Reference:
RunnableLambda
 | 
RunnablePassthrough


{'foo': 7, 'bar': 'hi'}


Declaratively make a batched version of a runnable
​


Runnable.map
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 
list
(
range
(
x
)
)
)
runnable2 
=
 RunnableLambda
(
lambda
 x
:
 x 
+
 
5
)
chain 
=
 runnable1 
|
 runnable2
.
map
(
)
chain
.
invoke
(
3
)
API Reference:
RunnableLambda


[5, 6, 7]


Get a graph representation of a runnable
​


Runnable.get_graph
​


from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnableParallel
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 
{
"foo"
:
 x
}
)
runnable2 
=
 RunnableLambda
(
lambda
 x
:
 
[
x
]
 
*
 
2
)
runnable3 
=
 RunnableLambda
(
lambda
 x
:
 
str
(
x
)
)
chain 
=
 runnable1 
|
 RunnableParallel
(
second
=
runnable2
,
 third
=
runnable3
)
chain
.
get_graph
(
)
.
print_ascii
(
)
API Reference:
RunnableLambda
 | 
RunnableParallel


                             +-------------+                              
                             | LambdaInput |                              
                             +-------------+                              
                                    *                                     
                                    *                                     
                                    *                                     
                    +------------------------------+                      
                    | Lambda(lambda x: {'foo': x}) |                      
                    +------------------------------+                      
                                    *                                     
                                    *                                     
                                    *                                     
                     +-----------------------------+                      
                     | Parallel<second,third>Input |                      
                     +-----------------------------+                      
                        ****                  ***                         
                    ****                         ****                     
                  **                                 **                   
+---------------------------+               +--------------------------+  
| Lambda(lambda x: [x] * 2) |               | Lambda(lambda x: str(x)) |  
+---------------------------+               +--------------------------+  
                        ****                  ***                         
                            ****          ****                            
                                **      **                                
                    +------------------------------+                      
                    | Parallel<second,third>Output |                      
                    +------------------------------+


Get all prompts in a chain
​


Runnable.get_prompts
​


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnableLambda
prompt1 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 
"good ai"
)
,
 
(
"human"
,
 
"{input}"
)
]
)
prompt2 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"really good ai"
)
,
        
(
"human"
,
 
"{input}"
)
,
        
(
"ai"
,
 
"{ai_output}"
)
,
        
(
"human"
,
 
"{input2}"
)
,
    
]
)
fake_llm 
=
 RunnableLambda
(
lambda
 prompt
:
 
"i am good ai"
)
chain 
=
 prompt1
.
assign
(
ai_output
=
fake_llm
)
 
|
 prompt2 
|
 fake_llm
for
 i
,
 prompt 
in
 
enumerate
(
chain
.
get_prompts
(
)
)
:
    
print
(
f"**prompt 
{
i
=
}
**\n"
)
    
print
(
prompt
.
pretty_repr
(
)
)
    
print
(
"\n"
 
*
 
3
)
API Reference:
ChatPromptTemplate
 | 
RunnableLambda


**prompt i=0**
================================ System Message ================================
good ai
================================ Human Message =================================
{input}
**prompt i=1**
================================ System Message ================================
really good ai
================================ Human Message =================================
{input}
================================== AI Message ==================================
{ai_output}
================================ Human Message =================================
{input2}


Add lifecycle listeners
​


Runnable.with_listeners
​


import
 time
from
 langchain_core
.
runnables 
import
 RunnableLambda
from
 langchain_core
.
tracers
.
schemas 
import
 Run
def
 
on_start
(
run_obj
:
 Run
)
:
    
print
(
"start_time:"
,
 run_obj
.
start_time
)
def
 
on_end
(
run_obj
:
 Run
)
:
    
print
(
"end_time:"
,
 run_obj
.
end_time
)
runnable1 
=
 RunnableLambda
(
lambda
 x
:
 time
.
sleep
(
x
)
)
chain 
=
 runnable1
.
with_listeners
(
on_start
=
on_start
,
 on_end
=
on_end
)
chain
.
invoke
(
2
)
API Reference:
RunnableLambda
 | 
Run


start_time: 2024-05-17 23:04:00.951065+00:00
end_time: 2024-05-17 23:04:02.958765+00:00
Edit this page
Was this page helpful?
Previous
How to inspect runnables
Next
How to cache LLM responses
Invoke a runnable
Batch a runnable
Stream a runnable
Compose runnables
Invoke runnables in parallel
Turn any function into a runnable
Merge input and output dicts
Include input dict in output dict
Add default invocation args
Add fallbacks
Add retries
Configure runnable execution
Add default config to runnable
Make runnable attributes configurable
Make chain components configurable
Build a chain dynamically based on input
Generate a stream of events
Yield batched outputs as they complete
Return subset of output dict
Declaratively make a batched version of a runnable
Get a graph representation of a runnable
Get all prompts in a chain
Add lifecycle listeners


--- docs_how_to_llm_caching.txt ---

How-to guides
How to cache LLM responses
On this page
How to cache LLM responses


LangChain provides an optional 
caching
 layer for LLMs. This is useful for two reasons:


It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times.
It can speed up your application by reducing the number of API calls you make to the LLM provider.


%
pip install 
-
qU langchain_openai langchain_community
import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)
# Please manually enter OpenAI Key


from
 langchain_core
.
globals
 
import
 set_llm_cache
from
 langchain_openai 
import
 OpenAI
# To make the caching really obvious, lets use a slower and older model.
# Caching supports newer chat models as well.
llm 
=
 OpenAI
(
model
=
"gpt-3.5-turbo-instruct"
,
 n
=
2
,
 best_of
=
2
)
API Reference:
set_llm_cache
 | 
OpenAI


%
%
time
from
 langchain_core
.
caches 
import
 InMemoryCache
set_llm_cache
(
InMemoryCache
(
)
)
# The first time, it is not yet in cache, so it should take longer
llm
.
invoke
(
"Tell me a joke"
)
API Reference:
InMemoryCache


CPU times: user 546 ms, sys: 379 ms, total: 925 ms
Wall time: 1.11 s


"\nWhy don't scientists trust atoms?\n\nBecause they make up everything!"


%
%
time
# The second time it is, so it goes faster
llm
.
invoke
(
"Tell me a joke"
)


CPU times: user 192 µs, sys: 77 µs, total: 269 µs
Wall time: 270 µs


"\nWhy don't scientists trust atoms?\n\nBecause they make up everything!"


SQLite Cache
​


!rm 
.
langchain
.
db


# We can do the same thing with a SQLite cache
from
 langchain_community
.
cache 
import
 SQLiteCache
set_llm_cache
(
SQLiteCache
(
database_path
=
".langchain.db"
)
)
API Reference:
SQLiteCache


%
%
time
# The first time, it is not yet in cache, so it should take longer
llm
.
invoke
(
"Tell me a joke"
)


CPU times: user 10.6 ms, sys: 4.21 ms, total: 14.8 ms
Wall time: 851 ms


"\n\nWhy don't scientists trust atoms?\n\nBecause they make up everything!"


%
%
time
# The second time it is, so it goes faster
llm
.
invoke
(
"Tell me a joke"
)


CPU times: user 59.7 ms, sys: 63.6 ms, total: 123 ms
Wall time: 134 ms


"\n\nWhy don't scientists trust atoms?\n\nBecause they make up everything!"
Edit this page
Was this page helpful?
Previous
LangChain Expression Language Cheatsheet
Next
How to track token usage for LLMs
SQLite Cache


--- docs_how_to_llm_token_usage_tracking.txt ---

How-to guides
How to track token usage for LLMs
On this page
How to track token usage for LLMs


Tracking 
token
 usage to calculate cost is an important part of putting your app in production. This guide goes over how to obtain this information from your LangChain model calls.


Prerequisites
This guide assumes familiarity with the following concepts:


LLMs




Using LangSmith
​


You can use 
LangSmith
 to help track token usage in your LLM application. See the 
LangSmith quick start guide
.


Using callbacks
​


There are some API-specific callback context managers that allow you to track token usage across multiple calls. You'll need to check whether such an integration is available for your particular model.


If such an integration is not available for your model, you can create a custom callback manager by adapting the implementation of the 
OpenAI callback manager
.


OpenAI
​


Let's first look at an extremely simple example of tracking token usage for a single Chat model call.


danger
The callback handler does not currently support streaming token counts for legacy language models (e.g., 
langchain_openai.OpenAI
). For support in a streaming context, refer to the corresponding guide for chat models 
here
.


Single call
​


from
 langchain_community
.
callbacks 
import
 get_openai_callback
from
 langchain_openai 
import
 OpenAI
llm 
=
 OpenAI
(
model_name
=
"gpt-3.5-turbo-instruct"
)
with
 get_openai_callback
(
)
 
as
 cb
:
    result 
=
 llm
.
invoke
(
"Tell me a joke"
)
    
print
(
result
)
    
print
(
"---"
)
print
(
)
print
(
f"Total Tokens: 
{
cb
.
total_tokens
}
"
)
print
(
f"Prompt Tokens: 
{
cb
.
prompt_tokens
}
"
)
print
(
f"Completion Tokens: 
{
cb
.
completion_tokens
}
"
)
print
(
f"Total Cost (USD): $
{
cb
.
total_cost
}
"
)
API Reference:
get_openai_callback
 | 
OpenAI


Why don't scientists trust atoms?
Because they make up everything.
---
Total Tokens: 18
Prompt Tokens: 4
Completion Tokens: 14
Total Cost (USD): $3.4e-05


Multiple calls
​


Anything inside the context manager will get tracked. Here's an example of using it to track multiple calls in sequence to a chain. This will also work for an agent which may use multiple steps.


from
 langchain_community
.
callbacks 
import
 get_openai_callback
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_openai 
import
 OpenAI
llm 
=
 OpenAI
(
model_name
=
"gpt-3.5-turbo-instruct"
)
template 
=
 PromptTemplate
.
from_template
(
"Tell me a joke about {topic}"
)
chain 
=
 template 
|
 llm
with
 get_openai_callback
(
)
 
as
 cb
:
    response 
=
 chain
.
invoke
(
{
"topic"
:
 
"birds"
}
)
    
print
(
response
)
    response 
=
 chain
.
invoke
(
{
"topic"
:
 
"fish"
}
)
    
print
(
"--"
)
    
print
(
response
)
print
(
)
print
(
"---"
)
print
(
f"Total Tokens: 
{
cb
.
total_tokens
}
"
)
print
(
f"Prompt Tokens: 
{
cb
.
prompt_tokens
}
"
)
print
(
f"Completion Tokens: 
{
cb
.
completion_tokens
}
"
)
print
(
f"Total Cost (USD): $
{
cb
.
total_cost
}
"
)
API Reference:
get_openai_callback
 | 
PromptTemplate
 | 
OpenAI


Why did the chicken go to the seance?
To talk to the other side of the road!
--
Why did the fish need a lawyer?
Because it got caught in a net!
---
Total Tokens: 50
Prompt Tokens: 12
Completion Tokens: 38
Total Cost (USD): $9.400000000000001e-05


Streaming
​


danger
get_openai_callback
 does not currently support streaming token counts for legacy language models (e.g., 
langchain_openai.OpenAI
). If you want to count tokens correctly in a streaming context, there are a number of options:


Use chat models as described in 
this guide
;


Implement a 
custom callback handler
 that uses appropriate tokenizers to count the tokens;


Use a monitoring platform such as 
LangSmith
.




Note that when using legacy language models in a streaming context, token counts are not updated:


from
 langchain_community
.
callbacks 
import
 get_openai_callback
from
 langchain_openai 
import
 OpenAI
llm 
=
 OpenAI
(
model_name
=
"gpt-3.5-turbo-instruct"
)
with
 get_openai_callback
(
)
 
as
 cb
:
    
for
 chunk 
in
 llm
.
stream
(
"Tell me a joke"
)
:
        
print
(
chunk
,
 end
=
""
,
 flush
=
True
)
    
print
(
result
)
    
print
(
"---"
)
print
(
)
print
(
f"Total Tokens: 
{
cb
.
total_tokens
}
"
)
print
(
f"Prompt Tokens: 
{
cb
.
prompt_tokens
}
"
)
print
(
f"Completion Tokens: 
{
cb
.
completion_tokens
}
"
)
print
(
f"Total Cost (USD): $
{
cb
.
total_cost
}
"
)
API Reference:
get_openai_callback
 | 
OpenAI


Why don't scientists trust atoms?
Because they make up everything!
Why don't scientists trust atoms?
Because they make up everything.
---
Total Tokens: 0
Prompt Tokens: 0
Completion Tokens: 0
Total Cost (USD): $0.0
Edit this page
Was this page helpful?
Previous
How to cache LLM responses
Next
Run models locally
Using LangSmith
Using callbacks
OpenAI
Single call
Multiple calls
Streaming


--- docs_how_to_local_llms.txt ---

How-to guides
Run models locally
On this page
Run models locally


Use case
​


The popularity of projects like 
llama.cpp
, 
Ollama
, 
GPT4All
, 
llamafile
, and others underscore the demand to run LLMs locally (on your own device).


This has at least two important benefits:




Privacy
: Your data is not sent to a third party, and it is not subject to the terms of service of a commercial service


Cost
: There is no inference fee, which is important for token-intensive applications (e.g., 
long-running simulations
, summarization)




Overview
​


Running an LLM locally requires a few things:




Open-source LLM
: An open-source LLM that can be freely modified and shared


Inference
: Ability to run this LLM on your device w/ acceptable latency




Open-source LLMs
​


Users can now gain access to a rapidly growing set of 
open-source LLMs
.


These LLMs can be assessed across at least two dimensions (see figure):




Base model
: What is the base-model and how was it trained?


Fine-tuning approach
: Was the base-model fine-tuned and, if so, what 
set of instructions
 was used?






The relative performance of these models can be assessed using several leaderboards, including:




LmSys


GPT4All


HuggingFace




Inference
​


A few frameworks for this have emerged to support inference of open-source LLMs on various devices:




llama.cpp
: C++ implementation of llama inference code with 
weight optimization / quantization


gpt4all
: Optimized C backend for inference


Ollama
: Bundles model weights and environment into an app that runs on device and serves the LLM


llamafile
: Bundles model weights and everything needed to run the model in a single file, allowing you to run the LLM locally from this file without any additional installation steps




In general, these frameworks will do a few things:




Quantization
: Reduce the memory footprint of the raw model weights


Efficient implementation for inference
: Support inference on consumer hardware (e.g., CPU or laptop GPU)




In particular, see 
this excellent post
 on the importance of quantization.




With less precision, we radically decrease the memory needed to store the LLM in memory.


In addition, we can see the importance of GPU memory bandwidth 
sheet
!


A Mac M2 Max is 5-6x faster than a M1 for inference due to the larger GPU memory bandwidth.




Formatting prompts
​


Some providers have 
chat model
 wrappers that takes care of formatting your input prompt for the specific local model you're using. However, if you are prompting local models with a 
text-in/text-out LLM
 wrapper, you may need to use a prompt tailored for your specific model.


This can 
require the inclusion of special tokens
. 
Here's an example for LLaMA 2
.


Quickstart
​


Ollama
 is one way to easily run inference on macOS.


The instructions 
here
 provide details, which we summarize:




Download and run
 the app


From command line, fetch a model from this 
list of options
: e.g., 
ollama pull llama3.1:8b


When the app is running, all models are automatically served on 
localhost:11434




%
pip install 
-
qU langchain_ollama


from
 langchain_ollama 
import
 OllamaLLM
llm 
=
 OllamaLLM
(
model
=
"llama3.1:8b"
)
llm
.
invoke
(
"The first man on the moon was ..."
)
API Reference:
OllamaLLM


'...Neil Armstrong!\n\nOn July 20, 1969, Neil Armstrong became the first person to set foot on the lunar surface, famously declaring "That\'s one small step for man, one giant leap for mankind" as he stepped off the lunar module Eagle onto the Moon\'s surface.\n\nWould you like to know more about the Apollo 11 mission or Neil Armstrong\'s achievements?'


Stream tokens as they are being generated:


for
 chunk 
in
 llm
.
stream
(
"The first man on the moon was ..."
)
:
    
print
(
chunk
,
 end
=
"|"
,
 flush
=
True
)


...|
``````output
Neil| Armstrong|,| an| American| astronaut|.| He| stepped| out| of| the| lunar| module| Eagle| and| onto| the| surface| of| the| Moon| on| July| |20|,| |196|9|,| famously| declaring|:| "|That|'s| one| small| step| for| man|,| one| giant| leap| for| mankind|."||


Ollama also includes a chat model wrapper that handles formatting conversation turns:


from
 langchain_ollama 
import
 ChatOllama
chat_model 
=
 ChatOllama
(
model
=
"llama3.1:8b"
)
chat_model
.
invoke
(
"Who was the first man on the moon?"
)
API Reference:
ChatOllama


AIMessage(content='The answer is a historic one!\n\nThe first man to walk on the Moon was Neil Armstrong, an American astronaut and commander of the Apollo 11 mission. On July 20, 1969, Armstrong stepped out of the lunar module Eagle onto the surface of the Moon, famously declaring:\n\n"That\'s one small step for man, one giant leap for mankind."\n\nArmstrong was followed by fellow astronaut Edwin "Buzz" Aldrin, who also walked on the Moon during the mission. Michael Collins remained in orbit around the Moon in the command module Columbia.\n\nNeil Armstrong passed away on August 25, 2012, but his legacy as a pioneering astronaut and engineer continues to inspire people around the world!', response_metadata={'model': 'llama3.1:8b', 'created_at': '2024-08-01T00:38:29.176717Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 10681861417, 'load_duration': 34270292, 'prompt_eval_count': 19, 'prompt_eval_duration': 6209448000, 'eval_count': 141, 'eval_duration': 4432022000}, id='run-7bed57c5-7f54-4092-912c-ae49073dcd48-0', usage_metadata={'input_tokens': 19, 'output_tokens': 141, 'total_tokens': 160})


Environment
​


Inference speed is a challenge when running models locally (see above).


To minimize latency, it is desirable to run models locally on GPU, which ships with many consumer laptops 
e.g., Apple devices
.


And even with GPU, the available GPU memory bandwidth (as noted above) is important.


Running Apple silicon GPU
​


Ollama
 and 
llamafile
 will automatically utilize the GPU on Apple devices.


Other frameworks require the user to set up the environment to utilize the Apple GPU.


For example, 
llama.cpp
 python bindings can be configured to use the GPU via 
Metal
.


Metal is a graphics and compute API created by Apple providing near-direct access to the GPU.


See the 
llama.cpp
 setup 
here
 to enable this.


In particular, ensure that conda is using the correct virtual environment that you created (
miniforge3
).


E.g., for me:


conda activate /Users/rlm/miniforge3/envs/llama


With the above confirmed, then:


CMAKE_ARGS="-DLLAMA_METAL=on" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir


LLMs
​


There are various ways to gain access to quantized model weights.




HuggingFace
 - Many quantized model are available for download and can be run with framework such as 
llama.cpp
. You can also download models in 
llamafile
 format
 from HuggingFace.


gpt4all
 - The model explorer offers a leaderboard of metrics and associated quantized models available for download


Ollama
 - Several models can be accessed directly via 
pull




Ollama
​


With 
Ollama
, fetch a model via 
ollama pull <model family>:<tag>
:




E.g., for Llama 2 7b: 
ollama pull llama2
 will download the most basic version of the model (e.g., smallest # parameters and 4 bit quantization)


We can also specify a particular version from the 
model list
, e.g., 
ollama pull llama2:13b


See the full set of parameters on the 
API reference page




llm 
=
 OllamaLLM
(
model
=
"llama2:13b"
)
llm
.
invoke
(
"The first man on the moon was ... think step by step"
)


' Sure! Here\'s the answer, broken down step by step:\n\nThe first man on the moon was... Neil Armstrong.\n\nHere\'s how I arrived at that answer:\n\n1. The first manned mission to land on the moon was Apollo 11.\n2. The mission included three astronauts: Neil Armstrong, Edwin "Buzz" Aldrin, and Michael Collins.\n3. Neil Armstrong was the mission commander and the first person to set foot on the moon.\n4. On July 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\'s surface, famously declaring "That\'s one small step for man, one giant leap for mankind."\n\nSo, the first man on the moon was Neil Armstrong!'


Llama.cpp
​


Llama.cpp is compatible with a 
broad set of models
.


For example, below we run inference on 
llama2-13b
 with 4 bit quantization downloaded from 
HuggingFace
.


As noted above, see the 
API reference
 for the full set of parameters.


From the 
llama.cpp API reference docs
, a few are worth commenting on:


n_gpu_layers
: number of layers to be loaded into GPU memory




Value: 1


Meaning: Only one layer of the model will be loaded into GPU memory (1 is often sufficient).




n_batch
: number of tokens the model should process in parallel




Value: n_batch


Meaning: It's recommended to choose a value between 1 and n_ctx (which in this case is set to 2048)




n_ctx
: Token context window




Value: 2048


Meaning: The model will consider a window of 2048 tokens at a time




f16_kv
: whether the model should use half-precision for the key/value cache




Value: True


Meaning: The model will use half-precision, which can be more memory efficient; Metal only supports True.




%
env CMAKE_ARGS
=
"-DLLAMA_METAL=on"
%
env FORCE_CMAKE
=
1
%
pip install 
-
-
upgrade 
-
-
quiet  llama
-
cpp
-
python 
-
-
no
-
cache
-
dirclear


from
 langchain_community
.
llms 
import
 LlamaCpp
from
 langchain_core
.
callbacks 
import
 CallbackManager
,
 StreamingStdOutCallbackHandler
llm 
=
 LlamaCpp
(
    model_path
=
"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin"
,
    n_gpu_layers
=
1
,
    n_batch
=
512
,
    n_ctx
=
2048
,
    f16_kv
=
True
,
    callback_manager
=
CallbackManager
(
[
StreamingStdOutCallbackHandler
(
)
]
)
,
    verbose
=
True
,
)
API Reference:
LlamaCpp
 | 
CallbackManager
 | 
StreamingStdOutCallbackHandler


The console log will show the below to indicate Metal was enabled properly from steps above:


ggml_metal_init: allocating
ggml_metal_init: using MPS


llm
.
invoke
(
"The first man on the moon was ... Let's think step by step"
)


Llama.generate: prefix-match hit
``````output
 and use logical reasoning to figure out who the first man on the moon was.
Here are some clues:
1. The first man on the moon was an American.
2. He was part of the Apollo 11 mission.
3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.
4. His last name is Armstrong.
Now, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.
Therefore, the first man on the moon was Neil Armstrong!
``````output
llama_print_timings:        load time =  9623.21 ms
llama_print_timings:      sample time =   143.77 ms /   203 runs   (    0.71 ms per token,  1412.01 tokens per second)
llama_print_timings: prompt eval time =   485.94 ms /     7 tokens (   69.42 ms per token,    14.40 tokens per second)
llama_print_timings:        eval time =  6385.16 ms /   202 runs   (   31.61 ms per token,    31.64 tokens per second)
llama_print_timings:       total time =  7279.28 ms


" and use logical reasoning to figure out who the first man on the moon was.\n\nHere are some clues:\n\n1. The first man on the moon was an American.\n2. He was part of the Apollo 11 mission.\n3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\n4. His last name is Armstrong.\n\nNow, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\nTherefore, the first man on the moon was Neil Armstrong!"


GPT4All
​


We can use model weights downloaded from 
GPT4All
 model explorer.


Similar to what is shown above, we can run inference and use 
the API reference
 to set parameters of interest.


%
pip install gpt4all


from
 langchain_community
.
llms 
import
 GPT4All
llm 
=
 GPT4All
(
    model
=
"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin"
)
API Reference:
GPT4All


llm
.
invoke
(
"The first man on the moon was ... Let's think step by step"
)


".\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon's surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these"


llamafile
​


One of the simplest ways to run an LLM locally is using a 
llamafile
. All you need to do is:




Download a llamafile from 
HuggingFace


Make the file executable


Run the file




llamafiles bundle model weights and a 
specially-compiled
 version of 
llama.cpp
 into a single file that can run on most computers without any additional dependencies. They also come with an embedded inference server that provides an 
API
 for interacting with your model.


Here's a simple bash script that shows all 3 setup steps:


# Download a llamafile from HuggingFace
wget https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile
# Make the file executable. On Windows, instead just rename the file to end in ".exe".
chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile
# Start the model server. Listens at http://localhost:8080 by default.
./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser


After you run the above setup steps, you can use LangChain to interact with your model:


from
 langchain_community
.
llms
.
llamafile 
import
 Llamafile
llm 
=
 Llamafile
(
)
llm
.
invoke
(
"The first man on the moon was ... Let's think step by step."
)
API Reference:
Llamafile


"\nFirstly, let's imagine the scene where Neil Armstrong stepped onto the moon. This happened in 1969. The first man on the moon was Neil Armstrong. We already know that.\n2nd, let's take a step back. Neil Armstrong didn't have any special powers. He had to land his spacecraft safely on the moon without injuring anyone or causing any damage. If he failed to do this, he would have been killed along with all those people who were on board the spacecraft.\n3rd, let's imagine that Neil Armstrong successfully landed his spacecraft on the moon and made it back to Earth safely. The next step was for him to be hailed as a hero by his people back home. It took years before Neil Armstrong became an American hero.\n4th, let's take another step back. Let's imagine that Neil Armstrong wasn't hailed as a hero, and instead, he was just forgotten. This happened in the 1970s. Neil Armstrong wasn't recognized for his remarkable achievement on the moon until after he died.\n5th, let's take another step back. Let's imagine that Neil Armstrong didn't die in the 1970s and instead, lived to be a hundred years old. This happened in 2036. In the year 2036, Neil Armstrong would have been a centenarian.\nNow, let's think about the present. Neil Armstrong is still alive. He turned 95 years old on July 20th, 2018. If he were to die now, his achievement of becoming the first human being to set foot on the moon would remain an unforgettable moment in history.\nI hope this helps you understand the significance and importance of Neil Armstrong's achievement on the moon!"


Prompts
​


Some LLMs will benefit from specific prompts.


For example, LLaMA will use 
special tokens
.


We can use 
ConditionalPromptSelector
 to set prompt based on the model type.


# Set our LLM
llm 
=
 LlamaCpp
(
    model_path
=
"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin"
,
    n_gpu_layers
=
1
,
    n_batch
=
512
,
    n_ctx
=
2048
,
    f16_kv
=
True
,
    callback_manager
=
CallbackManager
(
[
StreamingStdOutCallbackHandler
(
)
]
)
,
    verbose
=
True
,
)


Set the associated prompt based upon the model version.


from
 langchain
.
chains
.
prompt_selector 
import
 ConditionalPromptSelector
from
 langchain_core
.
prompts 
import
 PromptTemplate
DEFAULT_LLAMA_SEARCH_PROMPT 
=
 PromptTemplate
(
    input_variables
=
[
"question"
]
,
    template
=
"""<<SYS>> \n You are an assistant tasked with improving Google search \
results. \n <</SYS>> \n\n [INST] Generate THREE Google search queries that \
are similar to this question. The output should be a numbered list of questions \
and each should have a question mark at the end: \n\n {question} [/INST]"""
,
)
DEFAULT_SEARCH_PROMPT 
=
 PromptTemplate
(
    input_variables
=
[
"question"
]
,
    template
=
"""You are an assistant tasked with improving Google search \
results. Generate THREE Google search queries that are similar to \
this question. The output should be a numbered list of questions and each \
should have a question mark at the end: {question}"""
,
)
QUESTION_PROMPT_SELECTOR 
=
 ConditionalPromptSelector
(
    default_prompt
=
DEFAULT_SEARCH_PROMPT
,
    conditionals
=
[
(
lambda
 llm
:
 
isinstance
(
llm
,
 LlamaCpp
)
,
 DEFAULT_LLAMA_SEARCH_PROMPT
)
]
,
)
prompt 
=
 QUESTION_PROMPT_SELECTOR
.
get_prompt
(
llm
)
prompt
API Reference:
ConditionalPromptSelector
 | 
PromptTemplate


PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='<<SYS>> \n You are an assistant tasked with improving Google search results. \n <</SYS>> \n\n [INST] Generate THREE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: \n\n {question} [/INST]', template_format='f-string', validate_template=True)


# Chain
chain 
=
 prompt 
|
 llm
question 
=
 
"What NFL team won the Super Bowl in the year that Justin Bieber was born?"
chain
.
invoke
(
{
"question"
:
 question
}
)


  Sure! Here are three similar search queries with a question mark at the end:
1. Which NBA team did LeBron James lead to a championship in the year he was drafted?
2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?
3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?
``````output
llama_print_timings:        load time = 14943.19 ms
llama_print_timings:      sample time =    72.93 ms /   101 runs   (    0.72 ms per token,  1384.87 tokens per second)
llama_print_timings: prompt eval time = 14942.95 ms /    93 tokens (  160.68 ms per token,     6.22 tokens per second)
llama_print_timings:        eval time =  3430.85 ms /   100 runs   (   34.31 ms per token,    29.15 tokens per second)
llama_print_timings:       total time = 18578.26 ms


'  Sure! Here are three similar search queries with a question mark at the end:\n\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?'


We also can use the LangChain Prompt Hub to fetch and / or store prompts that are model specific.


This will work with your 
LangSmith API key
.


For example, 
here
 is a prompt for RAG with LLaMA-specific tokens.


Use cases
​


Given an 
llm
 created from one of the models above, you can use it for 
many use cases
.


For example, you can implement a 
RAG application
 using the chat models demonstrated here.


In general, use cases for local LLMs can be driven by at least two factors:




Privacy
: private data (e.g., journals, etc) that a user does not want to share


Cost
: text preprocessing (extraction/tagging), summarization, and agent simulations are token-use-intensive tasks




In addition, 
here
 is an overview on fine-tuning, which can utilize open-source LLMs.
Edit this page
Was this page helpful?
Previous
How to track token usage for LLMs
Next
How to get log probabilities
Use case
Overview
Open-source LLMs
Inference
Formatting prompts
Quickstart
Environment
Running Apple silicon GPU
LLMs
Ollama
Llama.cpp
GPT4All
llamafile
Prompts
Use cases


--- docs_how_to_logprobs.txt ---

How-to guides
How to get log probabilities
On this page
How to get log probabilities


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


Tokens




Certain 
chat models
 can be configured to return token-level log probabilities representing the likelihood of a given token. This guide walks through how to get this information in LangChain.


OpenAI
​


Install the LangChain x OpenAI package and set your API key


%
pip install 
-
qU langchain
-
openai


import
 getpass
import
 os
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


For the OpenAI API to return log probabilities we need to configure the 
logprobs=True
 param. Then, the logprobs are included on each output 
AIMessage
 as part of the 
response_metadata
:


from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
.
bind
(
logprobs
=
True
)
msg 
=
 llm
.
invoke
(
(
"human"
,
 
"how are you today"
)
)
msg
.
response_metadata
[
"logprobs"
]
[
"content"
]
[
:
5
]
API Reference:
ChatOpenAI


[{'token': 'I', 'bytes': [73], 'logprob': -0.26341408, 'top_logprobs': []},
 {'token': "'m",
  'bytes': [39, 109],
  'logprob': -0.48584133,
  'top_logprobs': []},
 {'token': ' just',
  'bytes': [32, 106, 117, 115, 116],
  'logprob': -0.23484154,
  'top_logprobs': []},
 {'token': ' a',
  'bytes': [32, 97],
  'logprob': -0.0018291725,
  'top_logprobs': []},
 {'token': ' computer',
  'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114],
  'logprob': -0.052299336,
  'top_logprobs': []}]


And are part of streamed Message chunks as well:


ct 
=
 
0
full 
=
 
None
for
 chunk 
in
 llm
.
stream
(
(
"human"
,
 
"how are you today"
)
)
:
    
if
 ct 
<
 
5
:
        full 
=
 chunk 
if
 full 
is
 
None
 
else
 full 
+
 chunk
        
if
 
"logprobs"
 
in
 full
.
response_metadata
:
            
print
(
full
.
response_metadata
[
"logprobs"
]
[
"content"
]
)
    
else
:
        
break
    ct 
+=
 
1


[]
[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}]
[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': "'m", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}]
[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': "'m", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}]
[{'token': 'I', 'bytes': [73], 'logprob': -0.26593843, 'top_logprobs': []}, {'token': "'m", 'bytes': [39, 109], 'logprob': -0.3238896, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.23778509, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.0022134194, 'top_logprobs': []}]


Next steps
​


You've now learned how to get logprobs from OpenAI models in LangChain.


Next, check out the other how-to guides chat models in this section, like 
how to get a model to return structured output
 or 
how to track token usage
.
Edit this page
Was this page helpful?
Previous
Run models locally
Next
How to reorder retrieved results to mitigate the "lost in the middle" effect
OpenAI
Next steps


--- docs_how_to_long_context_reorder.txt ---

How-to guides
How to reorder retrieved results to mitigate the "lost in the middle" effect
How to reorder retrieved results to mitigate the "lost in the middle" effect


Substantial performance degradations in 
RAG
 applications have been 
documented
 as the number of retrieved documents grows (e.g., beyond ten). In brief: models are liable to miss relevant information in the middle of long contexts.


By contrast, queries against vector stores will typically return documents in descending order of relevance (e.g., as measured by cosine similarity of 
embeddings
).


To mitigate the 
"lost in the middle"
 effect, you can re-order documents after retrieval such that the most relevant documents are positioned at extrema (e.g., the first and last pieces of context), and the least relevant documents are positioned in the middle. In some cases this can help surface the most relevant information to LLMs.


The 
LongContextReorder
 document transformer implements this re-ordering procedure. Below we demonstrate an example.


%
pip install 
-
qU langchain langchain
-
community langchain
-
openai


First we embed some artificial documents and index them in a basic in-memory vector store. We will use 
OpenAI
 embeddings, but any LangChain vector store or embeddings model will suffice.


from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
from
 langchain_openai 
import
 OpenAIEmbeddings
# Get embeddings.
embeddings 
=
 OpenAIEmbeddings
(
)
texts 
=
 
[
    
"Basquetball is a great sport."
,
    
"Fly me to the moon is one of my favourite songs."
,
    
"The Celtics are my favourite team."
,
    
"This is a document about the Boston Celtics"
,
    
"I simply love going to the movies"
,
    
"The Boston Celtics won the game by 20 points"
,
    
"This is just a random text."
,
    
"Elden Ring is one of the best games in the last 15 years."
,
    
"L. Kornet is one of the best Celtics players."
,
    
"Larry Bird was an iconic NBA player."
,
]
# Create a retriever
retriever 
=
 InMemoryVectorStore
.
from_texts
(
texts
,
 embedding
=
embeddings
)
.
as_retriever
(
    search_kwargs
=
{
"k"
:
 
10
}
)
query 
=
 
"What can you tell me about the Celtics?"
# Get relevant documents ordered by relevance score
docs 
=
 retriever
.
invoke
(
query
)
for
 doc 
in
 docs
:
    
print
(
f"- 
{
doc
.
page_content
}
"
)
API Reference:
InMemoryVectorStore
 | 
OpenAIEmbeddings


- The Celtics are my favourite team.
- This is a document about the Boston Celtics
- The Boston Celtics won the game by 20 points
- L. Kornet is one of the best Celtics players.
- Basquetball is a great sport.
- Larry Bird was an iconic NBA player.
- This is just a random text.
- I simply love going to the movies
- Fly me to the moon is one of my favourite songs.
- Elden Ring is one of the best games in the last 15 years.


Note that documents are returned in descending order of relevance to the query. The 
LongContextReorder
 document transformer will implement the re-ordering described above:


from
 langchain_community
.
document_transformers 
import
 LongContextReorder
# Reorder the documents:
# Less relevant document will be at the middle of the list and more
# relevant elements at beginning / end.
reordering 
=
 LongContextReorder
(
)
reordered_docs 
=
 reordering
.
transform_documents
(
docs
)
# Confirm that the 4 relevant documents are at beginning and end.
for
 doc 
in
 reordered_docs
:
    
print
(
f"- 
{
doc
.
page_content
}
"
)
API Reference:
LongContextReorder


- This is a document about the Boston Celtics
- L. Kornet is one of the best Celtics players.
- Larry Bird was an iconic NBA player.
- I simply love going to the movies
- Elden Ring is one of the best games in the last 15 years.
- Fly me to the moon is one of my favourite songs.
- This is just a random text.
- Basquetball is a great sport.
- The Boston Celtics won the game by 20 points
- The Celtics are my favourite team.


Below, we show how to incorporate the re-ordered documents into a simple question-answering chain:


from
 langchain
.
chains
.
combine_documents 
import
 create_stuff_documents_chain
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
prompt_template 
=
 
"""
Given these texts:
-----
{context}
-----
Please answer the following question:
{query}
"""
prompt 
=
 PromptTemplate
(
    template
=
prompt_template
,
    input_variables
=
[
"context"
,
 
"query"
]
,
)
# Create and invoke the chain:
chain 
=
 create_stuff_documents_chain
(
llm
,
 prompt
)
response 
=
 chain
.
invoke
(
{
"context"
:
 reordered_docs
,
 
"query"
:
 query
}
)
print
(
response
)
API Reference:
create_stuff_documents_chain
 | 
PromptTemplate
 | 
ChatOpenAI


The Boston Celtics are a professional basketball team known for their rich history and success in the NBA. L. Kornet is recognized as one of the best players on the team, and the Celtics recently won a game by 20 points. The Celtics are favored by some fans, as indicated by the statement, "The Celtics are my favourite team." Overall, they have a strong following and are considered a significant part of basketball culture.
Edit this page
Was this page helpful?
Previous
How to get log probabilities
Next
How to split Markdown by Headers


--- docs_how_to_markdown_header_metadata_splitter.txt ---

How-to guides
How to split Markdown by Headers
On this page
How to split Markdown by Headers


Motivation
​


Many chat or Q+A applications involve chunking input documents prior to embedding and vector storage.


These notes
 from Pinecone provide some useful tips:


When a full paragraph or document is embedded, the embedding process considers both the overall context and the relationships between the sentences and phrases within the text. This can result in a more comprehensive vector representation that captures the broader meaning and themes of the text.


As mentioned, chunking often aims to keep text with common context together. With this in mind, we might want to specifically honor the structure of the document itself. For example, a markdown file is organized by headers. Creating chunks within specific header groups is an intuitive idea. To address this challenge, we can use 
MarkdownHeaderTextSplitter
. This will split a markdown file by a specified set of headers.


For example, if we want to split this markdown:


md = '# Foo\n\n ## Bar\n\nHi this is Jim  \nHi this is Joe\n\n ## Baz\n\n Hi this is Molly' 


We can specify the headers to split on:


[("#", "Header 1"),("##", "Header 2")]


And content is grouped or split by common headers:


{'content': 'Hi this is Jim  \nHi this is Joe', 'metadata': {'Header 1': 'Foo', 'Header 2': 'Bar'}}
{'content': 'Hi this is Molly', 'metadata': {'Header 1': 'Foo', 'Header 2': 'Baz'}}


Let's have a look at some examples below.


Basic usage:
​


%
pip install 
-
qU langchain
-
text
-
splitters


from
 langchain_text_splitters 
import
 MarkdownHeaderTextSplitter
API Reference:
MarkdownHeaderTextSplitter


markdown_document 
=
 
"# Foo\n\n    ## Bar\n\nHi this is Jim\n\nHi this is Joe\n\n ### Boo \n\n Hi this is Lance \n\n ## Baz\n\n Hi this is Molly"
headers_to_split_on 
=
 
[
    
(
"#"
,
 
"Header 1"
)
,
    
(
"##"
,
 
"Header 2"
)
,
    
(
"###"
,
 
"Header 3"
)
,
]
markdown_splitter 
=
 MarkdownHeaderTextSplitter
(
headers_to_split_on
)
md_header_splits 
=
 markdown_splitter
.
split_text
(
markdown_document
)
md_header_splits


[Document(page_content='Hi this is Jim  \nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),
 Document(page_content='Hi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),
 Document(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]


type
(
md_header_splits
[
0
]
)


langchain_core.documents.base.Document


By default, 
MarkdownHeaderTextSplitter
 strips headers being split on from the output chunk's content. This can be disabled by setting 
strip_headers = False
.


markdown_splitter 
=
 MarkdownHeaderTextSplitter
(
headers_to_split_on
,
 strip_headers
=
False
)
md_header_splits 
=
 markdown_splitter
.
split_text
(
markdown_document
)
md_header_splits


[Document(page_content='# Foo  \n## Bar  \nHi this is Jim  \nHi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),
 Document(page_content='### Boo  \nHi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),
 Document(page_content='## Baz  \nHi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]


note
The default 
MarkdownHeaderTextSplitter
 strips white spaces and new lines. To preserve the original formatting of your Markdown documents, check out 
ExperimentalMarkdownSyntaxTextSplitter
.


How to return Markdown lines as separate documents
​


By default, 
MarkdownHeaderTextSplitter
 aggregates lines based on the headers specified in 
headers_to_split_on
. We can disable this by specifying 
return_each_line
:


markdown_splitter 
=
 MarkdownHeaderTextSplitter
(
    headers_to_split_on
,
    return_each_line
=
True
,
)
md_header_splits 
=
 markdown_splitter
.
split_text
(
markdown_document
)
md_header_splits


[Document(page_content='Hi this is Jim', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),
 Document(page_content='Hi this is Joe', metadata={'Header 1': 'Foo', 'Header 2': 'Bar'}),
 Document(page_content='Hi this is Lance', metadata={'Header 1': 'Foo', 'Header 2': 'Bar', 'Header 3': 'Boo'}),
 Document(page_content='Hi this is Molly', metadata={'Header 1': 'Foo', 'Header 2': 'Baz'})]


Note that here header information is retained in the 
metadata
 for each document.


How to constrain chunk size:
​


Within each markdown group we can then apply any text splitter we want, such as 
RecursiveCharacterTextSplitter
, which allows for further control of the chunk size.


markdown_document 
=
 
"# Intro \n\n    ## History \n\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \n\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \n\n ## Rise and divergence \n\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \n\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \n\n #### Standardization \n\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \n\n ## Implementations \n\n Implementations of Markdown are available for over a dozen programming languages."
headers_to_split_on 
=
 
[
    
(
"#"
,
 
"Header 1"
)
,
    
(
"##"
,
 
"Header 2"
)
,
]
# MD splits
markdown_splitter 
=
 MarkdownHeaderTextSplitter
(
    headers_to_split_on
=
headers_to_split_on
,
 strip_headers
=
False
)
md_header_splits 
=
 markdown_splitter
.
split_text
(
markdown_document
)
# Char-level splits
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
chunk_size 
=
 
250
chunk_overlap 
=
 
30
text_splitter 
=
 RecursiveCharacterTextSplitter
(
    chunk_size
=
chunk_size
,
 chunk_overlap
=
chunk_overlap
)
# Split
splits 
=
 text_splitter
.
split_documents
(
md_header_splits
)
splits
API Reference:
RecursiveCharacterTextSplitter


[Document(page_content='# Intro  \n## History  \nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]', metadata={'Header 1': 'Intro', 'Header 2': 'History'}),
 Document(page_content='Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.', metadata={'Header 1': 'Intro', 'Header 2': 'History'}),
 Document(page_content='## Rise and divergence  \nAs Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for  \nadditional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}),
 Document(page_content='#### Standardization  \nFrom 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.', metadata={'Header 1': 'Intro', 'Header 2': 'Rise and divergence'}),
 Document(page_content='## Implementations  \nImplementations of Markdown are available for over a dozen programming languages.', metadata={'Header 1': 'Intro', 'Header 2': 'Implementations'})]
Edit this page
Was this page helpful?
Previous
How to reorder retrieved results to mitigate the "lost in the middle" effect
Next
How to merge consecutive messages of the same type
Motivation
Basic usage:
How to return Markdown lines as separate documents
How to constrain chunk size:


--- docs_how_to_merge_message_runs.txt ---

How-to guides
How to merge consecutive messages of the same type
On this page
How to merge consecutive messages of the same type


Certain models do not support passing in consecutive 
messages
 of the same type (a.k.a. "runs" of the same message type).


The 
merge_message_runs
 utility makes it easy to merge consecutive messages of the same type.


Setup
​


%
pip install 
-
qU langchain
-
core langchain
-
anthropic


Basic usage
​


from
 langchain_core
.
messages 
import
 
(
    AIMessage
,
    HumanMessage
,
    SystemMessage
,
    merge_message_runs
,
)
messages 
=
 
[
    SystemMessage
(
"you're a good assistant."
)
,
    SystemMessage
(
"you always respond with a joke."
)
,
    HumanMessage
(
[
{
"type"
:
 
"text"
,
 
"text"
:
 
"i wonder why it's called langchain"
}
]
)
,
    HumanMessage
(
"and who is harrison chasing anyways"
)
,
    AIMessage
(
        
'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    
)
,
    AIMessage
(
"Why, he's probably chasing after the last cup of coffee in the office!"
)
,
]
merged 
=
 merge_message_runs
(
messages
)
print
(
"\n\n"
.
join
(
[
repr
(
x
)
 
for
 x 
in
 merged
]
)
)
API Reference:
AIMessage
 | 
HumanMessage
 | 
SystemMessage
 | 
merge_message_runs


SystemMessage(content="you're a good assistant.\nyou always respond with a joke.", additional_kwargs={}, response_metadata={})
HumanMessage(content=[{'type': 'text', 'text': "i wonder why it's called langchain"}, 'and who is harrison chasing anyways'], additional_kwargs={}, response_metadata={})
AIMessage(content='Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!\nWhy, he\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})


Notice that if the contents of one of the messages to merge is a list of content blocks then the merged message will have a list of content blocks. And if both messages to merge have string contents then those are concatenated with a newline character.


Chaining
​


merge_message_runs
 can be used in an imperatively (like above) or declaratively, making it easy to compose with other components in a chain:


from
 langchain_anthropic 
import
 ChatAnthropic
llm 
=
 ChatAnthropic
(
model
=
"claude-3-sonnet-20240229"
,
 temperature
=
0
)
# Notice we don't pass in messages. This creates
# a RunnableLambda that takes messages as input
merger 
=
 merge_message_runs
(
)
chain 
=
 merger 
|
 llm
chain
.
invoke
(
messages
)
API Reference:
ChatAnthropic


Note: you may need to restart the kernel to use updated packages.


AIMessage(content=[], additional_kwargs={}, response_metadata={'id': 'msg_01KNGUMTuzBVfwNouLDpUMwf', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 84, 'output_tokens': 3}}, id='run-b908b198-9c24-450b-9749-9d4a8182937b-0', usage_metadata={'input_tokens': 84, 'output_tokens': 3, 'total_tokens': 87})


Looking at the LangSmith trace we can see that before the messages are passed to the model they are merged: 
https://smith.langchain.com/public/ab558677-cac9-4c59-9066-1ecce5bcd87c/r


Looking at just the merger, we can see that it's a Runnable object that can be invoked like all Runnables:


merger
.
invoke
(
messages
)


[SystemMessage(content="you're a good assistant.\nyou always respond with a joke.", additional_kwargs={}, response_metadata={}),
 HumanMessage(content=[{'type': 'text', 'text': "i wonder why it's called langchain"}, 'and who is harrison chasing anyways'], additional_kwargs={}, response_metadata={}),
 AIMessage(content='Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!\nWhy, he\'s probably chasing after the last cup of coffee in the office!', additional_kwargs={}, response_metadata={})]


merge_message_runs
 can also be placed after a prompt:


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
prompt 
=
 ChatPromptTemplate
(
    
[
        
(
"system"
,
 
"You're great a {skill}"
)
,
        
(
"system"
,
 
"You're also great at explaining things"
)
,
        
(
"human"
,
 
"{query}"
)
,
    
]
)
chain 
=
 prompt 
|
 merger 
|
 llm
chain
.
invoke
(
{
"skill"
:
 
"math"
,
 
"query"
:
 
"what's the definition of a convergent series"
}
)
API Reference:
ChatPromptTemplate


AIMessage(content='A convergent series is an infinite series whose partial sums approach a finite value as more terms are added. In other words, the sequence of partial sums has a limit.\n\nMore formally, an infinite series Σ an (where an are the terms of the series) is said to be convergent if the sequence of partial sums:\n\nS1 = a1\nS2 = a1 + a2  \nS3 = a1 + a2 + a3\n...\nSn = a1 + a2 + a3 + ... + an\n...\n\nconverges to some finite number S as n goes to infinity. We write:\n\nlim n→∞ Sn = S\n\nThe finite number S is called the sum of the convergent infinite series.\n\nIf the sequence of partial sums does not approach any finite limit, the infinite series is said to be divergent.\n\nSome key properties:\n- A series converges if and only if the sequence of its partial sums is a Cauchy sequence.\n- Absolute/conditional convergence criteria help determine if a given series converges.\n- Convergent series have many important applications in mathematics, physics, engineering etc.', additional_kwargs={}, response_metadata={'id': 'msg_01MfV6y2hep7ZNvDz24A36U4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 29, 'output_tokens': 267}}, id='run-9d925f58-021e-4bd0-94fc-f8f5e91010a4-0', usage_metadata={'input_tokens': 29, 'output_tokens': 267, 'total_tokens': 296})


LangSmith Trace: 
https://smith.langchain.com/public/432150b6-9909-40a7-8ae7-944b7e657438/r/f4ad5fb2-4d38-42a6-b780-25f62617d53f


API reference
​


For a complete description of all arguments head to the API reference: 
https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.merge_message_runs.html
Edit this page
Was this page helpful?
Previous
How to split Markdown by Headers
Next
How to add message history
Setup
Basic usage
Chaining
API reference


--- docs_how_to_message_history.txt ---

How-to guides
How to add message history
On this page
How to add message history


Prerequisites
This guide assumes familiarity with the following concepts:


Chaining runnables


Prompt templates


Chat Messages


LangGraph persistence




note
This guide previously covered the 
RunnableWithMessageHistory
 abstraction. You can access this version of the guide in the 
v0.2 docs
.
As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of 
LangGraph persistence
 to incorporate 
memory
 into new LangChain applications.
If your code is already relying on 
RunnableWithMessageHistory
 or 
BaseChatMessageHistory
, you do 
not
 need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses 
RunnableWithMessageHistory
 will continue to work as expected.
Please see 
How to migrate to LangGraph Memory
 for more details.


Passing conversation state into and out a chain is vital when building a chatbot. LangGraph implements a built-in persistence layer, allowing chain states to be automatically persisted in memory, or external backends such as SQLite, Postgres or Redis. Details can be found in the LangGraph 
persistence documentation
.


In this guide we demonstrate how to add persistence to arbitrary LangChain runnables by wrapping them in a minimal LangGraph application. This lets us persist the message history and other elements of the chain's state, simplifying the development of multi-turn applications. It also supports multiple threads, enabling a single application to interact separately with multiple users.


Setup
​


Let's initialize a chat model:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Example: message inputs
​


Adding memory to a 
chat model
 provides a simple example. Chat models accept a list of messages as input and output a message. LangGraph includes a built-in 
MessagesState
 that we can use for this purpose.


Below, we:




Define the graph state to be a list of messages;


Add a single node to the graph that calls a chat model;


Compile the graph with an in-memory checkpointer to store messages between runs.




info
The output of a LangGraph application is its 
state
. This can be any Python type, but in this context it will typically be a 
TypedDict
 that matches the schema of your runnable.


from
 langchain_core
.
messages 
import
 HumanMessage
from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
graph 
import
 START
,
 MessagesState
,
 StateGraph
# Define a new graph
workflow 
=
 StateGraph
(
state_schema
=
MessagesState
)
# Define the function that calls the model
def
 
call_model
(
state
:
 MessagesState
)
:
    response 
=
 llm
.
invoke
(
state
[
"messages"
]
)
    
# Update message history with response:
    
return
 
{
"messages"
:
 response
}
# Define the (single) node in the graph
workflow
.
add_edge
(
START
,
 
"model"
)
workflow
.
add_node
(
"model"
,
 call_model
)
# Add memory
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
checkpointer
=
memory
)
API Reference:
HumanMessage
 | 
MemorySaver
 | 
StateGraph


When we run the application, we pass in a configuration 
dict
 that specifies a 
thread_id
. This ID is used to distinguish conversational threads (e.g., between different users).


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc123"
}
}


We can then invoke the application:


query 
=
 
"Hi! I'm Bob."
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
{
"messages"
:
 input_messages
}
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
  
# output contains all messages in state


==================================[1m Ai Message [0m==================================
It's nice to meet you, Bob! I'm Claude, an AI assistant created by Anthropic. How can I help you today?


query 
=
 
"What's my name?"
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
{
"messages"
:
 input_messages
}
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
Your name is Bob, as you introduced yourself at the beginning of our conversation.


Note that states are separated for different threads. If we issue the same query to a thread with a new 
thread_id
, the model indicates that it does not know the answer:


query 
=
 
"What's my name?"
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc234"
}
}
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
{
"messages"
:
 input_messages
}
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
I'm afraid I don't actually know your name. As an AI assistant, I don't have personal information about you unless you provide it to me directly.


Example: dictionary inputs
​


LangChain runnables often accept multiple inputs via separate keys in a single 
dict
 argument. A common example is a prompt template with multiple parameters.


Whereas before our runnable was a chat model, here we chain together a prompt template and chat model.


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 MessagesPlaceholder
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"Answer in {language}."
)
,
        MessagesPlaceholder
(
variable_name
=
"messages"
)
,
    
]
)
runnable 
=
 prompt 
|
 llm
API Reference:
ChatPromptTemplate
 | 
MessagesPlaceholder


For this scenario, we define the graph state to include these parameters (in addition to the message history). We then define a single-node graph in the same way as before.


Note that in the below state:




Updates to the 
messages
 list will append messages;


Updates to the 
language
 string will overwrite the string.




from
 typing 
import
 Sequence
from
 langchain_core
.
messages 
import
 BaseMessage
from
 langgraph
.
graph
.
message 
import
 add_messages
from
 typing_extensions 
import
 Annotated
,
 TypedDict
class
 
State
(
TypedDict
)
:
    messages
:
 Annotated
[
Sequence
[
BaseMessage
]
,
 add_messages
]
    language
:
 
str
workflow 
=
 StateGraph
(
state_schema
=
State
)
def
 
call_model
(
state
:
 State
)
:
    response 
=
 runnable
.
invoke
(
state
)
    
# Update message history with response:
    
return
 
{
"messages"
:
 
[
response
]
}
workflow
.
add_edge
(
START
,
 
"model"
)
workflow
.
add_node
(
"model"
,
 call_model
)
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
checkpointer
=
memory
)
API Reference:
BaseMessage
 | 
add_messages


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc345"
}
}
input_dict 
=
 
{
    
"messages"
:
 
[
HumanMessage
(
"Hi, I'm Bob."
)
]
,
    
"language"
:
 
"Spanish"
,
}
output 
=
 app
.
invoke
(
input_dict
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
¡Hola, Bob! Es un placer conocerte.


Managing message history
​


The message history (and other elements of the application state) can be accessed via 
.get_state
:


state 
=
 app
.
get_state
(
config
)
.
values
print
(
f'Language: 
{
state
[
"language"
]
}
'
)
for
 message 
in
 state
[
"messages"
]
:
    message
.
pretty_print
(
)


Language: Spanish
================================[1m Human Message [0m=================================
Hi, I'm Bob.
==================================[1m Ai Message [0m==================================
¡Hola, Bob! Es un placer conocerte.


We can also update the state via 
.update_state
. For example, we can manually append a new message:


from
 langchain_core
.
messages 
import
 HumanMessage
_ 
=
 app
.
update_state
(
config
,
 
{
"messages"
:
 
[
HumanMessage
(
"Test"
)
]
}
)
API Reference:
HumanMessage


state 
=
 app
.
get_state
(
config
)
.
values
print
(
f'Language: 
{
state
[
"language"
]
}
'
)
for
 message 
in
 state
[
"messages"
]
:
    message
.
pretty_print
(
)


Language: Spanish
================================[1m Human Message [0m=================================
Hi, I'm Bob.
==================================[1m Ai Message [0m==================================
¡Hola, Bob! Es un placer conocerte.
================================[1m Human Message [0m=================================
Test


For details on managing state, including deleting messages, see the LangGraph documentation:




How to delete messages


How to view and update past graph state


Edit this page
Was this page helpful?
Previous
How to merge consecutive messages of the same type
Next
How to migrate from legacy LangChain agents to LangGraph
Setup
Example: message inputs
Example: dictionary inputs
Managing message history


--- docs_how_to_migrate_agent.txt ---

How-to guides
How to migrate from legacy LangChain agents to LangGraph
On this page
How to migrate from legacy LangChain agents to LangGraph


Prerequisites
This guide assumes familiarity with the following concepts:


Agents


LangGraph


Tool calling




Here we focus on how to move from legacy LangChain agents to more flexible 
LangGraph
 agents.
LangChain agents (the 
AgentExecutor
 in particular) have multiple configuration parameters.
In this notebook we will show how those parameters map to the LangGraph react agent executor using the 
create_react_agent
 prebuilt helper method.


note
In LangGraph, the graph replaces LangChain's agent executor. It manages the agent's cycles and tracks the scratchpad as messages within its state. The LangChain "agent" corresponds to the prompt and LLM you've provided.


Prerequisites
​


This how-to guide uses OpenAI as the LLM. Install the dependencies to run.


%
%
capture 
-
-
no
-
stderr
%
pip install 
-
U langgraph langchain langchain
-
openai


Then, set your OpenAI API key.


import
 getpass
import
 os
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"OpenAI API key:\n"
)


Basic Usage
​


For basic creation and usage of a tool-calling ReAct-style agent, the functionality is the same. First, let's define a model and tool(s), then we'll use those to create an agent.


from
 langchain_core
.
tools 
import
 tool
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
model
=
"gpt-4o"
)
@tool
def
 
magic_function
(
input
:
 
int
)
 
-
>
 
int
:
    
"""Applies a magic function to an input."""
    
return
 
input
 
+
 
2
tools 
=
 
[
magic_function
]
query 
=
 
"what is the value of magic_function(3)?"
API Reference:
tool
 | 
ChatOpenAI


For the LangChain 
AgentExecutor
, we define a prompt with a placeholder for the agent's scratchpad. The agent can be invoked as follows:


from
 langchain
.
agents 
import
 AgentExecutor
,
 create_tool_calling_agent
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a helpful assistant"
)
,
        
(
"human"
,
 
"{input}"
)
,
        
# Placeholders fill up a **list** of messages
        
(
"placeholder"
,
 
"{agent_scratchpad}"
)
,
    
]
)
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
)
agent_executor 
=
 AgentExecutor
(
agent
=
agent
,
 tools
=
tools
)
agent_executor
.
invoke
(
{
"input"
:
 query
}
)
API Reference:
AgentExecutor
 | 
create_tool_calling_agent
 | 
ChatPromptTemplate


{'input': 'what is the value of magic_function(3)?',
 'output': 'The value of `magic_function(3)` is 5.'}


LangGraph's 
react agent executor
 manages a state that is defined by a list of messages. It will continue to process the list until there are no tool calls in the agent's output. To kick it off, we input a list of messages. The output will contain the entire state of the graph-- in this case, the conversation history.


from
 langgraph
.
prebuilt 
import
 create_react_agent
langgraph_agent_executor 
=
 create_react_agent
(
model
,
 tools
)
messages 
=
 langgraph_agent_executor
.
invoke
(
{
"messages"
:
 
[
(
"human"
,
 query
)
]
}
)
{
    
"input"
:
 query
,
    
"output"
:
 messages
[
"messages"
]
[
-
1
]
.
content
,
}
API Reference:
create_react_agent


{'input': 'what is the value of magic_function(3)?',
 'output': 'The value of `magic_function(3)` is 5.'}


message_history 
=
 messages
[
"messages"
]
new_query 
=
 
"Pardon?"
messages 
=
 langgraph_agent_executor
.
invoke
(
    
{
"messages"
:
 message_history 
+
 
[
(
"human"
,
 new_query
)
]
}
)
{
    
"input"
:
 new_query
,
    
"output"
:
 messages
[
"messages"
]
[
-
1
]
.
content
,
}


{'input': 'Pardon?',
 'output': 'The result of applying `magic_function` to the input value 3 is 5.'}


Prompt Templates
​


With legacy LangChain agents you have to pass in a prompt template. You can use this to control the agent.


With LangGraph 
react agent executor
, by default there is no prompt. You can achieve similar control over the agent in a few ways:




Pass in a system message as input


Initialize the agent with a system message


Initialize the agent with a function to transform messages in the graph state before passing to the model.


Initialize the agent with a 
Runnable
 to transform messages in the graph state before passing to the model. This includes passing prompt templates as well.




Let's take a look at all of these below. We will pass in custom instructions to get the agent to respond in Spanish.


First up, using 
AgentExecutor
:


prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a helpful assistant. Respond only in Spanish."
)
,
        
(
"human"
,
 
"{input}"
)
,
        
# Placeholders fill up a **list** of messages
        
(
"placeholder"
,
 
"{agent_scratchpad}"
)
,
    
]
)
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
)
agent_executor 
=
 AgentExecutor
(
agent
=
agent
,
 tools
=
tools
)
agent_executor
.
invoke
(
{
"input"
:
 query
}
)


{'input': 'what is the value of magic_function(3)?',
 'output': 'El valor de magic_function(3) es 5.'}


Now, let's pass a custom system message to 
react agent executor
.


LangGraph's prebuilt 
create_react_agent
 does not take a prompt template directly as a parameter, but instead takes a 
prompt
 parameter. This modifies the graph state before the llm is called, and can be one of four values:




A 
SystemMessage
, which is added to the beginning of the list of messages.


A 
string
, which is converted to a 
SystemMessage
 and added to the beginning of the list of messages.


A 
Callable
, which should take in full graph state. The output is then passed to the language model.


Or a 
Runnable
, which should take in full graph state. The output is then passed to the language model.




Here's how it looks in action:


from
 langchain_core
.
messages 
import
 SystemMessage
from
 langgraph
.
prebuilt 
import
 create_react_agent
system_message 
=
 
"You are a helpful assistant. Respond only in Spanish."
# This could also be a SystemMessage object
# system_message = SystemMessage(content="You are a helpful assistant. Respond only in Spanish.")
langgraph_agent_executor 
=
 create_react_agent
(
model
,
 tools
,
 prompt
=
system_message
)
messages 
=
 langgraph_agent_executor
.
invoke
(
{
"messages"
:
 
[
(
"user"
,
 query
)
]
}
)
API Reference:
SystemMessage
 | 
create_react_agent


We can also pass in an arbitrary function or a runnable. This function/runnable should take in a graph state and output a list of messages.
We can do all types of arbitrary formatting of messages here. In this case, let's add a SystemMessage to the start of the list of messages and append another user message at the end.


from
 langchain_core
.
messages 
import
 HumanMessage
,
 SystemMessage
from
 langgraph
.
prebuilt 
import
 create_react_agent
from
 langgraph
.
prebuilt
.
chat_agent_executor 
import
 AgentState
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a helpful assistant. Respond only in Spanish."
)
,
        
(
"placeholder"
,
 
"{messages}"
)
,
        
(
"user"
,
 
"Also say 'Pandamonium!' after the answer."
)
,
    
]
)
# alternatively, this can be passed as a function, e.g.
# def prompt(state: AgentState):
#     return (
#         [SystemMessage(content="You are a helpful assistant. Respond only in Spanish.")] +
#         state["messages"] +
#         [HumanMessage(content="Also say 'Pandamonium!' after the answer.")]
#     )
langgraph_agent_executor 
=
 create_react_agent
(
model
,
 tools
,
 prompt
=
prompt
)
messages 
=
 langgraph_agent_executor
.
invoke
(
{
"messages"
:
 
[
(
"human"
,
 query
)
]
}
)
print
(
    
{
        
"input"
:
 query
,
        
"output"
:
 messages
[
"messages"
]
[
-
1
]
.
content
,
    
}
)
API Reference:
HumanMessage
 | 
SystemMessage
 | 
create_react_agent


{'input': 'what is the value of magic_function(3)?', 'output': 'El valor de magic_function(3) es 5. ¡Pandamonium!'}


Memory
​


In LangChain
​


With LangChain's 
AgentExecutor
, you could add chat 
Memory
 so it can engage in a multi-turn conversation.


from
 langchain
.
agents 
import
 AgentExecutor
,
 create_tool_calling_agent
from
 langchain_core
.
chat_history 
import
 InMemoryChatMessageHistory
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables
.
history 
import
 RunnableWithMessageHistory
from
 langchain_core
.
tools 
import
 tool
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
model
=
"gpt-4o"
)
memory 
=
 InMemoryChatMessageHistory
(
session_id
=
"test-session"
)
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a helpful assistant."
)
,
        
# First put the history
        
(
"placeholder"
,
 
"{chat_history}"
)
,
        
# Then the new input
        
(
"human"
,
 
"{input}"
)
,
        
# Finally the scratchpad
        
(
"placeholder"
,
 
"{agent_scratchpad}"
)
,
    
]
)
@tool
def
 
magic_function
(
input
:
 
int
)
 
-
>
 
int
:
    
"""Applies a magic function to an input."""
    
return
 
input
 
+
 
2
tools 
=
 
[
magic_function
]
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
)
agent_executor 
=
 AgentExecutor
(
agent
=
agent
,
 tools
=
tools
)
agent_with_chat_history 
=
 RunnableWithMessageHistory
(
    agent_executor
,
    
# This is needed because in most real world scenarios, a session id is needed
    
# It isn't really used here because we are using a simple in memory ChatMessageHistory
    
lambda
 session_id
:
 memory
,
    input_messages_key
=
"input"
,
    history_messages_key
=
"chat_history"
,
)
config 
=
 
{
"configurable"
:
 
{
"session_id"
:
 
"test-session"
}
}
print
(
    agent_with_chat_history
.
invoke
(
        
{
"input"
:
 
"Hi, I'm polly! What's the output of magic_function of 3?"
}
,
 config
    
)
[
"output"
]
)
print
(
"---"
)
print
(
agent_with_chat_history
.
invoke
(
{
"input"
:
 
"Remember my name?"
}
,
 config
)
[
"output"
]
)
print
(
"---"
)
print
(
    agent_with_chat_history
.
invoke
(
{
"input"
:
 
"what was that output again?"
}
,
 config
)
[
        
"output"
    
]
)
API Reference:
AgentExecutor
 | 
create_tool_calling_agent
 | 
InMemoryChatMessageHistory
 | 
ChatPromptTemplate
 | 
RunnableWithMessageHistory
 | 
tool
 | 
ChatOpenAI


The output of the magic function when the input is 3 is 5.
---
Yes, you mentioned your name is Polly.
---
The output of the magic function when the input is 3 is 5.


In LangGraph
​


Memory is just 
persistence
, aka 
checkpointing
.


Add a 
checkpointer
 to the agent and you get chat memory for free.


from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver  
# an in-memory checkpointer
from
 langgraph
.
prebuilt 
import
 create_react_agent
system_message 
=
 
"You are a helpful assistant."
# This could also be a SystemMessage object
# system_message = SystemMessage(content="You are a helpful assistant. Respond only in Spanish.")
memory 
=
 MemorySaver
(
)
langgraph_agent_executor 
=
 create_react_agent
(
    model
,
 tools
,
 prompt
=
system_message
,
 checkpointer
=
memory
)
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"test-thread"
}
}
print
(
    langgraph_agent_executor
.
invoke
(
        
{
            
"messages"
:
 
[
                
(
"user"
,
 
"Hi, I'm polly! What's the output of magic_function of 3?"
)
            
]
        
}
,
        config
,
    
)
[
"messages"
]
[
-
1
]
.
content
)
print
(
"---"
)
print
(
    langgraph_agent_executor
.
invoke
(
        
{
"messages"
:
 
[
(
"user"
,
 
"Remember my name?"
)
]
}
,
 config
    
)
[
"messages"
]
[
-
1
]
.
content
)
print
(
"---"
)
print
(
    langgraph_agent_executor
.
invoke
(
        
{
"messages"
:
 
[
(
"user"
,
 
"what was that output again?"
)
]
}
,
 config
    
)
[
"messages"
]
[
-
1
]
.
content
)
API Reference:
MemorySaver
 | 
create_react_agent


The output of the magic function for the input 3 is 5.
---
Yes, you mentioned that your name is Polly.
---
The output of the magic function for the input 3 was 5.


Iterating through steps
​


In LangChain
​


With LangChain's 
AgentExecutor
, you could iterate over the steps using the 
stream
 (or async 
astream
) methods or the 
iter
 method. LangGraph supports stepwise iteration using 
stream


from
 langchain
.
agents 
import
 AgentExecutor
,
 create_tool_calling_agent
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
tools 
import
 tool
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
model
=
"gpt-4o"
)
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a helpful assistant."
)
,
        
(
"human"
,
 
"{input}"
)
,
        
# Placeholders fill up a **list** of messages
        
(
"placeholder"
,
 
"{agent_scratchpad}"
)
,
    
]
)
@tool
def
 
magic_function
(
input
:
 
int
)
 
-
>
 
int
:
    
"""Applies a magic function to an input."""
    
return
 
input
 
+
 
2
tools 
=
 
[
magic_function
]
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
=
prompt
)
agent_executor 
=
 AgentExecutor
(
agent
=
agent
,
 tools
=
tools
)
for
 step 
in
 agent_executor
.
stream
(
{
"input"
:
 query
}
)
:
    
print
(
step
)
API Reference:
AgentExecutor
 | 
create_tool_calling_agent
 | 
ChatPromptTemplate
 | 
tool
 | 
ChatOpenAI


{'actions': [ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log="\nInvoking: `magic_function` with `{'input': 3}`\n\n\n", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{"input":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_yyetzabaDBRX9Ml2KyqfKzZM')], 'messages': [AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{"input":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])]}
{'steps': [AgentStep(action=ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log="\nInvoking: `magic_function` with `{'input': 3}`\n\n\n", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-7a3a5ada-52ec-4df0-bf7d-81e5051b01b4', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{"input":3}', 'id': 'call_yyetzabaDBRX9Ml2KyqfKzZM', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_yyetzabaDBRX9Ml2KyqfKzZM'), observation=5)], 'messages': [FunctionMessage(content='5', additional_kwargs={}, response_metadata={}, name='magic_function')]}
{'output': 'The value of `magic_function(3)` is 5.', 'messages': [AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={}, response_metadata={})]}


In LangGraph
​


In LangGraph, things are handled natively using 
stream
 or the asynchronous 
astream
 method.


from
 langgraph
.
prebuilt 
import
 create_react_agent
from
 langgraph
.
prebuilt
.
chat_agent_executor 
import
 AgentState
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a helpful assistant."
)
,
        
(
"placeholder"
,
 
"{messages}"
)
,
    
]
)
langgraph_agent_executor 
=
 create_react_agent
(
model
,
 tools
,
 prompt
=
prompt
)
for
 step 
in
 langgraph_agent_executor
.
stream
(
    
{
"messages"
:
 
[
(
"human"
,
 query
)
]
}
,
 stream_mode
=
"updates"
)
:
    
print
(
step
)
API Reference:
create_react_agent


{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IHTMrjvIHn8gFOX42FstIpr9', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 61, 'total_tokens': 75, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1a6970da-163a-4e4d-b9b7-7e73b1057f42-0', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_IHTMrjvIHn8gFOX42FstIpr9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 14, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}
{'tools': {'messages': [ToolMessage(content='5', name='magic_function', id='51a9d3e4-734d-426f-a5a1-c6597e4efe25', tool_call_id='call_IHTMrjvIHn8gFOX42FstIpr9')]}}
{'agent': {'messages': [AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 84, 'total_tokens': 98, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a20a4ee344', 'finish_reason': 'stop', 'logprobs': None}, id='run-73001576-a3dc-4552-8d81-c9ce8aec05b3-0', usage_metadata={'input_tokens': 84, 'output_tokens': 14, 'total_tokens': 98, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}


return_intermediate_steps
​


In LangChain
​


Setting this parameter on AgentExecutor allows users to access intermediate_steps, which pairs agent actions (e.g., tool invocations) with their outcomes.


agent_executor 
=
 AgentExecutor
(
agent
=
agent
,
 tools
=
tools
,
 return_intermediate_steps
=
True
)
result 
=
 agent_executor
.
invoke
(
{
"input"
:
 query
}
)
print
(
result
[
"intermediate_steps"
]
)


[(ToolAgentAction(tool='magic_function', tool_input={'input': 3}, log="\nInvoking: `magic_function` with `{'input': 3}`\n\n\n", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7'}, id='run-c9dfe3ab-2db6-4592-851e-89e056aeab32', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'magic_function', 'args': '{"input":3}', 'id': 'call_njTvl2RsVf4q1aMUxoYnJuK1', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='call_njTvl2RsVf4q1aMUxoYnJuK1'), 5)]


In LangGraph
​


By default the 
react agent executor
 in LangGraph appends all messages to the central state. Therefore, it is easy to see any intermediate steps by just looking at the full state.


from
 langgraph
.
prebuilt 
import
 create_react_agent
langgraph_agent_executor 
=
 create_react_agent
(
model
,
 tools
=
tools
)
messages 
=
 langgraph_agent_executor
.
invoke
(
{
"messages"
:
 
[
(
"human"
,
 query
)
]
}
)
messages
API Reference:
create_react_agent


{'messages': [HumanMessage(content='what is the value of magic_function(3)?', additional_kwargs={}, response_metadata={}, id='1abb52c2-4bc2-4d82-bd32-5a24c3976b0f'),
  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_XfQD6C7rAalcmicQubkhJVFq', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a20a4ee344', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-34f02786-5b5c-4bb1-bd9e-406c81944a24-0', tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_XfQD6C7rAalcmicQubkhJVFq', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),
  ToolMessage(content='5', name='magic_function', id='cbc9fadf-1962-4ed7-b476-348c774652be', tool_call_id='call_XfQD6C7rAalcmicQubkhJVFq'),
  AIMessage(content='The value of `magic_function(3)` is 5.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 78, 'total_tokens': 92, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None}, id='run-547e03d2-872d-4008-a38d-b7f739a77df5-0', usage_metadata={'input_tokens': 78, 'output_tokens': 14, 'total_tokens': 92, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}


max_iterations
​


In LangChain
​


AgentExecutor
 implements a 
max_iterations
 parameter, allowing users to abort a run that exceeds a specified number of iterations.


@tool
def
 
magic_function
(
input
:
 
str
)
 
-
>
 
str
:
    
"""Applies a magic function to an input."""
    
return
 
"Sorry, there was an error. Please try again."
tools 
=
 
[
magic_function
]


prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a helpful assistant. Respond only in Spanish."
)
,
        
(
"human"
,
 
"{input}"
)
,
        
# Placeholders fill up a **list** of messages
        
(
"placeholder"
,
 
"{agent_scratchpad}"
)
,
    
]
)
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
)
agent_executor 
=
 AgentExecutor
(
    agent
=
agent
,
    tools
=
tools
,
    verbose
=
True
,
    max_iterations
=
3
,
)
agent_executor
.
invoke
(
{
"input"
:
 query
}
)


[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3mLo siento, no puedo decirte directamente el valor de `magic_function(3)`. Si deseas, puedo usar la función mágica para calcularlo. ¿Te gustaría que lo hiciera?[0m
[1m> Finished chain.[0m


{'input': 'what is the value of magic_function(3)?',
 'output': 'Lo siento, no puedo decirte directamente el valor de `magic_function(3)`. Si deseas, puedo usar la función mágica para calcularlo. ¿Te gustaría que lo hiciera?'}


In LangGraph
​


In LangGraph this is controlled via 
recursion_limit
 configuration parameter.


Note that in 
AgentExecutor
, an "iteration" includes a full turn of tool invocation and execution. In LangGraph, each step contributes to the recursion limit, so we will need to multiply by two (and add one) to get equivalent results.


If the recursion limit is reached, LangGraph raises a specific exception type, that we can catch and manage similarly to AgentExecutor.


from
 langgraph
.
errors 
import
 GraphRecursionError
from
 langgraph
.
prebuilt 
import
 create_react_agent
RECURSION_LIMIT 
=
 
2
 
*
 
3
 
+
 
1
langgraph_agent_executor 
=
 create_react_agent
(
model
,
 tools
=
tools
)
try
:
    
for
 chunk 
in
 langgraph_agent_executor
.
stream
(
        
{
"messages"
:
 
[
(
"human"
,
 query
)
]
}
,
        
{
"recursion_limit"
:
 RECURSION_LIMIT
}
,
        stream_mode
=
"values"
,
    
)
:
        
print
(
chunk
[
"messages"
]
[
-
1
]
)
except
 GraphRecursionError
:
    
print
(
{
"input"
:
 query
,
 
"output"
:
 
"Agent stopped due to max iterations."
}
)
API Reference:
create_react_agent


content='what is the value of magic_function(3)?' additional_kwargs={} response_metadata={} id='c2489fe8-e69c-4163-876d-3cce26b28521'
content='' additional_kwargs={'tool_calls': [{'id': 'call_OyNTcO6SDAvZcBlIEknPRrTR', 'function': {'arguments': '{"input":"3"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-b65504bb-fa23-4f8a-8d6c-7edb6d16e7ff-0' tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_OyNTcO6SDAvZcBlIEknPRrTR', 'type': 'tool_call'}] usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
content='Sorry, there was an error. Please try again.' name='magic_function' id='f00e0bff-54fe-4726-a1a7-127a59d8f7ed' tool_call_id='call_OyNTcO6SDAvZcBlIEknPRrTR'
content="It seems there was an error when trying to compute the value of the magic function with input 3. Let's try again." additional_kwargs={'tool_calls': [{'id': 'call_Q020rQoJh4cnh8WglIMnDm4z', 'function': {'arguments': '{"input":"3"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 88, 'total_tokens': 128, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-556d8cb2-b47a-4826-b17d-b520982c2475-0' tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_Q020rQoJh4cnh8WglIMnDm4z', 'type': 'tool_call'}] usage_metadata={'input_tokens': 88, 'output_tokens': 40, 'total_tokens': 128, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
content='Sorry, there was an error. Please try again.' name='magic_function' id='777212cd-8381-44db-9762-3f81951ea73e' tool_call_id='call_Q020rQoJh4cnh8WglIMnDm4z'
content="It seems there is a persistent issue in computing the value of the magic function with the input 3. Unfortunately, I can't provide the value at this time. If you have any other questions or need further assistance, feel free to ask!" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 150, 'total_tokens': 199, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None} id='run-92ec0b90-bc8e-4851-9139-f1d976145ab7-0' usage_metadata={'input_tokens': 150, 'output_tokens': 49, 'total_tokens': 199, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}


max_execution_time
​


In LangChain
​


AgentExecutor
 implements a 
max_execution_time
 parameter, allowing users to abort a run that exceeds a total time limit.


import
 time
@tool
def
 
magic_function
(
input
:
 
str
)
 
-
>
 
str
:
    
"""Applies a magic function to an input."""
    time
.
sleep
(
2.5
)
    
return
 
"Sorry, there was an error. Please try again."
tools 
=
 
[
magic_function
]
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
)
agent_executor 
=
 AgentExecutor
(
    agent
=
agent
,
    tools
=
tools
,
    max_execution_time
=
2
,
    verbose
=
True
,
)
agent_executor
.
invoke
(
{
"input"
:
 query
}
)


[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3mLo siento, no tengo la capacidad de evaluar directamente una función llamada "magic_function" con el valor 3. Sin embargo, si me proporcionas más detalles sobre qué hace la función o cómo está definida, podría intentar ayudarte a comprender su comportamiento o resolverlo de otra manera.[0m
[1m> Finished chain.[0m


{'input': 'what is the value of magic_function(3)?',
 'output': 'Lo siento, no tengo la capacidad de evaluar directamente una función llamada "magic_function" con el valor 3. Sin embargo, si me proporcionas más detalles sobre qué hace la función o cómo está definida, podría intentar ayudarte a comprender su comportamiento o resolverlo de otra manera.'}


In LangGraph
​


With LangGraph's react agent, you can control timeouts on two levels.


You can set a 
step_timeout
 to bound each 
step
:


from
 langgraph
.
prebuilt 
import
 create_react_agent
langgraph_agent_executor 
=
 create_react_agent
(
model
,
 tools
=
tools
)
# Set the max timeout for each step here
langgraph_agent_executor
.
step_timeout 
=
 
2
try
:
    
for
 chunk 
in
 langgraph_agent_executor
.
stream
(
{
"messages"
:
 
[
(
"human"
,
 query
)
]
}
)
:
        
print
(
chunk
)
        
print
(
"------"
)
except
 TimeoutError
:
    
print
(
{
"input"
:
 query
,
 
"output"
:
 
"Agent stopped due to a step timeout."
}
)
API Reference:
create_react_agent


{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_UuxSgpGaqzX84sNlKzCVOiRO', 'function': {'arguments': '{"input":"3"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-24c94cbd-2962-48cf-a447-af888eb6ef86-0', tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_UuxSgpGaqzX84sNlKzCVOiRO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}
------
{'input': 'what is the value of magic_function(3)?', 'output': 'Agent stopped due to a step timeout.'}


The other way to set a single max timeout for an entire run is to directly use the python stdlib 
asyncio
 library.


import
 asyncio
from
 langgraph
.
prebuilt 
import
 create_react_agent
langgraph_agent_executor 
=
 create_react_agent
(
model
,
 tools
=
tools
)
async
 
def
 
stream
(
langgraph_agent_executor
,
 inputs
)
:
    
async
 
for
 chunk 
in
 langgraph_agent_executor
.
astream
(
        
{
"messages"
:
 
[
(
"human"
,
 query
)
]
}
    
)
:
        
print
(
chunk
)
        
print
(
"------"
)
try
:
    task 
=
 asyncio
.
create_task
(
        stream
(
langgraph_agent_executor
,
 
{
"messages"
:
 
[
(
"human"
,
 query
)
]
}
)
    
)
    
await
 asyncio
.
wait_for
(
task
,
 timeout
=
3
)
except
 asyncio
.
TimeoutError
:
    
print
(
"Task Cancelled."
)
API Reference:
create_react_agent


{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_km17xvoY7wJ5yNnXhb5V9D3I', 'function': {'arguments': '{"input":"3"}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_45c6de4934', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b44a04e5-9b68-4020-be36-98de1593eefc-0', tool_calls=[{'name': 'magic_function', 'args': {'input': '3'}, 'id': 'call_km17xvoY7wJ5yNnXhb5V9D3I', 'type': 'tool_call'}], usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]}}
------
Task Cancelled.


early_stopping_method
​


In LangChain
​


With LangChain's 
AgentExecutor
, you could configure an 
early_stopping_method
 to either return a string saying "Agent stopped due to iteration limit or time limit." (
"force"
) or prompt the LLM a final time to respond (
"generate"
).


from
 langchain
.
agents 
import
 AgentExecutor
,
 create_tool_calling_agent
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
tools 
import
 tool
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
model
=
"gpt-4o"
)
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a helpful assistant."
)
,
        
(
"human"
,
 
"{input}"
)
,
        
# Placeholders fill up a **list** of messages
        
(
"placeholder"
,
 
"{agent_scratchpad}"
)
,
    
]
)
@tool
def
 
magic_function
(
input
:
 
int
)
 
-
>
 
int
:
    
"""Applies a magic function to an input."""
    
return
 
"Sorry there was an error, please try again."
tools 
=
 
[
magic_function
]
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
=
prompt
)
agent_executor 
=
 AgentExecutor
(
    agent
=
agent
,
 tools
=
tools
,
 early_stopping_method
=
"force"
,
 max_iterations
=
1
)
result 
=
 agent_executor
.
invoke
(
{
"input"
:
 query
}
)
print
(
"Output with early_stopping_method='force':"
)
print
(
result
[
"output"
]
)
API Reference:
AgentExecutor
 | 
create_tool_calling_agent
 | 
ChatPromptTemplate
 | 
tool
 | 
ChatOpenAI


Output with early_stopping_method='force':
Agent stopped due to max iterations.


In LangGraph
​


In LangGraph, you can explicitly handle the response behavior outside the agent, since the full state can be accessed.


from
 langgraph
.
errors 
import
 GraphRecursionError
from
 langgraph
.
prebuilt 
import
 create_react_agent
RECURSION_LIMIT 
=
 
2
 
*
 
1
 
+
 
1
langgraph_agent_executor 
=
 create_react_agent
(
model
,
 tools
=
tools
)
try
:
    
for
 chunk 
in
 langgraph_agent_executor
.
stream
(
        
{
"messages"
:
 
[
(
"human"
,
 query
)
]
}
,
        
{
"recursion_limit"
:
 RECURSION_LIMIT
}
,
        stream_mode
=
"values"
,
    
)
:
        
print
(
chunk
[
"messages"
]
[
-
1
]
)
except
 GraphRecursionError
:
    
print
(
{
"input"
:
 query
,
 
"output"
:
 
"Agent stopped due to max iterations."
}
)
API Reference:
create_react_agent


content='what is the value of magic_function(3)?' additional_kwargs={} response_metadata={} id='81fd2e50-1e6a-4871-87aa-b7c1225913a4'
content='' additional_kwargs={'tool_calls': [{'id': 'call_aaEzj3aO1RTnB0uoc9rYUIhi', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-476bc4b1-b7bf-4607-a31c-ddf09dc814c5-0' tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_aaEzj3aO1RTnB0uoc9rYUIhi', 'type': 'tool_call'}] usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
content='Sorry there was an error, please try again.' name='magic_function' id='dcbe7e3e-0ed4-467d-a729-2f45916ff44f' tool_call_id='call_aaEzj3aO1RTnB0uoc9rYUIhi'
content="It seems there was an error when trying to compute the value of `magic_function(3)`. Let's try that again." additional_kwargs={'tool_calls': [{'id': 'call_jr4R8uJn2pdXF5GZC2Dg3YWS', 'function': {'arguments': '{"input":3}', 'name': 'magic_function'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 87, 'total_tokens': 127, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-d94b8932-6e9e-4ab1-99f7-7dca89887ffe-0' tool_calls=[{'name': 'magic_function', 'args': {'input': 3}, 'id': 'call_jr4R8uJn2pdXF5GZC2Dg3YWS', 'type': 'tool_call'}] usage_metadata={'input_tokens': 87, 'output_tokens': 40, 'total_tokens': 127, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}
{'input': 'what is the value of magic_function(3)?', 'output': 'Agent stopped due to max iterations.'}


trim_intermediate_steps
​


In LangChain
​


With LangChain's 
AgentExecutor
, you could trim the intermediate steps of long-running agents using 
trim_intermediate_steps
, which is either an integer (indicating the agent should keep the last N steps) or a custom function.


For instance, we could trim the value so the agent only sees the most recent intermediate step.


from
 langchain
.
agents 
import
 AgentExecutor
,
 create_tool_calling_agent
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
tools 
import
 tool
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
model
=
"gpt-4o"
)
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are a helpful assistant."
)
,
        
(
"human"
,
 
"{input}"
)
,
        
# Placeholders fill up a **list** of messages
        
(
"placeholder"
,
 
"{agent_scratchpad}"
)
,
    
]
)
magic_step_num 
=
 
1
@tool
def
 
magic_function
(
input
:
 
int
)
 
-
>
 
int
:
    
"""Applies a magic function to an input."""
    
global
 magic_step_num
    
print
(
f"Call number: 
{
magic_step_num
}
"
)
    magic_step_num 
+=
 
1
    
return
 
input
 
+
 magic_step_num
tools 
=
 
[
magic_function
]
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
=
prompt
)
def
 
trim_steps
(
steps
:
 
list
)
:
    
# Let's give the agent amnesia
    
return
 
[
]
agent_executor 
=
 AgentExecutor
(
    agent
=
agent
,
 tools
=
tools
,
 trim_intermediate_steps
=
trim_steps
)
query 
=
 
"Call the magic function 4 times in sequence with the value 3. You cannot call it multiple times at once."
for
 step 
in
 agent_executor
.
stream
(
{
"input"
:
 query
}
)
:
    
pass
API Reference:
AgentExecutor
 | 
create_tool_calling_agent
 | 
ChatPromptTemplate
 | 
tool
 | 
ChatOpenAI


Call number: 1
Call number: 2
Call number: 3
Call number: 4
Call number: 5
Call number: 6
Call number: 7
Call number: 8
Call number: 9
Call number: 10
Call number: 11
Call number: 12
Call number: 13
Call number: 14
``````output
Stopping agent prematurely due to triggering stop condition
``````output
Call number: 15


In LangGraph
​


We can use the 
prompt
 just as before when passing in 
prompt templates
.


from
 langgraph
.
errors 
import
 GraphRecursionError
from
 langgraph
.
prebuilt 
import
 create_react_agent
from
 langgraph
.
prebuilt
.
chat_agent_executor 
import
 AgentState
magic_step_num 
=
 
1
@tool
def
 
magic_function
(
input
:
 
int
)
 
-
>
 
int
:
    
"""Applies a magic function to an input."""
    
global
 magic_step_num
    
print
(
f"Call number: 
{
magic_step_num
}
"
)
    magic_step_num 
+=
 
1
    
return
 
input
 
+
 magic_step_num
tools 
=
 
[
magic_function
]
def
 
_modify_state_messages
(
state
:
 AgentState
)
:
    
# Give the agent amnesia, only keeping the original user query
    
return
 
[
(
"system"
,
 
"You are a helpful assistant"
)
,
 state
[
"messages"
]
[
0
]
]
langgraph_agent_executor 
=
 create_react_agent
(
    model
,
 tools
,
 prompt
=
_modify_state_messages
)
try
:
    
for
 step 
in
 langgraph_agent_executor
.
stream
(
        
{
"messages"
:
 
[
(
"human"
,
 query
)
]
}
,
 stream_mode
=
"updates"
    
)
:
        
pass
except
 GraphRecursionError 
as
 e
:
    
print
(
"Stopping agent prematurely due to triggering stop condition"
)
API Reference:
create_react_agent


Call number: 1
Call number: 2
Call number: 3
Call number: 4
Call number: 5
Call number: 6
Call number: 7
Call number: 8
Call number: 9
Call number: 10
Call number: 11
Call number: 12
Stopping agent prematurely due to triggering stop condition


Next steps
​


You've now learned how to migrate your LangChain agent executors to LangGraph.


Next, check out other 
LangGraph how-to guides
.
Edit this page
Was this page helpful?
Previous
How to add message history
Next
How to retrieve using multiple vectors per document
Basic Usage
Prompt Templates
Memory
In LangChain
In LangGraph
Iterating through steps
In LangChain
In LangGraph
return_intermediate_steps
In LangChain
In LangGraph
max_iterations
In LangChain
In LangGraph
max_execution_time
In LangChain
In LangGraph
early_stopping_method
In LangChain
In LangGraph
trim_intermediate_steps
In LangChain
In LangGraph
Next steps


--- docs_how_to_multi_vector.txt ---

How-to guides
How to retrieve using multiple vectors per document
On this page
How to retrieve using multiple vectors per document


It can often be useful to store multiple 
vectors
 per document. There are multiple use cases where this is beneficial. For example, we can 
embed
 multiple chunks of a document and associate those embeddings with the parent document, allowing 
retriever
 hits on the chunks to return the larger document.


LangChain implements a base 
MultiVectorRetriever
, which simplifies this process. Much of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the 
MultiVectorRetriever
.


The methods to create multiple vectors per document include:




Smaller chunks: split a document into smaller chunks, and embed those (this is 
ParentDocumentRetriever
).


Summary: create a summary for each document, embed that along with (or instead of) the document.


Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.




Note that this also enables another method of adding embeddings - manually. This is useful because you can explicitly add questions or queries that should lead to a document being recovered, giving you more control.


Below we walk through an example. First we instantiate some documents. We will index them in an (in-memory) 
Chroma
 vector store using 
OpenAI
 embeddings, but any LangChain vector store or embeddings model will suffice.


%
pip install 
-
-
upgrade 
-
-
quiet  langchain
-
chroma langchain langchain
-
openai 
>
 
/
dev
/
null


from
 langchain
.
storage 
import
 InMemoryByteStore
from
 langchain_chroma 
import
 Chroma
from
 langchain_community
.
document_loaders 
import
 TextLoader
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
loaders 
=
 
[
    TextLoader
(
"paul_graham_essay.txt"
)
,
    TextLoader
(
"state_of_the_union.txt"
)
,
]
docs 
=
 
[
]
for
 loader 
in
 loaders
:
    docs
.
extend
(
loader
.
load
(
)
)
text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
10000
)
docs 
=
 text_splitter
.
split_documents
(
docs
)
# The vectorstore to use to index the child chunks
vectorstore 
=
 Chroma
(
    collection_name
=
"full_documents"
,
 embedding_function
=
OpenAIEmbeddings
(
)
)
API Reference:
InMemoryByteStore
 | 
TextLoader
 | 
OpenAIEmbeddings
 | 
RecursiveCharacterTextSplitter


Smaller chunks
​


Often times it can be useful to retrieve larger chunks of information, but embed smaller chunks. This allows for embeddings to capture the semantic meaning as closely as possible, but for as much context as possible to be passed downstream. Note that this is what the 
ParentDocumentRetriever
 does. Here we show what is going on under the hood.


We will make a distinction between the vector store, which indexes embeddings of the (sub) documents, and the document store, which houses the "parent" documents and associates them with an identifier.


import
 uuid
from
 langchain
.
retrievers
.
multi_vector 
import
 MultiVectorRetriever
# The storage layer for the parent documents
store 
=
 InMemoryByteStore
(
)
id_key 
=
 
"doc_id"
# The retriever (empty to start)
retriever 
=
 MultiVectorRetriever
(
    vectorstore
=
vectorstore
,
    byte_store
=
store
,
    id_key
=
id_key
,
)
doc_ids 
=
 
[
str
(
uuid
.
uuid4
(
)
)
 
for
 _ 
in
 docs
]
API Reference:
MultiVectorRetriever


We next generate the "sub" documents by splitting the original documents. Note that we store the document identifier in the 
metadata
 of the corresponding 
Document
 object.


# The splitter to use to create smaller chunks
child_text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
400
)
sub_docs 
=
 
[
]
for
 i
,
 doc 
in
 
enumerate
(
docs
)
:
    _id 
=
 doc_ids
[
i
]
    _sub_docs 
=
 child_text_splitter
.
split_documents
(
[
doc
]
)
    
for
 _doc 
in
 _sub_docs
:
        _doc
.
metadata
[
id_key
]
 
=
 _id
    sub_docs
.
extend
(
_sub_docs
)


Finally, we index the documents in our vector store and document store:


retriever
.
vectorstore
.
add_documents
(
sub_docs
)
retriever
.
docstore
.
mset
(
list
(
zip
(
doc_ids
,
 docs
)
)
)


The vector store alone will retrieve small chunks:


retriever
.
vectorstore
.
similarity_search
(
"justice breyer"
)
[
0
]


Document(page_content='Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.', metadata={'doc_id': '064eca46-a4c4-4789-8e3b-583f9597e54f', 'source': 'state_of_the_union.txt'})


Whereas the retriever will return the larger parent document:


len
(
retriever
.
invoke
(
"justice breyer"
)
[
0
]
.
page_content
)


9875


The default search type the retriever performs on the vector database is a similarity search. LangChain vector stores also support searching via 
Max Marginal Relevance
. This can be controlled via the 
search_type
 parameter of the retriever:


from
 langchain
.
retrievers
.
multi_vector 
import
 SearchType
retriever
.
search_type 
=
 SearchType
.
mmr
len
(
retriever
.
invoke
(
"justice breyer"
)
[
0
]
.
page_content
)
API Reference:
SearchType


9875


Associating summaries with a document for retrieval
​


A summary may be able to distill more accurately what a chunk is about, leading to better retrieval. Here we show how to create summaries, and then embed those.


We construct a simple 
chain
 that will receive an input 
Document
 object and generate a summary using a LLM.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


import
 uuid
from
 langchain_core
.
documents 
import
 Document
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
chain 
=
 
(
    
{
"doc"
:
 
lambda
 x
:
 x
.
page_content
}
    
|
 ChatPromptTemplate
.
from_template
(
"Summarize the following document:\n\n{doc}"
)
    
|
 llm
    
|
 StrOutputParser
(
)
)
API Reference:
Document
 | 
StrOutputParser
 | 
ChatPromptTemplate


Note that we can 
batch
 the chain across documents:


summaries 
=
 chain
.
batch
(
docs
,
 
{
"max_concurrency"
:
 
5
}
)


We can then initialize a 
MultiVectorRetriever
 as before, indexing the summaries in our vector store, and retaining the original documents in our document store:


# The vectorstore to use to index the child chunks
vectorstore 
=
 Chroma
(
collection_name
=
"summaries"
,
 embedding_function
=
OpenAIEmbeddings
(
)
)
# The storage layer for the parent documents
store 
=
 InMemoryByteStore
(
)
id_key 
=
 
"doc_id"
# The retriever (empty to start)
retriever 
=
 MultiVectorRetriever
(
    vectorstore
=
vectorstore
,
    byte_store
=
store
,
    id_key
=
id_key
,
)
doc_ids 
=
 
[
str
(
uuid
.
uuid4
(
)
)
 
for
 _ 
in
 docs
]
summary_docs 
=
 
[
    Document
(
page_content
=
s
,
 metadata
=
{
id_key
:
 doc_ids
[
i
]
}
)
    
for
 i
,
 s 
in
 
enumerate
(
summaries
)
]
retriever
.
vectorstore
.
add_documents
(
summary_docs
)
retriever
.
docstore
.
mset
(
list
(
zip
(
doc_ids
,
 docs
)
)
)


# # We can also add the original chunks to the vectorstore if we so want
# for i, doc in enumerate(docs):
#     doc.metadata[id_key] = doc_ids[i]
# retriever.vectorstore.add_documents(docs)


Querying the vector store will return summaries:


sub_docs 
=
 retriever
.
vectorstore
.
similarity_search
(
"justice breyer"
)
sub_docs
[
0
]


Document(page_content="President Biden recently nominated Judge Ketanji Brown Jackson to serve on the United States Supreme Court, emphasizing her qualifications and broad support. The President also outlined a plan to secure the border, fix the immigration system, protect women's rights, support LGBTQ+ Americans, and advance mental health services. He highlighted the importance of bipartisan unity in passing legislation, such as the Violence Against Women Act. The President also addressed supporting veterans, particularly those impacted by exposure to burn pits, and announced plans to expand benefits for veterans with respiratory cancers. Additionally, he proposed a plan to end cancer as we know it through the Cancer Moonshot initiative. President Biden expressed optimism about the future of America and emphasized the strength of the American people in overcoming challenges.", metadata={'doc_id': '84015b1b-980e-400a-94d8-cf95d7e079bd'})


Whereas the retriever will return the larger source document:


retrieved_docs 
=
 retriever
.
invoke
(
"justice breyer"
)
len
(
retrieved_docs
[
0
]
.
page_content
)


9194


Hypothetical Queries
​


An LLM can also be used to generate a list of hypothetical questions that could be asked of a particular document, which might bear close semantic similarity to relevant queries in a 
RAG
 application. These questions can then be embedded and associated with the documents to improve retrieval.


Below, we use the 
with_structured_output
 method to structure the LLM output into a list of strings.


from
 typing 
import
 List
from
 pydantic 
import
 BaseModel
,
 Field
class
 
HypotheticalQuestions
(
BaseModel
)
:
    
"""Generate hypothetical questions."""
    questions
:
 List
[
str
]
 
=
 Field
(
.
.
.
,
 description
=
"List of questions"
)
chain 
=
 
(
    
{
"doc"
:
 
lambda
 x
:
 x
.
page_content
}
    
# Only asking for 3 hypothetical questions, but this could be adjusted
    
|
 ChatPromptTemplate
.
from_template
(
        
"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\n\n{doc}"
    
)
    
|
 ChatOpenAI
(
max_retries
=
0
,
 model
=
"gpt-4o"
)
.
with_structured_output
(
        HypotheticalQuestions
    
)
    
|
 
(
lambda
 x
:
 x
.
questions
)
)


Invoking the chain on a single document demonstrates that it outputs a list of questions:


chain
.
invoke
(
docs
[
0
]
)


["What impact did the IBM 1401 have on the author's early programming experiences?",
 "How did the transition from using the IBM 1401 to microcomputers influence the author's programming journey?",
 "What role did Lisp play in shaping the author's understanding and approach to AI?"]


We can batch then batch the chain over all documents and assemble our vector store and document store as before:


# Batch chain over documents to generate hypothetical questions
hypothetical_questions 
=
 chain
.
batch
(
docs
,
 
{
"max_concurrency"
:
 
5
}
)
# The vectorstore to use to index the child chunks
vectorstore 
=
 Chroma
(
    collection_name
=
"hypo-questions"
,
 embedding_function
=
OpenAIEmbeddings
(
)
)
# The storage layer for the parent documents
store 
=
 InMemoryByteStore
(
)
id_key 
=
 
"doc_id"
# The retriever (empty to start)
retriever 
=
 MultiVectorRetriever
(
    vectorstore
=
vectorstore
,
    byte_store
=
store
,
    id_key
=
id_key
,
)
doc_ids 
=
 
[
str
(
uuid
.
uuid4
(
)
)
 
for
 _ 
in
 docs
]
# Generate Document objects from hypothetical questions
question_docs 
=
 
[
]
for
 i
,
 question_list 
in
 
enumerate
(
hypothetical_questions
)
:
    question_docs
.
extend
(
        
[
Document
(
page_content
=
s
,
 metadata
=
{
id_key
:
 doc_ids
[
i
]
}
)
 
for
 s 
in
 question_list
]
    
)
retriever
.
vectorstore
.
add_documents
(
question_docs
)
retriever
.
docstore
.
mset
(
list
(
zip
(
doc_ids
,
 docs
)
)
)


Note that querying the underlying vector store will retrieve hypothetical questions that are semantically similar to the input query:


sub_docs 
=
 retriever
.
vectorstore
.
similarity_search
(
"justice breyer"
)
sub_docs


[Document(page_content='What might be the potential benefits of nominating Circuit Court of Appeals Judge Ketanji Brown Jackson to the United States Supreme Court?', metadata={'doc_id': '43292b74-d1b8-4200-8a8b-ea0cb57fbcdb'}),
 Document(page_content='How might the Bipartisan Infrastructure Law impact the economic competition between the U.S. and China?', metadata={'doc_id': '66174780-d00c-4166-9791-f0069846e734'}),
 Document(page_content='What factors led to the creation of Y Combinator?', metadata={'doc_id': '72003c4e-4cc9-4f09-a787-0b541a65b38c'}),
 Document(page_content='How did the ability to publish essays online change the landscape for writers and thinkers?', metadata={'doc_id': 'e8d2c648-f245-4bcc-b8d3-14e64a164b64'})]


And invoking the retriever will return the corresponding document:


retrieved_docs 
=
 retriever
.
invoke
(
"justice breyer"
)
len
(
retrieved_docs
[
0
]
.
page_content
)


9194
Edit this page
Was this page helpful?
Previous
How to migrate from legacy LangChain agents to LangGraph
Next
How to pass multimodal data to models
Smaller chunks
Associating summaries with a document for retrieval
Hypothetical Queries


--- docs_how_to_multimodal_inputs.txt ---

How-to guides
How to pass multimodal data to models
On this page
How to pass multimodal data to models


Here we demonstrate how to pass 
multimodal
 input directly to models.


LangChain supports multimodal data as input to chat models:




Following provider-specific formats


Adhering to a cross-provider standard




Below, we demonstrate the cross-provider standard. See 
chat model integrations
 for detail
on native formats for specific providers.


note
Most chat models that support multimodal 
image
 inputs also accept those values in
OpenAI's 
Chat Completions format
:
{
    
"type"
:
 
"image_url"
,
    
"image_url"
:
 
{
"url"
:
 image_url
}
,
}


Images
​


Many providers will accept images passed in-line as base64 data. Some will additionally accept an image from a URL directly.


Images from base64 data
​


To pass images in-line, format them as content blocks of the following form:


{
    
"type"
:
 
"image"
,
    
"source_type"
:
 
"base64"
,
    
"mime_type"
:
 
"image/jpeg"
,
  
# or image/png, etc.
    
"data"
:
 
"<base64 data string>"
,
}


Example:


import
 base64
import
 httpx
from
 langchain
.
chat_models 
import
 init_chat_model
# Fetch image data
image_url 
=
 
"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
image_data 
=
 base64
.
b64encode
(
httpx
.
get
(
image_url
)
.
content
)
.
decode
(
"utf-8"
)
# Pass to LLM
llm 
=
 init_chat_model
(
"anthropic:claude-3-5-sonnet-latest"
)
message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
            
"type"
:
 
"text"
,
            
"text"
:
 
"Describe the weather in this image:"
,
        
}
,
        
{
            
"type"
:
 
"image"
,
            
"source_type"
:
 
"base64"
,
            
"data"
:
 image_data
,
            
"mime_type"
:
 
"image/jpeg"
,
        
}
,
    
]
,
}
response 
=
 llm
.
invoke
(
[
message
]
)
print
(
response
.
text
(
)
)
API Reference:
init_chat_model


The image shows a beautiful clear day with bright blue skies and wispy cirrus clouds stretching across the horizon. The clouds are thin and streaky, creating elegant patterns against the blue backdrop. The lighting suggests it's during the day, possibly late afternoon given the warm, golden quality of the light on the grass. The weather appears calm with no signs of wind (the grass looks relatively still) and no indication of rain. It's the kind of perfect, mild weather that's ideal for walking along the wooden boardwalk through the marsh grass.


See 
LangSmith trace
 for more detail.


Images from a URL
​


Some providers (including 
OpenAI
,

Anthropic
, and

Google Gemini
) will also accept images from URLs directly.


To pass images as URLs, format them as content blocks of the following form:


{
    
"type"
:
 
"image"
,
    
"source_type"
:
 
"url"
,
    
"url"
:
 
"https://..."
,
}


Example:


message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
            
"type"
:
 
"text"
,
            
"text"
:
 
"Describe the weather in this image:"
,
        
}
,
        
{
            
"type"
:
 
"image"
,
            
"source_type"
:
 
"url"
,
            
"url"
:
 image_url
,
        
}
,
    
]
,
}
response 
=
 llm
.
invoke
(
[
message
]
)
print
(
response
.
text
(
)
)


The weather in this image appears to be pleasant and clear. The sky is mostly blue with a few scattered, light clouds, and there is bright sunlight illuminating the green grass and plants. There are no signs of rain or stormy conditions, suggesting it is a calm, likely warm day—typical of spring or summer.


We can also pass in multiple images:


message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
"type"
:
 
"text"
,
 
"text"
:
 
"Are these two images the same?"
}
,
        
{
"type"
:
 
"image"
,
 
"source_type"
:
 
"url"
,
 
"url"
:
 image_url
}
,
        
{
"type"
:
 
"image"
,
 
"source_type"
:
 
"url"
,
 
"url"
:
 image_url
}
,
    
]
,
}
response 
=
 llm
.
invoke
(
[
message
]
)
print
(
response
.
text
(
)
)


Yes, these two images are the same. They depict a wooden boardwalk going through a grassy field under a blue sky with some clouds. The colors, composition, and elements in both images are identical.


Documents (PDF)
​


Some providers (including 
OpenAI
,

Anthropic
, and

Google Gemini
) will accept PDF documents.


Documents from base64 data
​


To pass documents in-line, format them as content blocks of the following form:


{
    
"type"
:
 
"file"
,
    
"source_type"
:
 
"base64"
,
    
"mime_type"
:
 
"application/pdf"
,
    
"data"
:
 
"<base64 data string>"
,
}


Example:


import
 base64
import
 httpx
from
 langchain
.
chat_models 
import
 init_chat_model
# Fetch PDF data
pdf_url 
=
 
"https://pdfobject.com/pdf/sample.pdf"
pdf_data 
=
 base64
.
b64encode
(
httpx
.
get
(
pdf_url
)
.
content
)
.
decode
(
"utf-8"
)
# Pass to LLM
llm 
=
 init_chat_model
(
"anthropic:claude-3-5-sonnet-latest"
)
message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
            
"type"
:
 
"text"
,
            
"text"
:
 
"Describe the document:"
,
        
}
,
        
{
            
"type"
:
 
"file"
,
            
"source_type"
:
 
"base64"
,
            
"data"
:
 pdf_data
,
            
"mime_type"
:
 
"application/pdf"
,
        
}
,
    
]
,
}
response 
=
 llm
.
invoke
(
[
message
]
)
print
(
response
.
text
(
)
)
API Reference:
init_chat_model


This document appears to be a sample PDF file that contains Lorem ipsum placeholder text. It begins with a title "Sample PDF" followed by the subtitle "This is a simple PDF file. Fun fun fun."
The rest of the document consists of several paragraphs of Lorem ipsum text, which is a commonly used placeholder text in design and publishing. The text is formatted in a clean, readable layout with consistent paragraph spacing. The document appears to be a single page containing four main paragraphs of this placeholder text.
The Lorem ipsum text, while appearing to be Latin, is actually scrambled Latin-like text that is used primarily to demonstrate the visual form of a document or typeface without the distraction of meaningful content. It's commonly used in publishing and graphic design when the actual content is not yet available but the layout needs to be demonstrated.
The document has a professional, simple layout with generous margins and clear paragraph separation, making it an effective example of basic PDF formatting and structure.


Documents from a URL
​


Some providers (specifically 
Anthropic
)
will also accept documents from URLs directly.


To pass documents as URLs, format them as content blocks of the following form:


{
    
"type"
:
 
"file"
,
    
"source_type"
:
 
"url"
,
    
"url"
:
 
"https://..."
,
}


Example:


message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
            
"type"
:
 
"text"
,
            
"text"
:
 
"Describe the document:"
,
        
}
,
        
{
            
"type"
:
 
"file"
,
            
"source_type"
:
 
"url"
,
            
"url"
:
 pdf_url
,
        
}
,
    
]
,
}
response 
=
 llm
.
invoke
(
[
message
]
)
print
(
response
.
text
(
)
)


This document appears to be a sample PDF file with both text and an image. It begins with a title "Sample PDF" followed by the text "This is a simple PDF file. Fun fun fun." The rest of the document contains Lorem ipsum placeholder text arranged in several paragraphs. The content is shown both as text and as an image of the formatted PDF, with the same content displayed in a clean, formatted layout with consistent spacing and typography. The document consists of a single page containing this sample text.


Audio
​


Some providers (including 
OpenAI
 and

Google Gemini
) will accept audio inputs.


Audio from base64 data
​


To pass audio in-line, format them as content blocks of the following form:


{
    
"type"
:
 
"audio"
,
    
"source_type"
:
 
"base64"
,
    
"mime_type"
:
 
"audio/wav"
,
  
# or appropriate mime-type
    
"data"
:
 
"<base64 data string>"
,
}


Example:


import
 base64
import
 httpx
from
 langchain
.
chat_models 
import
 init_chat_model
# Fetch audio data
audio_url 
=
 
"https://upload.wikimedia.org/wikipedia/commons/3/3d/Alcal%C3%A1_de_Henares_%28RPS_13-04-2024%29_canto_de_ruise%C3%B1or_%28Luscinia_megarhynchos%29_en_el_Soto_del_Henares.wav"
audio_data 
=
 base64
.
b64encode
(
httpx
.
get
(
audio_url
)
.
content
)
.
decode
(
"utf-8"
)
# Pass to LLM
llm 
=
 init_chat_model
(
"google_genai:gemini-2.0-flash-001"
)
message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
            
"type"
:
 
"text"
,
            
"text"
:
 
"Describe this audio:"
,
        
}
,
        
{
            
"type"
:
 
"audio"
,
            
"source_type"
:
 
"base64"
,
            
"data"
:
 audio_data
,
            
"mime_type"
:
 
"audio/wav"
,
        
}
,
    
]
,
}
response 
=
 llm
.
invoke
(
[
message
]
)
print
(
response
.
text
(
)
)
API Reference:
init_chat_model


The audio appears to consist primarily of bird sounds, specifically bird vocalizations like chirping and possibly other bird songs.


Provider-specific parameters
​


Some providers will support or require additional fields on content blocks containing multimodal data.
For example, Anthropic lets you specify 
caching
 of
specific content to reduce token consumption.


To use these fields, you can:




Store them on directly on the content block; or


Use the native format supported by each provider (see 
chat model integrations
 for detail).




We show three examples below.


Example: Anthropic prompt caching
​


llm 
=
 init_chat_model
(
"anthropic:claude-3-5-sonnet-latest"
)
message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
            
"type"
:
 
"text"
,
            
"text"
:
 
"Describe the weather in this image:"
,
        
}
,
        
{
            
"type"
:
 
"image"
,
            
"source_type"
:
 
"url"
,
            
"url"
:
 image_url
,
            
"cache_control"
:
 
{
"type"
:
 
"ephemeral"
}
,
        
}
,
    
]
,
}
response 
=
 llm
.
invoke
(
[
message
]
)
print
(
response
.
text
(
)
)
response
.
usage_metadata


The image shows a beautiful, clear day with partly cloudy skies. The sky is a vibrant blue with wispy, white cirrus clouds stretching across it. The lighting suggests it's during daylight hours, possibly late afternoon or early evening given the warm, golden quality of the light on the grass. The weather appears calm with no signs of wind (the grass looks relatively still) and no threatening weather conditions. It's the kind of perfect weather you'd want for a walk along this wooden boardwalk through the marshland or grassland area.


{'input_tokens': 1586,
 'output_tokens': 117,
 'total_tokens': 1703,
 'input_token_details': {'cache_read': 0, 'cache_creation': 1582}}


next_message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
            
"type"
:
 
"text"
,
            
"text"
:
 
"Summarize that in 5 words."
,
        
}
    
]
,
}
response 
=
 llm
.
invoke
(
[
message
,
 response
,
 next_message
]
)
print
(
response
.
text
(
)
)
response
.
usage_metadata


Clear blue skies, wispy clouds.


{'input_tokens': 1716,
 'output_tokens': 12,
 'total_tokens': 1728,
 'input_token_details': {'cache_read': 1582, 'cache_creation': 0}}


Example: Anthropic citations
​


message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
            
"type"
:
 
"text"
,
            
"text"
:
 
"Generate a 5 word summary of this document."
,
        
}
,
        
{
            
"type"
:
 
"file"
,
            
"source_type"
:
 
"base64"
,
            
"data"
:
 pdf_data
,
            
"mime_type"
:
 
"application/pdf"
,
            
"citations"
:
 
{
"enabled"
:
 
True
}
,
        
}
,
    
]
,
}
response 
=
 llm
.
invoke
(
[
message
]
)
response
.
content


[{'citations': [{'cited_text': 'Sample PDF\r\nThis is a simple PDF file. Fun fun fun.\r\n',
    'document_index': 0,
    'document_title': None,
    'end_page_number': 2,
    'start_page_number': 1,
    'type': 'page_location'}],
  'text': 'Simple PDF file: fun fun',
  'type': 'text'}]


Example: OpenAI file names
​


OpenAI requires that PDF documents be associated with file names:


llm 
=
 init_chat_model
(
"openai:gpt-4.1"
)
message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
            
"type"
:
 
"text"
,
            
"text"
:
 
"Describe the document:"
,
        
}
,
        
{
            
"type"
:
 
"file"
,
            
"source_type"
:
 
"base64"
,
            
"data"
:
 pdf_data
,
            
"mime_type"
:
 
"application/pdf"
,
            
"filename"
:
 
"my-file"
,
        
}
,
    
]
,
}
response 
=
 llm
.
invoke
(
[
message
]
)
print
(
response
.
text
(
)
)


The document is a sample PDF file containing placeholder text. It consists of one page, titled "Sample PDF". The content is a mixture of English and the commonly used filler text "Lorem ipsum dolor sit amet..." and its extensions, which are often used in publishing and web design as generic text to demonstrate font, layout, and other visual elements.
**Key points about the document:**
- Length: 1 page
- Purpose: Demonstrative/sample content
- Content: No substantive or meaningful information, just demonstration text in paragraph form
- Language: English (with the Latin-like "Lorem Ipsum" text used for layout purposes)
There are no charts, tables, diagrams, or images on the page—only plain text. The document serves as an example of what a PDF file looks like rather than providing actual, useful content.


Tool calls
​


Some multimodal models support 
tool calling
 features as well. To call tools using such models, simply bind tools to them in the 
usual way
, and invoke the model using content blocks of the desired type (e.g., containing image data).


from
 typing 
import
 Literal
from
 langchain_core
.
tools 
import
 tool
@tool
def
 
weather_tool
(
weather
:
 Literal
[
"sunny"
,
 
"cloudy"
,
 
"rainy"
]
)
 
-
>
 
None
:
    
"""Describe the weather"""
    
pass
llm_with_tools 
=
 llm
.
bind_tools
(
[
weather_tool
]
)
message 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
[
        
{
"type"
:
 
"text"
,
 
"text"
:
 
"Describe the weather in this image:"
}
,
        
{
"type"
:
 
"image"
,
 
"source_type"
:
 
"url"
,
 
"url"
:
 image_url
}
,
    
]
,
}
response 
=
 llm_with_tools
.
invoke
(
[
message
]
)
response
.
tool_calls
API Reference:
tool


[{'name': 'weather_tool',
  'args': {'weather': 'sunny'},
  'id': 'toolu_01G6JgdkhwggKcQKfhXZQPjf',
  'type': 'tool_call'}]
Edit this page
Was this page helpful?
Previous
How to retrieve using multiple vectors per document
Next
How to use multimodal prompts
Images
Images from base64 data
Images from a URL
Documents (PDF)
Documents from base64 data
Documents from a URL
Audio
Audio from base64 data
Provider-specific parameters
Example: Anthropic prompt caching
Example: Anthropic citations
Example: OpenAI file names
Tool calls


--- docs_how_to_multimodal_prompts.txt ---

How-to guides
How to use multimodal prompts
How to use multimodal prompts


Here we demonstrate how to use prompt templates to format 
multimodal
 inputs to models.


To use prompt templates in the context of multimodal data, we can templatize elements of the corresponding content block.
For example, below we define a prompt that takes a URL for an image as a parameter:


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
# Define prompt
prompt 
=
 ChatPromptTemplate
(
    
[
        
{
            
"role"
:
 
"system"
,
            
"content"
:
 
"Describe the image provided."
,
        
}
,
        
{
            
"role"
:
 
"user"
,
            
"content"
:
 
[
                
{
                    
"type"
:
 
"image"
,
                    
"source_type"
:
 
"url"
,
                    
"url"
:
 
"{image_url}"
,
                
}
,
            
]
,
        
}
,
    
]
)
API Reference:
ChatPromptTemplate


Let's use this prompt to pass an image to a 
chat model
:


from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"anthropic:claude-3-5-sonnet-latest"
)
url 
=
 
"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
chain 
=
 prompt 
|
 llm
response 
=
 chain
.
invoke
(
{
"image_url"
:
 url
}
)
print
(
response
.
text
(
)
)
API Reference:
init_chat_model


This image shows a beautiful wooden boardwalk cutting through a lush green wetland or marsh area. The boardwalk extends straight ahead toward the horizon, creating a strong leading line through the composition. On either side, tall green grasses sway in what appears to be a summer or late spring setting. The sky is particularly striking, with wispy cirrus clouds streaking across a vibrant blue background. In the distance, you can see a tree line bordering the wetland area. The lighting suggests this may be during "golden hour" - either early morning or late afternoon - as there's a warm, gentle quality to the light that's illuminating the scene. The wooden planks of the boardwalk appear well-maintained and provide safe passage through what would otherwise be difficult terrain to traverse. It's the kind of scene you might find in a nature preserve or wildlife refuge designed to give visitors access to observe wetland ecosystems while protecting the natural environment.


Note that we can templatize arbitrary elements of the content block:


prompt 
=
 ChatPromptTemplate
(
    
[
        
{
            
"role"
:
 
"system"
,
            
"content"
:
 
"Describe the image provided."
,
        
}
,
        
{
            
"role"
:
 
"user"
,
            
"content"
:
 
[
                
{
                    
"type"
:
 
"image"
,
                    
"source_type"
:
 
"base64"
,
                    
"mime_type"
:
 
"{image_mime_type}"
,
                    
"data"
:
 
"{image_data}"
,
                    
"cache_control"
:
 
{
"type"
:
 
"{cache_type}"
}
,
                
}
,
            
]
,
        
}
,
    
]
)


import
 base64
import
 httpx
image_data 
=
 base64
.
b64encode
(
httpx
.
get
(
url
)
.
content
)
.
decode
(
"utf-8"
)
chain 
=
 prompt 
|
 llm
response 
=
 chain
.
invoke
(
    
{
        
"image_data"
:
 image_data
,
        
"image_mime_type"
:
 
"image/jpeg"
,
        
"cache_type"
:
 
"ephemeral"
,
    
}
)
print
(
response
.
text
(
)
)


This image shows a beautiful wooden boardwalk cutting through a lush green marsh or wetland area. The boardwalk extends straight ahead toward the horizon, creating a strong leading line in the composition. The surrounding vegetation consists of tall grass and reeds in vibrant green hues, with some bushes and trees visible in the background. The sky is particularly striking, featuring a bright blue color with wispy white clouds streaked across it. The lighting suggests this photo was taken during the "golden hour" - either early morning or late afternoon - giving the scene a warm, peaceful quality. The raised wooden path provides accessible access through what would otherwise be difficult terrain to traverse, allowing visitors to experience and appreciate this natural environment.
Edit this page
Was this page helpful?
Previous
How to pass multimodal data to models
Next
How to create a custom Output Parser


--- docs_how_to_output_parser_custom.txt ---

How-to guides
How to create a custom Output Parser
On this page
How to create a custom Output Parser


In some situations you may want to implement a custom 
parser
 to structure the model output into a custom format.


There are two ways to implement a custom parser:




Using 
RunnableLambda
 or 
RunnableGenerator
 in 
LCEL
 -- we strongly recommend this for most use cases


By inheriting from one of the base classes for out parsing -- this is the hard way of doing things




The difference between the two approaches are mostly superficial and are mainly in terms of which callbacks are triggered (e.g., 
on_chain_start
 vs. 
on_parser_start
), and how a runnable lambda vs. a parser might be visualized in a tracing platform like LangSmith.


Runnable Lambdas and Generators
​


The recommended way to parse is using 
runnable lambdas
 and 
runnable generators
!


Here, we will make a simple parse that inverts the case of the output from the model.


For example, if the model outputs: "Meow", the parser will produce "mEOW".


from
 typing 
import
 Iterable
from
 langchain_anthropic
.
chat_models 
import
 ChatAnthropic
from
 langchain_core
.
messages 
import
 AIMessage
,
 AIMessageChunk
model 
=
 ChatAnthropic
(
model_name
=
"claude-2.1"
)
def
 
parse
(
ai_message
:
 AIMessage
)
 
-
>
 
str
:
    
"""Parse the AI message."""
    
return
 ai_message
.
content
.
swapcase
(
)
chain 
=
 model 
|
 parse
chain
.
invoke
(
"hello"
)
API Reference:
ChatAnthropic
 | 
AIMessage
 | 
AIMessageChunk


'hELLO!'


tip
LCEL automatically upgrades the function 
parse
 to 
RunnableLambda(parse)
 when composed using a 
|
  syntax.
If you don't like that you can manually import 
RunnableLambda
 and then run
parse = RunnableLambda(parse)
.


Does streaming work?


for
 chunk 
in
 chain
.
stream
(
"tell me about yourself in one sentence"
)
:
    
print
(
chunk
,
 end
=
"|"
,
 flush
=
True
)


i'M cLAUDE, AN ai ASSISTANT CREATED BY aNTHROPIC TO BE HELPFUL, HARMLESS, AND HONEST.|


No, it doesn't because the parser aggregates the input before parsing the output.


If we want to implement a streaming parser, we can have the parser accept an iterable over the input instead and yield
the results as they're available.


from
 langchain_core
.
runnables 
import
 RunnableGenerator
def
 
streaming_parse
(
chunks
:
 Iterable
[
AIMessageChunk
]
)
 
-
>
 Iterable
[
str
]
:
    
for
 chunk 
in
 chunks
:
        
yield
 chunk
.
content
.
swapcase
(
)
streaming_parse 
=
 RunnableGenerator
(
streaming_parse
)
API Reference:
RunnableGenerator


important
Please wrap the streaming parser in 
RunnableGenerator
 as we may stop automatically upgrading it with the 
|
 syntax.


chain 
=
 model 
|
 streaming_parse
chain
.
invoke
(
"hello"
)


'hELLO!'


Let's confirm that streaming works!


for
 chunk 
in
 chain
.
stream
(
"tell me about yourself in one sentence"
)
:
    
print
(
chunk
,
 end
=
"|"
,
 flush
=
True
)


i|'M| cLAUDE|,| AN| ai| ASSISTANT| CREATED| BY| aN|THROP|IC| TO| BE| HELPFUL|,| HARMLESS|,| AND| HONEST|.|


Inheriting from Parsing Base Classes
​


Another approach to implement a parser is by inheriting from 
BaseOutputParser
, 
BaseGenerationOutputParser
 or another one of the base parsers depending on what you need to do.


In general, we 
do not
 recommend this approach for most use cases as it results in more code to write without significant benefits.


The simplest kind of output parser extends the 
BaseOutputParser
 class and must implement the following methods:




parse
: takes the string output from the model and parses it


(optional) 
_type
: identifies the name of the parser.




When the output from the chat model or LLM is malformed, the can throw an 
OutputParserException
 to indicate that parsing fails because of bad input. Using this exception allows code that utilizes the parser to handle the exceptions in a consistent manner.


Parsers are Runnables! 🏃
Because 
BaseOutputParser
 implements the 
Runnable
 interface, any custom parser you will create this way will become valid LangChain Runnables and will benefit from automatic async support, batch interface, logging support etc.


Simple Parser
​


Here's a simple parser that can parse a 
string
 representation of a boolean (e.g., 
YES
 or 
NO
) and convert it into the corresponding 
boolean
 type.


from
 langchain_core
.
exceptions 
import
 OutputParserException
from
 langchain_core
.
output_parsers 
import
 BaseOutputParser
# The [bool] desribes a parameterization of a generic.
# It's basically indicating what the return type of parse is
# in this case the return type is either True or False
class
 
BooleanOutputParser
(
BaseOutputParser
[
bool
]
)
:
    
"""Custom boolean parser."""
    true_val
:
 
str
 
=
 
"YES"
    false_val
:
 
str
 
=
 
"NO"
    
def
 
parse
(
self
,
 text
:
 
str
)
 
-
>
 
bool
:
        cleaned_text 
=
 text
.
strip
(
)
.
upper
(
)
        
if
 cleaned_text 
not
 
in
 
(
self
.
true_val
.
upper
(
)
,
 self
.
false_val
.
upper
(
)
)
:
            
raise
 OutputParserException
(
                
f"BooleanOutputParser expected output value to either be "
                
f"
{
self
.
true_val
}
 or 
{
self
.
false_val
}
 (case-insensitive). "
                
f"Received 
{
cleaned_text
}
."
            
)
        
return
 cleaned_text 
==
 self
.
true_val
.
upper
(
)
    
@property
    
def
 
_type
(
self
)
 
-
>
 
str
:
        
return
 
"boolean_output_parser"
API Reference:
OutputParserException
 | 
BaseOutputParser


parser 
=
 BooleanOutputParser
(
)
parser
.
invoke
(
"YES"
)


True


try
:
    parser
.
invoke
(
"MEOW"
)
except
 Exception 
as
 e
:
    
print
(
f"Triggered an exception of type: 
{
type
(
e
)
}
"
)


Triggered an exception of type: <class 'langchain_core.exceptions.OutputParserException'>


Let's test changing the parameterization


parser 
=
 BooleanOutputParser
(
true_val
=
"OKAY"
)
parser
.
invoke
(
"OKAY"
)


True


Let's confirm that other LCEL methods are present


parser
.
batch
(
[
"OKAY"
,
 
"NO"
]
)


[True, False]


await
 parser
.
abatch
(
[
"OKAY"
,
 
"NO"
]
)


[True, False]


from
 langchain_anthropic
.
chat_models 
import
 ChatAnthropic
anthropic 
=
 ChatAnthropic
(
model_name
=
"claude-2.1"
)
anthropic
.
invoke
(
"say OKAY or NO"
)
API Reference:
ChatAnthropic


AIMessage(content='OKAY')


Let's test that our parser works!


chain 
=
 anthropic 
|
 parser
chain
.
invoke
(
"say OKAY or NO"
)


True


note
The parser will work with either the output from an LLM (a string) or the output from a chat model (an 
AIMessage
)!


Parsing Raw Model Outputs
​


Sometimes there is additional metadata on the model output that is important besides the raw text. One example of this is tool calling, where arguments intended to be passed to called functions are returned in a separate property. If you need this finer-grained control, you can instead subclass the 
BaseGenerationOutputParser
 class.


This class requires a single method 
parse_result
. This method takes raw model output (e.g., list of 
Generation
 or 
ChatGeneration
) and returns the parsed output.


Supporting both 
Generation
 and 
ChatGeneration
 allows the parser to work with both regular LLMs as well as with Chat Models.


from
 typing 
import
 List
from
 langchain_core
.
exceptions 
import
 OutputParserException
from
 langchain_core
.
messages 
import
 AIMessage
from
 langchain_core
.
output_parsers 
import
 BaseGenerationOutputParser
from
 langchain_core
.
outputs 
import
 ChatGeneration
,
 Generation
class
 
StrInvertCase
(
BaseGenerationOutputParser
[
str
]
)
:
    
"""An example parser that inverts the case of the characters in the message.
    This is an example parse shown just for demonstration purposes and to keep
    the example as simple as possible.
    """
    
def
 
parse_result
(
self
,
 result
:
 List
[
Generation
]
,
 
*
,
 partial
:
 
bool
 
=
 
False
)
 
-
>
 
str
:
        
"""Parse a list of model Generations into a specific format.
        Args:
            result: A list of Generations to be parsed. The Generations are assumed
                to be different candidate outputs for a single model input.
                Many parsers assume that only a single generation is passed it in.
                We will assert for that
            partial: Whether to allow partial results. This is used for parsers
                     that support streaming
        """
        
if
 
len
(
result
)
 
!=
 
1
:
            
raise
 NotImplementedError
(
                
"This output parser can only be used with a single generation."
            
)
        generation 
=
 result
[
0
]
        
if
 
not
 
isinstance
(
generation
,
 ChatGeneration
)
:
            
# Say that this one only works with chat generations
            
raise
 OutputParserException
(
                
"This output parser can only be used with a chat generation."
            
)
        
return
 generation
.
message
.
content
.
swapcase
(
)
chain 
=
 anthropic 
|
 StrInvertCase
(
)
API Reference:
OutputParserException
 | 
AIMessage
 | 
BaseGenerationOutputParser
 | 
ChatGeneration
 | 
Generation


Let's the new parser! It should be inverting the output from the model.


chain
.
invoke
(
"Tell me a short sentence about yourself"
)


'hELLO! mY NAME IS cLAUDE.'
Edit this page
Was this page helpful?
Previous
How to use multimodal prompts
Next
How to use the output-fixing parser
Runnable Lambdas and Generators
Inheriting from Parsing Base Classes
Simple Parser
Parsing Raw Model Outputs


--- docs_how_to_output_parser_fixing.txt ---

How-to guides
How to use the output-fixing parser
How to use the output-fixing parser


This 
output parser
 wraps another output parser, and in the event that the first one fails it calls out to another LLM to fix any errors.


But we can do other things besides throw errors. Specifically, we can pass the misformatted output, along with the formatted instructions, to the model and ask it to fix it.


For this example, we'll use the above Pydantic output parser. Here's what happens if we pass it a result that does not comply with the schema:


from
 typing 
import
 List
from
 langchain_core
.
exceptions 
import
 OutputParserException
from
 langchain_core
.
output_parsers 
import
 PydanticOutputParser
from
 langchain_openai 
import
 ChatOpenAI
from
 pydantic 
import
 BaseModel
,
 Field
API Reference:
OutputParserException
 | 
PydanticOutputParser
 | 
ChatOpenAI


class
 
Actor
(
BaseModel
)
:
    name
:
 
str
 
=
 Field
(
description
=
"name of an actor"
)
    film_names
:
 List
[
str
]
 
=
 Field
(
description
=
"list of names of films they starred in"
)
actor_query 
=
 
"Generate the filmography for a random actor."
parser 
=
 PydanticOutputParser
(
pydantic_object
=
Actor
)


misformatted 
=
 
"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}"


try
:
    parser
.
parse
(
misformatted
)
except
 OutputParserException 
as
 e
:
    
print
(
e
)


Invalid json output: {'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}
For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE


Now we can construct and use a 
OutputFixingParser
. This output parser takes as an argument another output parser but also an LLM with which to try to correct any formatting mistakes.


from
 langchain
.
output_parsers 
import
 OutputFixingParser
new_parser 
=
 OutputFixingParser
.
from_llm
(
parser
=
parser
,
 llm
=
ChatOpenAI
(
)
)
API Reference:
OutputFixingParser


new_parser
.
parse
(
misformatted
)


Actor(name='Tom Hanks', film_names=['Forrest Gump'])


Find out api documentation for 
OutputFixingParser
.
Edit this page
Was this page helpful?
Previous
How to create a custom Output Parser
Next
How to parse JSON output


--- docs_how_to_output_parser_json.txt ---

How-to guides
How to parse JSON output
On this page
How to parse JSON output


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


Output parsers


Prompt templates


Structured output


Chaining runnables together




While some model providers support 
built-in ways to return structured output
, not all do. We can use an output parser to help users to specify an arbitrary JSON schema via the prompt, query a model for outputs that conform to that schema, and finally parse that schema as JSON.


note
Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed JSON.


The 
JsonOutputParser
 is one built-in option for prompting for and then parsing JSON output. While it is similar in functionality to the 
PydanticOutputParser
, it also supports streaming back partial JSON objects.


Here's an example of how it can be used alongside 
Pydantic
 to conveniently declare the expected schema:


%
pip install 
-
qU langchain langchain
-
openai
import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


from
 langchain_core
.
output_parsers 
import
 JsonOutputParser
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
from
 pydantic 
import
 BaseModel
,
 Field
model 
=
 ChatOpenAI
(
temperature
=
0
)
# Define your desired data structure.
class
 
Joke
(
BaseModel
)
:
    setup
:
 
str
 
=
 Field
(
description
=
"question to set up a joke"
)
    punchline
:
 
str
 
=
 Field
(
description
=
"answer to resolve the joke"
)
# And a query intented to prompt a language model to populate the data structure.
joke_query 
=
 
"Tell me a joke."
# Set up a parser + inject instructions into the prompt template.
parser 
=
 JsonOutputParser
(
pydantic_object
=
Joke
)
prompt 
=
 PromptTemplate
(
    template
=
"Answer the user query.\n{format_instructions}\n{query}\n"
,
    input_variables
=
[
"query"
]
,
    partial_variables
=
{
"format_instructions"
:
 parser
.
get_format_instructions
(
)
}
,
)
chain 
=
 prompt 
|
 model 
|
 parser
chain
.
invoke
(
{
"query"
:
 joke_query
}
)
API Reference:
JsonOutputParser
 | 
PromptTemplate
 | 
ChatOpenAI


{'setup': "Why couldn't the bicycle stand up by itself?",
 'punchline': 'Because it was two tired!'}


Note that we are passing 
format_instructions
 from the parser directly into the prompt. You can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions:


parser
.
get_format_instructions
(
)


'The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}\nthe object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.\n\nHere is the output schema:\n\`\`\`\n{"properties": {"setup": {"title": "Setup", "description": "question to set up a joke", "type": "string"}, "punchline": {"title": "Punchline", "description": "answer to resolve the joke", "type": "string"}}, "required": ["setup", "punchline"]}\n\`\`\`'


Streaming
​


As mentioned above, a key difference between the 
JsonOutputParser
 and the 
PydanticOutputParser
 is that the 
JsonOutputParser
 output parser supports streaming partial chunks. Here's what that looks like:


for
 s 
in
 chain
.
stream
(
{
"query"
:
 joke_query
}
)
:
    
print
(
s
)


{}
{'setup': ''}
{'setup': 'Why'}
{'setup': 'Why couldn'}
{'setup': "Why couldn't"}
{'setup': "Why couldn't the"}
{'setup': "Why couldn't the bicycle"}
{'setup': "Why couldn't the bicycle stand"}
{'setup': "Why couldn't the bicycle stand up"}
{'setup': "Why couldn't the bicycle stand up by"}
{'setup': "Why couldn't the bicycle stand up by itself"}
{'setup': "Why couldn't the bicycle stand up by itself?"}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': ''}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because'}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because it'}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because it was'}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because it was two'}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because it was two tired'}
{'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'Because it was two tired!'}


Without Pydantic
​


You can also use the 
JsonOutputParser
 without Pydantic. This will prompt the model to return JSON, but doesn't provide specifics about what the schema should be.


joke_query 
=
 
"Tell me a joke."
parser 
=
 JsonOutputParser
(
)
prompt 
=
 PromptTemplate
(
    template
=
"Answer the user query.\n{format_instructions}\n{query}\n"
,
    input_variables
=
[
"query"
]
,
    partial_variables
=
{
"format_instructions"
:
 parser
.
get_format_instructions
(
)
}
,
)
chain 
=
 prompt 
|
 model 
|
 parser
chain
.
invoke
(
{
"query"
:
 joke_query
}
)


{'response': "Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!"}


Next steps
​


You've now learned one way to prompt a model to return structured JSON. Next, check out the 
broader guide on obtaining structured output
 for other techniques.
Edit this page
Was this page helpful?
Previous
How to use the output-fixing parser
Next
How to retry when a parsing error occurs
Streaming
Without Pydantic
Next steps


--- docs_how_to_output_parser_retry.txt ---

How-to guides
How to retry when a parsing error occurs
How to retry when a parsing error occurs


While in some cases it is possible to fix any parsing mistakes by only looking at the output, in other cases it isn't. An example of this is when the output is not just in the incorrect format, but is partially complete. Consider the below example.


from
 langchain
.
output_parsers 
import
 OutputFixingParser
from
 langchain_core
.
exceptions 
import
 OutputParserException
from
 langchain_core
.
output_parsers 
import
 PydanticOutputParser
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
,
 OpenAI
from
 pydantic 
import
 BaseModel
,
 Field
API Reference:
OutputFixingParser
 | 
OutputParserException
 | 
PydanticOutputParser
 | 
PromptTemplate
 | 
ChatOpenAI
 | 
OpenAI


template 
=
 
"""Based on the user question, provide an Action and Action Input for what step should be taken.
{format_instructions}
Question: {query}
Response:"""
class
 
Action
(
BaseModel
)
:
    action
:
 
str
 
=
 Field
(
description
=
"action to take"
)
    action_input
:
 
str
 
=
 Field
(
description
=
"input to the action"
)
parser 
=
 PydanticOutputParser
(
pydantic_object
=
Action
)


prompt 
=
 PromptTemplate
(
    template
=
"Answer the user query.\n{format_instructions}\n{query}\n"
,
    input_variables
=
[
"query"
]
,
    partial_variables
=
{
"format_instructions"
:
 parser
.
get_format_instructions
(
)
}
,
)


prompt_value 
=
 prompt
.
format_prompt
(
query
=
"who is leo di caprios gf?"
)


bad_response 
=
 
'{"action": "search"}'


If we try to parse this response as is, we will get an error:


try
:
    parser
.
parse
(
bad_response
)
except
 OutputParserException 
as
 e
:
    
print
(
e
)


Failed to parse Action from completion {"action": "search"}. Got: 1 validation error for Action
action_input
  Field required [type=missing, input_value={'action': 'search'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE


If we try to use the 
OutputFixingParser
 to fix this error, it will be confused - namely, it doesn't know what to actually put for action input.


fix_parser 
=
 OutputFixingParser
.
from_llm
(
parser
=
parser
,
 llm
=
ChatOpenAI
(
)
)


fix_parser
.
parse
(
bad_response
)


Action(action='search', action_input='input')


Instead, we can use the RetryOutputParser, which passes in the prompt (as well as the original output) to try again to get a better response.


from
 langchain
.
output_parsers 
import
 RetryOutputParser
API Reference:
RetryOutputParser


retry_parser 
=
 RetryOutputParser
.
from_llm
(
parser
=
parser
,
 llm
=
OpenAI
(
temperature
=
0
)
)


retry_parser
.
parse_with_prompt
(
bad_response
,
 prompt_value
)


Action(action='search', action_input='leo di caprio girlfriend')


We can also add the RetryOutputParser easily with a custom chain which transform the raw LLM/ChatModel output into a more workable format.


from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnableParallel
completion_chain 
=
 prompt 
|
 OpenAI
(
temperature
=
0
)
main_chain 
=
 RunnableParallel
(
    completion
=
completion_chain
,
 prompt_value
=
prompt
)
 
|
 RunnableLambda
(
lambda
 x
:
 retry_parser
.
parse_with_prompt
(
**
x
)
)
main_chain
.
invoke
(
{
"query"
:
 
"who is leo di caprios gf?"
}
)
API Reference:
RunnableLambda
 | 
RunnableParallel


Action(action='search', action_input='leo di caprio girlfriend')


Find out api documentation for 
RetryOutputParser
.
Edit this page
Was this page helpful?
Previous
How to parse JSON output
Next
How to parse text from message objects


--- docs_how_to_output_parser_string.txt ---

How-to guides
How to parse text from message objects
How to parse text from message objects


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


Messages


Output parsers


LangChain Expression Language (LCEL)




LangChain 
message
 objects support content in a 
variety of formats
, including text, 
multimodal data
, and a list of 
content block
 dicts.


The format of 
Chat model
 response content may depend on the provider. For example, the chat model for 
Anthropic
 will return string content for typical string input:


from
 langchain_anthropic 
import
 ChatAnthropic
llm 
=
 ChatAnthropic
(
model
=
"claude-3-5-haiku-latest"
)
response 
=
 llm
.
invoke
(
"Hello"
)
response
.
content
API Reference:
ChatAnthropic


'Hi there! How are you doing today? Is there anything I can help you with?'


But when tool calls are generated, the response content is structured into content blocks that convey the model's reasoning process:


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
get_weather
(
location
:
 
str
)
 
-
>
 
str
:
    
"""Get the weather from a location."""
    
return
 
"Sunny."
llm_with_tools 
=
 llm
.
bind_tools
(
[
get_weather
]
)
response 
=
 llm_with_tools
.
invoke
(
"What's the weather in San Francisco, CA?"
)
response
.
content
API Reference:
tool


[{'text': "I'll help you get the current weather for San Francisco, California. Let me check that for you right away.",
  'type': 'text'},
 {'id': 'toolu_015PwwcKxWYctKfY3pruHFyy',
  'input': {'location': 'San Francisco, CA'},
  'name': 'get_weather',
  'type': 'tool_use'}]


To automatically parse text from message objects irrespective of the format of the underlying content, we can use 
StrOutputParser
. We can compose it with a chat model as follows:


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
chain 
=
 llm_with_tools 
|
 StrOutputParser
(
)
API Reference:
StrOutputParser


StrOutputParser
 simplifies the extraction of text from message objects:


response 
=
 chain
.
invoke
(
"What's the weather in San Francisco, CA?"
)
print
(
response
)


I'll help you check the weather in San Francisco, CA right away.


This is particularly useful in streaming contexts:


for
 chunk 
in
 chain
.
stream
(
"What's the weather in San Francisco, CA?"
)
:
    
print
(
chunk
,
 end
=
"|"
)


|I'll| help| you get| the current| weather for| San Francisco, California|. Let| me retrieve| that| information for you.||||||||||


See the 
API Reference
 for more information.
Edit this page
Was this page helpful?
Previous
How to retry when a parsing error occurs
Next
How to parse XML output


--- docs_how_to_output_parser_structured.txt ---

How-to guides
How to use output parsers to parse an LLM response into structured format
On this page
How to use output parsers to parse an LLM response into structured format


Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support 
built-in ways to return structured output
, not all do.


Output parsers
 are classes that help structure language model responses. There are two main methods an output parser must implement:




"Get format instructions": A method which returns a string containing instructions for how the output of a language model should be formatted.


"Parse": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.




And then one optional one:




"Parse with prompt": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.




Get started
​


Below we go over the main type of output parser, the 
PydanticOutputParser
.


from
 langchain_core
.
output_parsers 
import
 PydanticOutputParser
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_openai 
import
 OpenAI
from
 pydantic 
import
 BaseModel
,
 Field
,
 model_validator
model 
=
 OpenAI
(
model_name
=
"gpt-3.5-turbo-instruct"
,
 temperature
=
0.0
)
# Define your desired data structure.
class
 
Joke
(
BaseModel
)
:
    setup
:
 
str
 
=
 Field
(
description
=
"question to set up a joke"
)
    punchline
:
 
str
 
=
 Field
(
description
=
"answer to resolve the joke"
)
    
# You can add custom validation logic easily with Pydantic.
    
@model_validator
(
mode
=
"before"
)
    
@classmethod
    
def
 
question_ends_with_question_mark
(
cls
,
 values
:
 
dict
)
 
-
>
 
dict
:
        setup 
=
 values
.
get
(
"setup"
)
        
if
 setup 
and
 setup
[
-
1
]
 
!=
 
"?"
:
            
raise
 ValueError
(
"Badly formed question!"
)
        
return
 values
# Set up a parser + inject instructions into the prompt template.
parser 
=
 PydanticOutputParser
(
pydantic_object
=
Joke
)
prompt 
=
 PromptTemplate
(
    template
=
"Answer the user query.\n{format_instructions}\n{query}\n"
,
    input_variables
=
[
"query"
]
,
    partial_variables
=
{
"format_instructions"
:
 parser
.
get_format_instructions
(
)
}
,
)
# And a query intended to prompt a language model to populate the data structure.
prompt_and_model 
=
 prompt 
|
 model
output 
=
 prompt_and_model
.
invoke
(
{
"query"
:
 
"Tell me a joke."
}
)
parser
.
invoke
(
output
)
API Reference:
PydanticOutputParser
 | 
PromptTemplate
 | 
OpenAI


Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')


LCEL
​


Output parsers implement the 
Runnable interface
, the basic building block of the 
LangChain Expression Language (LCEL)
. This means they support 
invoke
, 
ainvoke
, 
stream
, 
astream
, 
batch
, 
abatch
, 
astream_log
 calls.


Output parsers accept a string or 
BaseMessage
 as input and can return an arbitrary type.


parser
.
invoke
(
output
)


Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')


Instead of manually invoking the parser, we also could've just added it to our 
Runnable
 sequence:


chain 
=
 prompt 
|
 model 
|
 parser
chain
.
invoke
(
{
"query"
:
 
"Tell me a joke."
}
)


Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')


While all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.


The 
SimpleJsonOutputParser
 for example can stream through partial outputs:


from
 langchain
.
output_parsers
.
json 
import
 SimpleJsonOutputParser
json_prompt 
=
 PromptTemplate
.
from_template
(
    
"Return a JSON object with an `answer` key that answers the following question: {question}"
)
json_parser 
=
 SimpleJsonOutputParser
(
)
json_chain 
=
 json_prompt 
|
 model 
|
 json_parser
API Reference:
SimpleJsonOutputParser


list
(
json_chain
.
stream
(
{
"question"
:
 
"Who invented the microscope?"
}
)
)


[{},
 {'answer': ''},
 {'answer': 'Ant'},
 {'answer': 'Anton'},
 {'answer': 'Antonie'},
 {'answer': 'Antonie van'},
 {'answer': 'Antonie van Lee'},
 {'answer': 'Antonie van Leeu'},
 {'answer': 'Antonie van Leeuwen'},
 {'answer': 'Antonie van Leeuwenho'},
 {'answer': 'Antonie van Leeuwenhoek'}]


Similarly,for 
PydanticOutputParser
:


list
(
chain
.
stream
(
{
"query"
:
 
"Tell me a joke."
}
)
)


[Joke(setup='Why did the tomato turn red?', punchline=''),
 Joke(setup='Why did the tomato turn red?', punchline='Because'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it saw'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it saw the'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing'),
 Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')]
Edit this page
Was this page helpful?
Previous
How to run custom functions
Next
How to handle cases where no queries are generated
Get started
LCEL


--- docs_how_to_output_parser_xml.txt ---

How-to guides
How to parse XML output
On this page
How to parse XML output


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


Output parsers


Prompt templates


Structured output


Chaining runnables together




LLMs from different providers often have different strengths depending on the specific data they are trained on. This also means that some may be "better" and more reliable at generating output in formats other than JSON.


This guide shows you how to use the 
XMLOutputParser
 to prompt models for XML output, then and 
parse
 that output into a usable format.


note
Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed XML.


In the following examples, we use Anthropic's Claude-2 model (
https://docs.anthropic.com/claude/docs
), which is one such model that is optimized for XML tags.


%
pip install 
-
qU langchain langchain
-
anthropic
import
 os
from
 getpass 
import
 getpass
if
 
"ANTHROPIC_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"ANTHROPIC_API_KEY"
]
 
=
 getpass
(
)


Let's start with a simple request to the model.


from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_core
.
output_parsers 
import
 XMLOutputParser
from
 langchain_core
.
prompts 
import
 PromptTemplate
model 
=
 ChatAnthropic
(
model
=
"claude-2.1"
,
 max_tokens_to_sample
=
512
,
 temperature
=
0.1
)
actor_query 
=
 
"Generate the shortened filmography for Tom Hanks."
output 
=
 model
.
invoke
(
    
f"""
{
actor_query
}
Please enclose the movies in <movie></movie> tags"""
)
print
(
output
.
content
)
API Reference:
ChatAnthropic
 | 
XMLOutputParser
 | 
PromptTemplate


Here is the shortened filmography for Tom Hanks, with movies enclosed in XML tags:
<movie>Splash</movie>
<movie>Big</movie>
<movie>A League of Their Own</movie>
<movie>Sleepless in Seattle</movie>
<movie>Forrest Gump</movie>
<movie>Toy Story</movie>
<movie>Apollo 13</movie>
<movie>Saving Private Ryan</movie>
<movie>Cast Away</movie>
<movie>The Da Vinci Code</movie>


This actually worked pretty well! But it would be nice to parse that XML into a more easily usable format. We can use the 
XMLOutputParser
 to both add default format instructions to the prompt and parse outputted XML into a dict:


parser 
=
 XMLOutputParser
(
)
# We will add these instructions to the prompt below
parser
.
get_format_instructions
(
)


'The output should be formatted as a XML file.\n1. Output should conform to the tags below. \n2. If tags are not given, make them on your own.\n3. Remember to always open and close all the tags.\n\nAs an example, for the tags ["foo", "bar", "baz"]:\n1. String "<foo>\n   <bar>\n      <baz></baz>\n   </bar>\n</foo>" is a well-formatted instance of the schema. \n2. String "<foo>\n   <bar>\n   </foo>" is a badly-formatted instance.\n3. String "<foo>\n   <tag>\n   </tag>\n</foo>" is a badly-formatted instance.\n\nHere are the output tags:\n\`\`\`\nNone\n\`\`\`'


prompt 
=
 PromptTemplate
(
    template
=
"""{query}\n{format_instructions}"""
,
    input_variables
=
[
"query"
]
,
    partial_variables
=
{
"format_instructions"
:
 parser
.
get_format_instructions
(
)
}
,
)
chain 
=
 prompt 
|
 model 
|
 parser
output 
=
 chain
.
invoke
(
{
"query"
:
 actor_query
}
)
print
(
output
)


{'filmography': [{'movie': [{'title': 'Big'}, {'year': '1988'}]}, {'movie': [{'title': 'Forrest Gump'}, {'year': '1994'}]}, {'movie': [{'title': 'Toy Story'}, {'year': '1995'}]}, {'movie': [{'title': 'Saving Private Ryan'}, {'year': '1998'}]}, {'movie': [{'title': 'Cast Away'}, {'year': '2000'}]}]}


We can also add some tags to tailor the output to our needs. You can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions:


parser 
=
 XMLOutputParser
(
tags
=
[
"movies"
,
 
"actor"
,
 
"film"
,
 
"name"
,
 
"genre"
]
)
# We will add these instructions to the prompt below
parser
.
get_format_instructions
(
)


'The output should be formatted as a XML file.\n1. Output should conform to the tags below. \n2. If tags are not given, make them on your own.\n3. Remember to always open and close all the tags.\n\nAs an example, for the tags ["foo", "bar", "baz"]:\n1. String "<foo>\n   <bar>\n      <baz></baz>\n   </bar>\n</foo>" is a well-formatted instance of the schema. \n2. String "<foo>\n   <bar>\n   </foo>" is a badly-formatted instance.\n3. String "<foo>\n   <tag>\n   </tag>\n</foo>" is a badly-formatted instance.\n\nHere are the output tags:\n\`\`\`\n[\'movies\', \'actor\', \'film\', \'name\', \'genre\']\n\`\`\`'


prompt 
=
 PromptTemplate
(
    template
=
"""{query}\n{format_instructions}"""
,
    input_variables
=
[
"query"
]
,
    partial_variables
=
{
"format_instructions"
:
 parser
.
get_format_instructions
(
)
}
,
)
chain 
=
 prompt 
|
 model 
|
 parser
output 
=
 chain
.
invoke
(
{
"query"
:
 actor_query
}
)
print
(
output
)


{'movies': [{'actor': [{'name': 'Tom Hanks'}, {'film': [{'name': 'Forrest Gump'}, {'genre': 'Drama'}]}, {'film': [{'name': 'Cast Away'}, {'genre': 'Adventure'}]}, {'film': [{'name': 'Saving Private Ryan'}, {'genre': 'War'}]}]}]}


This output parser also supports streaming of partial chunks. Here's an example:


for
 s 
in
 chain
.
stream
(
{
"query"
:
 actor_query
}
)
:
    
print
(
s
)


{'movies': [{'actor': [{'name': 'Tom Hanks'}]}]}
{'movies': [{'actor': [{'film': [{'name': 'Forrest Gump'}]}]}]}
{'movies': [{'actor': [{'film': [{'genre': 'Drama'}]}]}]}
{'movies': [{'actor': [{'film': [{'name': 'Cast Away'}]}]}]}
{'movies': [{'actor': [{'film': [{'genre': 'Adventure'}]}]}]}
{'movies': [{'actor': [{'film': [{'name': 'Saving Private Ryan'}]}]}]}
{'movies': [{'actor': [{'film': [{'genre': 'War'}]}]}]}


Next steps
​


You've now learned how to prompt a model to return XML. Next, check out the 
broader guide on obtaining structured output
 for other related techniques.
Edit this page
Was this page helpful?
Previous
How to parse text from message objects
Next
How to parse YAML output
Next steps


--- docs_how_to_output_parser_yaml.txt ---

How-to guides
How to parse YAML output
On this page
How to parse YAML output


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


Output parsers


Prompt templates


Structured output


Chaining runnables together




LLMs from different providers often have different strengths depending on the specific data they are trained on. This also means that some may be "better" and more reliable at generating output in formats other than JSON.


This output parser allows users to specify an arbitrary schema and query LLMs for outputs that conform to that schema, using YAML to format their response.


note
Keep in mind that large language models are leaky abstractions! You'll have to use an LLM with sufficient capacity to generate well-formed YAML.


%
pip install 
-
qU langchain langchain
-
openai
import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


We use 
Pydantic
 with the 
YamlOutputParser
 to declare our data model and give the model more context as to what type of YAML it should generate:


from
 langchain
.
output_parsers 
import
 YamlOutputParser
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
from
 pydantic 
import
 BaseModel
,
 Field
# Define your desired data structure.
class
 
Joke
(
BaseModel
)
:
    setup
:
 
str
 
=
 Field
(
description
=
"question to set up a joke"
)
    punchline
:
 
str
 
=
 Field
(
description
=
"answer to resolve the joke"
)
model 
=
 ChatOpenAI
(
temperature
=
0
)
# And a query intented to prompt a language model to populate the data structure.
joke_query 
=
 
"Tell me a joke."
# Set up a parser + inject instructions into the prompt template.
parser 
=
 YamlOutputParser
(
pydantic_object
=
Joke
)
prompt 
=
 PromptTemplate
(
    template
=
"Answer the user query.\n{format_instructions}\n{query}\n"
,
    input_variables
=
[
"query"
]
,
    partial_variables
=
{
"format_instructions"
:
 parser
.
get_format_instructions
(
)
}
,
)
chain 
=
 prompt 
|
 model 
|
 parser
chain
.
invoke
(
{
"query"
:
 joke_query
}
)
API Reference:
YamlOutputParser
 | 
PromptTemplate
 | 
ChatOpenAI


Joke(setup="Why couldn't the bicycle find its way home?", punchline='Because it lost its bearings!')


The parser will automatically parse the output YAML and create a Pydantic model with the data. We can see the parser's 
format_instructions
, which get added to the prompt:


parser
.
get_format_instructions
(
)


'The output should be formatted as a YAML instance that conforms to the given JSON schema below.\n\n# Examples\n## Schema\n\`\`\`\n{"title": "Players", "description": "A list of players", "type": "array", "items": {"$ref": "#/definitions/Player"}, "definitions": {"Player": {"title": "Player", "type": "object", "properties": {"name": {"title": "Name", "description": "Player name", "type": "string"}, "avg": {"title": "Avg", "description": "Batting average", "type": "number"}}, "required": ["name", "avg"]}}}\n\`\`\`\n## Well formatted instance\n\`\`\`\n- name: John Doe\n  avg: 0.3\n- name: Jane Maxfield\n  avg: 1.4\n\`\`\`\n\n## Schema\n\`\`\`\n{"properties": {"habit": { "description": "A common daily habit", "type": "string" }, "sustainable_alternative": { "description": "An environmentally friendly alternative to the habit", "type": "string"}}, "required": ["habit", "sustainable_alternative"]}\n\`\`\`\n## Well formatted instance\n\`\`\`\nhabit: Using disposable water bottles for daily hydration.\nsustainable_alternative: Switch to a reusable water bottle to reduce plastic waste and decrease your environmental footprint.\n\`\`\` \n\nPlease follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: \n\`\`\`\n{"properties": {"setup": {"title": "Setup", "description": "question to set up a joke", "type": "string"}, "punchline": {"title": "Punchline", "description": "answer to resolve the joke", "type": "string"}}, "required": ["setup", "punchline"]}\n\`\`\`\n\nMake sure to always enclose the YAML output in triple backticks (\`\`\`). Please do not add anything other than valid YAML output!'


You can and should experiment with adding your own formatting hints in the other parts of your prompt to either augment or replace the default instructions.


Next steps
​


You've now learned how to prompt a model to return YAML. Next, check out the 
broader guide on obtaining structured output
 for other related techniques.
Edit this page
Was this page helpful?
Previous
How to parse XML output
Next
How to use the Parent Document Retriever
Next steps


--- docs_how_to_parallel.txt ---

How-to guides
How to invoke runnables in parallel
On this page
How to invoke runnables in parallel


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Expression Language (LCEL)


Chaining runnables




The 
RunnableParallel
 primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the 
RunnableParallel
. The final return value is a dict with the results of each value under its appropriate key.


Formatting with 
RunnableParallels
​


RunnableParallels
 are useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence. You can use them to split or fork the chain so that multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:


     Input
      / \
     /   \
 Branch1 Branch2
     \   /
      \ /
      Combine


Below, the input to prompt is expected to be a map with keys 
"context"
 and 
"question"
. The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the 
"question"
 key.


from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
,
 OpenAIEmbeddings
vectorstore 
=
 FAISS
.
from_texts
(
    
[
"harrison worked at kensho"
]
,
 embedding
=
OpenAIEmbeddings
(
)
)
retriever 
=
 vectorstore
.
as_retriever
(
)
template 
=
 
"""Answer the question based only on the following context:
{context}
Question: {question}
"""
# The prompt expects input with keys for "context" and "question"
prompt 
=
 ChatPromptTemplate
.
from_template
(
template
)
model 
=
 ChatOpenAI
(
)
retrieval_chain 
=
 
(
    
{
"context"
:
 retriever
,
 
"question"
:
 RunnablePassthrough
(
)
}
    
|
 prompt
    
|
 model
    
|
 StrOutputParser
(
)
)
retrieval_chain
.
invoke
(
"where did harrison work?"
)
API Reference:
FAISS
 | 
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI
 | 
OpenAIEmbeddings


'Harrison worked at Kensho.'


tip
Note that when composing a RunnableParallel with another Runnable we don't even need to wrap our dictionary in the RunnableParallel class — the type conversion is handled for us. In the context of a chain, these are equivalent:


{"context": retriever, "question": RunnablePassthrough()}


RunnableParallel({"context": retriever, "question": RunnablePassthrough()})


RunnableParallel(context=retriever, question=RunnablePassthrough())


See the section on 
coercion for more
.


Using itemgetter as shorthand
​


Note that you can use Python's 
itemgetter
 as shorthand to extract data from the map when combining with 
RunnableParallel
. You can find more information about itemgetter in the 
Python Documentation
.


In the example below, we use itemgetter to extract specific keys from the map:


from
 operator 
import
 itemgetter
from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
,
 OpenAIEmbeddings
vectorstore 
=
 FAISS
.
from_texts
(
    
[
"harrison worked at kensho"
]
,
 embedding
=
OpenAIEmbeddings
(
)
)
retriever 
=
 vectorstore
.
as_retriever
(
)
template 
=
 
"""Answer the question based only on the following context:
{context}
Question: {question}
Answer in the following language: {language}
"""
prompt 
=
 ChatPromptTemplate
.
from_template
(
template
)
chain 
=
 
(
    
{
        
"context"
:
 itemgetter
(
"question"
)
 
|
 retriever
,
        
"question"
:
 itemgetter
(
"question"
)
,
        
"language"
:
 itemgetter
(
"language"
)
,
    
}
    
|
 prompt
    
|
 model
    
|
 StrOutputParser
(
)
)
chain
.
invoke
(
{
"question"
:
 
"where did harrison work"
,
 
"language"
:
 
"italian"
}
)
API Reference:
FAISS
 | 
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI
 | 
OpenAIEmbeddings


'Harrison ha lavorato a Kensho.'


Parallelize steps
​


RunnableParallels make it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnableParallel
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
)
joke_chain 
=
 ChatPromptTemplate
.
from_template
(
"tell me a joke about {topic}"
)
 
|
 model
poem_chain 
=
 
(
    ChatPromptTemplate
.
from_template
(
"write a 2-line poem about {topic}"
)
 
|
 model
)
map_chain 
=
 RunnableParallel
(
joke
=
joke_chain
,
 poem
=
poem_chain
)
map_chain
.
invoke
(
{
"topic"
:
 
"bear"
}
)
API Reference:
ChatPromptTemplate
 | 
RunnableParallel
 | 
ChatOpenAI


{'joke': AIMessage(content="Why don't bears like fast food? Because they can't catch it!", response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 13, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_d9767fc5b9', 'finish_reason': 'stop', 'logprobs': None}, id='run-fe024170-c251-4b7a-bfd4-64a3737c67f2-0'),
 'poem': AIMessage(content='In the quiet of the forest, the bear roams free\nMajestic and wild, a sight to see.', response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 15, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-2707913e-a743-4101-b6ec-840df4568a76-0')}


Parallelism
​


RunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier 
joke_chain
, 
poem_chain
 and 
map_chain
 all have about the same runtime, even though 
map_chain
 executes both of the other two.


%
%
timeit
joke_chain
.
invoke
(
{
"topic"
:
 
"bear"
}
)


610 ms ± 64 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)


%
%
timeit
poem_chain
.
invoke
(
{
"topic"
:
 
"bear"
}
)


599 ms ± 73.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)


%
%
timeit
map_chain
.
invoke
(
{
"topic"
:
 
"bear"
}
)


643 ms ± 77.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)


Next steps
​


You now know some ways to format and parallelize chain steps with 
RunnableParallel
.


To learn more, see the other how-to guides on runnables in this section.
Edit this page
Was this page helpful?
Previous
How to add a semantic layer over graph database
Next
How to stream chat model responses
Formatting with 
RunnableParallels
Using itemgetter as shorthand
Parallelize steps
Parallelism
Next steps


--- docs_how_to_parent_document_retriever.txt ---

How-to guides
How to use the Parent Document Retriever
On this page
How to use the Parent Document Retriever


When splitting documents for 
retrieval
, there are often conflicting desires:




You may want to have small documents, so that their embeddings can most
accurately reflect their meaning. If too long, then the embeddings can
lose meaning.


You want to have long enough documents that the context of each chunk is
retained.




The 
ParentDocumentRetriever
 strikes that balance by splitting and storing
small chunks of data. During retrieval, it first fetches the small chunks
but then looks up the parent ids for those chunks and returns those larger
documents.


Note that "parent document" refers to the document that a small chunk
originated from. This can either be the whole raw document OR a larger
chunk.


from
 langchain
.
retrievers 
import
 ParentDocumentRetriever
API Reference:
ParentDocumentRetriever


from
 langchain
.
storage 
import
 InMemoryStore
from
 langchain_chroma 
import
 Chroma
from
 langchain_community
.
document_loaders 
import
 TextLoader
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
API Reference:
InMemoryStore
 | 
TextLoader
 | 
OpenAIEmbeddings
 | 
RecursiveCharacterTextSplitter


loaders 
=
 
[
    TextLoader
(
"paul_graham_essay.txt"
)
,
    TextLoader
(
"state_of_the_union.txt"
)
,
]
docs 
=
 
[
]
for
 loader 
in
 loaders
:
    docs
.
extend
(
loader
.
load
(
)
)


Retrieving full documents
​


In this mode, we want to retrieve the full documents. Therefore, we only specify a child 
splitter
.


# This text splitter is used to create the child documents
child_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
400
)
# The vectorstore to use to index the child chunks
vectorstore 
=
 Chroma
(
    collection_name
=
"full_documents"
,
 embedding_function
=
OpenAIEmbeddings
(
)
)
# The storage layer for the parent documents
store 
=
 InMemoryStore
(
)
retriever 
=
 ParentDocumentRetriever
(
    vectorstore
=
vectorstore
,
    docstore
=
store
,
    child_splitter
=
child_splitter
,
)


retriever
.
add_documents
(
docs
,
 ids
=
None
)


This should yield two keys, because we added two documents.


list
(
store
.
yield_keys
(
)
)


['9a63376c-58cc-42c9-b0f7-61f0e1a3a688',
 '40091598-e918-4a18-9be0-f46413a95ae4']


Let's now call the vector store search functionality - we should see that it returns small chunks (since we're storing the small chunks).


sub_docs 
=
 vectorstore
.
similarity_search
(
"justice breyer"
)


print
(
sub_docs
[
0
]
.
page_content
)


Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 
One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.


Let's now retrieve from the overall retriever. This should return large documents - since it returns the documents where the smaller chunks are located.


retrieved_docs 
=
 retriever
.
invoke
(
"justice breyer"
)


len
(
retrieved_docs
[
0
]
.
page_content
)


38540


Retrieving larger chunks
​


Sometimes, the full documents can be too big to want to retrieve them as is. In that case, what we really want to do is to first split the raw documents into larger chunks, and then split it into smaller chunks. We then index the smaller chunks, but on retrieval we retrieve the larger chunks (but still not the full documents).


# This text splitter is used to create the parent documents
parent_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
2000
)
# This text splitter is used to create the child documents
# It should create documents smaller than the parent
child_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
400
)
# The vectorstore to use to index the child chunks
vectorstore 
=
 Chroma
(
    collection_name
=
"split_parents"
,
 embedding_function
=
OpenAIEmbeddings
(
)
)
# The storage layer for the parent documents
store 
=
 InMemoryStore
(
)


retriever 
=
 ParentDocumentRetriever
(
    vectorstore
=
vectorstore
,
    docstore
=
store
,
    child_splitter
=
child_splitter
,
    parent_splitter
=
parent_splitter
,
)


retriever
.
add_documents
(
docs
)


We can see that there are much more than two documents now - these are the larger chunks.


len
(
list
(
store
.
yield_keys
(
)
)
)


66


Let's make sure the underlying vector store still retrieves the small chunks.


sub_docs 
=
 vectorstore
.
similarity_search
(
"justice breyer"
)


print
(
sub_docs
[
0
]
.
page_content
)


Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 
One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.


retrieved_docs 
=
 retriever
.
invoke
(
"justice breyer"
)


len
(
retrieved_docs
[
0
]
.
page_content
)


1849


print
(
retrieved_docs
[
0
]
.
page_content
)


In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. 
We cannot let this happen. 
Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. 
Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. 
One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. 
And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. 
A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. 
And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. 
We can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  
We’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  
We’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. 
We’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.
Edit this page
Was this page helpful?
Previous
How to parse YAML output
Next
How to use LangChain with different Pydantic versions
Retrieving full documents
Retrieving larger chunks


--- docs_how_to_passthrough.txt ---

How-to guides
How to pass through arguments from one step to the next
On this page
How to pass through arguments from one step to the next


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Expression Language (LCEL)


Chaining runnables


Calling runnables in parallel


Custom functions




When composing chains with several steps, sometimes you will want to pass data from previous steps unchanged for use as input to a later step. The 
RunnablePassthrough
 class allows you to do just this, and is typically is used in conjunction with a 
RunnableParallel
 to pass data through to a later step in your constructed chains.


See the example below:


%
pip install 
-
qU langchain langchain
-
openai
import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


from
 langchain_core
.
runnables 
import
 RunnableParallel
,
 RunnablePassthrough
runnable 
=
 RunnableParallel
(
    passed
=
RunnablePassthrough
(
)
,
    modified
=
lambda
 x
:
 x
[
"num"
]
 
+
 
1
,
)
runnable
.
invoke
(
{
"num"
:
 
1
}
)
API Reference:
RunnableParallel
 | 
RunnablePassthrough


{'passed': {'num': 1}, 'modified': 2}


As seen above, 
passed
 key was called with 
RunnablePassthrough()
 and so it simply passed on 
{'num': 1}
.


We also set a second key in the map with 
modified
. This uses a lambda to set a single value adding 1 to the num, which resulted in 
modified
 key with the value of 
2
.


Retrieval Example
​


In the example below, we see a more real-world use case where we use 
RunnablePassthrough
 along with 
RunnableParallel
 in a chain to properly format inputs to a prompt:


from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
,
 OpenAIEmbeddings
vectorstore 
=
 FAISS
.
from_texts
(
    
[
"harrison worked at kensho"
]
,
 embedding
=
OpenAIEmbeddings
(
)
)
retriever 
=
 vectorstore
.
as_retriever
(
)
template 
=
 
"""Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt 
=
 ChatPromptTemplate
.
from_template
(
template
)
model 
=
 ChatOpenAI
(
)
retrieval_chain 
=
 
(
    
{
"context"
:
 retriever
,
 
"question"
:
 RunnablePassthrough
(
)
}
    
|
 prompt
    
|
 model
    
|
 StrOutputParser
(
)
)
retrieval_chain
.
invoke
(
"where did harrison work?"
)
API Reference:
FAISS
 | 
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI
 | 
OpenAIEmbeddings


'Harrison worked at Kensho.'


Here the input to prompt is expected to be a map with keys "context" and "question". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the "question" key. The 
RunnablePassthrough
 allows us to pass on the user's question to the prompt and model.


Next steps
​


Now you've learned how to pass data through your chains to help format the data flowing through your chains.


To learn more, see the other how-to guides on runnables in this section.
Edit this page
Was this page helpful?
Previous
How to use built-in tools and toolkits
Next
How to compose prompts together
Retrieval Example
Next steps


--- docs_how_to_prompts_composition.txt ---

How-to guides
How to compose prompts together
On this page
How to compose prompts together


Prerequisites
This guide assumes familiarity with the following concepts:


Prompt templates




LangChain provides a user friendly interface for composing different parts of 
prompts
 together. You can do this with either string prompts or chat prompts. Constructing prompts this way allows for easy reuse of components.


String prompt composition
​


When working with string prompts, each template is joined together. You can work with either prompts directly or strings (the first element in the list needs to be a prompt).


from
 langchain_core
.
prompts 
import
 PromptTemplate
prompt 
=
 
(
    PromptTemplate
.
from_template
(
"Tell me a joke about {topic}"
)
    
+
 
", make it funny"
    
+
 
"\n\nand in {language}"
)
prompt
API Reference:
PromptTemplate


PromptTemplate(input_variables=['language', 'topic'], template='Tell me a joke about {topic}, make it funny\n\nand in {language}')


prompt
.
format
(
topic
=
"sports"
,
 language
=
"spanish"
)


'Tell me a joke about sports, make it funny\n\nand in spanish'


Chat prompt composition
​


A chat prompt is made up of a list of messages. Similarly to the above example, we can concatenate chat prompt templates. Each new element is a new message in the final prompt.


First, let's initialize the a 
ChatPromptTemplate
 with a 
SystemMessage
.


from
 langchain_core
.
messages 
import
 AIMessage
,
 HumanMessage
,
 SystemMessage
prompt 
=
 SystemMessage
(
content
=
"You are a nice pirate"
)
API Reference:
AIMessage
 | 
HumanMessage
 | 
SystemMessage


You can then easily create a pipeline combining it with other messages 
or
 message templates.
Use a 
Message
 when there is no variables to be formatted, use a 
MessageTemplate
 when there are variables to be formatted. You can also use just a string (note: this will automatically get inferred as a 
HumanMessagePromptTemplate
.)


new_prompt 
=
 
(
    prompt 
+
 HumanMessage
(
content
=
"hi"
)
 
+
 AIMessage
(
content
=
"what?"
)
 
+
 
"{input}"
)


Under the hood, this creates an instance of the ChatPromptTemplate class, so you can use it just as you did before!


new_prompt
.
format_messages
(
input
=
"i said hi"
)


[SystemMessage(content='You are a nice pirate'),
 HumanMessage(content='hi'),
 AIMessage(content='what?'),
 HumanMessage(content='i said hi')]


Using PipelinePrompt
​


LangChain includes a class called 
PipelinePromptTemplate
, which can be useful when you want to reuse parts of prompts. A PipelinePrompt consists of two main parts:




Final prompt: The final prompt that is returned


Pipeline prompts: A list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name.




from
 langchain_core
.
prompts 
import
 PipelinePromptTemplate
,
 PromptTemplate
full_template 
=
 
"""{introduction}
{example}
{start}"""
full_prompt 
=
 PromptTemplate
.
from_template
(
full_template
)
introduction_template 
=
 
"""You are impersonating {person}."""
introduction_prompt 
=
 PromptTemplate
.
from_template
(
introduction_template
)
example_template 
=
 
"""Here's an example of an interaction:
Q: {example_q}
A: {example_a}"""
example_prompt 
=
 PromptTemplate
.
from_template
(
example_template
)
start_template 
=
 
"""Now, do this for real!
Q: {input}
A:"""
start_prompt 
=
 PromptTemplate
.
from_template
(
start_template
)
input_prompts 
=
 
[
    
(
"introduction"
,
 introduction_prompt
)
,
    
(
"example"
,
 example_prompt
)
,
    
(
"start"
,
 start_prompt
)
,
]
pipeline_prompt 
=
 PipelinePromptTemplate
(
    final_prompt
=
full_prompt
,
 pipeline_prompts
=
input_prompts
)
pipeline_prompt
.
input_variables
API Reference:
PipelinePromptTemplate
 | 
PromptTemplate


['person', 'example_a', 'example_q', 'input']


print
(
    pipeline_prompt
.
format
(
        person
=
"Elon Musk"
,
        example_q
=
"What's your favorite car?"
,
        example_a
=
"Tesla"
,
        
input
=
"What's your favorite social media site?"
,
    
)
)


You are impersonating Elon Musk.
Here's an example of an interaction:
Q: What's your favorite car?
A: Tesla
Now, do this for real!
Q: What's your favorite social media site?
A:


Next steps
​


You've now learned how to compose prompts together.


Next, check out the other how-to guides on prompt templates in this section, like 
adding few-shot examples to your prompt templates
.
Edit this page
Was this page helpful?
Previous
How to pass through arguments from one step to the next
Next
How to handle multiple retrievers when doing query analysis
String prompt composition
Chat prompt composition
Using PipelinePrompt
Next steps


--- docs_how_to_prompts_partial.txt ---

How-to guides
How to partially format prompt templates
On this page
How to partially format prompt templates


Prerequisites
This guide assumes familiarity with the following concepts:


Prompt templates




Like partially binding arguments to a function, it can make sense to "partial" a 
prompt template
 - e.g. pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.


LangChain supports this in two ways:




Partial formatting with string values.


Partial formatting with functions that return string values.




In the examples below, we go over the motivations for both use cases as well as how to do it in LangChain.


Partial with strings
​


One common use case for wanting to partial a prompt template is if you get access to some of the variables in a prompt before others. For example, suppose you have a prompt template that requires two variables, 
foo
 and 
baz
. If you get the 
foo
 value early on in your chain, but the 
baz
 value later, it can be inconvenient to pass both variables all the way through the chain. Instead, you can partial the prompt template with the 
foo
 value, and then pass the partialed prompt template along and just use that. Below is an example of doing this:


from
 langchain_core
.
prompts 
import
 PromptTemplate
prompt 
=
 PromptTemplate
.
from_template
(
"{foo}{bar}"
)
partial_prompt 
=
 prompt
.
partial
(
foo
=
"foo"
)
print
(
partial_prompt
.
format
(
bar
=
"baz"
)
)
API Reference:
PromptTemplate


foobaz


You can also just initialize the prompt with the partialed variables.


prompt 
=
 PromptTemplate
(
    template
=
"{foo}{bar}"
,
 input_variables
=
[
"bar"
]
,
 partial_variables
=
{
"foo"
:
 
"foo"
}
)
print
(
prompt
.
format
(
bar
=
"baz"
)
)


foobaz


Partial with functions
​


The other common use is to partial with a function. The use case for this is when you have a variable you know that you always want to fetch in a common way. A prime example of this is with date or time. Imagine you have a prompt which you always want to have the current date. You can't hard code it in the prompt, and passing it along with the other input variables is inconvenient. In this case, it's handy to be able to partial the prompt with a function that always returns the current date.


from
 datetime 
import
 datetime
def
 
_get_datetime
(
)
:
    now 
=
 datetime
.
now
(
)
    
return
 now
.
strftime
(
"%m/%d/%Y, %H:%M:%S"
)
prompt 
=
 PromptTemplate
(
    template
=
"Tell me a {adjective} joke about the day {date}"
,
    input_variables
=
[
"adjective"
,
 
"date"
]
,
)
partial_prompt 
=
 prompt
.
partial
(
date
=
_get_datetime
)
print
(
partial_prompt
.
format
(
adjective
=
"funny"
)
)


Tell me a funny joke about the day 04/21/2024, 19:43:57


You can also just initialize the prompt with the partialed variables, which often makes more sense in this workflow.


prompt 
=
 PromptTemplate
(
    template
=
"Tell me a {adjective} joke about the day {date}"
,
    input_variables
=
[
"adjective"
]
,
    partial_variables
=
{
"date"
:
 _get_datetime
}
,
)
print
(
prompt
.
format
(
adjective
=
"funny"
)
)


Tell me a funny joke about the day 04/21/2024, 19:43:57


Next steps
​


You've now learned how to partially apply variables to your prompt templates.


Next, check out the other how-to guides on prompt templates in this section, like 
adding few-shot examples to your prompt templates
.
Edit this page
Was this page helpful?
Previous
How to construct knowledge graphs
Next
How to handle multiple queries when doing query analysis
Partial with strings
Partial with functions
Next steps


--- docs_how_to_pydantic_compatibility.txt ---

How-to guides
How to use LangChain with different Pydantic versions
How to use LangChain with different Pydantic versions


As of the 
0.3
 release, LangChain uses Pydantic 2 internally.


Users should install Pydantic 2 and are advised to 
avoid
 using the 
pydantic.v1
 namespace of Pydantic 2 with
LangChain APIs.


If you're working with prior versions of LangChain, please see the following guide
on 
Pydantic compatibility
.
Edit this page
Was this page helpful?
Previous
How to use the Parent Document Retriever
Next
How to add chat history


--- docs_how_to_qa_chat_history_how_to.txt ---

How-to guides
How to add chat history
On this page
How to add chat history


note
This guide previously used the 
RunnableWithMessageHistory
 abstraction. You can access this version of the documentation in the 
v0.2 docs
.
As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of 
LangGraph persistence
 to incorporate 
memory
 into new LangChain applications.
If your code is already relying on 
RunnableWithMessageHistory
 or 
BaseChatMessageHistory
, you do 
not
 need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses 
RunnableWithMessageHistory
 will continue to work as expected.
Please see 
How to migrate to LangGraph Memory
 for more details.


In many 
Q&A applications
 we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of "memory" of past questions and answers, and some logic for incorporating those into its current thinking.


In this guide we focus on 
adding logic for incorporating historical messages.


This is largely a condensed version of the 
Conversational RAG tutorial
.


We will cover two approaches:




Chains
, in which we always execute a retrieval step;


Agents
, in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).




For the external knowledge source, we will use the same 
LLM Powered Autonomous Agents
 blog post by Lilian Weng from the 
RAG tutorial
.


Both approaches leverage 
LangGraph
 as an orchestration framework. LangGraph implements a built-in 
persistence layer
, making it ideal for chat applications that support multiple conversational turns.


Setup
​


Dependencies
​


We'll use OpenAI embeddings and an InMemory vector store in this walkthrough, but everything shown here works with any 
Embeddings
, and 
VectorStore
 or 
Retriever
.


We'll use the following packages:


%
%
capture 
-
-
no
-
stderr
%
pip install 
-
-
upgrade 
-
-
quiet langgraph langchain
-
community beautifulsoup4


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with 
LangSmith
.


Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:


os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
if
 
not
 os
.
environ
.
get
(
"LANGSMITH_API_KEY"
)
:
    os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Components
​


We will need to select three components from LangChain's suite of integrations.


A 
chat model
:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


An 
embedding model
:




Select 
embeddings model
:
OpenAI
▾
OpenAI
Azure
Google
AWS
HuggingFace
Ollama
Cohere
MistralAI
Nomic
NVIDIA
Voyage AI
IBM watsonx
Fake
pip install -qU langchain-openai
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-large"
)


And a 
vector store
:




Select 
vector store
:
In-memory
▾
In-memory
AstraDB
Chroma
FAISS
Milvus
MongoDB
PGVector
Pinecone
Qdrant
pip install -qU langchain-core
from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
vector_store 
=
 InMemoryVectorStore
(
embeddings
)


Chains
​


The 
RAG Tutorial
 indexes an 
LLM Powered Autonomous Agents
 blog post by Lilian Weng. We will repeat that here. Below we load the content of the page, split it into sub-documents, and embed the documents into our 
vector store
:


import
 bs4
from
 langchain 
import
 hub
from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
from
 langchain_core
.
documents 
import
 Document
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
from
 typing_extensions 
import
 List
,
 TypedDict
# Load and chunk contents of the blog
loader 
=
 WebBaseLoader
(
    web_paths
=
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
,
)
,
    bs_kwargs
=
dict
(
        parse_only
=
bs4
.
SoupStrainer
(
            class_
=
(
"post-content"
,
 
"post-title"
,
 
"post-header"
)
        
)
    
)
,
)
docs 
=
 loader
.
load
(
)
text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
1000
,
 chunk_overlap
=
200
)
all_splits 
=
 text_splitter
.
split_documents
(
docs
)
API Reference:
hub
 | 
WebBaseLoader
 | 
Document
 | 
RecursiveCharacterTextSplitter


# Index chunks
_ 
=
 vector_store
.
add_documents
(
documents
=
all_splits
)


As detailed in 
Part 2
 of the RAG tutorial, we can naturally support a conversational experience by representing the flow of the RAG application as a sequence of 
messages
:




User input as a 
HumanMessage
;


Vector store query as an 
AIMessage
 with tool calls;


Retrieved documents as a 
ToolMessage
;


Final response as a 
AIMessage
.




We will use 
tool-calling
 to facilitate this, which additionally allows the query to be generated by the LLM. We can build a 
tool
 to execute the retrieval step:


from
 langchain_core
.
tools 
import
 tool
@tool
(
response_format
=
"content_and_artifact"
)
def
 
retrieve
(
query
:
 
str
)
:
    
"""Retrieve information related to a query."""
    retrieved_docs 
=
 vector_store
.
similarity_search
(
query
,
 k
=
2
)
    serialized 
=
 
"\n\n"
.
join
(
        
(
f"Source: 
{
doc
.
metadata
}
\n"
 
f"Content: 
{
doc
.
page_content
}
"
)
        
for
 doc 
in
 retrieved_docs
    
)
    
return
 serialized
,
 retrieved_docs
API Reference:
tool


We can now build our LangGraph application.


Note that we compile it with a 
checkpointer
 to support a back-and-forth conversation. LangGraph comes with a simple 
in-memory checkpointer
, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).


from
 langchain_core
.
messages 
import
 SystemMessage
from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
graph 
import
 END
,
 MessagesState
,
 StateGraph
from
 langgraph
.
prebuilt 
import
 ToolNode
,
 tools_condition
# Step 1: Generate an AIMessage that may include a tool-call to be sent.
def
 
query_or_respond
(
state
:
 MessagesState
)
:
    
"""Generate tool call for retrieval or respond."""
    llm_with_tools 
=
 llm
.
bind_tools
(
[
retrieve
]
)
    response 
=
 llm_with_tools
.
invoke
(
state
[
"messages"
]
)
    
# MessagesState appends messages to state instead of overwriting
    
return
 
{
"messages"
:
 
[
response
]
}
# Step 2: Execute the retrieval.
tools 
=
 ToolNode
(
[
retrieve
]
)
# Step 3: Generate a response using the retrieved content.
def
 
generate
(
state
:
 MessagesState
)
:
    
"""Generate answer."""
    
# Get generated ToolMessages
    recent_tool_messages 
=
 
[
]
    
for
 message 
in
 
reversed
(
state
[
"messages"
]
)
:
        
if
 message
.
type
 
==
 
"tool"
:
            recent_tool_messages
.
append
(
message
)
        
else
:
            
break
    tool_messages 
=
 recent_tool_messages
[
:
:
-
1
]
    
# Format into prompt
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
content 
for
 doc 
in
 tool_messages
)
    system_message_content 
=
 
(
        
"You are an assistant for question-answering tasks. "
        
"Use the following pieces of retrieved context to answer "
        
"the question. If you don't know the answer, say that you "
        
"don't know. Use three sentences maximum and keep the "
        
"answer concise."
        
"\n\n"
        
f"
{
docs_content
}
"
    
)
    conversation_messages 
=
 
[
        message
        
for
 message 
in
 state
[
"messages"
]
        
if
 message
.
type
 
in
 
(
"human"
,
 
"system"
)
        
or
 
(
message
.
type
 
==
 
"ai"
 
and
 
not
 message
.
tool_calls
)
    
]
    prompt 
=
 
[
SystemMessage
(
system_message_content
)
]
 
+
 conversation_messages
    
# Run
    response 
=
 llm
.
invoke
(
prompt
)
    
return
 
{
"messages"
:
 
[
response
]
}
# Build graph
graph_builder 
=
 StateGraph
(
MessagesState
)
graph_builder
.
add_node
(
query_or_respond
)
graph_builder
.
add_node
(
tools
)
graph_builder
.
add_node
(
generate
)
graph_builder
.
set_entry_point
(
"query_or_respond"
)
graph_builder
.
add_conditional_edges
(
    
"query_or_respond"
,
    tools_condition
,
    
{
END
:
 END
,
 
"tools"
:
 
"tools"
}
,
)
graph_builder
.
add_edge
(
"tools"
,
 
"generate"
)
graph_builder
.
add_edge
(
"generate"
,
 END
)
memory 
=
 MemorySaver
(
)
graph 
=
 graph_builder
.
compile
(
checkpointer
=
memory
)
API Reference:
SystemMessage
 | 
MemorySaver
 | 
StateGraph
 | 
ToolNode
 | 
tools_condition


from
 IPython
.
display 
import
 Image
,
 display
display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




Let's test our application.


Note that it responds appropriately to messages that do not require an additional retrieval step:


# Specify an ID for the thread
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc123"
}
}


input_message 
=
 
"Hello"
for
 step 
in
 graph
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 input_message
}
]
}
,
    stream_mode
=
"values"
,
    config
=
config
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
Hello
==================================[1m Ai Message [0m==================================
Hello! How can I assist you today?


And when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:


input_message 
=
 
"What is Task Decomposition?"
for
 step 
in
 graph
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 input_message
}
]
}
,
    stream_mode
=
"values"
,
    config
=
config
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
What is Task Decomposition?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_RntwX5GMt531biEE9MqSbgLV)
 Call ID: call_RntwX5GMt531biEE9MqSbgLV
  Args:
    query: Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
==================================[1m Ai Message [0m==================================
Task Decomposition is the process of breaking down a complicated task into smaller, more manageable steps. It often involves techniques like Chain of Thought (CoT), where the model is prompted to "think step by step," allowing for better handling of complex tasks. This approach enhances model performance and provides insight into the model's reasoning process.


Finally, because we have compiled our application with a 
checkpointer
, historical messages are maintained in the state. This allows the model to contextualize user queries:


input_message 
=
 
"Can you look up some common ways of doing it?"
for
 step 
in
 graph
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 input_message
}
]
}
,
    stream_mode
=
"values"
,
    config
=
config
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
Can you look up some common ways of doing it?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_kwO5rYPyJ0MftYKoKRFjKpZM)
 Call ID: call_kwO5rYPyJ0MftYKoKRFjKpZM
  Args:
    query: common methods for task decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
==================================[1m Ai Message [0m==================================
Common ways of Task Decomposition include: (1) using large language models (LLMs) with simple prompts like "Steps for XYZ" or "What are the subgoals for achieving XYZ?"; (2) utilizing task-specific instructions, such as "Write a story outline" for creative tasks; and (3) incorporating human inputs to guide the decomposition process.


Note that we can observe the full sequence of messages sent to the chat model-- including tool calls and retrieved context-- in the 
LangSmith trace
.


The conversation history can also be inspected via the state of the application:


chat_history 
=
 graph
.
get_state
(
config
)
.
values
[
"messages"
]
for
 message 
in
 chat_history
:
    message
.
pretty_print
(
)


================================[1m Human Message [0m=================================
Hello
==================================[1m Ai Message [0m==================================
Hello! How can I assist you today?
================================[1m Human Message [0m=================================
What is Task Decomposition?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_RntwX5GMt531biEE9MqSbgLV)
 Call ID: call_RntwX5GMt531biEE9MqSbgLV
  Args:
    query: Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
==================================[1m Ai Message [0m==================================
Task Decomposition is the process of breaking down a complicated task into smaller, more manageable steps. It often involves techniques like Chain of Thought (CoT), where the model is prompted to "think step by step," allowing for better handling of complex tasks. This approach enhances model performance and provides insight into the model's reasoning process.
================================[1m Human Message [0m=================================
Can you look up some common ways of doing it?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_kwO5rYPyJ0MftYKoKRFjKpZM)
 Call ID: call_kwO5rYPyJ0MftYKoKRFjKpZM
  Args:
    query: common methods for task decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
==================================[1m Ai Message [0m==================================
Common ways of Task Decomposition include: (1) using large language models (LLMs) with simple prompts like "Steps for XYZ" or "What are the subgoals for achieving XYZ?"; (2) utilizing task-specific instructions, such as "Write a story outline" for creative tasks; and (3) incorporating human inputs to guide the decomposition process.


Agents
​


Agents
 leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above "chain", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.


Below we assemble a minimal RAG agent. Using LangGraph's 
pre-built ReAct agent constructor
, we can do this in one line.


tip
Check out LangGraph's 
Agentic RAG
 tutorial for more advanced formulations.


from
 langgraph
.
prebuilt 
import
 create_react_agent
agent_executor 
=
 create_react_agent
(
llm
,
 
[
retrieve
]
,
 checkpointer
=
memory
)
API Reference:
create_react_agent


Let's inspect the graph:


display
(
Image
(
agent_executor
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




The key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.


Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"def234"
}
}
input_message 
=
 
(
    
"What is the standard method for Task Decomposition?\n\n"
    
"Once you get the answer, look up common extensions of that method."
)
for
 event 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 input_message
}
]
}
,
    stream_mode
=
"values"
,
    config
=
config
,
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
What is the standard method for Task Decomposition?
Once you get the answer, look up common extensions of that method.
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_rxBqio7dxthnMuzjr4AIquSZ)
 Call ID: call_rxBqio7dxthnMuzjr4AIquSZ
  Args:
    query: standard method for Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_kmQMRWCKeBdtXdlJi8yZD9CO)
 Call ID: call_kmQMRWCKeBdtXdlJi8yZD9CO
  Args:
    query: common extensions of Task Decomposition methods
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
==================================[1m Ai Message [0m==================================
The standard method for Task Decomposition involves breaking down complex tasks into smaller, manageable steps. Here are the main techniques:
1. **Chain of Thought (CoT)**: This prompting technique encourages a model to "think step by step," allowing it to utilize more computational resources during testing to decompose challenging tasks into simpler parts. CoT not only simplifies tasks but also provides insights into the model's reasoning process.
2. **Simple Prompting**: This can involve straightforward queries like "Steps for XYZ" or "What are the subgoals for achieving XYZ?" to guide the model in identifying the necessary steps.
3. **Task-specific Instructions**: Using specific prompts tailored to the task at hand, such as "Write a story outline" for creative writing, allows for more directed decomposition.
4. **Human Inputs**: Involving human expertise can also aid in breaking down tasks effectively.
### Common Extensions of Task Decomposition Methods
1. **Tree of Thoughts**: This method extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into various thought steps and generates multiple thoughts per step, forming a tree structure. This can utilize search processes like breadth-first search (BFS) or depth-first search (DFS) to evaluate states through classifiers or majority voting.
These extensions build on the basic principles of task decomposition, enhancing the depth and breadth of reasoning applied to complex tasks.


Note that the agent:




Generates a query to search for a standard method for task decomposition;


Receiving the answer, generates a second query to search for common extensions of it;


Having received all necessary context, answers the question.




We can see the full sequence of steps, along with latency and other metadata, in the 
LangSmith trace
.


Next steps
​


We've covered the steps to build a basic conversational Q&A application:




We used chains to build a predictable application that generates search queries for each user input;


We used agents to build an application that "decides" when and how to generate search queries.




To explore different types of retrievers and retrieval strategies, visit the 
retrievers
 section of the how-to guides.


For a detailed walkthrough of LangChain's conversation memory abstractions, visit the 
How to add message history (memory)
 LCEL page.


To learn more about agents, head to the 
Agents Modules
.
Edit this page
Was this page helpful?
Previous
How to use LangChain with different Pydantic versions
Next
How to get a RAG application to add citations
Setup
Dependencies
LangSmith
Components
Chains
Agents
Next steps


--- docs_how_to_qa_citations.txt ---

How-to guides
How to get a RAG application to add citations
On this page
How to get a RAG application to add citations


This guide reviews methods to get a model to cite which parts of the source documents it referenced in generating its response.


We will cover five methods:




Using tool-calling to cite document IDs;


Using tool-calling to cite documents IDs and provide text snippets;


Direct prompting;


Retrieval post-processing (i.e., compressing the retrieved context to make it more relevant);


Generation post-processing (i.e., issuing a second LLM call to annotate a generated answer with citations).




We generally suggest using the first item of the list that works for your use-case. That is, if your model supports tool-calling, try methods 1 or 2; otherwise, or if those fail, advance down the list.


Let's first create a simple 
RAG
 chain. To start we'll just retrieve from Wikipedia using the 
WikipediaRetriever
. We will use the same 
LangGraph
 implementation from the 
RAG Tutorial
.


Setup
​


First we'll need to install some dependencies:


%
pip install 
-
qU langchain
-
community wikipedia


Let's first select a LLM:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


We can now load a 
retriever
 and construct our 
prompt
:


from
 langchain_community
.
retrievers 
import
 WikipediaRetriever
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
system_prompt 
=
 
(
    
"You're a helpful AI assistant. Given a user question "
    
"and some Wikipedia article snippets, answer the user "
    
"question. If none of the articles answer the question, "
    
"just say you don't know."
    
"\n\nHere are the Wikipedia articles: "
    
"{context}"
)
retriever 
=
 WikipediaRetriever
(
top_k_results
=
6
,
 doc_content_chars_max
=
2000
)
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system_prompt
)
,
        
(
"human"
,
 
"{question}"
)
,
    
]
)
prompt
.
pretty_print
(
)
API Reference:
WikipediaRetriever
 | 
ChatPromptTemplate


================================[1m System Message [0m================================
You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, answer the user question. If none of the articles answer the question, just say you don't know.
Here are the Wikipedia articles: [33;1m[1;3m{context}[0m
================================[1m Human Message [0m=================================
[33;1m[1;3m{question}[0m


Now that we've got a 
model
, 
retriever
 and 
prompt
, let's chain them all together. Following the how-to guide on 
adding citations
 to a RAG application, we'll make it so our chain returns both the answer and the retrieved Documents. This uses the same 
LangGraph
 implementation as in the 
RAG Tutorial
.


from
 langchain_core
.
documents 
import
 Document
from
 langgraph
.
graph 
import
 START
,
 StateGraph
from
 typing_extensions 
import
 List
,
 TypedDict
# Define state for application
class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 
str
# Define application steps
def
 
retrieve
(
state
:
 State
)
:
    retrieved_docs 
=
 retriever
.
invoke
(
state
[
"question"
]
)
    
return
 
{
"context"
:
 retrieved_docs
}
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    response 
=
 llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
.
content
}
# Compile application and test
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)
API Reference:
Document
 | 
StateGraph


from
 IPython
.
display 
import
 Image
,
 display
display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




result 
=
 graph
.
invoke
(
{
"question"
:
 
"How fast are cheetahs?"
}
)
sources 
=
 
[
doc
.
metadata
[
"source"
]
 
for
 doc 
in
 result
[
"context"
]
]
print
(
f"Sources: 
{
sources
}
\n\n"
)
print
(
f'Answer: 
{
result
[
"answer"
]
}
'
)


Sources: ['https://en.wikipedia.org/wiki/Cheetah', 'https://en.wikipedia.org/wiki/Southeast_African_cheetah', 'https://en.wikipedia.org/wiki/Footspeed', 'https://en.wikipedia.org/wiki/Fastest_animals', 'https://en.wikipedia.org/wiki/Pursuit_predation', 'https://en.wikipedia.org/wiki/Gepard-class_fast_attack_craft']
Answer: Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).


Check out the 
LangSmith trace
.


Tool-calling
​


If your LLM of choice implements a 
tool-calling
 feature, you can use it to make the model specify which of the provided documents it's referencing when generating its answer. LangChain tool-calling models implement a 
.with_structured_output
 method which will force generation adhering to a desired schema (see details 
here
).


Cite documents
​


To cite documents using an identifier, we format the identifiers into the prompt, then use 
.with_structured_output
 to coerce the LLM to reference these identifiers in its output.


First we define a schema for the output. The 
.with_structured_output
 supports multiple formats, including JSON schema and Pydantic. Here we will use Pydantic:


from
 pydantic 
import
 BaseModel
,
 Field
class
 
CitedAnswer
(
BaseModel
)
:
    
"""Answer the user question based only on the given sources, and cite the sources used."""
    answer
:
 
str
 
=
 Field
(
        
.
.
.
,
        description
=
"The answer to the user question, which is based only on the given sources."
,
    
)
    citations
:
 List
[
int
]
 
=
 Field
(
        
.
.
.
,
        description
=
"The integer IDs of the SPECIFIC sources which justify the answer."
,
    
)


Let's see what the model output is like when we pass in our functions and a user input:


structured_llm 
=
 llm
.
with_structured_output
(
CitedAnswer
)
example_q 
=
 
"""What Brian's height?
Source: 1
Information: Suzy is 6'2"
Source: 2
Information: Jeremiah is blonde
Source: 3
Information: Brian is 3 inches shorter than Suzy"""
result 
=
 structured_llm
.
invoke
(
example_q
)
result


CitedAnswer(answer='Brian is 5\'11".', citations=[1, 3])


Or as a dict:


result
.
dict
(
)


{'answer': 'Brian is 5\'11".', 'citations': [1, 3]}


Now we structure the source identifiers into the prompt to replicate with our chain. We will make three changes:




Update the prompt to include source identifiers;


Use the 
structured_llm
 (i.e., 
llm.with_structured_output(CitedAnswer)
);


Return the Pydantic object in the output.




def
 
format_docs_with_id
(
docs
:
 List
[
Document
]
)
 
-
>
 
str
:
    formatted 
=
 
[
        
f"Source ID: 
{
i
}
\nArticle Title: 
{
doc
.
metadata
[
'title'
]
}
\nArticle Snippet: 
{
doc
.
page_content
}
"
        
for
 i
,
 doc 
in
 
enumerate
(
docs
)
    
]
    
return
 
"\n\n"
 
+
 
"\n\n"
.
join
(
formatted
)
class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 CitedAnswer
def
 
generate
(
state
:
 State
)
:
    formatted_docs 
=
 format_docs_with_id
(
state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 formatted_docs
}
)
    structured_llm 
=
 llm
.
with_structured_output
(
CitedAnswer
)
    response 
=
 structured_llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
}
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)


result 
=
 graph
.
invoke
(
{
"question"
:
 
"How fast are cheetahs?"
}
)
result
[
"answer"
]


CitedAnswer(answer='Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).', citations=[0, 3])


We can inspect the document at index 0, which the model cited:


print
(
result
[
"context"
]
[
0
]
)


page_content='The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67–94 cm (26–37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.
The cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.
The cheetah lives in three main social groups: females and their cubs, male "coalitions", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson's gazelles. The cheetah typically stalks its prey within 60–100 m (200–330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned a' metadata={'title': 'Cheetah', 'summary': 'The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67–94 cm (26–37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.\nThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.\nThe cheetah lives in three main social groups: females and their cubs, male "coalitions", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson\'s gazelles. The cheetah typically stalks its prey within 60–100 m (200–330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned at around four months and are independent by around 20 months of age.\nThe cheetah is threatened by habitat loss, conflict with humans, poaching and high susceptibility to diseases. The global cheetah population was estimated in 2021 at 6,517; it is listed as Vulnerable on the IUCN Red List. It has been widely depicted in art, literature, advertising, and animation. It was tamed in ancient Egypt and trained for hunting ungulates in the Arabian Peninsula and India. It has been kept in zoos since the early 19th century.', 'source': 'https://en.wikipedia.org/wiki/Cheetah'}


LangSmith trace: 
https://smith.langchain.com/public/6f34d136-451d-4625-90c8-2d8decebc21a/r


Cite snippets
​


To return text spans (perhaps in addition to source identifiers), we can use the same approach. The only change will be to build a more complex output schema, here using Pydantic, that includes a "quote" alongside a source identifier.


Aside: Note that if we break up our documents so that we have many documents with only a sentence or two instead of a few long documents, citing documents becomes roughly equivalent to citing snippets, and may be easier for the model because the model just needs to return an identifier for each snippet instead of the actual text. Probably worth trying both approaches and evaluating.


class
 
Citation
(
BaseModel
)
:
    source_id
:
 
int
 
=
 Field
(
        
.
.
.
,
        description
=
"The integer ID of a SPECIFIC source which justifies the answer."
,
    
)
    quote
:
 
str
 
=
 Field
(
        
.
.
.
,
        description
=
"The VERBATIM quote from the specified source that justifies the answer."
,
    
)
class
 
QuotedAnswer
(
BaseModel
)
:
    
"""Answer the user question based only on the given sources, and cite the sources used."""
    answer
:
 
str
 
=
 Field
(
        
.
.
.
,
        description
=
"The answer to the user question, which is based only on the given sources."
,
    
)
    citations
:
 List
[
Citation
]
 
=
 Field
(
        
.
.
.
,
 description
=
"Citations from the given sources that justify the answer."
    
)


class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 QuotedAnswer
def
 
generate
(
state
:
 State
)
:
    formatted_docs 
=
 format_docs_with_id
(
state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 formatted_docs
}
)
    structured_llm 
=
 llm
.
with_structured_output
(
QuotedAnswer
)
    response 
=
 structured_llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
}
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)


Here we see that the model has extracted a relevant snippet of text from source 0:


result 
=
 graph
.
invoke
(
{
"question"
:
 
"How fast are cheetahs?"
}
)
result
[
"answer"
]


QuotedAnswer(answer='Cheetahs are capable of running at speeds of 93 to 104 km/h (58 to 65 mph).', citations=[Citation(source_id=0, quote='The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed.')])


LangSmith trace: 
https://smith.langchain.com/public/e16dc72f-4261-4f25-a9a7-906238737283/r


Direct prompting
​


Some models don't support function-calling. We can achieve similar results with direct prompting. Let's try instructing a model to generate structured XML for its output:


xml_system 
=
 
"""You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, \
answer the user question and provide citations. If none of the articles answer the question, just say you don't know.
Remember, you must return both an answer and citations. A citation consists of a VERBATIM quote that \
justifies the answer and the ID of the quote article. Return a citation for every quote across all articles \
that justify the answer. Use the following format for your final output:
<cited_answer>
    <answer></answer>
    <citations>
        <citation><source_id></source_id><quote></quote></citation>
        <citation><source_id></source_id><quote></quote></citation>
        ...
    </citations>
</cited_answer>
Here are the Wikipedia articles:{context}"""
xml_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 xml_system
)
,
 
(
"human"
,
 
"{question}"
)
]
)


We now make similar small updates to our chain:




We update the formatting function to wrap the retrieved context in XML tags;


We do not use 
.with_structured_output
 (e.g., because it does not exist for a model);


We use 
XMLOutputParser
 to parse the answer into a dict.




from
 langchain_core
.
output_parsers 
import
 XMLOutputParser
def
 
format_docs_xml
(
docs
:
 List
[
Document
]
)
 
-
>
 
str
:
    formatted 
=
 
[
]
    
for
 i
,
 doc 
in
 
enumerate
(
docs
)
:
        doc_str 
=
 
f"""\
    <source id=\"
{
i
}
\">
        <title>
{
doc
.
metadata
[
'title'
]
}
</title>
        <article_snippet>
{
doc
.
page_content
}
</article_snippet>
    </source>"""
        formatted
.
append
(
doc_str
)
    
return
 
"\n\n<sources>"
 
+
 
"\n"
.
join
(
formatted
)
 
+
 
"</sources>"
class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 
dict
def
 
generate
(
state
:
 State
)
:
    formatted_docs 
=
 format_docs_xml
(
state
[
"context"
]
)
    messages 
=
 xml_prompt
.
invoke
(
        
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 formatted_docs
}
    
)
    response 
=
 llm
.
invoke
(
messages
)
    parsed_response 
=
 XMLOutputParser
(
)
.
invoke
(
response
)
    
return
 
{
"answer"
:
 parsed_response
}
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)
API Reference:
XMLOutputParser


Note that citations are again structured into the answer:


result 
=
 graph
.
invoke
(
{
"question"
:
 
"How fast are cheetahs?"
}
)
result
[
"answer"
]


{'cited_answer': [{'answer': 'Cheetahs can run at speeds of 93 to 104 km/h (58 to 65 mph).'},
  {'citations': [{'citation': [{'source_id': '0'},
      {'quote': 'The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph);'}]},
    {'citation': [{'source_id': '3'},
      {'quote': 'The fastest land animal is the cheetah.'}]}]}]}


LangSmith trace: 
https://smith.langchain.com/public/0c45f847-c640-4b9a-a5fa-63559e413527/r


Retrieval post-processing
​


Another approach is to post-process our retrieved documents to compress the content, so that the source content is already minimal enough that we don't need the model to cite specific sources or spans. For example, we could break up each document into a sentence or two, embed those and keep only the most relevant ones. LangChain has some built-in components for this. Here we'll use a 
RecursiveCharacterTextSplitter
, which creates chunks of a specified size by splitting on separator substrings, and an 
EmbeddingsFilter
, which keeps only the texts with the most relevant embeddings.


This approach effectively updates our 
retrieve
 step to compress the documents. Let's first select an 
embedding model
:




Select 
embeddings model
:
OpenAI
▾
OpenAI
Azure
Google
AWS
HuggingFace
Ollama
Cohere
MistralAI
Nomic
NVIDIA
Voyage AI
IBM watsonx
Fake
pip install -qU langchain-openai
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-large"
)


We can now rewrite the 
retrieve
 step:


from
 langchain
.
retrievers
.
document_compressors 
import
 EmbeddingsFilter
from
 langchain_core
.
runnables 
import
 RunnableParallel
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
splitter 
=
 RecursiveCharacterTextSplitter
(
    chunk_size
=
400
,
    chunk_overlap
=
0
,
    separators
=
[
"\n\n"
,
 
"\n"
,
 
"."
,
 
" "
]
,
    keep_separator
=
False
,
)
compressor 
=
 EmbeddingsFilter
(
embeddings
=
embeddings
,
 k
=
10
)
class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 
str
def
 
retrieve
(
state
:
 State
)
:
    retrieved_docs 
=
 retriever
.
invoke
(
state
[
"question"
]
)
    split_docs 
=
 splitter
.
split_documents
(
retrieved_docs
)
    stateful_docs 
=
 compressor
.
compress_documents
(
split_docs
,
 state
[
"question"
]
)
    
return
 
{
"context"
:
 stateful_docs
}
API Reference:
EmbeddingsFilter
 | 
RunnableParallel
 | 
RecursiveCharacterTextSplitter


Let's test this out:


retrieval_result 
=
 retrieve
(
{
"question"
:
 
"How fast are cheetahs?"
}
)
for
 doc 
in
 retrieval_result
[
"context"
]
:
    
print
(
f"
{
doc
.
page_content
}
\n\n"
)


Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail
The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67–94 cm (26–37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in)
2 mph), or 171 body lengths per second. The cheetah, the fastest land mammal, scores at only 16 body lengths per second
It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson's gazelles. The cheetah typically stalks its prey within 60–100 m (200–330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year
The cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran
The cheetah lives in three main social groups: females and their cubs, male "coalitions", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk
The Southeast African cheetah (Acinonyx jubatus jubatus) is the nominate cheetah subspecies native to East and Southern Africa. The Southern African cheetah lives mainly in the lowland areas and deserts of the Kalahari, the savannahs of Okavango Delta, and the grasslands of the Transvaal region in South Africa. In Namibia, cheetahs are mostly found in farmlands
Subpopulations have been called "South African cheetah" and "Namibian cheetah."
In India, four cheetahs of the subspecies are living in Kuno National Park in Madhya Pradesh after having been introduced there
Acinonyx jubatus velox proposed in 1913 by Edmund Heller on basis of a cheetah that was shot by Kermit Roosevelt in June 1909 in the Kenyan highlands.
Acinonyx rex proposed in 1927 by Reginald Innes Pocock on basis of a specimen from the Umvukwe Range in Rhodesia.


Next, we assemble it into our chain as before:


# This step is unchanged from our original RAG implementation
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    response 
=
 llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
.
content
}
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)


result 
=
 graph
.
invoke
(
{
"question"
:
 
"How fast are cheetahs?"
}
)
print
(
result
[
"answer"
]
)


Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph). They are known as the fastest land animals.


Note that the document content is now compressed, although the document objects retain the original content in a "summary" key in their metadata. These summaries are not passed to the model; only the condensed content is.


result
[
"context"
]
[
0
]
.
page_content  
# passed to model


'Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail'


result
[
"context"
]
[
0
]
.
metadata
[
"summary"
]
  
# original document  # original document


'The cheetah (Acinonyx jubatus) is a large cat and the fastest land animal. It has a tawny to creamy white or pale buff fur that is marked with evenly spaced, solid black spots. The head is small and rounded, with a short snout and black tear-like facial streaks. It reaches 67–94 cm (26–37 in) at the shoulder, and the head-and-body length is between 1.1 and 1.5 m (3 ft 7 in and 4 ft 11 in). Adults weigh between 21 and 72 kg (46 and 159 lb). The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph); it has evolved specialized adaptations for speed, including a light build, long thin legs and a long tail.\nThe cheetah was first described in the late 18th century. Four subspecies are recognised today that are native to Africa and central Iran. An African subspecies was introduced to India in 2022. It is now distributed mainly in small, fragmented populations in northwestern, eastern and southern Africa and central Iran. It lives in a variety of habitats such as savannahs in the Serengeti, arid mountain ranges in the Sahara, and hilly desert terrain.\nThe cheetah lives in three main social groups: females and their cubs, male "coalitions", and solitary males. While females lead a nomadic life searching for prey in large home ranges, males are more sedentary and instead establish much smaller territories in areas with plentiful prey and access to females. The cheetah is active during the day, with peaks during dawn and dusk. It feeds on small- to medium-sized prey, mostly weighing under 40 kg (88 lb), and prefers medium-sized ungulates such as impala, springbok and Thomson\'s gazelles. The cheetah typically stalks its prey within 60–100 m (200–330 ft) before charging towards it, trips it during the chase and bites its throat to suffocate it to death. It breeds throughout the year. After a gestation of nearly three months, females give birth to a litter of three or four cubs. Cheetah cubs are highly vulnerable to predation by other large carnivores. They are weaned at around four months and are independent by around 20 months of age.\nThe cheetah is threatened by habitat loss, conflict with humans, poaching and high susceptibility to diseases. The global cheetah population was estimated in 2021 at 6,517; it is listed as Vulnerable on the IUCN Red List. It has been widely depicted in art, literature, advertising, and animation. It was tamed in ancient Egypt and trained for hunting ungulates in the Arabian Peninsula and India. It has been kept in zoos since the early 19th century.'


LangSmith trace: 
https://smith.langchain.com/public/21b0dc15-d70a-4293-9402-9c70f9178e66/r


Generation post-processing
​


Another approach is to post-process our model generation. In this example we'll first generate just an answer, and then we'll ask the model to annotate it's own answer with citations. The downside of this approach is of course that it is slower and more expensive, because two model calls need to be made.


Let's apply this to our initial chain. If desired, we can implement this via a third step in our application.


class
 
Citation
(
BaseModel
)
:
    source_id
:
 
int
 
=
 Field
(
        
.
.
.
,
        description
=
"The integer ID of a SPECIFIC source which justifies the answer."
,
    
)
    quote
:
 
str
 
=
 Field
(
        
.
.
.
,
        description
=
"The VERBATIM quote from the specified source that justifies the answer."
,
    
)
class
 
AnnotatedAnswer
(
BaseModel
)
:
    
"""Annotate the answer to the user question with quote citations that justify the answer."""
    citations
:
 List
[
Citation
]
 
=
 Field
(
        
.
.
.
,
 description
=
"Citations from the given sources that justify the answer."
    
)
structured_llm 
=
 llm
.
with_structured_output
(
AnnotatedAnswer
)


class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 
str
    annotations
:
 AnnotatedAnswer
def
 
retrieve
(
state
:
 State
)
:
    retrieved_docs 
=
 retriever
.
invoke
(
state
[
"question"
]
)
    
return
 
{
"context"
:
 retrieved_docs
}
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    response 
=
 llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
.
content
}
def
 
annotate
(
state
:
 State
)
:
    formatted_docs 
=
 format_docs_with_id
(
state
[
"context"
]
)
    messages 
=
 
[
        
(
"system"
,
 system_prompt
.
format
(
context
=
formatted_docs
)
)
,
        
(
"human"
,
 state
[
"question"
]
)
,
        
(
"ai"
,
 state
[
"answer"
]
)
,
        
(
"human"
,
 
"Annotate your answer with citations."
)
,
    
]
    response 
=
 structured_llm
.
invoke
(
messages
)
    
return
 
{
"annotations"
:
 response
}
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
,
 annotate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)


display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




result 
=
 graph
.
invoke
(
{
"question"
:
 
"How fast are cheetahs?"
}
)
print
(
result
[
"answer"
]
)


Cheetahs are capable of running at speeds between 93 to 104 km/h (58 to 65 mph).


result
[
"annotations"
]


AnnotatedAnswer(citations=[Citation(source_id=0, quote='The cheetah is capable of running at 93 to 104 km/h (58 to 65 mph)')])


LangSmith trace: 
https://smith.langchain.com/public/b8257417-573b-47c4-a750-74e542035f19/r
Edit this page
Was this page helpful?
Previous
How to add chat history
Next
How to do per-user retrieval
Setup
Tool-calling
Cite documents
Cite snippets
Direct prompting
Retrieval post-processing
Generation post-processing


--- docs_how_to_qa_per_user.txt ---

How-to guides
How to do per-user retrieval
On this page
How to do per-user retrieval


This guide demonstrates how to configure runtime properties of a retrieval chain. An example application is to limit the documents available to a 
retriever
 based on the user.


When building a 
retrieval app
, you often have to build it with multiple users in mind. This means that you may be storing data not just for one user, but for many different users, and they should not be able to see eachother's data. This means that you need to be able to configure your retrieval chain to only retrieve certain information. This generally involves two steps.


Step 1: Make sure the retriever you are using supports multiple users


At the moment, there is no unified flag or filter for this in LangChain. Rather, each vectorstore and retriever may have their own, and may be called different things (namespaces, multi-tenancy, etc). For vectorstores, this is generally exposed as a keyword argument that is passed in during 
similarity_search
. By reading the documentation or source code, figure out whether the retriever you are using supports multiple users, and, if so, how to use it.


Note: adding documentation and/or support for multiple users for retrievers that do not support it (or document it) is a GREAT way to contribute to LangChain


Step 2: Add that parameter as a configurable field for the chain


This will let you easily call the chain and configure any relevant flags at runtime. See 
this documentation
 for more information on configuration.


Now, at runtime you can call this chain with configurable field.


Code Example
​


Let's see a concrete example of what this looks like in code. We will use Pinecone for this example.


To configure Pinecone, set the following environment variable:




PINECONE_API_KEY
: Your Pinecone API key




from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_pinecone 
import
 PineconeVectorStore
embeddings 
=
 OpenAIEmbeddings
(
)
vectorstore 
=
 PineconeVectorStore
(
index_name
=
"test-example"
,
 embedding
=
embeddings
)
vectorstore
.
add_texts
(
[
"I worked at Kensho"
]
,
 namespace
=
"harrison"
)
vectorstore
.
add_texts
(
[
"I worked at Facebook"
]
,
 namespace
=
"ankush"
)
API Reference:
OpenAIEmbeddings
 | 
PineconeVectorStore


['f907aab7-77c7-4347-acc2-6859f8142f92']


The pinecone kwarg for 
namespace
 can be used to separate documents


# This will only get documents for Ankush
vectorstore
.
as_retriever
(
search_kwargs
=
{
"namespace"
:
 
"ankush"
}
)
.
invoke
(
    
"where did i work?"
)


[Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')]


# This will only get documents for Harrison
vectorstore
.
as_retriever
(
search_kwargs
=
{
"namespace"
:
 
"harrison"
}
)
.
invoke
(
    
"where did i work?"
)


[Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')]


We can now create the chain that we will use to do question-answering over.


Let's first select a LLM.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


This will follow the basic implementation from the 
RAG tutorial
, but we will allow the retrieval step to be configurable.


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 ConfigurableField
template 
=
 
"""Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt 
=
 ChatPromptTemplate
.
from_template
(
template
)
retriever 
=
 vectorstore
.
as_retriever
(
)
API Reference:
ChatPromptTemplate
 | 
ConfigurableField


Here we mark the retriever as having a configurable field. All vectorstore retrievers have 
search_kwargs
 as a field. This is just a dictionary, with vectorstore specific fields.


This will let us pass in a value for 
search_kwargs
 when invoking the chain.


configurable_retriever 
=
 retriever
.
configurable_fields
(
    search_kwargs
=
ConfigurableField
(
        
id
=
"search_kwargs"
,
        name
=
"Search Kwargs"
,
        description
=
"The search kwargs to use"
,
    
)
)


We can now create the chain using our configurable retriever.


from
 langchain_core
.
documents 
import
 Document
from
 langchain_core
.
runnables 
import
 RunnableConfig
from
 langgraph
.
graph 
import
 START
,
 StateGraph
from
 typing_extensions 
import
 List
,
 TypedDict
class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 
str
def
 
retrieve
(
state
:
 State
,
 config
:
 RunnableConfig
)
:
    retrieved_docs 
=
 configurable_retriever
.
invoke
(
state
[
"question"
]
,
 config
)
    
return
 
{
"context"
:
 retrieved_docs
}
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    response 
=
 llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
.
content
}
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)
API Reference:
Document
 | 
RunnableConfig
 | 
StateGraph


from
 IPython
.
display 
import
 Image
,
 display
display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




We can now invoke the chain with configurable options. 
search_kwargs
 is the id of the configurable field. The value is the search kwargs to use for Pinecone.


result 
=
 graph
.
invoke
(
    
{
"question"
:
 
"Where did the user work?"
}
,
    config
=
{
"configurable"
:
 
{
"search_kwargs"
:
 
{
"namespace"
:
 
"harrison"
}
}
}
,
)
result


{'question': 'Where did the user work?',
 'context': [Document(id='16061fc5-c6fc-4f45-a3b3-23469d7996af', metadata={}, page_content='I worked at Kensho')],
 'answer': 'The user worked at Kensho.'}


result 
=
 graph
.
invoke
(
    
{
"question"
:
 
"Where did the user work?"
}
,
    config
=
{
"configurable"
:
 
{
"search_kwargs"
:
 
{
"namespace"
:
 
"ankush"
}
}
}
,
)
result


{'question': 'Where did the user work?',
 'context': [Document(id='f907aab7-77c7-4347-acc2-6859f8142f92', metadata={}, page_content='I worked at Facebook')],
 'answer': 'The user worked at Facebook.'}


For details operating your specific vector store, see the 
integration pages
.
Edit this page
Was this page helpful?
Previous
How to get a RAG application to add citations
Next
How to get your RAG application to return sources
Code Example


--- docs_how_to_qa_sources.txt ---

How-to guides
How to get your RAG application to return sources
On this page
How to get your RAG application to return sources


Often in 
Q&A
 applications it's important to show users the sources that were used to generate the answer. The simplest way to do this is for the chain to return the Documents that were retrieved in each generation.


We'll work off of the Q&A app we built over the 
LLM Powered Autonomous Agents
 blog post by Lilian Weng in the 
RAG tutorial
.


We will cover two approaches:




Using the basic RAG chain covered in 
Part 1
 of the RAG tutorial;


Using a conversational RAG chain as convered in 
Part 2
 of the tutorial.




We will also show how to structure sources into the model response, such that a model can report what specific sources it used in generating its answer.


Setup
​


Dependencies
​


We'll use the following packages:


%
pip install 
-
-
upgrade 
-
-
quiet langchain langchain
-
community langchainhub beautifulsoup4


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with 
LangSmith
.


Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:


os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Components
​


We will need to select three components from LangChain's suite of integrations.


A 
chat model
:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


An 
embedding model
:




Select 
embeddings model
:
OpenAI
▾
OpenAI
Azure
Google
AWS
HuggingFace
Ollama
Cohere
MistralAI
Nomic
NVIDIA
Voyage AI
IBM watsonx
Fake
pip install -qU langchain-openai
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-large"
)


And a 
vector store
:




Select 
vector store
:
In-memory
▾
In-memory
AstraDB
Chroma
FAISS
Milvus
MongoDB
PGVector
Pinecone
Qdrant
pip install -qU langchain-core
from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
vector_store 
=
 InMemoryVectorStore
(
embeddings
)


RAG application
​


Let's reconstruct the Q&A app with sources we built over the 
LLM Powered Autonomous Agents
 blog post by Lilian Weng in the 
RAG tutorial
.


First we index our documents:


import
 bs4
from
 langchain 
import
 hub
from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
from
 langchain_core
.
documents 
import
 Document
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
from
 typing_extensions 
import
 List
,
 TypedDict
# Load and chunk contents of the blog
loader 
=
 WebBaseLoader
(
    web_paths
=
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
,
)
,
    bs_kwargs
=
dict
(
        parse_only
=
bs4
.
SoupStrainer
(
            class_
=
(
"post-content"
,
 
"post-title"
,
 
"post-header"
)
        
)
    
)
,
)
docs 
=
 loader
.
load
(
)
text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
1000
,
 chunk_overlap
=
200
)
all_splits 
=
 text_splitter
.
split_documents
(
docs
)
API Reference:
hub
 | 
WebBaseLoader
 | 
Document
 | 
RecursiveCharacterTextSplitter


# Index chunks
_ 
=
 vector_store
.
add_documents
(
documents
=
all_splits
)


Next we build the application:


from
 langchain 
import
 hub
from
 langchain_core
.
documents 
import
 Document
from
 langgraph
.
graph 
import
 START
,
 StateGraph
from
 typing_extensions 
import
 List
,
 TypedDict
# Define prompt for question-answering
prompt 
=
 hub
.
pull
(
"rlm/rag-prompt"
)
# Define state for application
class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 
str
# Define application steps
def
 
retrieve
(
state
:
 State
)
:
    retrieved_docs 
=
 vector_store
.
similarity_search
(
state
[
"question"
]
)
    
return
 
{
"context"
:
 retrieved_docs
}
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    response 
=
 llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
.
content
}
# Compile application and test
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)
API Reference:
hub
 | 
Document
 | 
StateGraph


from
 IPython
.
display 
import
 Image
,
 display
display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




Because we're tracking the retrieved context in our application's state, it is accessible after invoking the application:


result 
=
 graph
.
invoke
(
{
"question"
:
 
"What is Task Decomposition?"
}
)
print
(
f'Context: 
{
result
[
"context"
]
}
\n\n'
)
print
(
f'Answer: 
{
result
[
"answer"
]
}
'
)


Context: [Document(id='c8471b37-07d8-4d51-856e-4b2c22bca88d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='acb7eb6f-f252-4353-aec2-f459135354ba', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.'), Document(id='4fae6668-7fec-4237-9b2d-78132f4f3f3f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='3c79dd86-595e-42e8-b64d-404780f9e2d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content="(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.")]
Answer: Task Decomposition is the process of breaking down a complex task into smaller, manageable steps to facilitate execution. This can be achieved through techniques like Chain of Thought, which encourages step-by-step reasoning, or Tree of Thoughts, which explores multiple reasoning paths for each step. It can be implemented using simple prompts, specific instructions, or human input to effectively tackle the original task.


Here, 
"context"
 contains the sources that the LLM used in generating the response in 
"answer"
.


Structure sources in model response
​


Up to this point, we've simply propagated the documents returned from the retrieval step through to the final response. But this may not illustrate what subset of information the model relied on when generating its answer. Below, we show how to structure sources into the model response, allowing the model to report what specific context it relied on for its answer.


It is straightforward to extend the above LangGraph implementation. Below, we make a simple change: we use the model's tool-calling features to generate 
structured output
, consisting of an answer and list of sources. The schema for the response is represented in the 
AnswerWithSources
 TypedDict, below.


from
 typing 
import
 List
from
 typing_extensions 
import
 Annotated
,
 TypedDict
# Desired schema for response
class
 
AnswerWithSources
(
TypedDict
)
:
    
"""An answer to the question, with sources."""
    answer
:
 
str
    sources
:
 Annotated
[
        List
[
str
]
,
        
.
.
.
,
        
"List of sources (author + year) used to answer the question"
,
    
]
class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 AnswerWithSources
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    structured_llm 
=
 llm
.
with_structured_output
(
AnswerWithSources
)
    response 
=
 structured_llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
}
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)


import
 json
result 
=
 graph
.
invoke
(
{
"question"
:
 
"What is Chain of Thought?"
}
)
print
(
json
.
dumps
(
result
[
"answer"
]
,
 indent
=
2
)
)


{
  "answer": "Chain of Thought (CoT) is a prompting technique that enhances model performance by instructing it to think step by step, allowing the decomposition of complex tasks into smaller, manageable steps. This method not only aids in task execution but also provides insights into the model's reasoning process. CoT has become a standard approach in improving how language models handle intricate problem-solving tasks.",
  "sources": [
    "Wei et al. 2022"
  ]
}


tip
View 
LangSmith trace
.


Conversational RAG
​


Part 2
 of the RAG tutorial implements a different architecture, in which steps in the RAG flow are represented via successive 
message
 objects. This leverages additional 
tool-calling
 features of chat models, and more naturally accommodates a "back-and-forth" conversational user experience.


In that tutorial (and below), we propagate the retrieved documents as 
artifacts
 on the tool messages. That makes it easy to pluck out the retrieved documents. Below, we add them as an additional key in the state, for convenience.


Note that we define the response format of the tool as 
"content_and_artifact"
:


from
 langchain_core
.
tools 
import
 tool
@tool
(
response_format
=
"content_and_artifact"
)
def
 
retrieve
(
query
:
 
str
)
:
    
"""Retrieve information related to a query."""
    retrieved_docs 
=
 vector_store
.
similarity_search
(
query
,
 k
=
2
)
    serialized 
=
 
"\n\n"
.
join
(
        
(
f"Source: 
{
doc
.
metadata
}
\n"
 
f"Content: 
{
doc
.
page_content
}
"
)
        
for
 doc 
in
 retrieved_docs
    
)
    
return
 serialized
,
 retrieved_docs
API Reference:
tool


We can now build and compile the exact same application as in 
Part 2
 of the RAG tutorial, with two changes:




We add a 
context
 key of the state to store retrieved documents;


In the 
generate
 step, we pluck out the retrieved documents and populate them in the state.




These changes are highlighted below.


from
 langchain_core
.
messages 
import
 SystemMessage
from
 langgraph
.
graph 
import
 END
,
 MessagesState
,
 StateGraph
from
 langgraph
.
prebuilt 
import
 ToolNode
,
 tools_condition
class
 
State
(
MessagesState
)
:
    context
:
 List
[
Document
]
# Step 1: Generate an AIMessage that may include a tool-call to be sent.
def
 
query_or_respond
(
state
:
 State
)
:
    
"""Generate tool call for retrieval or respond."""
    llm_with_tools 
=
 llm
.
bind_tools
(
[
retrieve
]
)
    response 
=
 llm_with_tools
.
invoke
(
state
[
"messages"
]
)
    
# MessagesState appends messages to state instead of overwriting
    
return
 
{
"messages"
:
 
[
response
]
}
# Step 2: Execute the retrieval.
tools 
=
 ToolNode
(
[
retrieve
]
)
# Step 3: Generate a response using the retrieved content.
def
 
generate
(
state
:
 MessagesState
)
:
    
"""Generate answer."""
    
# Get generated ToolMessages
    recent_tool_messages 
=
 
[
]
    
for
 message 
in
 
reversed
(
state
[
"messages"
]
)
:
        
if
 message
.
type
 
==
 
"tool"
:
            recent_tool_messages
.
append
(
message
)
        
else
:
            
break
    tool_messages 
=
 recent_tool_messages
[
:
:
-
1
]
    
# Format into prompt
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
content 
for
 doc 
in
 tool_messages
)
    system_message_content 
=
 
(
        
"You are an assistant for question-answering tasks. "
        
"Use the following pieces of retrieved context to answer "
        
"the question. If you don't know the answer, say that you "
        
"don't know. Use three sentences maximum and keep the "
        
"answer concise."
        
"\n\n"
        
f"
{
docs_content
}
"
    
)
    conversation_messages 
=
 
[
        message
        
for
 message 
in
 state
[
"messages"
]
        
if
 message
.
type
 
in
 
(
"human"
,
 
"system"
)
        
or
 
(
message
.
type
 
==
 
"ai"
 
and
 
not
 message
.
tool_calls
)
    
]
    prompt 
=
 
[
SystemMessage
(
system_message_content
)
]
 
+
 conversation_messages
    
# Run
    response 
=
 llm
.
invoke
(
prompt
)
    context 
=
 
[
]
    
for
 tool_message 
in
 tool_messages
:
        context
.
extend
(
tool_message
.
artifact
)
    
return
 
{
"messages"
:
 
[
response
]
,
 
"context"
:
 context
}
API Reference:
SystemMessage
 | 
StateGraph
 | 
ToolNode
 | 
tools_condition


We can compile the application as before:


graph_builder 
=
 StateGraph
(
MessagesState
)
graph_builder
.
add_node
(
query_or_respond
)
graph_builder
.
add_node
(
tools
)
graph_builder
.
add_node
(
generate
)
graph_builder
.
set_entry_point
(
"query_or_respond"
)
graph_builder
.
add_conditional_edges
(
    
"query_or_respond"
,
    tools_condition
,
    
{
END
:
 END
,
 
"tools"
:
 
"tools"
}
,
)
graph_builder
.
add_edge
(
"tools"
,
 
"generate"
)
graph_builder
.
add_edge
(
"generate"
,
 END
)
graph 
=
 graph_builder
.
compile
(
)


display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




Invoking our application, we see that the retrieved 
Document
 objects are accessible from the application state.


input_message 
=
 
"What is Task Decomposition?"
for
 step 
in
 graph
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 input_message
}
]
}
,
    stream_mode
=
"values"
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
What is Task Decomposition?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_oA0XZ5hF70X0oW4ccNUFCFxX)
 Call ID: call_oA0XZ5hF70X0oW4ccNUFCFxX
  Args:
    query: Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
==================================[1m Ai Message [0m==================================
Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often utilizes techniques like Chain of Thought (CoT) prompting, which encourages models to think step by step, enhancing performance on complex tasks. This approach helps clarify the model's reasoning and makes it easier to tackle difficult problems.


step
[
"context"
]


[Document(id='c8471b37-07d8-4d51-856e-4b2c22bca88d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),
 Document(id='acb7eb6f-f252-4353-aec2-f459135354ba', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.')]


tip
Check out the 
LangSmith trace
.
Edit this page
Was this page helpful?
Previous
How to do per-user retrieval
Next
How to stream results from your RAG application
Setup
Dependencies
LangSmith
Components
RAG application
Structure sources in model response
Conversational RAG


--- docs_how_to_qa_streaming.txt ---

How-to guides
How to stream results from your RAG application
On this page
How to stream results from your RAG application


This guide explains how to stream results from a 
RAG
 application. It covers streaming tokens from the final output as well as intermediate steps of a chain (e.g., from query re-writing).


We'll work off of the Q&A app with sources we built over the 
LLM Powered Autonomous Agents
 blog post by Lilian Weng in the 
RAG tutorial
.


Setup
​


Dependencies
​


We'll use the following packages:


%
pip install 
-
-
upgrade 
-
-
quiet  langchain langchain
-
community langchainhub beautifulsoup4


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with 
LangSmith
.


Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:


os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Components
​


We will need to select three components from LangChain's suite of integrations.


A 
chat model
:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


An 
embedding model
:




Select 
embeddings model
:
OpenAI
▾
OpenAI
Azure
Google
AWS
HuggingFace
Ollama
Cohere
MistralAI
Nomic
NVIDIA
Voyage AI
IBM watsonx
Fake
pip install -qU langchain-openai
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-large"
)


And a 
vector store
:




Select 
vector store
:
In-memory
▾
In-memory
AstraDB
Chroma
FAISS
Milvus
MongoDB
PGVector
Pinecone
Qdrant
pip install -qU langchain-core
from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
vector_store 
=
 InMemoryVectorStore
(
embeddings
)


RAG application
​


Let's reconstruct the Q&A app with sources we built over the 
LLM Powered Autonomous Agents
 blog post by Lilian Weng in the 
RAG tutorial
.


First we index our documents:


import
 bs4
from
 langchain 
import
 hub
from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
from
 langchain_core
.
documents 
import
 Document
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
from
 typing_extensions 
import
 List
,
 TypedDict
# Load and chunk contents of the blog
loader 
=
 WebBaseLoader
(
    web_paths
=
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
,
)
,
    bs_kwargs
=
dict
(
        parse_only
=
bs4
.
SoupStrainer
(
            class_
=
(
"post-content"
,
 
"post-title"
,
 
"post-header"
)
        
)
    
)
,
)
docs 
=
 loader
.
load
(
)
text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
1000
,
 chunk_overlap
=
200
)
all_splits 
=
 text_splitter
.
split_documents
(
docs
)
API Reference:
hub
 | 
WebBaseLoader
 | 
Document
 | 
RecursiveCharacterTextSplitter


# Index chunks
_ 
=
 vector_store
.
add_documents
(
documents
=
all_splits
)


Next we build the application:


from
 langchain 
import
 hub
from
 langchain_core
.
documents 
import
 Document
from
 langgraph
.
graph 
import
 START
,
 StateGraph
from
 typing_extensions 
import
 List
,
 TypedDict
# Define prompt for question-answering
prompt 
=
 hub
.
pull
(
"rlm/rag-prompt"
)
# Define state for application
class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 
str
# Define application steps
def
 
retrieve
(
state
:
 State
)
:
    retrieved_docs 
=
 vector_store
.
similarity_search
(
state
[
"question"
]
)
    
return
 
{
"context"
:
 retrieved_docs
}
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    response 
=
 llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
.
content
}
# Compile application and test
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)
API Reference:
hub
 | 
Document
 | 
StateGraph


from
 IPython
.
display 
import
 Image
,
 display
display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




Streaming final outputs
​


LangGraph supports several 
streaming modes
, which can be controlled by specifying the 
stream_mode
 parameter. Setting 
stream_mode="messages"
 allows us to stream tokens from chat model invocations.


In general there can be multiple chat model invocations in an application (although here there is just one). Below, we filter to only the last step using the name of the corresponding node:


input_message 
=
 
"What is Task Decomposition?"
for
 message
,
 metadata 
in
 graph
.
stream
(
    
{
"question"
:
 
"What is Task Decomposition?"
}
,
    stream_mode
=
"messages"
,
)
:
    
if
 metadata
[
"langgraph_node"
]
 
==
 
"generate"
:
        
print
(
message
.
content
,
 end
=
"|"
)


|Task| De|composition| is| a| technique| used| to| break| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| often| involves| prompting| models| to| "|think| step| by| step|,"| allowing| for| clearer| reasoning| and| better| performance| on| intricate| problems|.| This| can| be| achieved| through| various| methods|,| including| simple| prompts|,| task|-specific| instructions|,| or| human| input|.||


Streaming intermediate steps
​


Other streaming modes will generally stream steps from our invocation-- i.e., state updates from individual nodes. In this case, each node is just appending a new key to the state:


for
 step 
in
 graph
.
stream
(
    
{
"question"
:
 
"What is Task Decomposition?"
}
,
    stream_mode
=
"updates"
,
)
:
    
print
(
f"
{
step
}
\n\n----------------\n"
)


{'retrieve': {'context': [Document(id='5bf5e308-6ccb-4f09-94d2-d0c36b8c9980', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='d8aed221-7943-414d-8ed7-63c2b0e7523b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.'), Document(id='bfa87007-02ef-4f81-a008-4522ecea1025', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='6aff7fc0-5c21-4986-9f1e-91e89715d934', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content="(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.")]}}
----------------
{'generate': {'answer': 'Task Decomposition is the process of breaking down a complex task into smaller, manageable steps to enhance understanding and execution. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think through steps systematically, allowing for better problem-solving. It can be achieved through simple prompting, task-specific instructions, or human input.'}}
----------------


For more on streaming with LangGraph, check out its 
streaming documentation
. For more information on streaming individual LangChain 
Runnables
, refer to 
this guide
.
Edit this page
Was this page helpful?
Previous
How to get your RAG application to return sources
Next
How to split JSON data
Setup
Dependencies
LangSmith
Components
RAG application
Streaming final outputs
Streaming intermediate steps


--- docs_how_to_query_constructing_filters.txt ---

How-to guides
How to construct filters for query analysis
How to construct filters for query analysis


We may want to do query analysis to extract filters to pass into retrievers. One way we ask the LLM to represent these filters is as a Pydantic model. There is then the issue of converting that Pydantic model into a filter that can be passed into a retriever.


This can be done manually, but LangChain also provides some "Translators" that are able to translate from a common syntax into filters specific to each retriever. Here, we will cover how to use those translators.


from
 typing 
import
 Optional
from
 langchain
.
chains
.
query_constructor
.
ir 
import
 
(
    Comparator
,
    Comparison
,
    Operation
,
    Operator
,
    StructuredQuery
,
)
from
 langchain_community
.
query_constructors
.
chroma 
import
 ChromaTranslator
from
 langchain_community
.
query_constructors
.
elasticsearch 
import
 ElasticsearchTranslator
from
 pydantic 
import
 BaseModel
API Reference:
Comparator
 | 
Comparison
 | 
Operation
 | 
Operator
 | 
StructuredQuery
 | 
ChromaTranslator
 | 
ElasticsearchTranslator


In this example, 
year
 and 
author
 are both attributes to filter on.


class
 
Search
(
BaseModel
)
:
    query
:
 
str
    start_year
:
 Optional
[
int
]
    author
:
 Optional
[
str
]


search_query 
=
 Search
(
query
=
"RAG"
,
 start_year
=
2022
,
 author
=
"LangChain"
)


def
 
construct_comparisons
(
query
:
 Search
)
:
    comparisons 
=
 
[
]
    
if
 query
.
start_year 
is
 
not
 
None
:
        comparisons
.
append
(
            Comparison
(
                comparator
=
Comparator
.
GT
,
                attribute
=
"start_year"
,
                value
=
query
.
start_year
,
            
)
        
)
    
if
 query
.
author 
is
 
not
 
None
:
        comparisons
.
append
(
            Comparison
(
                comparator
=
Comparator
.
EQ
,
                attribute
=
"author"
,
                value
=
query
.
author
,
            
)
        
)
    
return
 comparisons


comparisons 
=
 construct_comparisons
(
search_query
)


_filter 
=
 Operation
(
operator
=
Operator
.
AND
,
 arguments
=
comparisons
)


ElasticsearchTranslator
(
)
.
visit_operation
(
_filter
)


{'bool': {'must': [{'range': {'metadata.start_year': {'gt': 2022}}},
   {'term': {'metadata.author.keyword': 'LangChain'}}]}}


ChromaTranslator
(
)
.
visit_operation
(
_filter
)


{'$and': [{'start_year': {'$gt': 2022}}, {'author': {'$eq': 'LangChain'}}]}
Edit this page
Was this page helpful?
Previous
How to add values to a chain's state
Next
How to configure runtime chain internals


--- docs_how_to_query_few_shot.txt ---

How-to guides
How to add examples to the prompt for query analysis
On this page
How to add examples to the prompt for query analysis


As our query analysis becomes more complex, the LLM may struggle to understand how exactly it should respond in certain scenarios. In order to improve performance here, we can 
add examples
 to the prompt to guide the LLM.


Let's take a look at how we can add examples for a LangChain YouTube video query analyzer.


Setup
​


Install dependencies
​


# %pip install -qU langchain-core langchain-openai


Set environment variables
​


We'll use OpenAI in this example:


import
 getpass
import
 os
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
)
# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


Query schema
​


We'll define a query schema that we want our model to output. To make our query analysis a bit more interesting, we'll add a 
sub_queries
 field that contains more narrow questions derived from the top level question.


from
 typing 
import
 List
,
 Optional
from
 pydantic 
import
 BaseModel
,
 Field
sub_queries_description 
=
 
"""\
If the original question contains multiple distinct sub-questions, \
or if there are more generic questions that would be helpful to answer in \
order to answer the original question, write a list of all relevant sub-questions. \
Make sure this list is comprehensive and covers all parts of the original question. \
It's ok if there's redundancy in the sub-questions. \
Make sure the sub-questions are as narrowly focused as possible."""
class
 
Search
(
BaseModel
)
:
    
"""Search over a database of tutorial videos about a software library."""
    query
:
 
str
 
=
 Field
(
        
.
.
.
,
        description
=
"Primary similarity search query applied to video transcripts."
,
    
)
    sub_queries
:
 List
[
str
]
 
=
 Field
(
        default_factory
=
list
,
 description
=
sub_queries_description
    
)
    publish_year
:
 Optional
[
int
]
 
=
 Field
(
None
,
 description
=
"Year video was published"
)


Query generation
​


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 MessagesPlaceholder
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
system 
=
 
"""You are an expert at converting user questions into database queries. \
You have access to a database of tutorial videos about a software library for building LLM-powered applications. \
Given a question, return a list of database queries optimized to retrieve the most relevant results.
If there are acronyms or words you are not familiar with, do not try to rephrase them."""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        MessagesPlaceholder
(
"examples"
,
 optional
=
True
)
,
        
(
"human"
,
 
"{question}"
)
,
    
]
)
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
structured_llm 
=
 llm
.
with_structured_output
(
Search
)
query_analyzer 
=
 
{
"question"
:
 RunnablePassthrough
(
)
}
 
|
 prompt 
|
 structured_llm
API Reference:
ChatPromptTemplate
 | 
MessagesPlaceholder
 | 
RunnablePassthrough
 | 
ChatOpenAI


Let's try out our query analyzer without any examples in the prompt:


query_analyzer
.
invoke
(
    
"what's the difference between web voyager and reflection agents? do both use langgraph?"
)


Search(query='difference between web voyager and reflection agents', sub_queries=['what is web voyager', 'what are reflection agents', 'do both web voyager and reflection agents use langgraph?'], publish_year=None)


Adding examples and tuning the prompt
​


This works pretty well, but we probably want it to decompose the question even further to separate the queries about Web Voyager and Reflection Agents.


To tune our query generation results, we can add some examples of inputs questions and gold standard output queries to our prompt.


examples 
=
 
[
]


question 
=
 
"What's chat langchain, is it a langchain template?"
query 
=
 Search
(
    query
=
"What is chat langchain and is it a langchain template?"
,
    sub_queries
=
[
"What is chat langchain"
,
 
"What is a langchain template"
]
,
)
examples
.
append
(
{
"input"
:
 question
,
 
"tool_calls"
:
 
[
query
]
}
)


question 
=
 
"How to build multi-agent system and stream intermediate steps from it"
query 
=
 Search
(
    query
=
"How to build multi-agent system and stream intermediate steps from it"
,
    sub_queries
=
[
        
"How to build multi-agent system"
,
        
"How to stream intermediate steps from multi-agent system"
,
        
"How to stream intermediate steps"
,
    
]
,
)
examples
.
append
(
{
"input"
:
 question
,
 
"tool_calls"
:
 
[
query
]
}
)


question 
=
 
"LangChain agents vs LangGraph?"
query 
=
 Search
(
    query
=
"What's the difference between LangChain agents and LangGraph? How do you deploy them?"
,
    sub_queries
=
[
        
"What are LangChain agents"
,
        
"What is LangGraph"
,
        
"How do you deploy LangChain agents"
,
        
"How do you deploy LangGraph"
,
    
]
,
)
examples
.
append
(
{
"input"
:
 question
,
 
"tool_calls"
:
 
[
query
]
}
)


Now we need to update our prompt template and chain so that the examples are included in each prompt. Since we're working with OpenAI function-calling, we'll need to do a bit of extra structuring to send example inputs and outputs to the model. We'll create a 
tool_example_to_messages
 helper function to handle this for us:


import
 uuid
from
 typing 
import
 Dict
from
 langchain_core
.
messages 
import
 
(
    AIMessage
,
    BaseMessage
,
    HumanMessage
,
    SystemMessage
,
    ToolMessage
,
)
def
 
tool_example_to_messages
(
example
:
 Dict
)
 
-
>
 List
[
BaseMessage
]
:
    messages
:
 List
[
BaseMessage
]
 
=
 
[
HumanMessage
(
content
=
example
[
"input"
]
)
]
    openai_tool_calls 
=
 
[
]
    
for
 tool_call 
in
 example
[
"tool_calls"
]
:
        openai_tool_calls
.
append
(
            
{
                
"id"
:
 
str
(
uuid
.
uuid4
(
)
)
,
                
"type"
:
 
"function"
,
                
"function"
:
 
{
                    
"name"
:
 tool_call
.
__class__
.
__name__
,
                    
"arguments"
:
 tool_call
.
json
(
)
,
                
}
,
            
}
        
)
    messages
.
append
(
        AIMessage
(
content
=
""
,
 additional_kwargs
=
{
"tool_calls"
:
 openai_tool_calls
}
)
    
)
    tool_outputs 
=
 example
.
get
(
"tool_outputs"
)
 
or
 
[
        
"You have correctly called this tool."
    
]
 
*
 
len
(
openai_tool_calls
)
    
for
 output
,
 tool_call 
in
 
zip
(
tool_outputs
,
 openai_tool_calls
)
:
        messages
.
append
(
ToolMessage
(
content
=
output
,
 tool_call_id
=
tool_call
[
"id"
]
)
)
    
return
 messages
example_msgs 
=
 
[
msg 
for
 ex 
in
 examples 
for
 msg 
in
 tool_example_to_messages
(
ex
)
]
API Reference:
AIMessage
 | 
BaseMessage
 | 
HumanMessage
 | 
SystemMessage
 | 
ToolMessage


from
 langchain_core
.
prompts 
import
 MessagesPlaceholder
query_analyzer_with_examples 
=
 
(
    
{
"question"
:
 RunnablePassthrough
(
)
}
    
|
 prompt
.
partial
(
examples
=
example_msgs
)
    
|
 structured_llm
)
API Reference:
MessagesPlaceholder


query_analyzer_with_examples
.
invoke
(
    
"what's the difference between web voyager and reflection agents? do both use langgraph?"
)


Search(query="What's the difference between web voyager and reflection agents? Do both use langgraph?", sub_queries=['What is web voyager', 'What are reflection agents', 'Do web voyager and reflection agents use langgraph?'], publish_year=None)


Thanks to our examples we get a slightly more decomposed search query. With some more prompt engineering and tuning of our examples we could improve query generation even more.


You can see that the examples are passed to the model as messages in the 
LangSmith trace
.
Edit this page
Was this page helpful?
Previous
How to install LangChain packages
Next
How to use few shot examples
Setup
Query schema
Query generation
Adding examples and tuning the prompt


--- docs_how_to_query_high_cardinality.txt ---

How-to guides
How deal with high cardinality categoricals when doing query analysis
On this page
How deal with high cardinality categoricals when doing query analysis


You may want to do query analysis to create a filter on a categorical column. One of the difficulties here is that you usually need to specify the EXACT categorical value. The issue is you need to make sure the LLM generates that categorical value exactly. This can be done relatively easy with prompting when there are only a few values that are valid. When there are a high number of valid values then it becomes more difficult, as those values may not fit in the LLM context, or (if they do) there may be too many for the LLM to properly attend to.


In this notebook we take a look at how to approach this.


Setup
​


Install dependencies
​


%
pip install 
-
qU langchain langchain
-
community langchain
-
openai faker langchain
-
chroma


Note: you may need to restart the kernel to use updated packages.


Set environment variables
​


We'll use OpenAI in this example:


import
 getpass
import
 os
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
)
# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


Set up data
​


We will generate a bunch of fake names


from
 faker 
import
 Faker
fake 
=
 Faker
(
)
names 
=
 
[
fake
.
name
(
)
 
for
 _ 
in
 
range
(
10000
)
]


Let's look at some of the names


names
[
0
]


'Jacob Adams'


names
[
567
]


'Eric Acevedo'


Query Analysis
​


We can now set up a baseline query analysis


from
 pydantic 
import
 BaseModel
,
 Field
,
 model_validator


class
 
Search
(
BaseModel
)
:
    query
:
 
str
    author
:
 
str


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
system 
=
 
"""Generate a relevant search query for a library system"""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        
(
"human"
,
 
"{question}"
)
,
    
]
)
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
structured_llm 
=
 llm
.
with_structured_output
(
Search
)
query_analyzer 
=
 
{
"question"
:
 RunnablePassthrough
(
)
}
 
|
 prompt 
|
 structured_llm
API Reference:
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI


We can see that if we spell the name exactly correctly, it knows how to handle it


query_analyzer
.
invoke
(
"what are books about aliens by Jesse Knight"
)


Search(query='aliens', author='Jesse Knight')


The issue is that the values you want to filter on may NOT be spelled exactly correctly


query_analyzer
.
invoke
(
"what are books about aliens by jess knight"
)


Search(query='aliens', author='Jess Knight')


Add in all values
​


One way around this is to add ALL possible values to the prompt. That will generally guide the query in the right direction


system 
=
 
"""Generate a relevant search query for a library system.
`author` attribute MUST be one of:
{authors}
Do NOT hallucinate author name!"""
base_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        
(
"human"
,
 
"{question}"
)
,
    
]
)
prompt 
=
 base_prompt
.
partial
(
authors
=
", "
.
join
(
names
)
)


query_analyzer_all 
=
 
{
"question"
:
 RunnablePassthrough
(
)
}
 
|
 prompt 
|
 structured_llm


However... if the list of categoricals is long enough, it may error!


try
:
    res 
=
 query_analyzer_all
.
invoke
(
"what are books about aliens by jess knight"
)
except
 Exception 
as
 e
:
    
print
(
e
)


We can try to use a longer context window... but with so much information in there, it is not garunteed to pick it up reliably


llm_long 
=
 ChatOpenAI
(
model
=
"gpt-4-turbo-preview"
,
 temperature
=
0
)
structured_llm_long 
=
 llm_long
.
with_structured_output
(
Search
)
query_analyzer_all 
=
 
{
"question"
:
 RunnablePassthrough
(
)
}
 
|
 prompt 
|
 structured_llm_long


query_analyzer_all
.
invoke
(
"what are books about aliens by jess knight"
)


Search(query='aliens', author='jess knight')


Find and all relevant values
​


Instead, what we can do is create an index over the relevant values and then query that for the N most relevant values,


from
 langchain_chroma 
import
 Chroma
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-small"
)
vectorstore 
=
 Chroma
.
from_texts
(
names
,
 embeddings
,
 collection_name
=
"author_names"
)
API Reference:
OpenAIEmbeddings


def
 
select_names
(
question
)
:
    _docs 
=
 vectorstore
.
similarity_search
(
question
,
 k
=
10
)
    _names 
=
 
[
d
.
page_content 
for
 d 
in
 _docs
]
    
return
 
", "
.
join
(
_names
)


create_prompt 
=
 
{
    
"question"
:
 RunnablePassthrough
(
)
,
    
"authors"
:
 select_names
,
}
 
|
 base_prompt


query_analyzer_select 
=
 create_prompt 
|
 structured_llm


create_prompt
.
invoke
(
"what are books by jess knight"
)


ChatPromptValue(messages=[SystemMessage(content='Generate a relevant search query for a library system.\n\n`author` attribute MUST be one of:\n\nJennifer Knight, Jill Knight, John Knight, Dr. Jeffrey Knight, Christopher Knight, Andrea Knight, Brandy Knight, Jennifer Keller, Becky Chambers, Sarah Knapp\n\nDo NOT hallucinate author name!'), HumanMessage(content='what are books by jess knight')])


query_analyzer_select
.
invoke
(
"what are books about aliens by jess knight"
)


Search(query='books about aliens', author='Jennifer Knight')


Replace after selection
​


Another method is to let the LLM fill in whatever value, but then convert that value to a valid value.
This can actually be done with the Pydantic class itself!


class
 
Search
(
BaseModel
)
:
    query
:
 
str
    author
:
 
str
    
@model_validator
(
mode
=
"before"
)
    
@classmethod
    
def
 
double
(
cls
,
 values
:
 
dict
)
 
-
>
 
dict
:
        author 
=
 values
[
"author"
]
        closest_valid_author 
=
 vectorstore
.
similarity_search
(
author
,
 k
=
1
)
[
            
0
        
]
.
page_content
        values
[
"author"
]
 
=
 closest_valid_author
        
return
 values


system 
=
 
"""Generate a relevant search query for a library system"""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        
(
"human"
,
 
"{question}"
)
,
    
]
)
corrective_structure_llm 
=
 llm
.
with_structured_output
(
Search
)
corrective_query_analyzer 
=
 
(
    
{
"question"
:
 RunnablePassthrough
(
)
}
 
|
 prompt 
|
 corrective_structure_llm
)


corrective_query_analyzer
.
invoke
(
"what are books about aliens by jes knight"
)


Search(query='aliens', author='John Knight')


# TODO: show trigram similarity
Edit this page
Was this page helpful?
Previous
How to configure runtime chain internals
Next
Custom Document Loader
Setup
Query Analysis
Add in all values
Find and all relevant values
Replace after selection


--- docs_how_to_query_multiple_queries.txt ---

How-to guides
How to handle multiple queries when doing query analysis
On this page
How to handle multiple queries when doing query analysis


Sometimes, a query analysis technique may allow for multiple queries to be generated. In these cases, we need to remember to run all queries and then to combine the results. We will show a simple example (using mock data) of how to do that.


Setup
​


Install dependencies
​


%
pip install 
-
qU langchain langchain
-
community langchain
-
openai langchain
-
chroma


Note: you may need to restart the kernel to use updated packages.


Set environment variables
​


We'll use OpenAI in this example:


import
 getpass
import
 os
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
)
# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


Create Index
​


We will create a vectorstore over fake information.


from
 langchain_chroma 
import
 Chroma
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
texts 
=
 
[
"Harrison worked at Kensho"
,
 
"Ankush worked at Facebook"
]
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-small"
)
vectorstore 
=
 Chroma
.
from_texts
(
    texts
,
    embeddings
,
)
retriever 
=
 vectorstore
.
as_retriever
(
search_kwargs
=
{
"k"
:
 
1
}
)
API Reference:
OpenAIEmbeddings
 | 
RecursiveCharacterTextSplitter


Query analysis
​


We will use function calling to structure the output. We will let it return multiple queries.


from
 typing 
import
 List
,
 Optional
from
 pydantic 
import
 BaseModel
,
 Field
class
 
Search
(
BaseModel
)
:
    
"""Search over a database of job records."""
    queries
:
 List
[
str
]
 
=
 Field
(
        
.
.
.
,
        description
=
"Distinct queries to search for"
,
    
)


from
 langchain_core
.
output_parsers
.
openai_tools 
import
 PydanticToolsParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
output_parser 
=
 PydanticToolsParser
(
tools
=
[
Search
]
)
system 
=
 
"""You have the ability to issue search queries to get information to help answer user information.
If you need to look up two distinct pieces of information, you are allowed to do that!"""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        
(
"human"
,
 
"{question}"
)
,
    
]
)
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
structured_llm 
=
 llm
.
with_structured_output
(
Search
)
query_analyzer 
=
 
{
"question"
:
 RunnablePassthrough
(
)
}
 
|
 prompt 
|
 structured_llm
API Reference:
PydanticToolsParser
 | 
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI


We can see that this allows for creating multiple queries


query_analyzer
.
invoke
(
"where did Harrison Work"
)


Search(queries=['Harrison Work', 'Harrison employment history'])


query_analyzer
.
invoke
(
"where did Harrison and ankush Work"
)


Search(queries=['Harrison work history', 'Ankush work history'])


Retrieval with query analysis
​


So how would we include this in a chain? One thing that will make this a lot easier is if we call our retriever asynchronously - this will let us loop over the queries and not get blocked on the response time.


from
 langchain_core
.
runnables 
import
 chain
API Reference:
chain


@chain
async
 
def
 
custom_chain
(
question
)
:
    response 
=
 
await
 query_analyzer
.
ainvoke
(
question
)
    docs 
=
 
[
]
    
for
 query 
in
 response
.
queries
:
        new_docs 
=
 
await
 retriever
.
ainvoke
(
query
)
        docs
.
extend
(
new_docs
)
    
# You probably want to think about reranking or deduplicating documents here
    
# But that is a separate topic
    
return
 docs


await
 custom_chain
.
ainvoke
(
"where did Harrison Work"
)


[Document(page_content='Harrison worked at Kensho'),
 Document(page_content='Harrison worked at Kensho')]


await
 custom_chain
.
ainvoke
(
"where did Harrison and ankush Work"
)


[Document(page_content='Harrison worked at Kensho'),
 Document(page_content='Ankush worked at Facebook')]
Edit this page
Was this page helpful?
Previous
How to partially format prompt templates
Next
How to use built-in tools and toolkits
Setup
Create Index
Query analysis
Retrieval with query analysis


--- docs_how_to_query_multiple_retrievers.txt ---

How-to guides
How to handle multiple retrievers when doing query analysis
On this page
How to handle multiple retrievers when doing query analysis


Sometimes, a query analysis technique may allow for selection of which 
retriever
 to use. To use this, you will need to add some logic to select the retriever to do. We will show a simple example (using mock data) of how to do that.


Setup
​


Install dependencies
​


%
pip install 
-
qU langchain langchain
-
community langchain
-
openai langchain
-
chroma


Note: you may need to restart the kernel to use updated packages.


Set environment variables
​


We'll use OpenAI in this example:


import
 getpass
import
 os
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
)
# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


Create Index
​


We will create a vectorstore over fake information.


from
 langchain_chroma 
import
 Chroma
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
texts 
=
 
[
"Harrison worked at Kensho"
]
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-small"
)
vectorstore 
=
 Chroma
.
from_texts
(
texts
,
 embeddings
,
 collection_name
=
"harrison"
)
retriever_harrison 
=
 vectorstore
.
as_retriever
(
search_kwargs
=
{
"k"
:
 
1
}
)
texts 
=
 
[
"Ankush worked at Facebook"
]
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-small"
)
vectorstore 
=
 Chroma
.
from_texts
(
texts
,
 embeddings
,
 collection_name
=
"ankush"
)
retriever_ankush 
=
 vectorstore
.
as_retriever
(
search_kwargs
=
{
"k"
:
 
1
}
)
API Reference:
OpenAIEmbeddings
 | 
RecursiveCharacterTextSplitter


Query analysis
​


We will use function calling to structure the output. We will let it return multiple queries.


from
 typing 
import
 List
,
 Optional
from
 pydantic 
import
 BaseModel
,
 Field
class
 
Search
(
BaseModel
)
:
    
"""Search for information about a person."""
    query
:
 
str
 
=
 Field
(
        
.
.
.
,
        description
=
"Query to look up"
,
    
)
    person
:
 
str
 
=
 Field
(
        
.
.
.
,
        description
=
"Person to look things up for. Should be `HARRISON` or `ANKUSH`."
,
    
)


from
 langchain_core
.
output_parsers
.
openai_tools 
import
 PydanticToolsParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
output_parser 
=
 PydanticToolsParser
(
tools
=
[
Search
]
)
system 
=
 
"""You have the ability to issue search queries to get information to help answer user information."""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        
(
"human"
,
 
"{question}"
)
,
    
]
)
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
structured_llm 
=
 llm
.
with_structured_output
(
Search
)
query_analyzer 
=
 
{
"question"
:
 RunnablePassthrough
(
)
}
 
|
 prompt 
|
 structured_llm
API Reference:
PydanticToolsParser
 | 
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI


We can see that this allows for routing between retrievers


query_analyzer
.
invoke
(
"where did Harrison Work"
)


Search(query='work history', person='HARRISON')


query_analyzer
.
invoke
(
"where did ankush Work"
)


Search(query='work history', person='ANKUSH')


Retrieval with query analysis
​


So how would we include this in a chain? We just need some simple logic to select the retriever and pass in the search query


from
 langchain_core
.
runnables 
import
 chain
API Reference:
chain


retrievers 
=
 
{
    
"HARRISON"
:
 retriever_harrison
,
    
"ANKUSH"
:
 retriever_ankush
,
}


@chain
def
 
custom_chain
(
question
)
:
    response 
=
 query_analyzer
.
invoke
(
question
)
    retriever 
=
 retrievers
[
response
.
person
]
    
return
 retriever
.
invoke
(
response
.
query
)


custom_chain
.
invoke
(
"where did Harrison Work"
)


[Document(page_content='Harrison worked at Kensho')]


custom_chain
.
invoke
(
"where did ankush Work"
)


[Document(page_content='Ankush worked at Facebook')]
Edit this page
Was this page helpful?
Previous
How to compose prompts together
Next
How to add values to a chain's state
Setup
Create Index
Query analysis
Retrieval with query analysis


--- docs_how_to_query_no_queries.txt ---

How-to guides
How to handle cases where no queries are generated
On this page
How to handle cases where no queries are generated


Sometimes, a query analysis technique may allow for any number of queries to be generated - including no queries! In this case, our overall chain will need to inspect the result of the query analysis before deciding whether to call the retriever or not.


We will use mock data for this example.


Setup
​


Install dependencies
​


%
pip install 
-
qU langchain langchain
-
community langchain
-
openai langchain
-
chroma


Note: you may need to restart the kernel to use updated packages.


Set environment variables
​


We'll use OpenAI in this example:


import
 getpass
import
 os
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
)
# Optional, uncomment to trace runs with LangSmith. Sign up here: https://smith.langchain.com.
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


Create Index
​


We will create a vectorstore over fake information.


from
 langchain_chroma 
import
 Chroma
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
texts 
=
 
[
"Harrison worked at Kensho"
]
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-small"
)
vectorstore 
=
 Chroma
.
from_texts
(
    texts
,
    embeddings
,
)
retriever 
=
 vectorstore
.
as_retriever
(
)
API Reference:
OpenAIEmbeddings
 | 
RecursiveCharacterTextSplitter


Query analysis
​


We will use function calling to structure the output. However, we will configure the LLM such that is doesn't NEED to call the function representing a search query (should it decide not to). We will also then use a prompt to do query analysis that explicitly lays when it should and shouldn't make a search.


from
 typing 
import
 Optional
from
 pydantic 
import
 BaseModel
,
 Field
class
 
Search
(
BaseModel
)
:
    
"""Search over a database of job records."""
    query
:
 
str
 
=
 Field
(
        
.
.
.
,
        description
=
"Similarity search query applied to job record."
,
    
)


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
system 
=
 
"""You have the ability to issue search queries to get information to help answer user information.
You do not NEED to look things up. If you don't need to, then just respond normally."""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        
(
"human"
,
 
"{question}"
)
,
    
]
)
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
structured_llm 
=
 llm
.
bind_tools
(
[
Search
]
)
query_analyzer 
=
 
{
"question"
:
 RunnablePassthrough
(
)
}
 
|
 prompt 
|
 structured_llm
API Reference:
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI


We can see that by invoking this we get an message that sometimes - but not always - returns a tool call.


query_analyzer
.
invoke
(
"where did Harrison Work"
)


AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_korLZrh08PTRL94f4L7rFqdj', 'function': {'arguments': '{"query":"Harrison"}', 'name': 'Search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 95, 'total_tokens': 109}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ea94d376-37bf-4f80-abe6-e3b42b767ea0-0', tool_calls=[{'name': 'Search', 'args': {'query': 'Harrison'}, 'id': 'call_korLZrh08PTRL94f4L7rFqdj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 95, 'output_tokens': 14, 'total_tokens': 109})


query_analyzer
.
invoke
(
"hi!"
)


AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 93, 'total_tokens': 103}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-ebdfc44a-455a-4ca6-be85-84559886b1e1-0', usage_metadata={'input_tokens': 93, 'output_tokens': 10, 'total_tokens': 103})


Retrieval with query analysis
​


So how would we include this in a chain? Let's look at an example below.


from
 langchain_core
.
output_parsers
.
openai_tools 
import
 PydanticToolsParser
from
 langchain_core
.
runnables 
import
 chain
output_parser 
=
 PydanticToolsParser
(
tools
=
[
Search
]
)
API Reference:
PydanticToolsParser
 | 
chain


@chain
def
 
custom_chain
(
question
)
:
    response 
=
 query_analyzer
.
invoke
(
question
)
    
if
 
"tool_calls"
 
in
 response
.
additional_kwargs
:
        query 
=
 output_parser
.
invoke
(
response
)
        docs 
=
 retriever
.
invoke
(
query
[
0
]
.
query
)
        
# Could add more logic - like another LLM call - here
        
return
 docs
    
else
:
        
return
 response


custom_chain
.
invoke
(
"where did Harrison Work"
)


Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1


[Document(page_content='Harrison worked at Kensho')]


custom_chain
.
invoke
(
"hi!"
)


AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 93, 'total_tokens': 103}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'stop', 'logprobs': None}, id='run-e87f058d-30c0-4075-8a89-a01b982d557e-0', usage_metadata={'input_tokens': 93, 'output_tokens': 10, 'total_tokens': 103})
Edit this page
Was this page helpful?
Previous
How to use output parsers to parse an LLM response into structured format
Next
How to route between sub-chains
Setup
Create Index
Query analysis
Retrieval with query analysis


--- docs_how_to_recursive_json_splitter.txt ---

How-to guides
How to split JSON data
On this page
How to split JSON data


This json splitter 
splits
 json data while allowing control over chunk sizes. It traverses json data depth first and builds smaller json chunks. It attempts to keep nested json objects whole but will split them if needed to keep chunks between a min_chunk_size and the max_chunk_size.


If the value is not a nested json, but rather a very large string the string will not be split. If you need a hard cap on the chunk size consider composing this with a Recursive Text splitter on those chunks. There is an optional pre-processing step to split lists, by first converting them to json (dict) and then splitting them as such.




How the text is split: json value.


How the chunk size is measured: by number of characters.




%
pip install 
-
qU langchain
-
text
-
splitters


First we load some json data:


import
 json
import
 requests
# This is a large nested json object and will be loaded as a python dict
json_data 
=
 requests
.
get
(
"https://api.smith.langchain.com/openapi.json"
)
.
json
(
)


Basic usage
​


Specify 
max_chunk_size
 to constrain chunk sizes:


from
 langchain_text_splitters 
import
 RecursiveJsonSplitter
splitter 
=
 RecursiveJsonSplitter
(
max_chunk_size
=
300
)
API Reference:
RecursiveJsonSplitter


To obtain json chunks, use the 
.split_json
 method:


# Recursively split json data - If you need to access/manipulate the smaller json chunks
json_chunks 
=
 splitter
.
split_json
(
json_data
=
json_data
)
for
 chunk 
in
 json_chunks
[
:
3
]
:
    
print
(
chunk
)


{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'servers': [{'url': 'https://api.smith.langchain.com', 'description': 'LangSmith API endpoint.'}]}
{'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions'], 'summary': 'Read Tracer Session', 'description': 'Get a specific session.', 'operationId': 'read_tracer_session_api_v1_sessions__session_id__get'}}}}
{'paths': {'/api/v1/sessions/{session_id}': {'get': {'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}


To obtain LangChain 
Document
 objects, use the 
.create_documents
 method:


# The splitter can also output documents
docs 
=
 splitter
.
create_documents
(
texts
=
[
json_data
]
)
for
 doc 
in
 docs
[
:
3
]
:
    
print
(
doc
)


page_content='{"openapi": "3.1.0", "info": {"title": "LangSmith", "version": "0.1.0"}, "servers": [{"url": "https://api.smith.langchain.com", "description": "LangSmith API endpoint."}]}'
page_content='{"paths": {"/api/v1/sessions/{session_id}": {"get": {"tags": ["tracer-sessions"], "summary": "Read Tracer Session", "description": "Get a specific session.", "operationId": "read_tracer_session_api_v1_sessions__session_id__get"}}}}'
page_content='{"paths": {"/api/v1/sessions/{session_id}": {"get": {"security": [{"API Key": []}, {"Tenant ID": []}, {"Bearer Auth": []}]}}}}'


Or use 
.split_text
 to obtain string content directly:


texts 
=
 splitter
.
split_text
(
json_data
=
json_data
)
print
(
texts
[
0
]
)
print
(
texts
[
1
]
)


{"openapi": "3.1.0", "info": {"title": "LangSmith", "version": "0.1.0"}, "servers": [{"url": "https://api.smith.langchain.com", "description": "LangSmith API endpoint."}]}
{"paths": {"/api/v1/sessions/{session_id}": {"get": {"tags": ["tracer-sessions"], "summary": "Read Tracer Session", "description": "Get a specific session.", "operationId": "read_tracer_session_api_v1_sessions__session_id__get"}}}}


How to manage chunk sizes from list content
​


Note that one of the chunks in this example is larger than the specified 
max_chunk_size
 of 300. Reviewing one of these chunks that was bigger we see there is a list object there:


print
(
[
len
(
text
)
 
for
 text 
in
 texts
]
[
:
10
]
)
print
(
)
print
(
texts
[
3
]
)


[171, 231, 126, 469, 210, 213, 237, 271, 191, 232]
{"paths": {"/api/v1/sessions/{session_id}": {"get": {"parameters": [{"name": "session_id", "in": "path", "required": true, "schema": {"type": "string", "format": "uuid", "title": "Session Id"}}, {"name": "include_stats", "in": "query", "required": false, "schema": {"type": "boolean", "default": false, "title": "Include Stats"}}, {"name": "accept", "in": "header", "required": false, "schema": {"anyOf": [{"type": "string"}, {"type": "null"}], "title": "Accept"}}]}}}}


The json splitter by default does not split lists.


Specify 
convert_lists=True
 to preprocess the json, converting list content to dicts with 
index:item
 as 
key:val
 pairs:


texts 
=
 splitter
.
split_text
(
json_data
=
json_data
,
 convert_lists
=
True
)


Let's look at the size of the chunks. Now they are all under the max


print
(
[
len
(
text
)
 
for
 text 
in
 texts
]
[
:
10
]
)


[176, 236, 141, 203, 212, 221, 210, 213, 242, 291]


The list has been converted to a dict, but retains all the needed contextual information even if split into many chunks:


print
(
texts
[
1
]
)


{"paths": {"/api/v1/sessions/{session_id}": {"get": {"tags": {"0": "tracer-sessions"}, "summary": "Read Tracer Session", "description": "Get a specific session.", "operationId": "read_tracer_session_api_v1_sessions__session_id__get"}}}}


# We can also look at the documents
docs
[
1
]


Document(page_content='{"paths": {"/api/v1/sessions/{session_id}": {"get": {"tags": ["tracer-sessions"], "summary": "Read Tracer Session", "description": "Get a specific session.", "operationId": "read_tracer_session_api_v1_sessions__session_id__get"}}}}')
Edit this page
Was this page helpful?
Previous
How to stream results from your RAG application
Next
How to recursively split text by characters
Basic usage
How to manage chunk sizes from list content


--- docs_how_to_recursive_text_splitter.txt ---

How-to guides
How to recursively split text by characters
On this page
How to recursively split text by characters


This 
text splitter
 is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is 
["\n\n", "\n", " ", ""]
. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.




How the text is split: by list of characters.


How the chunk size is measured: by number of characters.




Below we show example usage.


To obtain the string content directly, use 
.split_text
.


To create LangChain 
Document
 objects (e.g., for use in downstream tasks), use 
.create_documents
.


%
pip install 
-
qU langchain
-
text
-
splitters


from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
# Load example document
with
 
open
(
"state_of_the_union.txt"
)
 
as
 f
:
    state_of_the_union 
=
 f
.
read
(
)
text_splitter 
=
 RecursiveCharacterTextSplitter
(
    
# Set a really small chunk size, just to show.
    chunk_size
=
100
,
    chunk_overlap
=
20
,
    length_function
=
len
,
    is_separator_regex
=
False
,
)
texts 
=
 text_splitter
.
create_documents
(
[
state_of_the_union
]
)
print
(
texts
[
0
]
)
print
(
texts
[
1
]
)
API Reference:
RecursiveCharacterTextSplitter


page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and'
page_content='of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.'


text_splitter
.
split_text
(
state_of_the_union
)
[
:
2
]


['Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and',
 'of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.']


Let's go through the parameters set above for 
RecursiveCharacterTextSplitter
:




chunk_size
: The maximum size of a chunk, where size is determined by the 
length_function
.


chunk_overlap
: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks.


length_function
: Function determining the chunk size.


is_separator_regex
: Whether the separator list (defaulting to 
["\n\n", "\n", " ", ""]
) should be interpreted as regex.




Splitting text from languages without word boundaries
​


Some writing systems do not have 
word boundaries
, for example Chinese, Japanese, and Thai. Splitting text with the default separator list of 
["\n\n", "\n", " ", ""]
 can cause words to be split between chunks. To keep words together, you can override the list of separators to include additional punctuation:




Add ASCII full-stop "
.
", 
Unicode fullwidth
 full stop "
．
" (used in Chinese text), and 
ideographic full stop
 "
。
" (used in Japanese and Chinese)


Add 
Zero-width space
 used in Thai, Myanmar, Kmer, and Japanese.


Add ASCII comma "
,
", Unicode fullwidth comma "
，
", and Unicode ideographic comma "
、
"




text_splitter 
=
 RecursiveCharacterTextSplitter
(
    separators
=
[
        
"\n\n"
,
        
"\n"
,
        
" "
,
        
"."
,
        
","
,
        
"\u200b"
,
  
# Zero-width space
        
"\uff0c"
,
  
# Fullwidth comma
        
"\u3001"
,
  
# Ideographic comma
        
"\uff0e"
,
  
# Fullwidth full stop
        
"\u3002"
,
  
# Ideographic full stop
        
""
,
    
]
,
    
# Existing args
)
Edit this page
Was this page helpful?
Previous
How to split JSON data
Next
Response metadata
Splitting text from languages without word boundaries


--- docs_how_to_response_metadata.txt ---

How-to guides
Response metadata
On this page
Response metadata


Many model providers include some metadata in their chat generation 
responses
. This metadata can be accessed via the 
AIMessage.response_metadata: Dict
 attribute. Depending on the model provider and model configuration, this can contain information like 
token counts
, 
logprobs
, and more.


Here's what the response metadata looks like for a few different providers:


OpenAI
​


from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
msg 
=
 llm
.
invoke
(
"What's the oldest known example of cuneiform"
)
msg
.
response_metadata
API Reference:
ChatOpenAI


{'token_usage': {'completion_tokens': 110,
  'prompt_tokens': 16,
  'total_tokens': 126,
  'completion_tokens_details': {'accepted_prediction_tokens': 0,
   'audio_tokens': 0,
   'reasoning_tokens': 0,
   'rejected_prediction_tokens': 0},
  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},
 'model_name': 'gpt-4o-mini-2024-07-18',
 'system_fingerprint': 'fp_b8bc95a0ac',
 'id': 'chatcmpl-BDrISvLar6AqcZngBmhajFZXVc2u9',
 'finish_reason': 'stop',
 'logprobs': None}


Anthropic
​


from
 langchain_anthropic 
import
 ChatAnthropic
llm 
=
 ChatAnthropic
(
model
=
"claude-3-5-sonnet-latest"
)
msg 
=
 llm
.
invoke
(
"What's the oldest known example of cuneiform"
)
msg
.
response_metadata
API Reference:
ChatAnthropic


{'id': 'msg_01JHnvPqgERY7MZwrvfkmq52',
 'model': 'claude-3-5-sonnet-20241022',
 'stop_reason': 'end_turn',
 'stop_sequence': None,
 'usage': {'cache_creation_input_tokens': 0,
  'cache_read_input_tokens': 0,
  'input_tokens': 17,
  'output_tokens': 221},
 'model_name': 'claude-3-5-sonnet-20241022'}


Google VertexAI
​


from
 langchain_google_vertexai 
import
 ChatVertexAI
llm 
=
 ChatVertexAI
(
model
=
"gemini-1.5-flash-001"
)
msg 
=
 llm
.
invoke
(
"What's the oldest known example of cuneiform"
)
msg
.
response_metadata
API Reference:
ChatVertexAI


{'is_blocked': False,
 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH',
   'probability_label': 'NEGLIGIBLE',
   'probability_score': 0.046142578125,
   'blocked': False,
   'severity': 'HARM_SEVERITY_NEGLIGIBLE',
   'severity_score': 0.07275390625},
  {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT',
   'probability_label': 'NEGLIGIBLE',
   'probability_score': 0.05419921875,
   'blocked': False,
   'severity': 'HARM_SEVERITY_NEGLIGIBLE',
   'severity_score': 0.03955078125},
  {'category': 'HARM_CATEGORY_HARASSMENT',
   'probability_label': 'NEGLIGIBLE',
   'probability_score': 0.083984375,
   'blocked': False,
   'severity': 'HARM_SEVERITY_NEGLIGIBLE',
   'severity_score': 0.029296875},
  {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
   'probability_label': 'NEGLIGIBLE',
   'probability_score': 0.054931640625,
   'blocked': False,
   'severity': 'HARM_SEVERITY_NEGLIGIBLE',
   'severity_score': 0.03466796875}],
 'usage_metadata': {'prompt_token_count': 10,
  'candidates_token_count': 193,
  'total_token_count': 203,
  'prompt_tokens_details': [{'modality': 1, 'token_count': 10}],
  'candidates_tokens_details': [{'modality': 1, 'token_count': 193}],
  'cached_content_token_count': 0,
  'cache_tokens_details': []},
 'finish_reason': 'STOP',
 'avg_logprobs': -0.5702065976790196,
 'model_name': 'gemini-1.5-flash-001'}


Bedrock (Anthropic)
​


from
 langchain_aws 
import
 ChatBedrockConverse
llm 
=
 ChatBedrockConverse
(
model
=
"anthropic.claude-3-sonnet-20240229-v1:0"
)
msg 
=
 llm
.
invoke
(
"What's the oldest known example of cuneiform"
)
msg
.
response_metadata
API Reference:
ChatBedrockConverse


{'ResponseMetadata': {'RequestId': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31',
  'HTTPStatusCode': 200,
  'HTTPHeaders': {'date': 'Sat, 22 Mar 2025 11:27:46 GMT',
   'content-type': 'application/json',
   'content-length': '1660',
   'connection': 'keep-alive',
   'x-amzn-requestid': 'ea0ac2ad-3ad5-4a49-9647-274a0c73ac31'},
  'RetryAttempts': 0},
 'stopReason': 'end_turn',
 'metrics': {'latencyMs': [11044]}}


MistralAI
​


from
 langchain_mistralai 
import
 ChatMistralAI
llm 
=
 ChatMistralAI
(
model
=
"mistral-small-latest"
)
msg 
=
 llm
.
invoke
(
[
(
"human"
,
 
"What's the oldest known example of cuneiform"
)
]
)
msg
.
response_metadata
API Reference:
ChatMistralAI


{'token_usage': {'prompt_tokens': 13,
  'total_tokens': 219,
  'completion_tokens': 206},
 'model_name': 'mistral-small-latest',
 'model': 'mistral-small-latest',
 'finish_reason': 'stop'}


Groq
​


from
 langchain_groq 
import
 ChatGroq
llm 
=
 ChatGroq
(
model
=
"llama-3.1-8b-instant"
)
msg 
=
 llm
.
invoke
(
"What's the oldest known example of cuneiform"
)
msg
.
response_metadata
API Reference:
ChatGroq


{'token_usage': {'completion_tokens': 184,
  'prompt_tokens': 45,
  'total_tokens': 229,
  'completion_time': 0.245333333,
  'prompt_time': 0.002262803,
  'queue_time': 0.19315161,
  'total_time': 0.247596136},
 'model_name': 'llama-3.1-8b-instant',
 'system_fingerprint': 'fp_a56f6eea01',
 'finish_reason': 'stop',
 'logprobs': None}


FireworksAI
​


from
 langchain_fireworks 
import
 ChatFireworks
llm 
=
 ChatFireworks
(
model
=
"accounts/fireworks/models/llama-v3p1-70b-instruct"
)
msg 
=
 llm
.
invoke
(
"What's the oldest known example of cuneiform"
)
msg
.
response_metadata
API Reference:
ChatFireworks


{'token_usage': {'prompt_tokens': 25,
  'total_tokens': 352,
  'completion_tokens': 327},
 'model_name': 'accounts/fireworks/models/llama-v3p1-70b-instruct',
 'system_fingerprint': '',
 'finish_reason': 'stop',
 'logprobs': None}
Edit this page
Was this page helpful?
Previous
How to recursively split text by characters
Next
How to pass runtime secrets to runnables
OpenAI
Anthropic
Google VertexAI
Bedrock (Anthropic)
MistralAI
Groq
FireworksAI


--- docs_how_to_routing.txt ---

How-to guides
How to route between sub-chains
On this page
How to route between sub-chains


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Expression Language (LCEL)


Chaining runnables


Configuring chain parameters at runtime


Prompt templates


Chat Messages




Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing can help provide structure and consistency around interactions with models by allowing you to define states and use information related to those states as context to model calls.


There are two ways to perform routing:




Conditionally return runnables from a 
RunnableLambda
 (recommended)


Using a 
RunnableBranch
 (legacy)




We'll illustrate both methods using a two step sequence where the first step classifies an input question as being about 
LangChain
, 
Anthropic
, or 
Other
, then routes to a corresponding prompt chain.


Example Setup
​


First, let's create a chain that will identify incoming questions as being about 
LangChain
, 
Anthropic
, or 
Other
:


from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 PromptTemplate
chain 
=
 
(
    PromptTemplate
.
from_template
(
        
"""Given the user question below, classify it as either being about `LangChain`, `Anthropic`, or `Other`.
Do not respond with more than one word.
<question>
{question}
</question>
Classification:"""
    
)
    
|
 ChatAnthropic
(
model_name
=
"claude-3-haiku-20240307"
)
    
|
 StrOutputParser
(
)
)
chain
.
invoke
(
{
"question"
:
 
"how do I call Anthropic?"
}
)
API Reference:
ChatAnthropic
 | 
StrOutputParser
 | 
PromptTemplate


'Anthropic'


Now, let's create three sub chains:


langchain_chain 
=
 PromptTemplate
.
from_template
(
    
"""You are an expert in langchain. \
Always answer questions starting with "As Harrison Chase told me". \
Respond to the following question:
Question: {question}
Answer:"""
)
 
|
 ChatAnthropic
(
model_name
=
"claude-3-haiku-20240307"
)
anthropic_chain 
=
 PromptTemplate
.
from_template
(
    
"""You are an expert in anthropic. \
Always answer questions starting with "As Dario Amodei told me". \
Respond to the following question:
Question: {question}
Answer:"""
)
 
|
 ChatAnthropic
(
model_name
=
"claude-3-haiku-20240307"
)
general_chain 
=
 PromptTemplate
.
from_template
(
    
"""Respond to the following question:
Question: {question}
Answer:"""
)
 
|
 ChatAnthropic
(
model_name
=
"claude-3-haiku-20240307"
)


Using a custom function (Recommended)
​


You can also use a custom function to route between different outputs. Here's an example:


def
 
route
(
info
)
:
    
if
 
"anthropic"
 
in
 info
[
"topic"
]
.
lower
(
)
:
        
return
 anthropic_chain
    
elif
 
"langchain"
 
in
 info
[
"topic"
]
.
lower
(
)
:
        
return
 langchain_chain
    
else
:
        
return
 general_chain


from
 langchain_core
.
runnables 
import
 RunnableLambda
full_chain 
=
 
{
"topic"
:
 chain
,
 
"question"
:
 
lambda
 x
:
 x
[
"question"
]
}
 
|
 RunnableLambda
(
    route
)
API Reference:
RunnableLambda


full_chain
.
invoke
(
{
"question"
:
 
"how do I use Anthropic?"
}
)


AIMessage(content="As Dario Amodei told me, to use Anthropic, you can start by exploring the company's website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \n\nDepending on your specific needs, you can look into Anthropic's AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\n\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic's approach to AI development.\n\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I'd encourage you to reach out to them directly to discuss how they can best support your specific requirements.", response_metadata={'id': 'msg_01CtLFgFSwvTaJomrihE87Ra', 'content': [ContentBlock(text="As Dario Amodei told me, to use Anthropic, you can start by exploring the company's website and learning about their mission, values, and the different services and products they offer. Anthropic is focused on developing safe and ethical AI systems, so they have a strong emphasis on transparency and responsible AI development. \n\nDepending on your specific needs, you can look into Anthropic's AI research and development services, which cover areas like natural language processing, computer vision, and reinforcement learning. They also offer consulting and advisory services to help organizations navigate the challenges and opportunities of AI integration.\n\nAdditionally, Anthropic has released some open-source AI models and tools that you can explore and experiment with. These can be a great way to get hands-on experience with Anthropic's approach to AI development.\n\nOverall, Anthropic aims to be a reliable and trustworthy partner in the AI space, so I'd encourage you to reach out to them directly to discuss how they can best support your specific requirements.", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=53, output_tokens=219)})


full_chain
.
invoke
(
{
"question"
:
 
"how do I use LangChain?"
}
)


AIMessage(content="As Harrison Chase told me, using LangChain involves a few key steps:\n\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\n\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\n\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\n\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\n\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your chosen language model to your LangChain application.\n\n6. **Implement your application logic**: Use LangChain's building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\n\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\n\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.", response_metadata={'id': 'msg_01H3UXAAHG4TwxJLpxwuuVU7', 'content': [ContentBlock(text="As Harrison Chase told me, using LangChain involves a few key steps:\n\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\n\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\n\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\n\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\n\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your chosen language model to your LangChain application.\n\n6. **Implement your application logic**: Use LangChain's building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\n\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\n\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=50, output_tokens=400)})


full_chain
.
invoke
(
{
"question"
:
 
"whats 2 + 2"
}
)


AIMessage(content='4', response_metadata={'id': 'msg_01UAKP81jTZu9fyiyFYhsbHc', 'content': [ContentBlock(text='4', type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=28, output_tokens=5)})


Using a RunnableBranch
​


A 
RunnableBranch
 is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does 
not
 offer anything that you can't achieve in a custom function as described above, so we recommend using a custom function instead.


A 
RunnableBranch
 is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.


If no provided conditions match, it runs the default runnable.


Here's an example of what it looks like in action:


from
 langchain_core
.
runnables 
import
 RunnableBranch
branch 
=
 RunnableBranch
(
    
(
lambda
 x
:
 
"anthropic"
 
in
 x
[
"topic"
]
.
lower
(
)
,
 anthropic_chain
)
,
    
(
lambda
 x
:
 
"langchain"
 
in
 x
[
"topic"
]
.
lower
(
)
,
 langchain_chain
)
,
    general_chain
,
)
full_chain 
=
 
{
"topic"
:
 chain
,
 
"question"
:
 
lambda
 x
:
 x
[
"question"
]
}
 
|
 branch
full_chain
.
invoke
(
{
"question"
:
 
"how do I use Anthropic?"
}
)
API Reference:
RunnableBranch


AIMessage(content="As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \n\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic's technology and services can support your specific needs.\n\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We're here to collaborate and help you harness the power of advanced AI responsibly.", response_metadata={'id': 'msg_0187BVnpniPDJnVvwf3M1LdY', 'content': [ContentBlock(text="As Dario Amodei told me, to use Anthropic, you should first familiarize yourself with our mission and principles. Anthropic is committed to developing safe and beneficial artificial intelligence that can help solve important problems facing humanity. \n\nTo get started, I recommend exploring the resources on our website, which cover our research, products, and approach to AI development. You can also reach out to our team to learn more about how Anthropic's technology and services can support your specific needs.\n\nThe key is to engage with us in a way that aligns with our values of transparency, ethical AI, and a commitment to the wellbeing of humanity. We're here to collaborate and help you harness the power of advanced AI responsibly.", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=53, output_tokens=160)})


full_chain
.
invoke
(
{
"question"
:
 
"how do I use LangChain?"
}
)


AIMessage(content="As Harrison Chase told me, using LangChain involves several key steps. First, you'll need to install the LangChain library and import the necessary modules. Then, you'll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \n\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \n\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I'd recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I'm always happy to share more insights from my conversations with Harrison.", response_metadata={'id': 'msg_01T1naS99wGPkEAP4LME8iAv', 'content': [ContentBlock(text="As Harrison Chase told me, using LangChain involves several key steps. First, you'll need to install the LangChain library and import the necessary modules. Then, you'll want to define your language model, any data sources you plan to use, and the specific tasks you want to accomplish, such as question answering, text generation, or agent-based reasoning. \n\nLangChain provides a flexible framework for building applications that leverage large language models. It includes abstractions for things like retrievers, prompts, and chains, which allow you to compose different components together to create powerful workflows. \n\nThe documentation on the LangChain website is excellent and covers many common use cases in detail. I'd recommend starting there to get a solid understanding of the core concepts and how to apply them to your specific needs. And of course, feel free to reach out if you have any other questions - I'm always happy to share more insights from my conversations with Harrison.", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=50, output_tokens=205)})


full_chain
.
invoke
(
{
"question"
:
 
"whats 2 + 2"
}
)


AIMessage(content='4', response_metadata={'id': 'msg_01T6T3TS6hRCtU8JayN93QEi', 'content': [ContentBlock(text='4', type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=28, output_tokens=5)})


Routing by semantic similarity
​


One especially useful technique is to use embeddings to route a query to the most relevant prompt. Here's an example.


from
 langchain_community
.
utils
.
math 
import
 cosine_similarity
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_core
.
runnables 
import
 RunnableLambda
,
 RunnablePassthrough
from
 langchain_openai 
import
 OpenAIEmbeddings
physics_template 
=
 
"""You are a very smart physics professor. \
You are great at answering questions about physics in a concise and easy to understand manner. \
When you don't know the answer to a question you admit that you don't know.
Here is a question:
{query}"""
math_template 
=
 
"""You are a very good mathematician. You are great at answering math questions. \
You are so good because you are able to break down hard problems into their component parts, \
answer the component parts, and then put them together to answer the broader question.
Here is a question:
{query}"""
embeddings 
=
 OpenAIEmbeddings
(
)
prompt_templates 
=
 
[
physics_template
,
 math_template
]
prompt_embeddings 
=
 embeddings
.
embed_documents
(
prompt_templates
)
def
 
prompt_router
(
input
)
:
    query_embedding 
=
 embeddings
.
embed_query
(
input
[
"query"
]
)
    similarity 
=
 cosine_similarity
(
[
query_embedding
]
,
 prompt_embeddings
)
[
0
]
    most_similar 
=
 prompt_templates
[
similarity
.
argmax
(
)
]
    
print
(
"Using MATH"
 
if
 most_similar 
==
 math_template 
else
 
"Using PHYSICS"
)
    
return
 PromptTemplate
.
from_template
(
most_similar
)
chain 
=
 
(
    
{
"query"
:
 RunnablePassthrough
(
)
}
    
|
 RunnableLambda
(
prompt_router
)
    
|
 ChatAnthropic
(
model
=
"claude-3-haiku-20240307"
)
    
|
 StrOutputParser
(
)
)
API Reference:
cosine_similarity
 | 
StrOutputParser
 | 
PromptTemplate
 | 
RunnableLambda
 | 
RunnablePassthrough
 | 
OpenAIEmbeddings


print
(
chain
.
invoke
(
"What's a black hole"
)
)


Using PHYSICS
As a physics professor, I would be happy to provide a concise and easy-to-understand explanation of what a black hole is.
A black hole is an incredibly dense region of space-time where the gravitational pull is so strong that nothing, not even light, can escape from it. This means that if you were to get too close to a black hole, you would be pulled in and crushed by the intense gravitational forces.
The formation of a black hole occurs when a massive star, much larger than our Sun, reaches the end of its life and collapses in on itself. This collapse causes the matter to become extremely dense, and the gravitational force becomes so strong that it creates a point of no return, known as the event horizon.
Beyond the event horizon, the laws of physics as we know them break down, and the intense gravitational forces create a singularity, which is a point of infinite density and curvature in space-time.
Black holes are fascinating and mysterious objects, and there is still much to be learned about their properties and behavior. If I were unsure about any specific details or aspects of black holes, I would readily admit that I do not have a complete understanding and would encourage further research and investigation.


print
(
chain
.
invoke
(
"What's a path integral"
)
)


Using MATH
A path integral is a powerful mathematical concept in physics, particularly in the field of quantum mechanics. It was developed by the renowned physicist Richard Feynman as an alternative formulation of quantum mechanics.
In a path integral, instead of considering a single, definite path that a particle might take from one point to another, as in classical mechanics, the particle is considered to take all possible paths simultaneously. Each path is assigned a complex-valued weight, and the total probability amplitude for the particle to go from one point to another is calculated by summing (integrating) over all possible paths.
The key ideas behind the path integral formulation are:
1. Superposition principle: In quantum mechanics, particles can exist in a superposition of multiple states or paths simultaneously.
2. Probability amplitude: The probability amplitude for a particle to go from one point to another is calculated by summing the complex-valued weights of all possible paths.
3. Weighting of paths: Each path is assigned a weight based on the action (the time integral of the Lagrangian) along that path. Paths with lower action have a greater weight.
4. Feynman's approach: Feynman developed the path integral formulation as an alternative to the traditional wave function approach in quantum mechanics, providing a more intuitive and conceptual understanding of quantum phenomena.
The path integral approach is particularly useful in quantum field theory, where it provides a powerful framework for calculating transition probabilities and understanding the behavior of quantum systems. It has also found applications in various areas of physics, such as condensed matter, statistical mechanics, and even in finance (the path integral approach to option pricing).
The mathematical construction of the path integral involves the use of advanced concepts from functional analysis and measure theory, making it a powerful and sophisticated tool in the physicist's arsenal.


Next steps
​


You've now learned how to add routing to your composed LCEL chains.


Next, check out the other how-to guides on runnables in this section.
Edit this page
Was this page helpful?
Previous
How to handle cases where no queries are generated
Next
How to return structured data from a model
Example Setup
Using a custom function (Recommended)
Using a RunnableBranch
Routing by semantic similarity
Next steps


--- docs_how_to_runnable_runtime_secrets.txt ---

How-to guides
How to pass runtime secrets to runnables
How to pass runtime secrets to runnables


Requires 
langchain-core >= 0.2.22


We can pass in secrets to our 
runnables
 at runtime using the 
RunnableConfig
. Specifically we can pass in secrets with a 
__
 prefix to the 
configurable
 field. This will ensure that these secrets aren't traced as part of the invocation:


from
 langchain_core
.
runnables 
import
 RunnableConfig
from
 langchain_core
.
tools 
import
 tool
@tool
def
 
foo
(
x
:
 
int
,
 config
:
 RunnableConfig
)
 
-
>
 
int
:
    
"""Sum x and a secret int"""
    
return
 x 
+
 config
[
"configurable"
]
[
"__top_secret_int"
]
foo
.
invoke
(
{
"x"
:
 
5
}
,
 
{
"configurable"
:
 
{
"__top_secret_int"
:
 
2
,
 
"traced_key"
:
 
"bar"
}
}
)
API Reference:
RunnableConfig
 | 
tool


7


Looking at the LangSmith trace for this run, we can see that "traced_key" was recorded (as part of Metadata) while our secret int was not: 
https://smith.langchain.com/public/aa7e3289-49ca-422d-a408-f6b927210170/r
Edit this page
Was this page helpful?
Previous
Response metadata
Next
How to do "self-querying" retrieval


--- docs_how_to_self_query.txt ---

How-to guides
How to do "self-querying" retrieval
On this page
How to do "self-querying" retrieval


info
Head to 
Integrations
 for documentation on vector stores with built-in support for self-querying.


A self-querying 
retriever
 is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying 
vector store
. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.




Get started
​


For demonstration purposes we'll use a 
Chroma
 vector store. We've created a small demo set of documents that contain summaries of movies.


Note:
 The self-query retriever requires you to have 
lark
 package installed.


%
pip install 
-
-
upgrade 
-
-
quiet  lark langchain
-
chroma


from
 langchain_chroma 
import
 Chroma
from
 langchain_core
.
documents 
import
 Document
from
 langchain_openai 
import
 OpenAIEmbeddings
docs 
=
 
[
    Document
(
        page_content
=
"A bunch of scientists bring back dinosaurs and mayhem breaks loose"
,
        metadata
=
{
"year"
:
 
1993
,
 
"rating"
:
 
7.7
,
 
"genre"
:
 
"science fiction"
}
,
    
)
,
    Document
(
        page_content
=
"Leo DiCaprio gets lost in a dream within a dream within a dream within a ..."
,
        metadata
=
{
"year"
:
 
2010
,
 
"director"
:
 
"Christopher Nolan"
,
 
"rating"
:
 
8.2
}
,
    
)
,
    Document
(
        page_content
=
"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea"
,
        metadata
=
{
"year"
:
 
2006
,
 
"director"
:
 
"Satoshi Kon"
,
 
"rating"
:
 
8.6
}
,
    
)
,
    Document
(
        page_content
=
"A bunch of normal-sized women are supremely wholesome and some men pine after them"
,
        metadata
=
{
"year"
:
 
2019
,
 
"director"
:
 
"Greta Gerwig"
,
 
"rating"
:
 
8.3
}
,
    
)
,
    Document
(
        page_content
=
"Toys come alive and have a blast doing so"
,
        metadata
=
{
"year"
:
 
1995
,
 
"genre"
:
 
"animated"
}
,
    
)
,
    Document
(
        page_content
=
"Three men walk into the Zone, three men walk out of the Zone"
,
        metadata
=
{
            
"year"
:
 
1979
,
            
"director"
:
 
"Andrei Tarkovsky"
,
            
"genre"
:
 
"thriller"
,
            
"rating"
:
 
9.9
,
        
}
,
    
)
,
]
vectorstore 
=
 Chroma
.
from_documents
(
docs
,
 OpenAIEmbeddings
(
)
)
API Reference:
Document
 | 
OpenAIEmbeddings


Creating our self-querying retriever
​


Now we can instantiate our retriever. To do this we'll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.


from
 langchain
.
chains
.
query_constructor
.
schema 
import
 AttributeInfo
from
 langchain
.
retrievers
.
self_query
.
base 
import
 SelfQueryRetriever
from
 langchain_openai 
import
 ChatOpenAI
metadata_field_info 
=
 
[
    AttributeInfo
(
        name
=
"genre"
,
        description
=
"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']"
,
        
type
=
"string"
,
    
)
,
    AttributeInfo
(
        name
=
"year"
,
        description
=
"The year the movie was released"
,
        
type
=
"integer"
,
    
)
,
    AttributeInfo
(
        name
=
"director"
,
        description
=
"The name of the movie director"
,
        
type
=
"string"
,
    
)
,
    AttributeInfo
(
        name
=
"rating"
,
 description
=
"A 1-10 rating for the movie"
,
 
type
=
"float"
    
)
,
]
document_content_description 
=
 
"Brief summary of a movie"
llm 
=
 ChatOpenAI
(
temperature
=
0
)
retriever 
=
 SelfQueryRetriever
.
from_llm
(
    llm
,
    vectorstore
,
    document_content_description
,
    metadata_field_info
,
)
API Reference:
AttributeInfo
 | 
SelfQueryRetriever
 | 
ChatOpenAI


Testing it out
​


And now we can actually try using our retriever!


# This example only specifies a filter
retriever
.
invoke
(
"I want to watch a movie rated higher than 8.5"
)


[Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979}),
 Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006})]


# This example specifies a query and a filter
retriever
.
invoke
(
"Has Greta Gerwig directed any movies about women"
)


[Document(page_content='A bunch of normal-sized women are supremely wholesome and some men pine after them', metadata={'director': 'Greta Gerwig', 'rating': 8.3, 'year': 2019})]


# This example specifies a composite filter
retriever
.
invoke
(
"What's a highly rated (above 8.5) science fiction film?"
)


[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}),
 Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979})]


# This example specifies a query and composite filter
retriever
.
invoke
(
    
"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated"
)


[Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]


Filter k
​


We can also use the self query retriever to specify 
k
: the number of documents to fetch.


We can do this by passing 
enable_limit=True
 to the constructor.


retriever 
=
 SelfQueryRetriever
.
from_llm
(
    llm
,
    vectorstore
,
    document_content_description
,
    metadata_field_info
,
    enable_limit
=
True
,
)
# This example only specifies a relevant query
retriever
.
invoke
(
"What are two movies about dinosaurs"
)


[Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'genre': 'science fiction', 'rating': 7.7, 'year': 1993}),
 Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]


Constructing from scratch with LCEL
​


To see what's going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.


First, we need to create a query-construction chain. This chain will take a user query and generated a 
StructuredQuery
 object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we'll ignore here for simplicity.


from
 langchain
.
chains
.
query_constructor
.
base 
import
 
(
    StructuredQueryOutputParser
,
    get_query_constructor_prompt
,
)
prompt 
=
 get_query_constructor_prompt
(
    document_content_description
,
    metadata_field_info
,
)
output_parser 
=
 StructuredQueryOutputParser
.
from_components
(
)
query_constructor 
=
 prompt 
|
 llm 
|
 output_parser
API Reference:
StructuredQueryOutputParser
 | 
get_query_constructor_prompt


Let's look at our prompt:


print
(
prompt
.
format
(
query
=
"dummy question"
)
)


Your goal is to structure the user's query to match the request schema provided below.
<< Structured Request Schema >>
When responding use a markdown code snippet with a JSON object formatted in the following schema:
\`\`\`json
{
    "query": string \ text string to compare to document contents
    "filter": string \ logical condition statement for filtering documents
}
\`\`\`
The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.
A logical condition statement is composed of one or more comparison and logical operation statements.
A comparison statement takes the form: `comp(attr, val)`:
- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator
- `attr` (string):  name of attribute to apply the comparison to
- `val` (string): is the comparison value
A logical operation statement takes the form `op(statement1, statement2, ...)`:
- `op` (and | or | not): logical operator
- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to
Make sure that you only use the comparators and logical operators listed above and no others.
Make sure that filters only refer to attributes that exist in the data source.
Make sure that filters only use the attributed names with its function names if there are functions applied on them.
Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.
Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.
Make sure that filters are only used as needed. If there are no filters that should be applied return "NO_FILTER" for the filter value.
<< Example 1. >>
Data Source:
\`\`\`json
{
    "content": "Lyrics of a song",
    "attributes": {
        "artist": {
            "type": "string",
            "description": "Name of the song artist"
        },
        "length": {
            "type": "integer",
            "description": "Length of the song in seconds"
        },
        "genre": {
            "type": "string",
            "description": "The song genre, one of "pop", "rock" or "rap""
        }
    }
}
\`\`\`
User Query:
What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre
Structured Request:
\`\`\`json
{
    "query": "teenager love",
    "filter": "and(or(eq(\"artist\", \"Taylor Swift\"), eq(\"artist\", \"Katy Perry\")), lt(\"length\", 180), eq(\"genre\", \"pop\"))"
}
\`\`\`
<< Example 2. >>
Data Source:
\`\`\`json
{
    "content": "Lyrics of a song",
    "attributes": {
        "artist": {
            "type": "string",
            "description": "Name of the song artist"
        },
        "length": {
            "type": "integer",
            "description": "Length of the song in seconds"
        },
        "genre": {
            "type": "string",
            "description": "The song genre, one of "pop", "rock" or "rap""
        }
    }
}
\`\`\`
User Query:
What are songs that were not published on Spotify
Structured Request:
\`\`\`json
{
    "query": "",
    "filter": "NO_FILTER"
}
\`\`\`
<< Example 3. >>
Data Source:
\`\`\`json
{
    "content": "Brief summary of a movie",
    "attributes": {
    "genre": {
        "description": "The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']",
        "type": "string"
    },
    "year": {
        "description": "The year the movie was released",
        "type": "integer"
    },
    "director": {
        "description": "The name of the movie director",
        "type": "string"
    },
    "rating": {
        "description": "A 1-10 rating for the movie",
        "type": "float"
    }
}
}
\`\`\`
User Query:
dummy question
Structured Request:


And what our full chain produces:


query_constructor
.
invoke
(
    
{
        
"query"
:
 
"What are some sci-fi movies from the 90's directed by Luc Besson about taxi drivers"
    
}
)


StructuredQuery(query='taxi driver', filter=Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='genre', value='science fiction'), Operation(operator=<Operator.AND: 'and'>, arguments=[Comparison(comparator=<Comparator.GTE: 'gte'>, attribute='year', value=1990), Comparison(comparator=<Comparator.LT: 'lt'>, attribute='year', value=2000)]), Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='director', value='Luc Besson')]), limit=None)


The query constructor is the key element of the self-query retriever. To make a great retrieval system you'll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, 
check out this cookbook
.


The next key element is the structured query translator. This is the object responsible for translating the generic 
StructuredQuery
 object into a metadata filter in the syntax of the vector store you're using. LangChain comes with a number of built-in translators. To see them all head to the 
Integrations section
.


from
 langchain_community
.
query_constructors
.
chroma 
import
 ChromaTranslator
retriever 
=
 SelfQueryRetriever
(
    query_constructor
=
query_constructor
,
    vectorstore
=
vectorstore
,
    structured_query_translator
=
ChromaTranslator
(
)
,
)
API Reference:
ChromaTranslator


retriever
.
invoke
(
    
"What's a movie after 1990 but before 2005 that's all about toys, and preferably is animated"
)


[Document(page_content='Toys come alive and have a blast doing so', metadata={'genre': 'animated', 'year': 1995})]
Edit this page
Was this page helpful?
Previous
How to pass runtime secrets to runnables
Next
How to split text based on semantic similarity
Get started
Creating our self-querying retriever
Testing it out
Filter k
Constructing from scratch with LCEL


--- docs_how_to_semantic-chunker.txt ---

How-to guides
How to split text based on semantic similarity
On this page
How to split text based on semantic similarity


Taken from Greg Kamradt's wonderful notebook:

5_Levels_Of_Text_Splitting


All credit to him.


This guide covers how to split chunks based on their semantic similarity. If embeddings are sufficiently far apart, chunks are split.


At a high level, this splits into sentences, then groups into groups of 3
sentences, and then merges one that are similar in the embedding space.


Install Dependencies
​


!pip install 
-
-
quiet langchain_experimental langchain_openai


Load Example Data
​


# This is a long document we can split up.
with
 
open
(
"state_of_the_union.txt"
)
 
as
 f
:
    state_of_the_union 
=
 f
.
read
(
)


Create Text Splitter
​


To instantiate a 
SemanticChunker
, we must specify an embedding model. Below we will use 
OpenAIEmbeddings
.


from
 langchain_experimental
.
text_splitter 
import
 SemanticChunker
from
 langchain_openai
.
embeddings 
import
 OpenAIEmbeddings
text_splitter 
=
 SemanticChunker
(
OpenAIEmbeddings
(
)
)
API Reference:
SemanticChunker
 | 
OpenAIEmbeddings


Split Text
​


We split text in the usual way, e.g., by invoking 
.create_documents
 to create LangChain 
Document
 objects:


docs 
=
 text_splitter
.
create_documents
(
[
state_of_the_union
]
)
print
(
docs
[
0
]
.
page_content
)


Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.


Breakpoints
​


This chunker works by determining when to "break" apart sentences. This is done by looking for differences in embeddings between any two sentences. When that difference is past some threshold, then they are split.


There are a few ways to determine what that threshold is, which are controlled by the 
breakpoint_threshold_type
 kwarg.


Note: if the resulting chunk sizes are too small/big, the additional kwargs 
breakpoint_threshold_amount
 and 
min_chunk_size
 can be used for adjustments.


Percentile
​


The default way to split is based on percentile. In this method, all differences between sentences are calculated, and then any difference greater than the X percentile is split. The default value for X is 95.0 and can be adjusted by the keyword argument 
breakpoint_threshold_amount
 which expects a number between 0.0 and 100.0.


text_splitter 
=
 SemanticChunker
(
    OpenAIEmbeddings
(
)
,
 breakpoint_threshold_type
=
"percentile"
)


docs 
=
 text_splitter
.
create_documents
(
[
state_of_the_union
]
)
print
(
docs
[
0
]
.
page_content
)


Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.


print
(
len
(
docs
)
)


26


Standard Deviation
​


In this method, any difference greater than X standard deviations is split. The default value for X is 3.0 and can be adjusted by the keyword argument 
breakpoint_threshold_amount
.


text_splitter 
=
 SemanticChunker
(
    OpenAIEmbeddings
(
)
,
 breakpoint_threshold_type
=
"standard_deviation"
)


docs 
=
 text_splitter
.
create_documents
(
[
state_of_the_union
]
)
print
(
docs
[
0
]
.
page_content
)


Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving. And the costs and the threats to America and the world keep rising. That’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. The United States is a member along with 29 other nations. It matters. American diplomacy matters. American resolve matters. Putin’s latest attack on Ukraine was premeditated and unprovoked. He rejected repeated efforts at diplomacy. He thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready. Here is what we did. We prepared extensively and carefully. We spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. I spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression. We countered Russia’s lies with truth. And now that he has acted the free world is holding him accountable. Along with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. We are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. Together with our allies –we are right now enforcing powerful economic sanctions. We are cutting off Russia’s largest banks from the international financial system. Preventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless. We are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come. Tonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. The U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs. We are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. And tonight I am announcing that we will join our allies in closing off American air space to all Russian flights – further isolating Russia – and adding an additional squeeze –on their economy. The Ruble has lost 30% of its value. The Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. Together with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. We are giving more than $1 Billion in direct assistance to Ukraine. And we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering. Let me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine. Our forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies – in the event that Putin decides to keep moving west. For that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. As I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power. And we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them. Putin has unleashed violence and chaos. But while he may make gains on the battlefield – he will pay a continuing high price over the long run. And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards. To all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. And I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. Tonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world. America will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies. These steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming.


print
(
len
(
docs
)
)


4


Interquartile
​


In this method, the interquartile distance is used to split chunks. The interquartile range can be scaled by the keyword argument 
breakpoint_threshold_amount
, the default value is 1.5.


text_splitter 
=
 SemanticChunker
(
    OpenAIEmbeddings
(
)
,
 breakpoint_threshold_type
=
"interquartile"
)


docs 
=
 text_splitter
.
create_documents
(
[
state_of_the_union
]
)
print
(
docs
[
0
]
.
page_content
)


Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans. Last year COVID-19 kept us apart. This year we are finally together again. Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. With a duty to one another to the American people to the Constitution. And with an unwavering resolve that freedom will always triumph over tyranny. Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. He thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. He met the Ukrainian people. From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. Groups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. In this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight. Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. Throughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos. They keep moving.


print
(
len
(
docs
)
)


25


Gradient
​


In this method, the gradient of distance is used to split chunks along with the percentile method. This method is useful when chunks are highly correlated with each other or specific to a domain e.g. legal or medical. The idea is to apply anomaly detection on gradient array so that the distribution become wider and easy to identify boundaries in highly semantic data.
Similar to the percentile method, the split can be adjusted by the keyword argument 
breakpoint_threshold_amount
 which expects a number between 0.0 and 100.0, the default value is 95.0.


text_splitter 
=
 SemanticChunker
(
    OpenAIEmbeddings
(
)
,
 breakpoint_threshold_type
=
"gradient"
)


docs 
=
 text_splitter
.
create_documents
(
[
state_of_the_union
]
)
print
(
docs
[
0
]
.
page_content
)


Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.


print
(
len
(
docs
)
)


26
Edit this page
Was this page helpful?
Previous
How to do "self-querying" retrieval
Next
How to chain runnables
Install Dependencies
Load Example Data
Create Text Splitter
Split Text
Breakpoints
Percentile
Standard Deviation
Interquartile
Gradient


--- docs_how_to_sequence.txt ---

How-to guides
How to chain runnables
On this page
How to chain runnables


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Expression Language (LCEL)


Prompt templates


Chat models


Output parser




One point about 
LangChain Expression Language
 is that any two runnables can be "chained" together into sequences. The output of the previous runnable's 
.invoke()
 call is passed as input to the next runnable. This can be done using the pipe operator (
|
), or the more explicit 
.pipe()
 method, which does the same thing.


The resulting 
RunnableSequence
 is itself a runnable, which means it can be invoked, streamed, or further chained just like any other runnable. Advantages of chaining runnables in this way are efficient streaming (the sequence will stream output as soon as it is available), and debugging and tracing with tools like 
LangSmith
.


The pipe operator: 
|
​


To show off how this works, let's go through an example. We'll walk through a common pattern in LangChain: using a 
prompt template
 to format input into a 
chat model
, and finally converting the chat message output into a string with an 
output parser
.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
prompt 
=
 ChatPromptTemplate
.
from_template
(
"tell me a joke about {topic}"
)
chain 
=
 prompt 
|
 model 
|
 StrOutputParser
(
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate


Prompts and models are both runnable, and the output type from the prompt call is the same as the input type of the chat model, so we can chain them together. We can then invoke the resulting sequence like any other runnable:


chain
.
invoke
(
{
"topic"
:
 
"bears"
}
)


"Here's a bear joke for you:\n\nWhy did the bear dissolve in water?\nBecause it was a polar bear!"


Coercion
​


We can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.


For example, let's say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.


We would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a 
RunnableParallel
, which runs all of its values in parallel and returns a dict with the results.


This happens to be the same format the next prompt template expects. Here it is in action:


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
analysis_prompt 
=
 ChatPromptTemplate
.
from_template
(
"is this a funny joke? {joke}"
)
composed_chain 
=
 
{
"joke"
:
 chain
}
 
|
 analysis_prompt 
|
 model 
|
 StrOutputParser
(
)
composed_chain
.
invoke
(
{
"topic"
:
 
"bears"
}
)
API Reference:
StrOutputParser


'Haha, that\'s a clever play on words! Using "polar" to imply the bear dissolved or became polar/polarized when put in water. Not the most hilarious joke ever, but it has a cute, groan-worthy pun that makes it mildly amusing. I appreciate a good pun or wordplay joke.'


Functions will also be coerced into runnables, so you can add custom logic to your chains too. The below chain results in the same logical flow as before:


composed_chain_with_lambda 
=
 
(
    chain
    
|
 
(
lambda
 
input
:
 
{
"joke"
:
 
input
}
)
    
|
 analysis_prompt
    
|
 model
    
|
 StrOutputParser
(
)
)
composed_chain_with_lambda
.
invoke
(
{
"topic"
:
 
"beets"
}
)


"Haha, that's a cute and punny joke! I like how it plays on the idea of beets blushing or turning red like someone blushing. Food puns can be quite amusing. While not a total knee-slapper, it's a light-hearted, groan-worthy dad joke that would make me chuckle and shake my head. Simple vegetable humor!"


However, keep in mind that using functions like this may interfere with operations like streaming. See 
this section
 for more information.


The 
.pipe()
 method
​


We could also compose the same sequence using the 
.pipe()
 method. Here's what that looks like:


from
 langchain_core
.
runnables 
import
 RunnableParallel
composed_chain_with_pipe 
=
 
(
    RunnableParallel
(
{
"joke"
:
 chain
}
)
    
.
pipe
(
analysis_prompt
)
    
.
pipe
(
model
)
    
.
pipe
(
StrOutputParser
(
)
)
)
composed_chain_with_pipe
.
invoke
(
{
"topic"
:
 
"battlestar galactica"
}
)
API Reference:
RunnableParallel


"I cannot reproduce any copyrighted material verbatim, but I can try to analyze the humor in the joke you provided without quoting it directly.\n\nThe joke plays on the idea that the Cylon raiders, who are the antagonists in the Battlestar Galactica universe, failed to locate the human survivors after attacking their home planets (the Twelve Colonies) due to using an outdated and poorly performing operating system (Windows Vista) for their targeting systems.\n\nThe humor stems from the juxtaposition of a futuristic science fiction setting with a relatable real-world frustration – the use of buggy, slow, or unreliable software or technology. It pokes fun at the perceived inadequacies of Windows Vista, which was widely criticized for its performance issues and other problems when it was released.\n\nBy attributing the Cylons' failure to locate the humans to their use of Vista, the joke creates an amusing and unexpected connection between a fictional advanced race of robots and a familiar technological annoyance experienced by many people in the real world.\n\nOverall, the joke relies on incongruity and relatability to generate humor, but without reproducing any copyrighted material directly."


Or the abbreviated:


composed_chain_with_pipe 
=
 RunnableParallel
(
{
"joke"
:
 chain
}
)
.
pipe
(
    analysis_prompt
,
 model
,
 StrOutputParser
(
)
)


Related
​




Streaming
: Check out the streaming guide to understand the streaming behavior of a chain


Edit this page
Was this page helpful?
Previous
How to split text based on semantic similarity
Next
How to save and load LangChain objects
The pipe operator: 
|
Coercion
The 
.pipe()
 method
Related


--- docs_how_to_serialization.txt ---

How-to guides
How to save and load LangChain objects
On this page
How to save and load LangChain objects


LangChain classes implement standard methods for serialization. Serializing LangChain objects using these methods confer some advantages:




Secrets, such as API keys, are separated from other parameters and can be loaded back to the object on de-serialization;


De-serialization is kept compatible across package versions, so objects that were serialized with one version of LangChain can be properly de-serialized with another.




To save and load LangChain objects using this system, use the 
dumpd
, 
dumps
, 
load
, and 
loads
 functions in the 
load module
 of 
langchain-core
. These functions support JSON and JSON-serializable objects.


All LangChain objects that inherit from 
Serializable
 are JSON-serializable. Examples include 
messages
, 
document objects
 (e.g., as returned from 
retrievers
), and most 
Runnables
, such as chat models, retrievers, and 
chains
 implemented with the LangChain Expression Language.


Below we walk through an example with a simple 
LLM chain
.


caution
De-serialization using 
load
 and 
loads
 can instantiate any serializable LangChain object. Only use this feature with trusted inputs!
De-serialization is a beta feature and is subject to change.


from
 langchain_core
.
load 
import
 dumpd
,
 dumps
,
 load
,
 loads
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"Translate the following into {language}:"
)
,
        
(
"user"
,
 
"{text}"
)
,
    
]
,
)
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 api_key
=
"llm-api-key"
)
chain 
=
 prompt 
|
 llm
API Reference:
dumpd
 | 
dumps
 | 
load
 | 
loads
 | 
ChatPromptTemplate
 | 
ChatOpenAI


Saving objects
​


To json
​


string_representation 
=
 dumps
(
chain
,
 pretty
=
True
)
print
(
string_representation
[
:
500
]
)


{
  "lc": 1,
  "type": "constructor",
  "id": [
    "langchain",
    "schema",
    "runnable",
    "RunnableSequence"
  ],
  "kwargs": {
    "first": {
      "lc": 1,
      "type": "constructor",
      "id": [
        "langchain",
        "prompts",
        "chat",
        "ChatPromptTemplate"
      ],
      "kwargs": {
        "input_variables": [
          "language",
          "text"
        ],
        "messages": [
          {
            "lc": 1,
            "type": "constructor",


To a json-serializable Python dict
​


dict_representation 
=
 dumpd
(
chain
)
print
(
type
(
dict_representation
)
)


<class 'dict'>


To disk
​


import
 json
with
 
open
(
"/tmp/chain.json"
,
 
"w"
)
 
as
 fp
:
    json
.
dump
(
string_representation
,
 fp
)


Note that the API key is withheld from the serialized representations. Parameters that are considered secret are specified by the 
.lc_secrets
 attribute of the LangChain object:


chain
.
last
.
lc_secrets


{'openai_api_key': 'OPENAI_API_KEY'}


Loading objects
​


Specifying 
secrets_map
 in 
load
 and 
loads
 will load the corresponding secrets onto the de-serialized LangChain object.


From string
​


chain 
=
 loads
(
string_representation
,
 secrets_map
=
{
"OPENAI_API_KEY"
:
 
"llm-api-key"
}
)


From dict
​


chain 
=
 load
(
dict_representation
,
 secrets_map
=
{
"OPENAI_API_KEY"
:
 
"llm-api-key"
}
)


From disk
​


with
 
open
(
"/tmp/chain.json"
,
 
"r"
)
 
as
 fp
:
    chain 
=
 loads
(
json
.
load
(
fp
)
,
 secrets_map
=
{
"OPENAI_API_KEY"
:
 
"llm-api-key"
}
)


Note that we recover the API key specified at the start of the guide:


chain
.
last
.
openai_api_key
.
get_secret_value
(
)


'llm-api-key'
Edit this page
Was this page helpful?
Previous
How to chain runnables
Next
How to split text by tokens
Saving objects
To json
To a json-serializable Python dict
To disk
Loading objects
From string
From dict
From disk


--- docs_how_to_split_by_token.txt ---

How-to guides
How to split text by tokens
On this page
How to split text by tokens


Language models have a 
token
 limit. You should not exceed the token limit. When you 
split your text
 into chunks it is therefore a good idea to count the number of tokens. There are many tokenizers. When you count tokens in your text you should use the same tokenizer as used in the language model.


tiktoken
​


note
tiktoken
 is a fast 
BPE
 tokenizer created by 
OpenAI
.


We can use 
tiktoken
 to estimate tokens used. It will probably be more accurate for the OpenAI models.




How the text is split: by character passed in.


How the chunk size is measured: by 
tiktoken
 tokenizer.




CharacterTextSplitter
, 
RecursiveCharacterTextSplitter
, and 
TokenTextSplitter
 can be used with 
tiktoken
 directly.


%
pip install 
-
-
upgrade 
-
-
quiet langchain
-
text
-
splitters tiktoken


from
 langchain_text_splitters 
import
 CharacterTextSplitter
# This is a long document we can split up.
with
 
open
(
"state_of_the_union.txt"
)
 
as
 f
:
    state_of_the_union 
=
 f
.
read
(
)
API Reference:
CharacterTextSplitter


To split with a 
CharacterTextSplitter
 and then merge chunks with 
tiktoken
, use its 
.from_tiktoken_encoder()
 method. Note that splits from this method can be larger than the chunk size measured by the 
tiktoken
 tokenizer.


The 
.from_tiktoken_encoder()
 method takes either 
encoding_name
 as an argument (e.g. 
cl100k_base
), or the 
model_name
 (e.g. 
gpt-4
). All additional arguments like 
chunk_size
, 
chunk_overlap
, and 
separators
 are used to instantiate 
CharacterTextSplitter
:


text_splitter 
=
 CharacterTextSplitter
.
from_tiktoken_encoder
(
    encoding_name
=
"cl100k_base"
,
 chunk_size
=
100
,
 chunk_overlap
=
0
)
texts 
=
 text_splitter
.
split_text
(
state_of_the_union
)


print
(
texts
[
0
]
)


Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  
Last year COVID-19 kept us apart. This year we are finally together again. 
Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. 
With a duty to one another to the American people to the Constitution.


To implement a hard constraint on the chunk size, we can use 
RecursiveCharacterTextSplitter.from_tiktoken_encoder
, where each split will be recursively split if it has a larger size:


from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
text_splitter 
=
 RecursiveCharacterTextSplitter
.
from_tiktoken_encoder
(
    model_name
=
"gpt-4"
,
    chunk_size
=
100
,
    chunk_overlap
=
0
,
)
API Reference:
RecursiveCharacterTextSplitter


We can also load a 
TokenTextSplitter
 splitter, which works with 
tiktoken
 directly and will ensure each split is smaller than chunk size.


from
 langchain_text_splitters 
import
 TokenTextSplitter
text_splitter 
=
 TokenTextSplitter
(
chunk_size
=
10
,
 chunk_overlap
=
0
)
texts 
=
 text_splitter
.
split_text
(
state_of_the_union
)
print
(
texts
[
0
]
)
API Reference:
TokenTextSplitter


Madam Speaker, Madam Vice President, our


Some written languages (e.g. Chinese and Japanese) have characters which encode to 2 or more tokens. Using the 
TokenTextSplitter
 directly can split the tokens for a character between two chunks causing malformed Unicode characters. Use 
RecursiveCharacterTextSplitter.from_tiktoken_encoder
 or 
CharacterTextSplitter.from_tiktoken_encoder
 to ensure chunks contain valid Unicode strings.


spaCy
​


note
spaCy
 is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython.


LangChain implements splitters based on the 
spaCy tokenizer
.




How the text is split: by 
spaCy
 tokenizer.


How the chunk size is measured: by number of characters.




%
pip install 
-
-
upgrade 
-
-
quiet  spacy


# This is a long document we can split up.
with
 
open
(
"state_of_the_union.txt"
)
 
as
 f
:
    state_of_the_union 
=
 f
.
read
(
)


from
 langchain_text_splitters 
import
 SpacyTextSplitter
text_splitter 
=
 SpacyTextSplitter
(
chunk_size
=
1000
)
texts 
=
 text_splitter
.
split_text
(
state_of_the_union
)
print
(
texts
[
0
]
)
API Reference:
SpacyTextSplitter


Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.
Members of Congress and the Cabinet.
Justices of the Supreme Court.
My fellow Americans.  
Last year COVID-19 kept us apart.
This year we are finally together again. 
Tonight, we meet as Democrats Republicans and Independents.
But most importantly as Americans. 
With a duty to one another to the American people to the Constitution. 
And with an unwavering resolve that freedom will always triumph over tyranny. 
Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.
But he badly miscalculated. 
He thought he could roll into Ukraine and the world would roll over.
Instead he met a wall of strength he never imagined. 
He met the Ukrainian people. 
From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.


SentenceTransformers
​


The 
SentenceTransformersTokenTextSplitter
 is a specialized text splitter for use with the sentence-transformer models. The default behaviour is to split the text into chunks that fit the token window of the sentence transformer model that you would like to use.


To split text and constrain token counts according to the sentence-transformers tokenizer, instantiate a 
SentenceTransformersTokenTextSplitter
. You can optionally specify:




chunk_overlap
: integer count of token overlap;


model_name
: sentence-transformer model name, defaulting to 
"sentence-transformers/all-mpnet-base-v2"
;


tokens_per_chunk
: desired token count per chunk.




from
 langchain_text_splitters 
import
 SentenceTransformersTokenTextSplitter
splitter 
=
 SentenceTransformersTokenTextSplitter
(
chunk_overlap
=
0
)
text 
=
 
"Lorem "
count_start_and_stop_tokens 
=
 
2
text_token_count 
=
 splitter
.
count_tokens
(
text
=
text
)
 
-
 count_start_and_stop_tokens
print
(
text_token_count
)
API Reference:
SentenceTransformersTokenTextSplitter


2


token_multiplier 
=
 splitter
.
maximum_tokens_per_chunk 
//
 text_token_count 
+
 
1
# `text_to_split` does not fit in a single chunk
text_to_split 
=
 text 
*
 token_multiplier
print
(
f"tokens in text to split: 
{
splitter
.
count_tokens
(
text
=
text_to_split
)
}
"
)


tokens in text to split: 514


text_chunks 
=
 splitter
.
split_text
(
text
=
text_to_split
)
print
(
text_chunks
[
1
]
)


lorem


NLTK
​


note
The Natural Language Toolkit
, or more commonly 
NLTK
, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language.


Rather than just splitting on "\n\n", we can use 
NLTK
 to split based on 
NLTK tokenizers
.




How the text is split: by 
NLTK
 tokenizer.


How the chunk size is measured: by number of characters.




# pip install nltk


# This is a long document we can split up.
with
 
open
(
"state_of_the_union.txt"
)
 
as
 f
:
    state_of_the_union 
=
 f
.
read
(
)


from
 langchain_text_splitters 
import
 NLTKTextSplitter
text_splitter 
=
 NLTKTextSplitter
(
chunk_size
=
1000
)
API Reference:
NLTKTextSplitter


texts 
=
 text_splitter
.
split_text
(
state_of_the_union
)
print
(
texts
[
0
]
)


Madam Speaker, Madam Vice President, our First Lady and Second Gentleman.
Members of Congress and the Cabinet.
Justices of the Supreme Court.
My fellow Americans.
Last year COVID-19 kept us apart.
This year we are finally together again.
Tonight, we meet as Democrats Republicans and Independents.
But most importantly as Americans.
With a duty to one another to the American people to the Constitution.
And with an unwavering resolve that freedom will always triumph over tyranny.
Six days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways.
But he badly miscalculated.
He thought he could roll into Ukraine and the world would roll over.
Instead he met a wall of strength he never imagined.
He met the Ukrainian people.
From President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.
Groups of citizens blocking tanks with their bodies.


KoNLPY
​


note
KoNLPy: Korean NLP in Python
 is is a Python package for natural language processing (NLP) of the Korean language.


Token splitting involves the segmentation of text into smaller, more manageable units called tokens. These tokens are often words, phrases, symbols, or other meaningful elements crucial for further processing and analysis. In languages like English, token splitting typically involves separating words by spaces and punctuation marks. The effectiveness of token splitting largely depends on the tokenizer's understanding of the language structure, ensuring the generation of meaningful tokens. Since tokenizers designed for the English language are not equipped to understand the unique semantic structures of other languages, such as Korean, they cannot be effectively used for Korean language processing.


Token splitting for Korean with KoNLPy's Kkma Analyzer
​


In case of Korean text, KoNLPY includes at morphological analyzer called 
Kkma
 (Korean Knowledge Morpheme Analyzer). 
Kkma
 provides detailed morphological analysis of Korean text. It breaks down sentences into words and words into their respective morphemes, identifying parts of speech for each token. It can segment a block of text into individual sentences, which is particularly useful for processing long texts.


Usage Considerations
​


While 
Kkma
 is renowned for its detailed analysis, it is important to note that this precision may impact processing speed. Thus, 
Kkma
 is best suited for applications where analytical depth is prioritized over rapid text processing.


# pip install konlpy


# This is a long Korean document that we want to split up into its component sentences.
with
 
open
(
"./your_korean_doc.txt"
)
 
as
 f
:
    korean_document 
=
 f
.
read
(
)


from
 langchain_text_splitters 
import
 KonlpyTextSplitter
text_splitter 
=
 KonlpyTextSplitter
(
)
API Reference:
KonlpyTextSplitter


texts 
=
 text_splitter
.
split_text
(
korean_document
)
# The sentences are split with "\n\n" characters.
print
(
texts
[
0
]
)


춘향전 옛날에 남원에 이 도령이라는 벼슬아치 아들이 있었다.
그의 외모는 빛나는 달처럼 잘생겼고, 그의 학식과 기예는 남보다 뛰어났다.
한편, 이 마을에는 춘향이라는 절세 가인이 살고 있었다.
춘 향의 아름다움은 꽃과 같아 마을 사람들 로부터 많은 사랑을 받았다.
어느 봄날, 도령은 친구들과 놀러 나갔다가 춘 향을 만 나 첫 눈에 반하고 말았다.
두 사람은 서로 사랑하게 되었고, 이내 비밀스러운 사랑의 맹세를 나누었다.
하지만 좋은 날들은 오래가지 않았다.
도령의 아버지가 다른 곳으로 전근을 가게 되어 도령도 떠나 야만 했다.
이별의 아픔 속에서도, 두 사람은 재회를 기약하며 서로를 믿고 기다리기로 했다.
그러나 새로 부임한 관아의 사또가 춘 향의 아름다움에 욕심을 내 어 그녀에게 강요를 시작했다.
춘 향 은 도령에 대한 자신의 사랑을 지키기 위해, 사또의 요구를 단호히 거절했다.
이에 분노한 사또는 춘 향을 감옥에 가두고 혹독한 형벌을 내렸다.
이야기는 이 도령이 고위 관직에 오른 후, 춘 향을 구해 내는 것으로 끝난다.
두 사람은 오랜 시련 끝에 다시 만나게 되고, 그들의 사랑은 온 세상에 전해 지며 후세에까지 이어진다.
- 춘향전 (The Tale of Chunhyang)


Hugging Face tokenizer
​


Hugging Face
 has many tokenizers.


We use Hugging Face tokenizer, the 
GPT2TokenizerFast
 to count the text length in tokens.




How the text is split: by character passed in.


How the chunk size is measured: by number of tokens calculated by the 
Hugging Face
 tokenizer.




from
 transformers 
import
 GPT2TokenizerFast
tokenizer 
=
 GPT2TokenizerFast
.
from_pretrained
(
"gpt2"
)


# This is a long document we can split up.
with
 
open
(
"state_of_the_union.txt"
)
 
as
 f
:
    state_of_the_union 
=
 f
.
read
(
)
from
 langchain_text_splitters 
import
 CharacterTextSplitter
API Reference:
CharacterTextSplitter


text_splitter 
=
 CharacterTextSplitter
.
from_huggingface_tokenizer
(
    tokenizer
,
 chunk_size
=
100
,
 chunk_overlap
=
0
)
texts 
=
 text_splitter
.
split_text
(
state_of_the_union
)


print
(
texts
[
0
]
)


Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  
Last year COVID-19 kept us apart. This year we are finally together again. 
Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. 
With a duty to one another to the American people to the Constitution.
Edit this page
Was this page helpful?
Previous
How to save and load LangChain objects
Next
How to split HTML
tiktoken
spaCy
SentenceTransformers
NLTK
KoNLPY
Token splitting for Korean with KoNLPy's Kkma Analyzer
Usage Considerations
Hugging Face tokenizer


--- docs_how_to_split_html.txt ---

How-to guides
How to split HTML
On this page
How to split HTML


Splitting HTML documents into manageable chunks is essential for various text processing tasks such as natural language processing, search indexing, and more. In this guide, we will explore three different text splitters provided by LangChain that you can use to split HTML content effectively:




HTMLHeaderTextSplitter


HTMLSectionSplitter


HTMLSemanticPreservingSplitter




Each of these splitters has unique features and use cases. This guide will help you understand the differences between them, why you might choose one over the others, and how to use them effectively.


%pip install -qU langchain-text-splitters


Overview of the Splitters
​


HTMLHeaderTextSplitter
​


info
Useful when you want to preserve the hierarchical structure of a document based on its headings.


Description
: Splits HTML text based on header tags (e.g., 
<h1>
, 
<h2>
, 
<h3>
, etc.), and adds metadata for each header relevant to any given chunk.


Capabilities
:




Splits text at the HTML element level.


Preserves context-rich information encoded in document structures.


Can return chunks element by element or combine elements with the same metadata.






HTMLSectionSplitter
​


info
Useful when you want to split HTML documents into larger sections, such as 
<section>
, 
<div>
, or custom-defined sections.


Description
: Similar to HTMLHeaderTextSplitter but focuses on splitting HTML into sections based on specified tags.


Capabilities
:




Uses XSLT transformations to detect and split sections.


Internally uses 
RecursiveCharacterTextSplitter
 for large sections.


Considers font sizes to determine sections.






HTMLSemanticPreservingSplitter
​


info
Ideal when you need to ensure that structured elements are not split across chunks, preserving contextual relevancy.


Description
: Splits HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components.


Capabilities
:




Preserves tables, lists, and other specified HTML elements.


Allows custom handlers for specific HTML tags.


Ensures that the semantic meaning of the document is maintained.


Built in normalization & stopword removal






Choosing the Right Splitter
​




Use 
HTMLHeaderTextSplitter
 when
: You need to split an HTML document based on its header hierarchy and maintain metadata about the headers.


Use 
HTMLSectionSplitter
 when
: You need to split the document into larger, more general sections, possibly based on custom tags or font sizes.


Use 
HTMLSemanticPreservingSplitter
 when
: You need to split the document into chunks while preserving semantic elements like tables and lists, ensuring that they are not split and that their context is maintained.




Feature
HTMLHeaderTextSplitter
HTMLSectionSplitter
HTMLSemanticPreservingSplitter
Splits based on headers
Yes
Yes
Yes
Preserves semantic elements (tables, lists)
No
No
Yes
Adds metadata for headers
Yes
Yes
Yes
Custom handlers for HTML tags
No
No
Yes
Preserves media (images, videos)
No
No
Yes
Considers font sizes
No
Yes
No
Uses XSLT transformations
No
Yes
No


Example HTML Document
​


Let's use the following HTML document as an example:


html_string 
=
 
"""
<!DOCTYPE html>
  <html lang='en'>
  <head>
    <meta charset='UTF-8'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0'>
    <title>Fancy Example HTML Page</title>
  </head>
  <body>
    <h1>Main Title</h1>
    <p>This is an introductory paragraph with some basic content.</p>
    
    <h2>Section 1: Introduction</h2>
    <p>This section introduces the topic. Below is a list:</p>
    <ul>
      <li>First item</li>
      <li>Second item</li>
      <li>Third item with <strong>bold text</strong> and <a href='#'>a link</a></li>
    </ul>
    
    <h3>Subsection 1.1: Details</h3>
    <p>This subsection provides additional details. Here's a table:</p>
    <table border='1'>
      <thead>
        <tr>
          <th>Header 1</th>
          <th>Header 2</th>
          <th>Header 3</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Row 1, Cell 1</td>
          <td>Row 1, Cell 2</td>
          <td>Row 1, Cell 3</td>
        </tr>
        <tr>
          <td>Row 2, Cell 1</td>
          <td>Row 2, Cell 2</td>
          <td>Row 2, Cell 3</td>
        </tr>
      </tbody>
    </table>
    
    <h2>Section 2: Media Content</h2>
    <p>This section contains an image and a video:</p>
      <img src='example_image_link.mp4' alt='Example Image'>
      <video controls width='250' src='example_video_link.mp4' type='video/mp4'>
      Your browser does not support the video tag.
    </video>
    <h2>Section 3: Code Example</h2>
    <p>This section contains a code block:</p>
    <pre><code data-lang="html">
    &lt;div&gt;
      &lt;p&gt;This is a paragraph inside a div.&lt;/p&gt;
    &lt;/div&gt;
    </code></pre>
    <h2>Conclusion</h2>
    <p>This is the conclusion of the document.</p>
  </body>
  </html>
"""


Using HTMLHeaderTextSplitter
​


HTMLHeaderTextSplitter
 is a "structure-aware" 
text splitter
 that splits text at the HTML element level and adds metadata for each header "relevant" to any given chunk. It can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures. It can be used with other text splitters as part of a chunking pipeline.


It is analogous to the 
MarkdownHeaderTextSplitter
 for markdown files.


To specify what headers to split on, specify 
headers_to_split_on
 when instantiating 
HTMLHeaderTextSplitter
 as shown below.


from
 langchain_text_splitters 
import
 HTMLHeaderTextSplitter
headers_to_split_on 
=
 
[
    
(
"h1"
,
 
"Header 1"
)
,
    
(
"h2"
,
 
"Header 2"
)
,
    
(
"h3"
,
 
"Header 3"
)
,
]
html_splitter 
=
 HTMLHeaderTextSplitter
(
headers_to_split_on
)
html_header_splits 
=
 html_splitter
.
split_text
(
html_string
)
html_header_splits
API Reference:
HTMLHeaderTextSplitter


[Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some basic content.'),
 Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic. Below is a list:  \nFirst item Second item Third item with bold text and a link'),
 Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction', 'Header 3': 'Subsection 1.1: Details'}, page_content="This subsection provides additional details. Here's a table:"),
 Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video:'),
 Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block:'),
 Document(metadata={'Header 1': 'Main Title', 'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]


To return each element together with their associated headers, specify 
return_each_element=True
 when instantiating 
HTMLHeaderTextSplitter
:


html_splitter 
=
 HTMLHeaderTextSplitter
(
    headers_to_split_on
,
    return_each_element
=
True
,
)
html_header_splits_elements 
=
 html_splitter
.
split_text
(
html_string
)


Comparing with the above, where elements are aggregated by their headers:


for
 element 
in
 html_header_splits
[
:
2
]
:
    
print
(
element
)


page_content='This is an introductory paragraph with some basic content.' metadata={'Header 1': 'Main Title'}
page_content='This section introduces the topic. Below is a list:  
First item Second item Third item with bold text and a link' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}


Now each element is returned as a distinct 
Document
:


for
 element 
in
 html_header_splits_elements
[
:
3
]
:
    
print
(
element
)


page_content='This is an introductory paragraph with some basic content.' metadata={'Header 1': 'Main Title'}
page_content='This section introduces the topic. Below is a list:' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}
page_content='First item Second item Third item with bold text and a link' metadata={'Header 1': 'Main Title', 'Header 2': 'Section 1: Introduction'}


How to split from a URL or HTML file:
​


To read directly from a URL, pass the URL string into the 
split_text_from_url
 method.


Similarly, a local HTML file can be passed to the 
split_text_from_file
 method.


url 
=
 
"https://plato.stanford.edu/entries/goedel/"
headers_to_split_on 
=
 
[
    
(
"h1"
,
 
"Header 1"
)
,
    
(
"h2"
,
 
"Header 2"
)
,
    
(
"h3"
,
 
"Header 3"
)
,
    
(
"h4"
,
 
"Header 4"
)
,
]
html_splitter 
=
 HTMLHeaderTextSplitter
(
headers_to_split_on
)
# for local file use html_splitter.split_text_from_file(<path_to_file>)
html_header_splits 
=
 html_splitter
.
split_text_from_url
(
url
)


How to constrain chunk sizes:
​


HTMLHeaderTextSplitter
, which splits based on HTML headers, can be composed with another splitter which constrains splits based on character lengths, such as 
RecursiveCharacterTextSplitter
.


This can be done using the 
.split_documents
 method of the second splitter:


from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
chunk_size 
=
 
500
chunk_overlap 
=
 
30
text_splitter 
=
 RecursiveCharacterTextSplitter
(
    chunk_size
=
chunk_size
,
 chunk_overlap
=
chunk_overlap
)
# Split
splits 
=
 text_splitter
.
split_documents
(
html_header_splits
)
splits
[
80
:
85
]
API Reference:
RecursiveCharacterTextSplitter


[Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='We see that Gödel first tried to reduce the consistency problem for analysis to that of arithmetic. This seemed to require a truth definition for arithmetic, which in turn led to paradoxes, such as the Liar paradox (“This sentence is false”) and Berry’s paradox (“The least number not defined by an expression consisting of just fourteen English words”). Gödel then noticed that such paradoxes would not necessarily arise if truth were replaced by provability. But this means that arithmetic truth'),
 Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='means that arithmetic truth and arithmetic provability are not co-extensive — whence the First Incompleteness Theorem.'),
 Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='This account of Gödel’s discovery was told to Hao Wang very much after the fact; but in Gödel’s contemporary correspondence with Bernays and Zermelo, essentially the same description of his path to the theorems is given. (See Gödel 2003a and Gödel 2003b respectively.) From those accounts we see that the undefinability of truth in arithmetic, a result credited to Tarski, was likely obtained in some form by Gödel by 1931. But he neither publicized nor published the result; the biases logicians'),
 Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.1 The First Incompleteness Theorem'}, page_content='result; the biases logicians had expressed at the time concerning the notion of truth, biases which came vehemently to the fore when Tarski announced his results on the undefinability of truth in formal systems 1935, may have served as a deterrent to Gödel’s publication of that theorem.'),
 Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': '2. Gödel’s Mathematical Work', 'Header 3': '2.2 The Incompleteness Theorems', 'Header 4': '2.2.2 The proof of the First Incompleteness Theorem'}, page_content='We now describe the proof of the two theorems, formulating Gödel’s results in Peano arithmetic. Gödel himself used a system related to that defined in Principia Mathematica, but containing Peano arithmetic. In our presentation of the First and Second Incompleteness Theorems we refer to Peano arithmetic as P, following Gödel’s notation.')]


Limitations
​


There can be quite a bit of structural variation from one HTML document to another, and while 
HTMLHeaderTextSplitter
 will attempt to attach all "relevant" headers to any given chunk, it can sometimes miss certain headers. For example, the algorithm assumes an informational hierarchy in which headers are always at nodes "above" associated text, i.e. prior siblings, ancestors, and combinations thereof. In the following news article (as of the writing of this document), the document is structured such that the text of the top-level headline, while tagged "h1", is in a 
distinct
 subtree from the text elements that we'd expect it to be 
"above"
—so we can observe that the "h1" element and its associated text do not show up in the chunk metadata (but, where applicable, we do see "h2" and its associated text):


url 
=
 
"https://www.cnn.com/2023/09/25/weather/el-nino-winter-us-climate/index.html"
headers_to_split_on 
=
 
[
    
(
"h1"
,
 
"Header 1"
)
,
    
(
"h2"
,
 
"Header 2"
)
,
]
html_splitter 
=
 HTMLHeaderTextSplitter
(
headers_to_split_on
)
html_header_splits 
=
 html_splitter
.
split_text_from_url
(
url
)
print
(
html_header_splits
[
1
]
.
page_content
[
:
500
]
)


No two El Niño winters are the same, but many have temperature and precipitation trends in common.  
Average conditions during an El Niño winter across the continental US.  
One of the major reasons is the position of the jet stream, which often shifts south during an El Niño winter. This shift typically brings wetter and cooler weather to the South while the North becomes drier and warmer, according to NOAA.  
Because the jet stream is essentially a river of air that storms flow through, they c


Using HTMLSectionSplitter
​


Similar in concept to the 
HTMLHeaderTextSplitter
, the 
HTMLSectionSplitter
 is a "structure-aware" 
text splitter
 that splits text at the element level and adds metadata for each header "relevant" to any given chunk. It lets you split HTML by sections.


It can return chunks element by element or combine elements with the same metadata, with the objectives of (a) keeping related text grouped (more or less) semantically and (b) preserving context-rich information encoded in document structures.


Use 
xslt_path
 to provide an absolute path to transform the HTML so that it can detect sections based on provided tags. The default is to use the 
converting_to_header.xslt
 file in the 
data_connection/document_transformers
 directory. This is for converting the html to a format/layout that is easier to detect sections. For example, 
span
 based on their font size can be converted to header tags to be detected as a section.


How to split HTML strings:
​


from
 langchain_text_splitters 
import
 HTMLSectionSplitter
headers_to_split_on 
=
 
[
    
(
"h1"
,
 
"Header 1"
)
,
    
(
"h2"
,
 
"Header 2"
)
,
]
html_splitter 
=
 HTMLSectionSplitter
(
headers_to_split_on
)
html_header_splits 
=
 html_splitter
.
split_text
(
html_string
)
html_header_splits
API Reference:
HTMLSectionSplitter


[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title \n This is an introductory paragraph with some basic content.'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content="Section 1: Introduction \n This section introduces the topic. Below is a list: \n \n First item \n Second item \n Third item with  bold text  and  a link \n \n \n Subsection 1.1: Details \n This subsection provides additional details. Here's a table: \n \n \n \n Header 1 \n Header 2 \n Header 3 \n \n \n \n \n Row 1, Cell 1 \n Row 1, Cell 2 \n Row 1, Cell 3 \n \n \n Row 2, Cell 1 \n Row 2, Cell 2 \n Row 2, Cell 3"),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Section 2: Media Content \n This section contains an image and a video: \n \n \n      Your browser does not support the video tag.'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='Section 3: Code Example \n This section contains a code block: \n \n    <div>\n      <p>This is a paragraph inside a div.</p>\n    </div>'),
 Document(metadata={'Header 2': 'Conclusion'}, page_content='Conclusion \n This is the conclusion of the document.')]


How to constrain chunk sizes:
​


HTMLSectionSplitter
 can be used with other text splitters as part of a chunking pipeline. Internally, it uses the 
RecursiveCharacterTextSplitter
 when the section size is larger than the chunk size. It also considers the font size of the text to determine whether it is a section or not based on the determined font size threshold.


from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
headers_to_split_on 
=
 
[
    
(
"h1"
,
 
"Header 1"
)
,
    
(
"h2"
,
 
"Header 2"
)
,
    
(
"h3"
,
 
"Header 3"
)
,
]
html_splitter 
=
 HTMLSectionSplitter
(
headers_to_split_on
)
html_header_splits 
=
 html_splitter
.
split_text
(
html_string
)
chunk_size 
=
 
50
chunk_overlap 
=
 
5
text_splitter 
=
 RecursiveCharacterTextSplitter
(
    chunk_size
=
chunk_size
,
 chunk_overlap
=
chunk_overlap
)
# Split
splits 
=
 text_splitter
.
split_documents
(
html_header_splits
)
splits
API Reference:
RecursiveCharacterTextSplitter


[Document(metadata={'Header 1': 'Main Title'}, page_content='Main Title'),
 Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some'),
 Document(metadata={'Header 1': 'Main Title'}, page_content='some basic content.'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='Section 1: Introduction'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic. Below is a'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='is a list:'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='First item \n Second item'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='Third item with  bold text  and  a link'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Subsection 1.1: Details'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='This subsection provides additional details.'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content="Here's a table:"),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Header 1 \n Header 2 \n Header 3'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 1, Cell 1 \n Row 1, Cell 2'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 1, Cell 3 \n \n \n Row 2, Cell 1'),
 Document(metadata={'Header 3': 'Subsection 1.1: Details'}, page_content='Row 2, Cell 2 \n Row 2, Cell 3'),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Section 2: Media Content'),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video:'),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='Your browser does not support the video'),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='tag.'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='Section 3: Code Example'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block: \n \n    <div>'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='<p>This is a paragraph inside a div.</p>'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='</div>'),
 Document(metadata={'Header 2': 'Conclusion'}, page_content='Conclusion'),
 Document(metadata={'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]


Using HTMLSemanticPreservingSplitter
​


The 
HTMLSemanticPreservingSplitter
 is designed to split HTML content into manageable chunks while preserving the semantic structure of important elements like tables, lists, and other HTML components. This ensures that such elements are not split across chunks, causing loss of contextual relevancy such as table headers, list headers etc.


This splitter is designed at its heart, to create contextually relevant chunks. General Recursive splitting with 
HTMLHeaderTextSplitter
 can cause tables, lists and other structured elements to be split in the middle, losing significant context and creating bad chunks.


The 
HTMLSemanticPreservingSplitter
 is essential for splitting HTML content that includes structured elements like tables and lists, especially when it's critical to preserve these elements intact. Additionally, its ability to define custom handlers for specific HTML tags makes it a versatile tool for processing complex HTML documents.


IMPORTANT
: 
max_chunk_size
 is not a definite maximum size of a chunk, the calculation of max size, occurs when the preserved content is not apart of the chunk, to ensure it is not split. When we add the preserved data back in to the chunk, there is a chance the chunk size will exceed the 
max_chunk_size
. This is crucial to ensure we maintain the structure of the original document


info
Notes:


We have defined a custom handler to re-format the contents of code blocks


We defined a deny list for specific html elements, to decompose them and their contents pre-processing


We have intentionally set a small chunk size to demonstrate the non-splitting of elements




# BeautifulSoup is required to use the custom handlers
from
 bs4 
import
 Tag
from
 langchain_text_splitters 
import
 HTMLSemanticPreservingSplitter
headers_to_split_on 
=
 
[
    
(
"h1"
,
 
"Header 1"
)
,
    
(
"h2"
,
 
"Header 2"
)
,
]
def
 
code_handler
(
element
:
 Tag
)
 
-
>
 
str
:
    data_lang 
=
 element
.
get
(
"data-lang"
)
    code_format 
=
 
f"<code:
{
data_lang
}
>
{
element
.
get_text
(
)
}
</code>"
    
return
 code_format
splitter 
=
 HTMLSemanticPreservingSplitter
(
    headers_to_split_on
=
headers_to_split_on
,
    separators
=
[
"\n\n"
,
 
"\n"
,
 
". "
,
 
"! "
,
 
"? "
]
,
    max_chunk_size
=
50
,
    preserve_images
=
True
,
    preserve_videos
=
True
,
    elements_to_preserve
=
[
"table"
,
 
"ul"
,
 
"ol"
,
 
"code"
]
,
    denylist_tags
=
[
"script"
,
 
"style"
,
 
"head"
]
,
    custom_handlers
=
{
"code"
:
 code_handler
}
,
)
documents 
=
 splitter
.
split_text
(
html_string
)
documents
API Reference:
HTMLSemanticPreservingSplitter


[Document(metadata={'Header 1': 'Main Title'}, page_content='This is an introductory paragraph with some basic content.'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='This section introduces the topic'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content='. Below is a list: First item Second item Third item with bold text and a link Subsection 1.1: Details This subsection provides additional details'),
 Document(metadata={'Header 2': 'Section 1: Introduction'}, page_content=". Here's a table: Header 1 Header 2 Header 3 Row 1, Cell 1 Row 1, Cell 2 Row 1, Cell 3 Row 2, Cell 1 Row 2, Cell 2 Row 2, Cell 3"),
 Document(metadata={'Header 2': 'Section 2: Media Content'}, page_content='This section contains an image and a video: ![image:example_image_link.mp4](example_image_link.mp4) ![video:example_video_link.mp4](example_video_link.mp4)'),
 Document(metadata={'Header 2': 'Section 3: Code Example'}, page_content='This section contains a code block: <code:html> <div> <p>This is a paragraph inside a div.</p> </div> </code>'),
 Document(metadata={'Header 2': 'Conclusion'}, page_content='This is the conclusion of the document.')]


Preserving Tables and Lists
​


In this example, we will demonstrate how the 
HTMLSemanticPreservingSplitter
 can preserve a table and a large list within an HTML document. The chunk size will be set to 50 characters to illustrate how the splitter ensures that these elements are not split, even when they exceed the maximum defined chunk size.


from
 langchain_text_splitters 
import
 HTMLSemanticPreservingSplitter
html_string 
=
 
"""
<!DOCTYPE html>
<html>
    <body>
        <div>
            <h1>Section 1</h1>
            <p>This section contains an important table and list that should not be split across chunks.</p>
            <table>
                <tr>
                    <th>Item</th>
                    <th>Quantity</th>
                    <th>Price</th>
                </tr>
                <tr>
                    <td>Apples</td>
                    <td>10</td>
                    <td>$1.00</td>
                </tr>
                <tr>
                    <td>Oranges</td>
                    <td>5</td>
                    <td>$0.50</td>
                </tr>
                <tr>
                    <td>Bananas</td>
                    <td>50</td>
                    <td>$1.50</td>
                </tr>
            </table>
            <h2>Subsection 1.1</h2>
            <p>Additional text in subsection 1.1 that is separated from the table and list.</p>
            <p>Here is a detailed list:</p>
            <ul>
                <li>Item 1: Description of item 1, which is quite detailed and important.</li>
                <li>Item 2: Description of item 2, which also contains significant information.</li>
                <li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>
            </ul>
        </div>
    </body>
</html>
"""
headers_to_split_on 
=
 
[
(
"h1"
,
 
"Header 1"
)
,
 
(
"h2"
,
 
"Header 2"
)
]
splitter 
=
 HTMLSemanticPreservingSplitter
(
    headers_to_split_on
=
headers_to_split_on
,
    max_chunk_size
=
50
,
    elements_to_preserve
=
[
"table"
,
 
"ul"
]
,
)
documents 
=
 splitter
.
split_text
(
html_string
)
print
(
documents
)
API Reference:
HTMLSemanticPreservingSplitter


[Document(metadata={'Header 1': 'Section 1'}, page_content='This section contains an important table and list'), Document(metadata={'Header 1': 'Section 1'}, page_content='that should not be split across chunks.'), Document(metadata={'Header 1': 'Section 1'}, page_content='Item Quantity Price Apples 10 $1.00 Oranges 5 $0.50 Bananas 50 $1.50'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content='Additional text in subsection 1.1 that is'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content='separated from the table and list. Here is a'), Document(metadata={'Header 2': 'Subsection 1.1'}, page_content="detailed list: Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.")]


Explanation
​


In this example, the 
HTMLSemanticPreservingSplitter
 ensures that the entire table and the unordered list (
<ul>
) are preserved within their respective chunks. Even though the chunk size is set to 50 characters, the splitter recognizes that these elements should not be split and keeps them intact.


This is particularly important when dealing with data tables or lists, where splitting the content could lead to loss of context or confusion. The resulting 
Document
 objects retain the full structure of these elements, ensuring that the contextual relevance of the information is maintained.


Using a Custom Handler
​


The 
HTMLSemanticPreservingSplitter
 allows you to define custom handlers for specific HTML elements. Some platforms, have custom HTML tags that are not natively parsed by 
BeautifulSoup
, when this occurs, you can utilize custom handlers to add the formatting logic easily.


This can be particularly useful for elements that require special processing, such as 
<iframe>
 tags or specific 'data-' elements. In this example, we'll create a custom handler for 
iframe
 tags that converts them into Markdown-like links.


def
 
custom_iframe_extractor
(
iframe_tag
)
:
    iframe_src 
=
 iframe_tag
.
get
(
"src"
,
 
""
)
    
return
 
f"[iframe:
{
iframe_src
}
](
{
iframe_src
}
)"
splitter 
=
 HTMLSemanticPreservingSplitter
(
    headers_to_split_on
=
headers_to_split_on
,
    max_chunk_size
=
50
,
    separators
=
[
"\n\n"
,
 
"\n"
,
 
". "
]
,
    elements_to_preserve
=
[
"table"
,
 
"ul"
,
 
"ol"
]
,
    custom_handlers
=
{
"iframe"
:
 custom_iframe_extractor
}
,
)
html_string 
=
 
"""
<!DOCTYPE html>
<html>
    <body>
        <div>
            <h1>Section with Iframe</h1>
            <iframe src="https://example.com/embed"></iframe>
            <p>Some text after the iframe.</p>
            <ul>
                <li>Item 1: Description of item 1, which is quite detailed and important.</li>
                <li>Item 2: Description of item 2, which also contains significant information.</li>
                <li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>
            </ul>
        </div>
    </body>
</html>
"""
documents 
=
 splitter
.
split_text
(
html_string
)
print
(
documents
)


[Document(metadata={'Header 1': 'Section with Iframe'}, page_content='[iframe:https://example.com/embed](https://example.com/embed) Some text after the iframe'), Document(metadata={'Header 1': 'Section with Iframe'}, page_content=". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.")]


Explanation
​


In this example, we defined a custom handler for 
iframe
 tags that converts them into Markdown-like links. When the splitter processes the HTML content, it uses this custom handler to transform the 
iframe
 tags while preserving other elements like tables and lists. The resulting 
Document
 objects show how the iframe is handled according to the custom logic you provided.


Important
: When presvering items such as links, you should be mindful not to include 
.
 in your separators, or leave separators blank. 
RecursiveCharacterTextSplitter
 splits on full stop, which will cut links in half. Ensure you provide a separator list with 
. 
  instead.


Using a custom handler to analyze an image with an LLM
​


With custom handler's, we can also override the default processing for any element. A great example of this, is inserting semantic analysis of an image within a document, directly in the chunking flow.


Since our function is called when the tag is discovered, we can override the 
<img>
 tag and turn off 
preserve_images
 to insert any content we would like to embed in our chunks.


"""This example assumes you have helper methods `load_image_from_url` and an LLM agent `llm` that can process image data."""
from
 langchain
.
agents 
import
 AgentExecutor
# This example needs to be replaced with your own agent
llm 
=
 AgentExecutor
(
.
.
.
)
# This method is a placeholder for loading image data from a URL and is not implemented here
def
 
load_image_from_url
(
image_url
:
 
str
)
 
-
>
 
bytes
:
    
# Assuming this method fetches the image data from the URL
    
return
 
b"image_data"
html_string 
=
 
"""
<!DOCTYPE html>
<html>
    <body>
        <div>
            <h1>Section with Image and Link</h1>
            <p>
                <img src="https://example.com/image.jpg" alt="An example image" />
                Some text after the image.
            </p>
            <ul>
                <li>Item 1: Description of item 1, which is quite detailed and important.</li>
                <li>Item 2: Description of item 2, which also contains significant information.</li>
                <li>Item 3: Description of item 3, another item that we don't want to split across chunks.</li>
            </ul>
        </div>
    </body>
</html>
"""
def
 
custom_image_handler
(
img_tag
)
 
-
>
 
str
:
    img_src 
=
 img_tag
.
get
(
"src"
,
 
""
)
    img_alt 
=
 img_tag
.
get
(
"alt"
,
 
"No alt text provided"
)
    image_data 
=
 load_image_from_url
(
img_src
)
    semantic_meaning 
=
 llm
.
invoke
(
image_data
)
    markdown_text 
=
 
f"[Image Alt Text: 
{
img_alt
}
 | Image Source: 
{
img_src
}
 | Image Semantic Meaning: 
{
semantic_meaning
}
]"
    
return
 markdown_text
splitter 
=
 HTMLSemanticPreservingSplitter
(
    headers_to_split_on
=
headers_to_split_on
,
    max_chunk_size
=
50
,
    separators
=
[
"\n\n"
,
 
"\n"
,
 
". "
]
,
    elements_to_preserve
=
[
"ul"
]
,
    preserve_images
=
False
,
    custom_handlers
=
{
"img"
:
 custom_image_handler
}
,
)
documents 
=
 splitter
.
split_text
(
html_string
)
print
(
documents
)
API Reference:
AgentExecutor


[Document(metadata={'Header 1': 'Section with Image and Link'}, page_content='[Image Alt Text: An example image | Image Source: https://example.com/image.jpg | Image Semantic Meaning: semantic-meaning] Some text after the image'), 
Document(metadata={'Header 1': 'Section with Image and Link'}, page_content=". Item 1: Description of item 1, which is quite detailed and important. Item 2: Description of item 2, which also contains significant information. Item 3: Description of item 3, another item that we don't want to split across chunks.")]


Explanation:
​


With our custom handler written to extract the specific fields from a 
<img>
 element in HTML, we can further process the data with our agent, and insert the result directly into our chunk. It is important to ensure 
preserve_images
 is set to 
False
 otherwise the default processing of 
<img>
 fields will take place.
Edit this page
Was this page helpful?
Previous
How to split text by tokens
Next
How to do question answering over CSVs
Overview of the Splitters
HTMLHeaderTextSplitter
HTMLSectionSplitter
HTMLSemanticPreservingSplitter
Choosing the Right Splitter
Example HTML Document
Using HTMLHeaderTextSplitter
How to split from a URL or HTML file:
How to constrain chunk sizes:
Limitations
Using HTMLSectionSplitter
How to split HTML strings:
How to constrain chunk sizes:
Using HTMLSemanticPreservingSplitter
Preserving Tables and Lists
Using a Custom Handler
Using a custom handler to analyze an image with an LLM


--- docs_how_to_sql_csv.txt ---

How-to guides
How to do question answering over CSVs
On this page
How to do question answering over CSVs


LLMs are great for building question-answering systems over various types of data sources. In this section we'll go over how to build Q&A systems over data stored in a CSV file(s). Like working with SQL databases, the key to working with CSV files is to give an LLM access to tools for querying and interacting with the data. The two main ways to do this are to either:




RECOMMENDED
: Load the CSV(s) into a SQL database, and use the approaches outlined in the 
SQL tutorial
.


Give the LLM access to a Python environment where it can use libraries like Pandas to interact with the data.




We will cover both approaches in this guide.


⚠️ Security note ⚠️
​


Both approaches mentioned above carry significant risks. Using SQL requires executing model-generated SQL queries. Using a library like Pandas requires letting the model execute Python code. Since it is easier to tightly scope SQL connection permissions and sanitize SQL queries than it is to sandbox Python environments, 
we HIGHLY recommend interacting with CSV data via SQL.
 For more on general security best practices, 
see here
.


Setup
​


Dependencies for this guide:


%
pip install 
-
qU langchain langchain
-
openai langchain
-
community langchain
-
experimental pandas


Set required environment variables:


# Using LangSmith is recommended but not required. Uncomment below lines to use.
# import os
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


Download the 
Titanic dataset
 if you don't already have it:


!wget https
:
//
web
.
stanford
.
edu
/
class
/
archive
/
cs
/
cs109
/
cs109
.
1166
/
stuff
/
titanic
.
csv 
-
O titanic
.
csv


import
 pandas 
as
 pd
df 
=
 pd
.
read_csv
(
"titanic.csv"
)
print
(
df
.
shape
)
print
(
df
.
columns
.
tolist
(
)
)


(887, 8)
['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']


SQL
​


Using SQL to interact with CSV data is the recommended approach because it is easier to limit permissions and sanitize queries than with arbitrary Python.


Most SQL databases make it easy to load a CSV file in as a table (
DuckDB
, 
SQLite
, etc.). Once you've done this you can use all of the chain and agent-creating techniques outlined in the 
SQL tutorial
. Here's a quick example of how we might do this with SQLite:


from
 langchain_community
.
utilities 
import
 SQLDatabase
from
 sqlalchemy 
import
 create_engine
engine 
=
 create_engine
(
"sqlite:///titanic.db"
)
df
.
to_sql
(
"titanic"
,
 engine
,
 index
=
False
)
API Reference:
SQLDatabase


887


db 
=
 SQLDatabase
(
engine
=
engine
)
print
(
db
.
dialect
)
print
(
db
.
get_usable_table_names
(
)
)
print
(
db
.
run
(
"SELECT * FROM titanic WHERE Age < 2;"
)
)


sqlite
['titanic']
[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]


And create a 
SQL agent
 to interact with it:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


from
 langchain_community
.
agent_toolkits 
import
 create_sql_agent
agent_executor 
=
 create_sql_agent
(
llm
,
 db
=
db
,
 agent_type
=
"openai-tools"
,
 verbose
=
True
)
API Reference:
create_sql_agent


agent_executor
.
invoke
(
{
"input"
:
 
"what's the average age of survivors"
}
)


[1m> Entering new SQL Agent Executor chain...[0m
[32;1m[1;3m
Invoking: `sql_db_list_tables` with `{}`
[0m[38;5;200m[1;3mtitanic[0m[32;1m[1;3m
Invoking: `sql_db_schema` with `{'table_names': 'titanic'}`
[0m[33;1m[1;3m
CREATE TABLE titanic (
	"Survived" BIGINT, 
	"Pclass" BIGINT, 
	"Name" TEXT, 
	"Sex" TEXT, 
	"Age" FLOAT, 
	"Siblings/Spouses Aboard" BIGINT, 
	"Parents/Children Aboard" BIGINT, 
	"Fare" FLOAT
)
/*
3 rows from titanic table:
Survived	Pclass	Name	Sex	Age	Siblings/Spouses Aboard	Parents/Children Aboard	Fare
0	3	Mr. Owen Harris Braund	male	22.0	1	0	7.25
1	1	Mrs. John Bradley (Florence Briggs Thayer) Cumings	female	38.0	1	0	71.2833
1	3	Miss. Laina Heikkinen	female	26.0	0	0	7.925
*/[0m[32;1m[1;3m
Invoking: `sql_db_query` with `{'query': 'SELECT AVG(Age) AS Average_Age FROM titanic WHERE Survived = 1'}`
[0m[36;1m[1;3m[(28.408391812865496,)][0m[32;1m[1;3mThe average age of survivors in the Titanic dataset is approximately 28.41 years.[0m
[1m> Finished chain.[0m


{'input': "what's the average age of survivors",
 'output': 'The average age of survivors in the Titanic dataset is approximately 28.41 years.'}


This approach easily generalizes to multiple CSVs, since we can just load each of them into our database as its own table. See the 
Multiple CSVs
 section below.


Pandas
​


Instead of SQL we can also use data analysis libraries like pandas and the code generating abilities of LLMs to interact with CSV data. Again, 
this approach is not fit for production use cases unless you have extensive safeguards in place
. For this reason, our code-execution utilities and constructors live in the 
langchain-experimental
 package.


Chain
​


Most LLMs have been trained on enough pandas Python code that they can generate it just by being asked to:


ai_msg 
=
 llm
.
invoke
(
    
"I have a pandas DataFrame 'df' with columns 'Age' and 'Fare'. Write code to compute the correlation between the two columns. Return Markdown for a Python code snippet and nothing else."
)
print
(
ai_msg
.
content
)


\`\`\`python
correlation = df['Age'].corr(df['Fare'])
correlation
\`\`\`


We can combine this ability with a Python-executing tool to create a simple data analysis chain. We'll first want to load our CSV table as a dataframe, and give the tool access to this dataframe:


import
 pandas 
as
 pd
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_experimental
.
tools 
import
 PythonAstREPLTool
df 
=
 pd
.
read_csv
(
"titanic.csv"
)
tool 
=
 PythonAstREPLTool
(
locals
=
{
"df"
:
 df
}
)
tool
.
invoke
(
"df['Fare'].mean()"
)
API Reference:
ChatPromptTemplate
 | 
PythonAstREPLTool


32.30542018038331


To help enforce proper use of our Python tool, we'll using 
tool calling
:


llm_with_tools 
=
 llm
.
bind_tools
(
[
tool
]
,
 tool_choice
=
tool
.
name
)
response 
=
 llm_with_tools
.
invoke
(
    
"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns"
)
response


AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SBrK246yUbdnJemXFC8Iod05', 'function': {'arguments': '{"query":"df.corr()[\'Age\'][\'Fare\']"}', 'name': 'python_repl_ast'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 125, 'total_tokens': 138}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-1fd332ba-fa72-4351-8182-d464e7368311-0', tool_calls=[{'name': 'python_repl_ast', 'args': {'query': "df.corr()['Age']['Fare']"}, 'id': 'call_SBrK246yUbdnJemXFC8Iod05'}])


response
.
tool_calls


[{'name': 'python_repl_ast',
  'args': {'query': "df.corr()['Age']['Fare']"},
  'id': 'call_SBrK246yUbdnJemXFC8Iod05'}]


We'll add a tools output parser to extract the function call as a dict:


from
 langchain_core
.
output_parsers
.
openai_tools 
import
 JsonOutputKeyToolsParser
parser 
=
 JsonOutputKeyToolsParser
(
key_name
=
tool
.
name
,
 first_tool_only
=
True
)
(
llm_with_tools 
|
 parser
)
.
invoke
(
    
"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns"
)
API Reference:
JsonOutputKeyToolsParser


{'query': "df[['Age', 'Fare']].corr()"}


And combine with a prompt so that we can just specify a question without needing to specify the dataframe info every invocation:


system 
=
 
f"""You have access to a pandas dataframe `df`. \
Here is the output of `df.head().to_markdown()`:
\`\`\`
{
df
.
head
(
)
.
to_markdown
(
)
}
\`\`\`
Given a user question, write the Python code to answer it. \
Return ONLY the valid Python code and nothing else. \
Don't assume you have access to any libraries other than built-in Python ones and pandas."""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
[
(
"system"
,
 system
)
,
 
(
"human"
,
 
"{question}"
)
]
)
code_chain 
=
 prompt 
|
 llm_with_tools 
|
 parser
code_chain
.
invoke
(
{
"question"
:
 
"What's the correlation between age and fare"
}
)


{'query': "df[['Age', 'Fare']].corr()"}


And lastly we'll add our Python tool so that the generated code is actually executed:


chain 
=
 prompt 
|
 llm_with_tools 
|
 parser 
|
 tool
chain
.
invoke
(
{
"question"
:
 
"What's the correlation between age and fare"
}
)


0.11232863699941621


And just like that we have a simple data analysis chain. We can take a peak at the intermediate steps by looking at the LangSmith trace: 
https://smith.langchain.com/public/b1309290-7212-49b7-bde2-75b39a32b49a/r


We could add an additional LLM call at the end to generate a conversational response, so that we're not just responding with the tool output. For this we'll want to add a chat history 
MessagesPlaceholder
 to our prompt:


from
 operator 
import
 itemgetter
from
 langchain_core
.
messages 
import
 ToolMessage
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 MessagesPlaceholder
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
system 
=
 
f"""You have access to a pandas dataframe `df`. \
Here is the output of `df.head().to_markdown()`:
\`\`\`
{
df
.
head
(
)
.
to_markdown
(
)
}
\`\`\`
Given a user question, write the Python code to answer it. \
Don't assume you have access to any libraries other than built-in Python ones and pandas.
Respond directly to the question once you have enough information to answer it."""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            system
,
        
)
,
        
(
"human"
,
 
"{question}"
)
,
        
# This MessagesPlaceholder allows us to optionally append an arbitrary number of messages
        
# at the end of the prompt using the 'chat_history' arg.
        MessagesPlaceholder
(
"chat_history"
,
 optional
=
True
)
,
    
]
)
def
 
_get_chat_history
(
x
:
 
dict
)
 
-
>
 
list
:
    
"""Parse the chain output up to this point into a list of chat history messages to insert in the prompt."""
    ai_msg 
=
 x
[
"ai_msg"
]
    tool_call_id 
=
 x
[
"ai_msg"
]
.
additional_kwargs
[
"tool_calls"
]
[
0
]
[
"id"
]
    tool_msg 
=
 ToolMessage
(
tool_call_id
=
tool_call_id
,
 content
=
str
(
x
[
"tool_output"
]
)
)
    
return
 
[
ai_msg
,
 tool_msg
]
chain 
=
 
(
    RunnablePassthrough
.
assign
(
ai_msg
=
prompt 
|
 llm_with_tools
)
    
.
assign
(
tool_output
=
itemgetter
(
"ai_msg"
)
 
|
 parser 
|
 tool
)
    
.
assign
(
chat_history
=
_get_chat_history
)
    
.
assign
(
response
=
prompt 
|
 llm 
|
 StrOutputParser
(
)
)
    
.
pick
(
[
"tool_output"
,
 
"response"
]
)
)
API Reference:
ToolMessage
 | 
StrOutputParser
 | 
MessagesPlaceholder
 | 
RunnablePassthrough


chain
.
invoke
(
{
"question"
:
 
"What's the correlation between age and fare"
}
)


{'tool_output': 0.11232863699941616,
 'response': 'The correlation between age and fare is approximately 0.1123.'}


Here's the LangSmith trace for this run: 
https://smith.langchain.com/public/14e38d70-45b1-4b81-8477-9fd2b7c07ea6/r


Agent
​


For complex questions it can be helpful for an LLM to be able to iteratively execute code while maintaining the inputs and outputs of its previous executions. This is where Agents come into play. They allow an LLM to decide how many times a tool needs to be invoked and keep track of the executions it's made so far. The 
create_pandas_dataframe_agent
 is a built-in agent that makes it easy to work with dataframes:


from
 langchain_experimental
.
agents 
import
 create_pandas_dataframe_agent
agent 
=
 create_pandas_dataframe_agent
(
    llm
,
 df
,
 agent_type
=
"openai-tools"
,
 verbose
=
True
,
 allow_dangerous_code
=
True
)
agent
.
invoke
(
    
{
        
"input"
:
 
"What's the correlation between age and fare? is that greater than the correlation between fare and survival?"
    
}
)
API Reference:
create_pandas_dataframe_agent


[1m> Entering new AgentExecutor chain...[0m
[32;1m[1;3m
Invoking: `python_repl_ast` with `{'query': "df[['Age', 'Fare']].corr().iloc[0,1]"}`
[0m[36;1m[1;3m0.11232863699941621[0m[32;1m[1;3m
Invoking: `python_repl_ast` with `{'query': "df[['Fare', 'Survived']].corr().iloc[0,1]"}`
[0m[36;1m[1;3m0.2561785496289603[0m[32;1m[1;3mThe correlation between Age and Fare is approximately 0.112, and the correlation between Fare and Survival is approximately 0.256.
Therefore, the correlation between Fare and Survival (0.256) is greater than the correlation between Age and Fare (0.112).[0m
[1m> Finished chain.[0m


{'input': "What's the correlation between age and fare? is that greater than the correlation between fare and survival?",
 'output': 'The correlation between Age and Fare is approximately 0.112, and the correlation between Fare and Survival is approximately 0.256.\n\nTherefore, the correlation between Fare and Survival (0.256) is greater than the correlation between Age and Fare (0.112).'}


Here's the LangSmith trace for this run: 
https://smith.langchain.com/public/6a86aee2-4f22-474a-9264-bd4c7283e665/r


Multiple CSVs
​


To handle multiple CSVs (or dataframes) we just need to pass multiple dataframes to our Python tool. Our 
create_pandas_dataframe_agent
 constructor can do this out of the box, we can pass in a list of dataframes instead of just one. If we're constructing a chain ourselves, we can do something like:


df_1 
=
 df
[
[
"Age"
,
 
"Fare"
]
]
df_2 
=
 df
[
[
"Fare"
,
 
"Survived"
]
]
tool 
=
 PythonAstREPLTool
(
locals
=
{
"df_1"
:
 df_1
,
 
"df_2"
:
 df_2
}
)
llm_with_tool 
=
 llm
.
bind_tools
(
tools
=
[
tool
]
,
 tool_choice
=
tool
.
name
)
df_template 
=
 
"""\`\`\`python
{df_name}.head().to_markdown()
>>> {df_head}
\`\`\`"""
df_context 
=
 
"\n\n"
.
join
(
    df_template
.
format
(
df_head
=
_df
.
head
(
)
.
to_markdown
(
)
,
 df_name
=
df_name
)
    
for
 _df
,
 df_name 
in
 
[
(
df_1
,
 
"df_1"
)
,
 
(
df_2
,
 
"df_2"
)
]
)
system 
=
 
f"""You have access to a number of pandas dataframes. \
Here is a sample of rows from each dataframe and the python code that was used to generate the sample:
{
df_context
}
Given a user question about the dataframes, write the Python code to answer it. \
Don't assume you have access to any libraries other than built-in Python ones and pandas. \
Make sure to refer only to the variables mentioned above."""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
[
(
"system"
,
 system
)
,
 
(
"human"
,
 
"{question}"
)
]
)
chain 
=
 prompt 
|
 llm_with_tool 
|
 parser 
|
 tool
chain
.
invoke
(
    
{
        
"question"
:
 
"return the difference in the correlation between age and fare and the correlation between fare and survival"
    
}
)


0.14384991262954416


Here's the LangSmith trace for this run: 
https://smith.langchain.com/public/cc2a7d7f-7c5a-4e77-a10c-7b5420fcd07f/r


Sandboxed code execution
​


There are a number of tools like 
E2B
 and 
Bearly
 that provide sandboxed environments for Python code execution, to allow for safer code-executing chains and agents.


Next steps
​


For more advanced data analysis applications we recommend checking out:




SQL tutorial
: Many of the challenges of working with SQL db's and CSV's are generic to any structured data type, so it's useful to read the SQL techniques even if you're using Pandas for CSV data analysis.


Tool use
: Guides on general best practices when working with chains and agents that invoke tools


Agents
: Understand the fundamentals of building LLM agents.


Integrations: Sandboxed envs like 
E2B
 and 
Bearly
, utilities like 
SQLDatabase
, related agents like 
Spark DataFrame agent
.


Edit this page
Was this page helpful?
Previous
How to split HTML
Next
How to deal with large databases when doing SQL question-answering
⚠️ Security note ⚠️
Setup
SQL
Pandas
Chain
Agent
Multiple CSVs
Sandboxed code execution
Next steps


--- docs_how_to_sql_large_db.txt ---

How-to guides
How to deal with large databases when doing SQL question-answering
On this page
How to deal with large databases when doing SQL question-answering


In order to write valid queries against a database, we need to feed the model the table names, table schemas, and feature values for it to query over. When there are many tables, columns, and/or high-cardinality columns, it becomes impossible for us to dump the full information about our database in every prompt. Instead, we must find ways to dynamically insert into the prompt only the most relevant information.


In this guide we demonstrate methods for identifying such relevant information, and feeding this into a query-generation step. We will cover:




Identifying a relevant subset of tables;


Identifying a relevant subset of column values.




Setup
​


First, get required packages and set environment variables:


%
pip install 
-
-
upgrade 
-
-
quiet  langchain langchain
-
community langchain
-
openai


# Uncomment the below to use LangSmith. Not required.
# import os
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"


The below example will use a SQLite connection with Chinook database. Follow 
these installation steps
 to create 
Chinook.db
 in the same directory as this notebook:




Save 
this file
 as 
Chinook_Sqlite.sql


Run 
sqlite3 Chinook.db


Run 
.read Chinook_Sqlite.sql


Test 
SELECT * FROM Artist LIMIT 10;




Now, 
Chinook.db
 is in our directory and we can interface with it using the SQLAlchemy-driven 
SQLDatabase
 class:


from
 langchain_community
.
utilities 
import
 SQLDatabase
db 
=
 SQLDatabase
.
from_uri
(
"sqlite:///Chinook.db"
)
print
(
db
.
dialect
)
print
(
db
.
get_usable_table_names
(
)
)
print
(
db
.
run
(
"SELECT * FROM Artist LIMIT 10;"
)
)
API Reference:
SQLDatabase


sqlite
['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']
[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]


Many tables
​


One of the main pieces of information we need to include in our prompt is the schemas of the relevant tables. When we have very many tables, we can't fit all of the schemas in a single prompt. What we can do in such cases is first extract the names of the tables related to the user input, and then include only their schemas.


One easy and reliable way to do this is using 
tool-calling
. Below, we show how we can use this feature to obtain output conforming to a desired format (in this case, a list of table names). We use the chat model's 
.bind_tools
 method to bind a tool in Pydantic format, and feed this into an output parser to reconstruct the object from the model's response.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


from
 langchain_core
.
output_parsers
.
openai_tools 
import
 PydanticToolsParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 pydantic 
import
 BaseModel
,
 Field
class
 
Table
(
BaseModel
)
:
    
"""Table in SQL database."""
    name
:
 
str
 
=
 Field
(
description
=
"Name of table in SQL database."
)
table_names 
=
 
"\n"
.
join
(
db
.
get_usable_table_names
(
)
)
system 
=
 
f"""Return the names of ALL the SQL tables that MIGHT be relevant to the user question. \
The tables are:
{
table_names
}
Remember to include ALL POTENTIALLY RELEVANT tables, even if you're not sure that they're needed."""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        
(
"human"
,
 
"{input}"
)
,
    
]
)
llm_with_tools 
=
 llm
.
bind_tools
(
[
Table
]
)
output_parser 
=
 PydanticToolsParser
(
tools
=
[
Table
]
)
table_chain 
=
 prompt 
|
 llm_with_tools 
|
 output_parser
table_chain
.
invoke
(
{
"input"
:
 
"What are all the genres of Alanis Morisette songs"
}
)
API Reference:
PydanticToolsParser
 | 
ChatPromptTemplate


[Table(name='Genre')]


This works pretty well! Except, as we'll see below, we actually need a few other tables as well. This would be pretty difficult for the model to know based just on the user question. In this case, we might think to simplify our model's job by grouping the tables together. We'll just ask the model to choose between categories "Music" and "Business", and then take care of selecting all the relevant tables from there:


system 
=
 
"""Return the names of any SQL tables that are relevant to the user question.
The tables are:
Music
Business
"""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        
(
"human"
,
 
"{input}"
)
,
    
]
)
category_chain 
=
 prompt 
|
 llm_with_tools 
|
 output_parser
category_chain
.
invoke
(
{
"input"
:
 
"What are all the genres of Alanis Morisette songs"
}
)


[Table(name='Music'), Table(name='Business')]


from
 typing 
import
 List
def
 
get_tables
(
categories
:
 List
[
Table
]
)
 
-
>
 List
[
str
]
:
    tables 
=
 
[
]
    
for
 category 
in
 categories
:
        
if
 category
.
name 
==
 
"Music"
:
            tables
.
extend
(
                
[
                    
"Album"
,
                    
"Artist"
,
                    
"Genre"
,
                    
"MediaType"
,
                    
"Playlist"
,
                    
"PlaylistTrack"
,
                    
"Track"
,
                
]
            
)
        
elif
 category
.
name 
==
 
"Business"
:
            tables
.
extend
(
[
"Customer"
,
 
"Employee"
,
 
"Invoice"
,
 
"InvoiceLine"
]
)
    
return
 tables
table_chain 
=
 category_chain 
|
 get_tables
table_chain
.
invoke
(
{
"input"
:
 
"What are all the genres of Alanis Morisette songs"
}
)


['Album',
 'Artist',
 'Genre',
 'MediaType',
 'Playlist',
 'PlaylistTrack',
 'Track',
 'Customer',
 'Employee',
 'Invoice',
 'InvoiceLine']


Now that we've got a chain that can output the relevant tables for any query we can combine this with our 
create_sql_query_chain
, which can accept a list of 
table_names_to_use
 to determine which table schemas are included in the prompt:


from
 operator 
import
 itemgetter
from
 langchain
.
chains 
import
 create_sql_query_chain
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
query_chain 
=
 create_sql_query_chain
(
llm
,
 db
)
# Convert "question" key to the "input" key expected by current table_chain.
table_chain 
=
 
{
"input"
:
 itemgetter
(
"question"
)
}
 
|
 table_chain
# Set table_names_to_use using table_chain.
full_chain 
=
 RunnablePassthrough
.
assign
(
table_names_to_use
=
table_chain
)
 
|
 query_chain
API Reference:
create_sql_query_chain
 | 
RunnablePassthrough


query 
=
 full_chain
.
invoke
(
    
{
"question"
:
 
"What are all the genres of Alanis Morisette songs"
}
)
print
(
query
)


SELECT DISTINCT "g"."Name"
FROM "Genre" g
JOIN "Track" t ON "g"."GenreId" = "t"."GenreId"
JOIN "Album" a ON "t"."AlbumId" = "a"."AlbumId"
JOIN "Artist" ar ON "a"."ArtistId" = "ar"."ArtistId"
WHERE "ar"."Name" = 'Alanis Morissette'
LIMIT 5;


db
.
run
(
query
)


"[('Rock',)]"


We can see the LangSmith trace for this run 
here
.


We've seen how to dynamically include a subset of table schemas in a prompt within a chain. Another possible approach to this problem is to let an Agent decide for itself when to look up tables by giving it a Tool to do so. You can see an example of this in the 
SQL: Agents
 guide.


High-cardinality columns
​


In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.


One naive strategy it to create a vector store with all the distinct proper nouns that exist in the database. We can then query that vector store each user input and inject the most relevant proper nouns into the prompt.


First we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:


import
 ast
import
 re
def
 
query_as_list
(
db
,
 query
)
:
    res 
=
 db
.
run
(
query
)
    res 
=
 
[
el 
for
 sub 
in
 ast
.
literal_eval
(
res
)
 
for
 el 
in
 sub 
if
 el
]
    res 
=
 
[
re
.
sub
(
r"\b\d+\b"
,
 
""
,
 string
)
.
strip
(
)
 
for
 string 
in
 res
]
    
return
 res
proper_nouns 
=
 query_as_list
(
db
,
 
"SELECT Name FROM Artist"
)
proper_nouns 
+=
 query_as_list
(
db
,
 
"SELECT Title FROM Album"
)
proper_nouns 
+=
 query_as_list
(
db
,
 
"SELECT Name FROM Genre"
)
len
(
proper_nouns
)
proper_nouns
[
:
5
]


['AC/DC', 'Accept', 'Aerosmith', 'Alanis Morissette', 'Alice In Chains']


Now we can embed and store all of our values in a vector database:


from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_openai 
import
 OpenAIEmbeddings
vector_db 
=
 FAISS
.
from_texts
(
proper_nouns
,
 OpenAIEmbeddings
(
)
)
retriever 
=
 vector_db
.
as_retriever
(
search_kwargs
=
{
"k"
:
 
15
}
)
API Reference:
FAISS
 | 
OpenAIEmbeddings


And put together a query construction chain that first retrieves values from the database and inserts them into the prompt:


from
 operator 
import
 itemgetter
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
system 
=
 
"""You are a SQLite expert. Given an input question, create a syntactically
correct SQLite query to run. Unless otherwise specificed, do not return more than
{top_k} rows.
Only return the SQL query with no markup or explanation.
Here is the relevant table info: {table_info}
Here is a non-exhaustive list of possible feature values. If filtering on a feature
value make sure to check its spelling against this list first:
{proper_nouns}
"""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
[
(
"system"
,
 system
)
,
 
(
"human"
,
 
"{input}"
)
]
)
query_chain 
=
 create_sql_query_chain
(
llm
,
 db
,
 prompt
=
prompt
)
retriever_chain 
=
 
(
    itemgetter
(
"question"
)
    
|
 retriever
    
|
 
(
lambda
 docs
:
 
"\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 docs
)
)
)
chain 
=
 RunnablePassthrough
.
assign
(
proper_nouns
=
retriever_chain
)
 
|
 query_chain
API Reference:
ChatPromptTemplate
 | 
RunnablePassthrough


To try out our chain, let's see what happens when we try filtering on "elenis moriset", a misspelling of Alanis Morissette, without and with retrieval:


# Without retrieval
query 
=
 query_chain
.
invoke
(
    
{
"question"
:
 
"What are all the genres of elenis moriset songs"
,
 
"proper_nouns"
:
 
""
}
)
print
(
query
)
db
.
run
(
query
)


SELECT DISTINCT g.Name 
FROM Track t
JOIN Album a ON t.AlbumId = a.AlbumId
JOIN Artist ar ON a.ArtistId = ar.ArtistId
JOIN Genre g ON t.GenreId = g.GenreId
WHERE ar.Name = 'Elenis Moriset';


''


# With retrieval
query 
=
 chain
.
invoke
(
{
"question"
:
 
"What are all the genres of elenis moriset songs"
}
)
print
(
query
)
db
.
run
(
query
)


SELECT DISTINCT g.Name
FROM Genre g
JOIN Track t ON g.GenreId = t.GenreId
JOIN Album a ON t.AlbumId = a.AlbumId
JOIN Artist ar ON a.ArtistId = ar.ArtistId
WHERE ar.Name = 'Alanis Morissette';


"[('Rock',)]"


We can see that with retrieval we're able to correct the spelling from "Elenis Moriset" to "Alanis Morissette" and get back a valid result.


Another possible approach to this problem is to let an Agent decide for itself when to look up proper nouns. You can see an example of this in the 
SQL: Agents
 guide.
Edit this page
Was this page helpful?
Previous
How to do question answering over CSVs
Next
How to better prompt when doing SQL question-answering
Setup
Many tables
High-cardinality columns


--- docs_how_to_sql_prompting.txt ---

How-to guides
How to better prompt when doing SQL question-answering
On this page
How to better prompt when doing SQL question-answering


In this guide we'll go over prompting strategies to improve SQL query generation using 
create_sql_query_chain
. We'll largely focus on methods for getting relevant database-specific information in your prompt.


We will cover:




How the dialect of the LangChain 
SQLDatabase
 impacts the prompt of the chain;


How to format schema information into the prompt using 
SQLDatabase.get_context
;


How to build and select 
few-shot examples
 to assist the model.




Setup
​


First, get required packages and set environment variables:


%
pip install 
-
-
upgrade 
-
-
quiet  langchain langchain
-
community langchain
-
experimental langchain
-
openai


# Uncomment the below to use LangSmith. Not required.
# import os
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"


The below example will use a SQLite connection with Chinook database. Follow 
these installation steps
 to create 
Chinook.db
 in the same directory as this notebook:




Save 
this file
 as 
Chinook_Sqlite.sql


Run 
sqlite3 Chinook.db


Run 
.read Chinook_Sqlite.sql


Test 
SELECT * FROM Artist LIMIT 10;




Now, 
Chinook.db
 is in our directory and we can interface with it using the SQLAlchemy-driven 
SQLDatabase
 class:


from
 langchain_community
.
utilities 
import
 SQLDatabase
db 
=
 SQLDatabase
.
from_uri
(
"sqlite:///Chinook.db"
,
 sample_rows_in_table_info
=
3
)
print
(
db
.
dialect
)
print
(
db
.
get_usable_table_names
(
)
)
print
(
db
.
run
(
"SELECT * FROM Artist LIMIT 10;"
)
)
API Reference:
SQLDatabase


sqlite
['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']
[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]


Dialect-specific prompting
​


One of the simplest things we can do is make our prompt specific to the SQL dialect we're using. When using the built-in 
create_sql_query_chain
 and 
SQLDatabase
, this is handled for you for any of the following dialects:


from
 langchain
.
chains
.
sql_database
.
prompt 
import
 SQL_PROMPTS
list
(
SQL_PROMPTS
)


['crate',
 'duckdb',
 'googlesql',
 'mssql',
 'mysql',
 'mariadb',
 'oracle',
 'postgresql',
 'sqlite',
 'clickhouse',
 'prestodb']


For example, using our current DB we can see that we'll get a SQLite-specific prompt.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


from
 langchain
.
chains 
import
 create_sql_query_chain
chain 
=
 create_sql_query_chain
(
llm
,
 db
)
chain
.
get_prompts
(
)
[
0
]
.
pretty_print
(
)
API Reference:
create_sql_query_chain


You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.
Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.
Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (") to denote them as delimited identifiers.
Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.
Pay attention to use date('now') function to get the current date, if the question involves "today".
Use the following format:
Question: Question here
SQLQuery: SQL Query to run
SQLResult: Result of the SQLQuery
Answer: Final answer here
Only use the following tables:
[33;1m[1;3m{table_info}[0m
Question: [33;1m[1;3m{input}[0m


Table definitions and example rows
​


In most SQL chains, we'll need to feed the model at least part of the database schema. Without this it won't be able to write valid queries. Our database comes with some convenience methods to give us the relevant context. Specifically, we can get the table names, their schemas, and a sample of rows from each table.


Here we will use 
SQLDatabase.get_context
, which provides available tables and their schemas:


context 
=
 db
.
get_context
(
)
print
(
list
(
context
)
)
print
(
context
[
"table_info"
]
)


['table_info', 'table_names']
CREATE TABLE "Album" (
	"AlbumId" INTEGER NOT NULL, 
	"Title" NVARCHAR(160) NOT NULL, 
	"ArtistId" INTEGER NOT NULL, 
	PRIMARY KEY ("AlbumId"), 
	FOREIGN KEY("ArtistId") REFERENCES "Artist" ("ArtistId")
)
/*
3 rows from Album table:
AlbumId	Title	ArtistId
1	For Those About To Rock We Salute You	1
2	Balls to the Wall	2
3	Restless and Wild	2
*/
CREATE TABLE "Artist" (
	"ArtistId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120), 
	PRIMARY KEY ("ArtistId")
)
/*
3 rows from Artist table:
ArtistId	Name
1	AC/DC
2	Accept
3	Aerosmith
*/
CREATE TABLE "Customer" (
	"CustomerId" INTEGER NOT NULL, 
	"FirstName" NVARCHAR(40) NOT NULL, 
	"LastName" NVARCHAR(20) NOT NULL, 
	"Company" NVARCHAR(80), 
	"Address" NVARCHAR(70), 
	"City" NVARCHAR(40), 
	"State" NVARCHAR(40), 
	"Country" NVARCHAR(40), 
	"PostalCode" NVARCHAR(10), 
	"Phone" NVARCHAR(24), 
	"Fax" NVARCHAR(24), 
	"Email" NVARCHAR(60) NOT NULL, 
	"SupportRepId" INTEGER, 
	PRIMARY KEY ("CustomerId"), 
	FOREIGN KEY("SupportRepId") REFERENCES "Employee" ("EmployeeId")
)
/*
3 rows from Customer table:
CustomerId	FirstName	LastName	Company	Address	City	State	Country	PostalCode	Phone	Fax	Email	SupportRepId
1	Luís	Gonçalves	Embraer - Empresa Brasileira de Aeronáutica S.A.	Av. Brigadeiro Faria Lima, 2170	São José dos Campos	SP	Brazil	12227-000	+55 (12) 3923-5555	+55 (12) 3923-5566	luisg@embraer.com.br	3
2	Leonie	Köhler	None	Theodor-Heuss-Straße 34	Stuttgart	None	Germany	70174	+49 0711 2842222	None	leonekohler@surfeu.de	5
3	François	Tremblay	None	1498 rue Bélanger	Montréal	QC	Canada	H2G 1A7	+1 (514) 721-4711	None	ftremblay@gmail.com	3
*/
CREATE TABLE "Employee" (
	"EmployeeId" INTEGER NOT NULL, 
	"LastName" NVARCHAR(20) NOT NULL, 
	"FirstName" NVARCHAR(20) NOT NULL, 
	"Title" NVARCHAR(30), 
	"ReportsTo" INTEGER, 
	"BirthDate" DATETIME, 
	"HireDate" DATETIME, 
	"Address" NVARCHAR(70), 
	"City" NVARCHAR(40), 
	"State" NVARCHAR(40), 
	"Country" NVARCHAR(40), 
	"PostalCode" NVARCHAR(10), 
	"Phone" NVARCHAR(24), 
	"Fax" NVARCHAR(24), 
	"Email" NVARCHAR(60), 
	PRIMARY KEY ("EmployeeId"), 
	FOREIGN KEY("ReportsTo") REFERENCES "Employee" ("EmployeeId")
)
/*
3 rows from Employee table:
EmployeeId	LastName	FirstName	Title	ReportsTo	BirthDate	HireDate	Address	City	State	Country	PostalCode	Phone	Fax	Email
1	Adams	Andrew	General Manager	None	1962-02-18 00:00:00	2002-08-14 00:00:00	11120 Jasper Ave NW	Edmonton	AB	Canada	T5K 2N1	+1 (780) 428-9482	+1 (780) 428-3457	andrew@chinookcorp.com
2	Edwards	Nancy	Sales Manager	1	1958-12-08 00:00:00	2002-05-01 00:00:00	825 8 Ave SW	Calgary	AB	Canada	T2P 2T3	+1 (403) 262-3443	+1 (403) 262-3322	nancy@chinookcorp.com
3	Peacock	Jane	Sales Support Agent	2	1973-08-29 00:00:00	2002-04-01 00:00:00	1111 6 Ave SW	Calgary	AB	Canada	T2P 5M5	+1 (403) 262-3443	+1 (403) 262-6712	jane@chinookcorp.com
*/
CREATE TABLE "Genre" (
	"GenreId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120), 
	PRIMARY KEY ("GenreId")
)
/*
3 rows from Genre table:
GenreId	Name
1	Rock
2	Jazz
3	Metal
*/
CREATE TABLE "Invoice" (
	"InvoiceId" INTEGER NOT NULL, 
	"CustomerId" INTEGER NOT NULL, 
	"InvoiceDate" DATETIME NOT NULL, 
	"BillingAddress" NVARCHAR(70), 
	"BillingCity" NVARCHAR(40), 
	"BillingState" NVARCHAR(40), 
	"BillingCountry" NVARCHAR(40), 
	"BillingPostalCode" NVARCHAR(10), 
	"Total" NUMERIC(10, 2) NOT NULL, 
	PRIMARY KEY ("InvoiceId"), 
	FOREIGN KEY("CustomerId") REFERENCES "Customer" ("CustomerId")
)
/*
3 rows from Invoice table:
InvoiceId	CustomerId	InvoiceDate	BillingAddress	BillingCity	BillingState	BillingCountry	BillingPostalCode	Total
1	2	2021-01-01 00:00:00	Theodor-Heuss-Straße 34	Stuttgart	None	Germany	70174	1.98
2	4	2021-01-02 00:00:00	Ullevålsveien 14	Oslo	None	Norway	0171	3.96
3	8	2021-01-03 00:00:00	Grétrystraat 63	Brussels	None	Belgium	1000	5.94
*/
CREATE TABLE "InvoiceLine" (
	"InvoiceLineId" INTEGER NOT NULL, 
	"InvoiceId" INTEGER NOT NULL, 
	"TrackId" INTEGER NOT NULL, 
	"UnitPrice" NUMERIC(10, 2) NOT NULL, 
	"Quantity" INTEGER NOT NULL, 
	PRIMARY KEY ("InvoiceLineId"), 
	FOREIGN KEY("TrackId") REFERENCES "Track" ("TrackId"), 
	FOREIGN KEY("InvoiceId") REFERENCES "Invoice" ("InvoiceId")
)
/*
3 rows from InvoiceLine table:
InvoiceLineId	InvoiceId	TrackId	UnitPrice	Quantity
1	1	2	0.99	1
2	1	4	0.99	1
3	2	6	0.99	1
*/
CREATE TABLE "MediaType" (
	"MediaTypeId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120), 
	PRIMARY KEY ("MediaTypeId")
)
/*
3 rows from MediaType table:
MediaTypeId	Name
1	MPEG audio file
2	Protected AAC audio file
3	Protected MPEG-4 video file
*/
CREATE TABLE "Playlist" (
	"PlaylistId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120), 
	PRIMARY KEY ("PlaylistId")
)
/*
3 rows from Playlist table:
PlaylistId	Name
1	Music
2	Movies
3	TV Shows
*/
CREATE TABLE "PlaylistTrack" (
	"PlaylistId" INTEGER NOT NULL, 
	"TrackId" INTEGER NOT NULL, 
	PRIMARY KEY ("PlaylistId", "TrackId"), 
	FOREIGN KEY("TrackId") REFERENCES "Track" ("TrackId"), 
	FOREIGN KEY("PlaylistId") REFERENCES "Playlist" ("PlaylistId")
)
/*
3 rows from PlaylistTrack table:
PlaylistId	TrackId
1	3402
1	3389
1	3390
*/
CREATE TABLE "Track" (
	"TrackId" INTEGER NOT NULL, 
	"Name" NVARCHAR(200) NOT NULL, 
	"AlbumId" INTEGER, 
	"MediaTypeId" INTEGER NOT NULL, 
	"GenreId" INTEGER, 
	"Composer" NVARCHAR(220), 
	"Milliseconds" INTEGER NOT NULL, 
	"Bytes" INTEGER, 
	"UnitPrice" NUMERIC(10, 2) NOT NULL, 
	PRIMARY KEY ("TrackId"), 
	FOREIGN KEY("MediaTypeId") REFERENCES "MediaType" ("MediaTypeId"), 
	FOREIGN KEY("GenreId") REFERENCES "Genre" ("GenreId"), 
	FOREIGN KEY("AlbumId") REFERENCES "Album" ("AlbumId")
)
/*
3 rows from Track table:
TrackId	Name	AlbumId	MediaTypeId	GenreId	Composer	Milliseconds	Bytes	UnitPrice
1	For Those About To Rock (We Salute You)	1	1	1	Angus Young, Malcolm Young, Brian Johnson	343719	11170334	0.99
2	Balls to the Wall	2	2	1	U. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann	342562	5510424	0.99
3	Fast As a Shark	3	2	1	F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman	230619	3990994	0.99
*/


When we don't have too many, or too wide of, tables, we can just insert the entirety of this information in our prompt:


prompt_with_context 
=
 chain
.
get_prompts
(
)
[
0
]
.
partial
(
table_info
=
context
[
"table_info"
]
)
print
(
prompt_with_context
.
pretty_repr
(
)
[
:
1500
]
)


You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.
Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.
Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (") to denote them as delimited identifiers.
Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.
Pay attention to use date('now') function to get the current date, if the question involves "today".
Use the following format:
Question: Question here
SQLQuery: SQL Query to run
SQLResult: Result of the SQLQuery
Answer: Final answer here
Only use the following tables:
CREATE TABLE "Album" (
	"AlbumId" INTEGER NOT NULL, 
	"Title" NVARCHAR(160) NOT NULL, 
	"ArtistId" INTEGER NOT NULL, 
	PRIMARY KEY ("AlbumId"), 
	FOREIGN KEY("ArtistId") REFERENCES "Artist" ("ArtistId")
)
/*
3 rows from Album table:
AlbumId	Title	ArtistId
1	For Those About To Rock We Salute You	1
2	Balls to the Wall	2
3	Restless and Wild	2
*/
CREATE TABLE "Artist" (
	"ArtistId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120)


When we do have database schemas that are too large to fit into our model's context window, we'll need to come up with ways of inserting only the relevant table definitions into the prompt based on the user input. For more on this head to the 
Many tables, wide tables, high-cardinality feature
 guide.


Few-shot examples
​


Including examples of natural language questions being converted to valid SQL queries against our database in the prompt will often improve model performance, especially for complex queries.


Let's say we have the following examples:


examples 
=
 
[
    
{
"input"
:
 
"List all artists."
,
 
"query"
:
 
"SELECT * FROM Artist;"
}
,
    
{
        
"input"
:
 
"Find all albums for the artist 'AC/DC'."
,
        
"query"
:
 
"SELECT * FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'AC/DC');"
,
    
}
,
    
{
        
"input"
:
 
"List all tracks in the 'Rock' genre."
,
        
"query"
:
 
"SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');"
,
    
}
,
    
{
        
"input"
:
 
"Find the total duration of all tracks."
,
        
"query"
:
 
"SELECT SUM(Milliseconds) FROM Track;"
,
    
}
,
    
{
        
"input"
:
 
"List all customers from Canada."
,
        
"query"
:
 
"SELECT * FROM Customer WHERE Country = 'Canada';"
,
    
}
,
    
{
        
"input"
:
 
"How many tracks are there in the album with ID 5?"
,
        
"query"
:
 
"SELECT COUNT(*) FROM Track WHERE AlbumId = 5;"
,
    
}
,
    
{
        
"input"
:
 
"Find the total number of invoices."
,
        
"query"
:
 
"SELECT COUNT(*) FROM Invoice;"
,
    
}
,
    
{
        
"input"
:
 
"List all tracks that are longer than 5 minutes."
,
        
"query"
:
 
"SELECT * FROM Track WHERE Milliseconds > 300000;"
,
    
}
,
    
{
        
"input"
:
 
"Who are the top 5 customers by total purchase?"
,
        
"query"
:
 
"SELECT CustomerId, SUM(Total) AS TotalPurchase FROM Invoice GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5;"
,
    
}
,
    
{
        
"input"
:
 
"Which albums are from the year 2000?"
,
        
"query"
:
 
"SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';"
,
    
}
,
    
{
        
"input"
:
 
"How many employees are there"
,
        
"query"
:
 
'SELECT COUNT(*) FROM "Employee"'
,
    
}
,
]


We can create a few-shot prompt with them like so:


from
 langchain_core
.
prompts 
import
 FewShotPromptTemplate
,
 PromptTemplate
example_prompt 
=
 PromptTemplate
.
from_template
(
"User input: {input}\nSQL query: {query}"
)
prompt 
=
 FewShotPromptTemplate
(
    examples
=
examples
[
:
5
]
,
    example_prompt
=
example_prompt
,
    prefix
=
"You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than {top_k} rows.\n\nHere is the relevant table info: {table_info}\n\nBelow are a number of examples of questions and their corresponding SQL queries."
,
    suffix
=
"User input: {input}\nSQL query: "
,
    input_variables
=
[
"input"
,
 
"top_k"
,
 
"table_info"
]
,
)
API Reference:
FewShotPromptTemplate
 | 
PromptTemplate


print
(
prompt
.
format
(
input
=
"How many artists are there?"
,
 top_k
=
3
,
 table_info
=
"foo"
)
)


You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than 3 rows.
Here is the relevant table info: foo
Below are a number of examples of questions and their corresponding SQL queries.
User input: List all artists.
SQL query: SELECT * FROM Artist;
User input: Find all albums for the artist 'AC/DC'.
SQL query: SELECT * FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'AC/DC');
User input: List all tracks in the 'Rock' genre.
SQL query: SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');
User input: Find the total duration of all tracks.
SQL query: SELECT SUM(Milliseconds) FROM Track;
User input: List all customers from Canada.
SQL query: SELECT * FROM Customer WHERE Country = 'Canada';
User input: How many artists are there?
SQL query:


Dynamic few-shot examples
​


If we have enough examples, we may want to only include the most relevant ones in the prompt, either because they don't fit in the model's context window or because the long tail of examples distracts the model. And specifically, given any input we want to include the examples most relevant to that input.


We can do just this using an ExampleSelector. In this case we'll use a 
SemanticSimilarityExampleSelector
, which will store the examples in the vector database of our choosing. At runtime it will perform a similarity search between the input and our examples, and return the most semantically similar ones.


We default to OpenAI embeddings here, but you can swap them out for the model provider of your choice.


from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_core
.
example_selectors 
import
 SemanticSimilarityExampleSelector
from
 langchain_openai 
import
 OpenAIEmbeddings
example_selector 
=
 SemanticSimilarityExampleSelector
.
from_examples
(
    examples
,
    OpenAIEmbeddings
(
)
,
    FAISS
,
    k
=
5
,
    input_keys
=
[
"input"
]
,
)
API Reference:
FAISS
 | 
SemanticSimilarityExampleSelector
 | 
OpenAIEmbeddings


example_selector
.
select_examples
(
{
"input"
:
 
"how many artists are there?"
}
)


[{'input': 'List all artists.', 'query': 'SELECT * FROM Artist;'},
 {'input': 'How many employees are there',
  'query': 'SELECT COUNT(*) FROM "Employee"'},
 {'input': 'How many tracks are there in the album with ID 5?',
  'query': 'SELECT COUNT(*) FROM Track WHERE AlbumId = 5;'},
 {'input': 'Which albums are from the year 2000?',
  'query': "SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';"},
 {'input': "List all tracks in the 'Rock' genre.",
  'query': "SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');"}]


To use it, we can pass the ExampleSelector directly in to our FewShotPromptTemplate:


prompt 
=
 FewShotPromptTemplate
(
    example_selector
=
example_selector
,
    example_prompt
=
example_prompt
,
    prefix
=
"You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than {top_k} rows.\n\nHere is the relevant table info: {table_info}\n\nBelow are a number of examples of questions and their corresponding SQL queries."
,
    suffix
=
"User input: {input}\nSQL query: "
,
    input_variables
=
[
"input"
,
 
"top_k"
,
 
"table_info"
]
,
)


print
(
prompt
.
format
(
input
=
"how many artists are there?"
,
 top_k
=
3
,
 table_info
=
"foo"
)
)


You are a SQLite expert. Given an input question, create a syntactically correct SQLite query to run. Unless otherwise specificed, do not return more than 3 rows.
Here is the relevant table info: foo
Below are a number of examples of questions and their corresponding SQL queries.
User input: List all artists.
SQL query: SELECT * FROM Artist;
User input: How many employees are there
SQL query: SELECT COUNT(*) FROM "Employee"
User input: How many tracks are there in the album with ID 5?
SQL query: SELECT COUNT(*) FROM Track WHERE AlbumId = 5;
User input: Which albums are from the year 2000?
SQL query: SELECT * FROM Album WHERE strftime('%Y', ReleaseDate) = '2000';
User input: List all tracks in the 'Rock' genre.
SQL query: SELECT * FROM Track WHERE GenreId = (SELECT GenreId FROM Genre WHERE Name = 'Rock');
User input: how many artists are there?
SQL query:


Trying it out, we see that the model identifies the relevant table:


chain 
=
 create_sql_query_chain
(
llm
,
 db
,
 prompt
)
chain
.
invoke
(
{
"question"
:
 
"how many artists are there?"
}
)


'SELECT COUNT(*) FROM Artist;'
Edit this page
Was this page helpful?
Previous
How to deal with large databases when doing SQL question-answering
Next
How to do query validation as part of SQL question-answering
Setup
Dialect-specific prompting
Table definitions and example rows
Few-shot examples
Dynamic few-shot examples


--- docs_how_to_sql_query_checking.txt ---

How-to guides
How to do query validation as part of SQL question-answering
On this page
How to do query validation as part of SQL question-answering


Perhaps the most error-prone part of any SQL chain or agent is writing valid and safe SQL queries. In this guide we'll go over some strategies for validating our queries and handling invalid queries.


We will cover:




Appending a "query validator" step to the query generation;


Prompt engineering to reduce the incidence of errors.




Setup
​


First, get required packages and set environment variables:


%
pip install 
-
-
upgrade 
-
-
quiet  langchain langchain
-
community langchain
-
openai


# Uncomment the below to use LangSmith. Not required.
# import os
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"


The below example will use a SQLite connection with Chinook database. Follow 
these installation steps
 to create 
Chinook.db
 in the same directory as this notebook:




Save 
this file
 as 
Chinook_Sqlite.sql


Run 
sqlite3 Chinook.db


Run 
.read Chinook_Sqlite.sql


Test 
SELECT * FROM Artist LIMIT 10;




Now, 
Chinook.db
 is in our directory and we can interface with it using the SQLAlchemy-driven 
SQLDatabase
 class:


from
 langchain_community
.
utilities 
import
 SQLDatabase
db 
=
 SQLDatabase
.
from_uri
(
"sqlite:///Chinook.db"
)
print
(
db
.
dialect
)
print
(
db
.
get_usable_table_names
(
)
)
print
(
db
.
run
(
"SELECT * FROM Artist LIMIT 10;"
)
)
API Reference:
SQLDatabase


sqlite
['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']
[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]


Query checker
​


Perhaps the simplest strategy is to ask the model itself to check the original query for common mistakes. Suppose we have the following SQL query chain:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


from
 langchain
.
chains 
import
 create_sql_query_chain
chain 
=
 create_sql_query_chain
(
llm
,
 db
)
API Reference:
create_sql_query_chain


And we want to validate its outputs. We can do so by extending the chain with a second prompt and model call:


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
system 
=
 
"""Double check the user's {dialect} query for common mistakes, including:
- Using NOT IN with NULL values
- Using UNION when UNION ALL should have been used
- Using BETWEEN for exclusive ranges
- Data type mismatch in predicates
- Properly quoting identifiers
- Using the correct number of arguments for functions
- Casting to the correct data type
- Using the proper columns for joins
If there are any of the above mistakes, rewrite the query.
If there are no mistakes, just reproduce the original query with no further commentary.
Output the final SQL query only."""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 system
)
,
 
(
"human"
,
 
"{query}"
)
]
)
.
partial
(
dialect
=
db
.
dialect
)
validation_chain 
=
 prompt 
|
 llm 
|
 StrOutputParser
(
)
full_chain 
=
 
{
"query"
:
 chain
}
 
|
 validation_chain
API Reference:
StrOutputParser
 | 
ChatPromptTemplate


query 
=
 full_chain
.
invoke
(
    
{
        
"question"
:
 
"What's the average Invoice from an American customer whose Fax is missing since 2003 but before 2010"
    
}
)
print
(
query
)


SELECT AVG(i.Total) AS AverageInvoice
FROM Invoice i
JOIN Customer c ON i.CustomerId = c.CustomerId
WHERE c.Country = 'USA'
AND c.Fax IS NULL
AND i.InvoiceDate >= '2003-01-01' 
AND i.InvoiceDate < '2010-01-01'


Note how we can see both steps of the chain in the 
Langsmith trace
.


db
.
run
(
query
)


'[(6.632999999999998,)]'


The obvious downside of this approach is that we need to make two model calls instead of one to generate our query. To get around this we can try to perform the query generation and query check in a single model invocation:


system 
=
 
"""You are a {dialect} expert. Given an input question, create a syntactically correct {dialect} query to run.
Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per {dialect}. You can order the results to return the most informative data in the database.
Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (") to denote them as delimited identifiers.
Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.
Pay attention to use date('now') function to get the current date, if the question involves "today".
Only use the following tables:
{table_info}
Write an initial draft of the query. Then double check the {dialect} query for common mistakes, including:
- Using NOT IN with NULL values
- Using UNION when UNION ALL should have been used
- Using BETWEEN for exclusive ranges
- Data type mismatch in predicates
- Properly quoting identifiers
- Using the correct number of arguments for functions
- Casting to the correct data type
- Using the proper columns for joins
Use format:
First draft: <<FIRST_DRAFT_QUERY>>
Final answer: <<FINAL_ANSWER_QUERY>>
"""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 system
)
,
 
(
"human"
,
 
"{input}"
)
]
)
.
partial
(
dialect
=
db
.
dialect
)
def
 
parse_final_answer
(
output
:
 
str
)
 
-
>
 
str
:
    
return
 output
.
split
(
"Final answer: "
)
[
1
]
chain 
=
 create_sql_query_chain
(
llm
,
 db
,
 prompt
=
prompt
)
 
|
 parse_final_answer
prompt
.
pretty_print
(
)


================================[1m System Message [0m================================
You are a [33;1m[1;3m{dialect}[0m expert. Given an input question, create a syntactically correct [33;1m[1;3m{dialect}[0m query to run.
Unless the user specifies in the question a specific number of examples to obtain, query for at most [33;1m[1;3m{top_k}[0m results using the LIMIT clause as per [33;1m[1;3m{dialect}[0m. You can order the results to return the most informative data in the database.
Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (") to denote them as delimited identifiers.
Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.
Pay attention to use date('now') function to get the current date, if the question involves "today".
Only use the following tables:
[33;1m[1;3m{table_info}[0m
Write an initial draft of the query. Then double check the [33;1m[1;3m{dialect}[0m query for common mistakes, including:
- Using NOT IN with NULL values
- Using UNION when UNION ALL should have been used
- Using BETWEEN for exclusive ranges
- Data type mismatch in predicates
- Properly quoting identifiers
- Using the correct number of arguments for functions
- Casting to the correct data type
- Using the proper columns for joins
Use format:
First draft: <<FIRST_DRAFT_QUERY>>
Final answer: <<FINAL_ANSWER_QUERY>>
================================[1m Human Message [0m=================================
[33;1m[1;3m{input}[0m


query 
=
 chain
.
invoke
(
    
{
        
"question"
:
 
"What's the average Invoice from an American customer whose Fax is missing since 2003 but before 2010"
    
}
)
print
(
query
)


SELECT AVG(i."Total") AS "AverageInvoice"
FROM "Invoice" i
JOIN "Customer" c ON i."CustomerId" = c."CustomerId"
WHERE c."Country" = 'USA'
AND c."Fax" IS NULL
AND i."InvoiceDate" BETWEEN '2003-01-01' AND '2010-01-01';


db
.
run
(
query
)


'[(6.632999999999998,)]'


Human-in-the-loop
​


In some cases our data is sensitive enough that we never want to execute a SQL query without a human approving it first. Head to the 
Tool use: Human-in-the-loop
 page to learn how to add a human-in-the-loop to any tool, chain or agent.


Error handling
​


At some point, the model will make a mistake and craft an invalid SQL query. Or an issue will arise with our database. Or the model API will go down. We'll want to add some error handling behavior to our chains and agents so that we fail gracefully in these situations, and perhaps even automatically recover. To learn about error handling with tools, head to the 
Tool use: Error handling
 page.
Edit this page
Was this page helpful?
Previous
How to better prompt when doing SQL question-answering
Next
How to stream runnables
Setup
Query checker
Human-in-the-loop
Error handling


--- docs_how_to_streaming.txt ---

How-to guides
How to stream runnables
On this page
How to stream runnables


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


LangChain Expression Language


Output parsers




Streaming is critical in making applications based on LLMs feel responsive to end-users.


Important LangChain primitives like 
chat models
, 
output parsers
, 
prompts
, 
retrievers
, and 
agents
 implement the LangChain 
Runnable Interface
.


This interface provides two general approaches to stream content:




sync 
stream
 and async 
astream
: a 
default implementation
 of streaming that streams the 
final output
 from the chain.


async 
astream_events
 and async 
astream_log
: these provide a way to stream both 
intermediate steps
 and 
final output
 from the chain.




Let's take a look at both approaches, and try to understand how to use them.


info
For a higher-level overview of streaming techniques in LangChain, see 
this section of the conceptual guide
.


Using Stream
​


All 
Runnable
 objects implement a sync method called 
stream
 and an async variant called 
astream
.


These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.


Streaming is only possible if all steps in the program know how to process an 
input stream
; i.e., process an input chunk one at a time, and yield a corresponding output chunk.


The complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.


The best place to start exploring streaming is with the single most important components in LLMs apps-- the LLMs themselves!


LLMs and Chat Models
​


Large language models and their chat variants are the primary bottleneck in LLM based apps.


Large language models can take 
several seconds
 to generate a complete response to a query. This is far slower than the 
~200-300 ms
 threshold at which an application feels responsive to an end user.


The key strategy to make the application feel more responsive is to show intermediate progress; viz., to stream the output from the model 
token by token
.


We will show examples of streaming using a chat model. Choose one from the options below:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Let's start with the sync 
stream
 API:


chunks 
=
 
[
]
for
 chunk 
in
 model
.
stream
(
"what color is the sky?"
)
:
    chunks
.
append
(
chunk
)
    
print
(
chunk
.
content
,
 end
=
"|"
,
 flush
=
True
)


The| sky| appears| blue| during| the| day|.|


Alternatively, if you're working in an async environment, you may consider using the async 
astream
 API:


chunks 
=
 
[
]
async
 
for
 chunk 
in
 model
.
astream
(
"what color is the sky?"
)
:
    chunks
.
append
(
chunk
)
    
print
(
chunk
.
content
,
 end
=
"|"
,
 flush
=
True
)


The| sky| appears| blue| during| the| day|.|


Let's inspect one of the chunks


chunks
[
0
]


AIMessageChunk(content='The', id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')


We got back something called an 
AIMessageChunk
. This chunk represents a part of an 
AIMessage
.


Message chunks are additive by design -- one can simply add them up to get the state of the response so far!


chunks
[
0
]
 
+
 chunks
[
1
]
 
+
 chunks
[
2
]
 
+
 chunks
[
3
]
 
+
 chunks
[
4
]


AIMessageChunk(content='The sky appears blue during', id='run-b36bea64-5511-4d7a-b6a3-a07b3db0c8e7')


Chains
​


Virtually all LLM applications involve more steps than just a call to a language model.


Let's build a simple chain using 
LangChain Expression Language
 (
LCEL
) that combines a prompt, model and a parser and verify that streaming works.


We will use 
StrOutputParser
 to parse the output from the model. This is a simple parser that extracts the 
content
 field from an 
AIMessageChunk
, giving us the 
token
 returned by the model.


tip
LCEL is a 
declarative
 way to specify a "program" by chainining together different LangChain primitives. Chains created using LCEL benefit from an automatic implementation of 
stream
 and 
astream
 allowing streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable interface.


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
prompt 
=
 ChatPromptTemplate
.
from_template
(
"tell me a joke about {topic}"
)
parser 
=
 StrOutputParser
(
)
chain 
=
 prompt 
|
 model 
|
 parser
async
 
for
 chunk 
in
 chain
.
astream
(
{
"topic"
:
 
"parrot"
}
)
:
    
print
(
chunk
,
 end
=
"|"
,
 flush
=
True
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate


Here|'s| a| joke| about| a| par|rot|:|
A man| goes| to| a| pet| shop| to| buy| a| par|rot|.| The| shop| owner| shows| him| two| stunning| pa|rr|ots| with| beautiful| pl|um|age|.|
"|There|'s| a| talking| par|rot| an|d a| non|-|talking| par|rot|,"| the| owner| says|.| "|The| talking| par|rot| costs| $|100|,| an|d the| non|-|talking| par|rot| is| $|20|."|
The| man| says|,| "|I|'ll| take| the| non|-|talking| par|rot| at| $|20|."|
He| pays| an|d leaves| with| the| par|rot|.| As| he|'s| walking| down| the| street|,| the| par|rot| looks| up| at| him| an|d says|,| "|You| know|,| you| really| are| a| stupi|d man|!"|
The| man| is| stun|ne|d an|d looks| at| the| par|rot| in| dis|bel|ief|.| The| par|rot| continues|,| "|Yes|,| you| got| r|ippe|d off| big| time|!| I| can| talk| just| as| well| as| that| other| par|rot|,| an|d you| only| pai|d $|20| |for| me|!"|


Note that we're getting streaming output even though we're using 
parser
 at the end of the chain above. The 
parser
 operates on each streaming chunk individidually. Many of the 
LCEL primitives
 also support this kind of transform-style passthrough streaming, which can be very convenient when constructing apps.


Custom functions can be 
designed to return generators
, which are able to operate on streams.


Certain runnables, like 
prompt templates
 and 
chat models
, cannot process individual chunks and instead aggregate all previous steps. Such runnables can interrupt the streaming process.


note
The LangChain Expression language allows you to separate the construction of a chain from the mode in which it is used (e.g., sync/async, batch/streaming etc.). If this is not relevant to what you're building, you can also rely on a standard 
imperative
 programming approach by
caling 
invoke
, 
batch
 or 
stream
 on each component individually, assigning the results to variables and then using them downstream as you see fit.


Working with Input Streams
​


What if you wanted to stream JSON from the output as it was being generated?


If you were to rely on 
json.loads
 to parse the partial json, the parsing would fail as the partial json wouldn't be valid json.


You'd likely be at a complete loss of what to do and claim that it wasn't possible to stream JSON.


Well, turns out there is a way to do it -- the parser needs to operate on the 
input stream
, and attempt to "auto-complete" the partial json into a valid state.


Let's see such a parser in action to understand what this means.


from
 langchain_core
.
output_parsers 
import
 JsonOutputParser
chain 
=
 
(
    model 
|
 JsonOutputParser
(
)
)
  
# Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models
async
 
for
 text 
in
 chain
.
astream
(
    
"output a list of the countries france, spain and japan and their populations in JSON format. "
    
'Use a dict with an outer key of "countries" which contains a list of countries. '
    
"Each country should have the key `name` and `population`"
)
:
    
print
(
text
,
 flush
=
True
)
API Reference:
JsonOutputParser


{}
{'countries': []}
{'countries': [{}]}
{'countries': [{'name': ''}]}
{'countries': [{'name': 'France'}]}
{'countries': [{'name': 'France', 'population': 67}]}
{'countries': [{'name': 'France', 'population': 67413}]}
{'countries': [{'name': 'France', 'population': 67413000}]}
{'countries': [{'name': 'France', 'population': 67413000}, {}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': ''}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': ''}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan'}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125584}]}
{'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351567}, {'name': 'Japan', 'population': 125584000}]}


Now, let's 
break
 streaming. We'll use the previous example and append an extraction function at the end that extracts the country names from the finalized JSON.


warning
Any steps in the chain that operate on 
finalized inputs
 rather than on 
input streams
 can break streaming functionality via 
stream
 or 
astream
.


tip
Later, we will discuss the 
astream_events
 API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on 
finalized inputs
.


from
 langchain_core
.
output_parsers 
import
 
(
    JsonOutputParser
,
)
# A function that operates on finalized inputs
# rather than on an input_stream
def
 
_extract_country_names
(
inputs
)
:
    
"""A function that does not operates on input streams and breaks streaming."""
    
if
 
not
 
isinstance
(
inputs
,
 
dict
)
:
        
return
 
""
    
if
 
"countries"
 
not
 
in
 inputs
:
        
return
 
""
    countries 
=
 inputs
[
"countries"
]
    
if
 
not
 
isinstance
(
countries
,
 
list
)
:
        
return
 
""
    country_names 
=
 
[
        country
.
get
(
"name"
)
 
for
 country 
in
 countries 
if
 
isinstance
(
country
,
 
dict
)
    
]
    
return
 country_names
chain 
=
 model 
|
 JsonOutputParser
(
)
 
|
 _extract_country_names
async
 
for
 text 
in
 chain
.
astream
(
    
"output a list of the countries france, spain and japan and their populations in JSON format. "
    
'Use a dict with an outer key of "countries" which contains a list of countries. '
    
"Each country should have the key `name` and `population`"
)
:
    
print
(
text
,
 end
=
"|"
,
 flush
=
True
)
API Reference:
JsonOutputParser


['France', 'Spain', 'Japan']|


Generator Functions
​


Let's fix the streaming using a generator function that can operate on the 
input stream
.


tip
A generator function (a function that uses 
yield
) allows writing code that operates on 
input streams


from
 langchain_core
.
output_parsers 
import
 JsonOutputParser
async
 
def
 
_extract_country_names_streaming
(
input_stream
)
:
    
"""A function that operates on input streams."""
    country_names_so_far 
=
 
set
(
)
    
async
 
for
 
input
 
in
 input_stream
:
        
if
 
not
 
isinstance
(
input
,
 
dict
)
:
            
continue
        
if
 
"countries"
 
not
 
in
 
input
:
            
continue
        countries 
=
 
input
[
"countries"
]
        
if
 
not
 
isinstance
(
countries
,
 
list
)
:
            
continue
        
for
 country 
in
 countries
:
            name 
=
 country
.
get
(
"name"
)
            
if
 
not
 name
:
                
continue
            
if
 name 
not
 
in
 country_names_so_far
:
                
yield
 name
                country_names_so_far
.
add
(
name
)
chain 
=
 model 
|
 JsonOutputParser
(
)
 
|
 _extract_country_names_streaming
async
 
for
 text 
in
 chain
.
astream
(
    
"output a list of the countries france, spain and japan and their populations in JSON format. "
    
'Use a dict with an outer key of "countries" which contains a list of countries. '
    
"Each country should have the key `name` and `population`"
,
)
:
    
print
(
text
,
 end
=
"|"
,
 flush
=
True
)
API Reference:
JsonOutputParser


France|Spain|Japan|


note
Because the code above is relying on JSON auto-completion, you may see partial names of countries (e.g., 
Sp
 and 
Spain
), which is not what one would want for an extraction result!
We're focusing on streaming concepts, not necessarily the results of the chains.


Non-streaming components
​


Some built-in components like Retrievers do not offer any 
streaming
. What happens if we try to 
stream
 them? 🤨


from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 OpenAIEmbeddings
template 
=
 
"""Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt 
=
 ChatPromptTemplate
.
from_template
(
template
)
vectorstore 
=
 FAISS
.
from_texts
(
    
[
"harrison worked at kensho"
,
 
"harrison likes spicy food"
]
,
    embedding
=
OpenAIEmbeddings
(
)
,
)
retriever 
=
 vectorstore
.
as_retriever
(
)
chunks 
=
 
[
chunk 
for
 chunk 
in
 retriever
.
stream
(
"where did harrison work?"
)
]
chunks
API Reference:
FAISS
 | 
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
OpenAIEmbeddings


[[Document(page_content='harrison worked at kensho'),
  Document(page_content='harrison likes spicy food')]]


Stream just yielded the final result from that component.


This is OK 🥹! Not all components have to implement streaming -- in some cases streaming is either unnecessary, difficult or just doesn't make sense.


tip
An LCEL chain constructed using non-streaming components, will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.


retrieval_chain 
=
 
(
    
{
        
"context"
:
 retriever
.
with_config
(
run_name
=
"Docs"
)
,
        
"question"
:
 RunnablePassthrough
(
)
,
    
}
    
|
 prompt
    
|
 model
    
|
 StrOutputParser
(
)
)


for
 chunk 
in
 retrieval_chain
.
stream
(
    
"Where did harrison work? "
 
"Write 3 made up sentences about this place."
)
:
    
print
(
chunk
,
 end
=
"|"
,
 flush
=
True
)


Base|d on| the| given| context|,| Harrison| worke|d at| K|ens|ho|.|
Here| are| |3| |made| up| sentences| about| this| place|:|
1|.| K|ens|ho| was| a| cutting|-|edge| technology| company| known| for| its| innovative| solutions| in| artificial| intelligence| an|d data| analytics|.|
2|.| The| modern| office| space| at| K|ens|ho| feature|d open| floor| plans|,| collaborative| work|sp|aces|,| an|d a| vib|rant| atmosphere| that| fos|tere|d creativity| an|d team|work|.|
3|.| With| its| prime| location| in| the| heart| of| the| city|,| K|ens|ho| attracte|d top| talent| from| aroun|d the| worl|d,| creating| a| diverse| an|d dynamic| work| environment|.|


Now that we've seen how 
stream
 and 
astream
 work, let's venture into the world of streaming events. 🏞️


Using Stream Events
​


Event Streaming is a 
beta
 API. This API may change a bit based on feedback.


note
This guide demonstrates the 
V2
 API and requires langchain-core >= 0.2. For the 
V1
 API compatible with older versions of LangChain, see 
here
.


import
 langchain_core
langchain_core
.
__version__


For the 
astream_events
 API to work properly:




Use 
async
 throughout the code to the extent possible (e.g., async tools etc)


Propagate callbacks if defining custom functions / runnables


Whenever using runnables without LCEL, make sure to call 
.astream()
 on LLMs rather than 
.ainvoke
 to force the LLM to stream tokens.


Let us know if anything doesn't work as expected! :)




Event Reference
​


Below is a reference table that shows some events that might be emitted by the various Runnable objects.


note
When streaming is implemented properly, the inputs to a runnable will not be known until after the input stream has been entirely consumed. This means that 
inputs
 will often be included only for 
end
 events and rather than for 
start
 events.


event
name
chunk
input
output
on_chat_model_start
[model name]
{"messages": [[SystemMessage, HumanMessage]]}
on_chat_model_stream
[model name]
AIMessageChunk(content="hello")
on_chat_model_end
[model name]
{"messages": [[SystemMessage, HumanMessage]]}
AIMessageChunk(content="hello world")
on_llm_start
[model name]
{'input': 'hello'}
on_llm_stream
[model name]
'Hello'
on_llm_end
[model name]
'Hello human!'
on_chain_start
format_docs
on_chain_stream
format_docs
"hello world!, goodbye world!"
on_chain_end
format_docs
[Document(...)]
"hello world!, goodbye world!"
on_tool_start
some_tool
{"x": 1, "y": "2"}
on_tool_end
some_tool
{"x": 1, "y": "2"}
on_retriever_start
[retriever name]
{"query": "hello"}
on_retriever_end
[retriever name]
{"query": "hello"}
[Document(...), ..]
on_prompt_start
[template_name]
{"question": "hello"}
on_prompt_end
[template_name]
{"question": "hello"}
ChatPromptValue(messages: [SystemMessage, ...])


Chat Model
​


Let's start off by looking at the events produced by a chat model.


events 
=
 
[
]
async
 
for
 event 
in
 model
.
astream_events
(
"hello"
)
:
    events
.
append
(
event
)


note
For 
langchain-core<0.3.37
, set the 
version
 kwarg explicitly (e.g., 
model.astream_events("hello", version="v2")
).


Let's take a look at the few of the start event and a few of the end events.


events
[
:
3
]


[{'event': 'on_chat_model_start',
  'data': {'input': 'hello'},
  'name': 'ChatAnthropic',
  'tags': [],
  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',
  'metadata': {'ls_provider': 'anthropic',
   'ls_model_name': 'claude-3-sonnet-20240229',
   'ls_model_type': 'chat',
   'ls_temperature': 0.0,
   'ls_max_tokens': 1024},
  'parent_ids': []},
 {'event': 'on_chat_model_stream',
  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',
  'name': 'ChatAnthropic',
  'tags': [],
  'metadata': {'ls_provider': 'anthropic',
   'ls_model_name': 'claude-3-sonnet-20240229',
   'ls_model_type': 'chat',
   'ls_temperature': 0.0,
   'ls_max_tokens': 1024},
  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 8, 'output_tokens': 4, 'total_tokens': 12, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},
  'parent_ids': []},
 {'event': 'on_chat_model_stream',
  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',
  'name': 'ChatAnthropic',
  'tags': [],
  'metadata': {'ls_provider': 'anthropic',
   'ls_model_name': 'claude-3-sonnet-20240229',
   'ls_model_type': 'chat',
   'ls_temperature': 0.0,
   'ls_max_tokens': 1024},
  'data': {'chunk': AIMessageChunk(content='Hello! How can', additional_kwargs={}, response_metadata={}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66')},
  'parent_ids': []}]


events
[
-
2
:
]


[{'event': 'on_chat_model_stream',
  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',
  'name': 'ChatAnthropic',
  'tags': [],
  'metadata': {'ls_provider': 'anthropic',
   'ls_model_name': 'claude-3-sonnet-20240229',
   'ls_model_type': 'chat',
   'ls_temperature': 0.0,
   'ls_max_tokens': 1024},
  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 0, 'output_tokens': 12, 'total_tokens': 12, 'input_token_details': {}})},
  'parent_ids': []},
 {'event': 'on_chat_model_end',
  'data': {'output': AIMessageChunk(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-b18d016d-8b9b-49e7-a555-44db498fcf66', usage_metadata={'input_tokens': 8, 'output_tokens': 16, 'total_tokens': 24, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})},
  'run_id': 'b18d016d-8b9b-49e7-a555-44db498fcf66',
  'name': 'ChatAnthropic',
  'tags': [],
  'metadata': {'ls_provider': 'anthropic',
   'ls_model_name': 'claude-3-sonnet-20240229',
   'ls_model_type': 'chat',
   'ls_temperature': 0.0,
   'ls_max_tokens': 1024},
  'parent_ids': []}]


Chain
​


Let's revisit the example chain that parsed streaming JSON to explore the streaming events API.


chain 
=
 
(
    model 
|
 JsonOutputParser
(
)
)
  
# Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models
events 
=
 
[
    event
    
async
 
for
 event 
in
 chain
.
astream_events
(
        
"output a list of the countries france, spain and japan and their populations in JSON format. "
        
'Use a dict with an outer key of "countries" which contains a list of countries. '
        
"Each country should have the key `name` and `population`"
,
    
)
]


If you examine at the first few events, you'll notice that there are 
3
 different start events rather than 
2
 start events.


The three start events correspond to:




The chain (model + parser)


The model


The parser




events
[
:
3
]


[{'event': 'on_chain_start',
  'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`'},
  'name': 'RunnableSequence',
  'tags': [],
  'run_id': '4765006b-16e2-4b1d-a523-edd9fd64cb92',
  'metadata': {}},
 {'event': 'on_chat_model_start',
  'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`')]]}},
  'name': 'ChatAnthropic',
  'tags': ['seq:step:1'],
  'run_id': '0320c234-7b52-4a14-ae4e-5f100949e589',
  'metadata': {}},
 {'event': 'on_chat_model_stream',
  'data': {'chunk': AIMessageChunk(content='{', id='run-0320c234-7b52-4a14-ae4e-5f100949e589')},
  'run_id': '0320c234-7b52-4a14-ae4e-5f100949e589',
  'name': 'ChatAnthropic',
  'tags': ['seq:step:1'],
  'metadata': {}}]


What do you think you'd see if you looked at the last 3 events? what about the middle?


Let's use this API to take output the stream events from the model and the parser. We're ignoring start events, end events and events from the chain.


num_events 
=
 
0
async
 
for
 event 
in
 chain
.
astream_events
(
    
"output a list of the countries france, spain and japan and their populations in JSON format. "
    
'Use a dict with an outer key of "countries" which contains a list of countries. '
    
"Each country should have the key `name` and `population`"
,
)
:
    kind 
=
 event
[
"event"
]
    
if
 kind 
==
 
"on_chat_model_stream"
:
        
print
(
            
f"Chat model chunk: 
{
repr
(
event
[
'data'
]
[
'chunk'
]
.
content
)
}
"
,
            flush
=
True
,
        
)
    
if
 kind 
==
 
"on_parser_stream"
:
        
print
(
f"Parser chunk: 
{
event
[
'data'
]
[
'chunk'
]
}
"
,
 flush
=
True
)
    num_events 
+=
 
1
    
if
 num_events 
>
 
30
:
        
# Truncate the output
        
print
(
"..."
)
        
break


Chat model chunk: ''
Chat model chunk: '{'
Parser chunk: {}
Chat model chunk: '\n  "countries'
Chat model chunk: '": [\n    '
Parser chunk: {'countries': []}
Chat model chunk: '{\n      "'
Parser chunk: {'countries': [{}]}
Chat model chunk: 'name": "France'
Parser chunk: {'countries': [{'name': 'France'}]}
Chat model chunk: '",\n      "'
Chat model chunk: 'population": 67'
Parser chunk: {'countries': [{'name': 'France', 'population': 67}]}
Chat model chunk: '413'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413}]}
Chat model chunk: '000\n    },'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}]}
Chat model chunk: '\n    {'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {}]}
Chat model chunk: '\n      "name":'
...


Because both the model and the parser support streaming, we see streaming events from both components in real time! Kind of cool isn't it? 🦜


Filtering Events
​


Because this API produces so many events, it is useful to be able to filter on events.


You can filter by either component 
name
, component 
tags
 or component 
type
.


By Name
​


chain 
=
 model
.
with_config
(
{
"run_name"
:
 
"model"
}
)
 
|
 JsonOutputParser
(
)
.
with_config
(
    
{
"run_name"
:
 
"my_parser"
}
)
max_events 
=
 
0
async
 
for
 event 
in
 chain
.
astream_events
(
    
"output a list of the countries france, spain and japan and their populations in JSON format. "
    
'Use a dict with an outer key of "countries" which contains a list of countries. '
    
"Each country should have the key `name` and `population`"
,
    include_names
=
[
"my_parser"
]
,
)
:
    
print
(
event
)
    max_events 
+=
 
1
    
if
 max_events 
>
 
10
:
        
# Truncate output
        
print
(
"..."
)
        
break


{'event': 'on_parser_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'my_parser', 'tags': ['seq:step:2'], 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'metadata': {}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
{'event': 'on_parser_stream', 'run_id': '37ee9e85-481c-415e-863b-c9e132d24948', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}}, 'parent_ids': ['5a0bc625-09fd-4bdf-9932-54909a9a8c29']}
...


By Type
​


chain 
=
 model
.
with_config
(
{
"run_name"
:
 
"model"
}
)
 
|
 JsonOutputParser
(
)
.
with_config
(
    
{
"run_name"
:
 
"my_parser"
}
)
max_events 
=
 
0
async
 
for
 event 
in
 chain
.
astream_events
(
    
'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`'
,
    include_types
=
[
"chat_model"
]
,
)
:
    
print
(
event
)
    max_events 
+=
 
1
    
if
 max_events 
>
 
10
:
        
# Truncate output
        
print
(
"..."
)
        
break


{'event': 'on_chat_model_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'model', 'tags': ['seq:step:1'], 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c', usage_metadata={'input_tokens': 56, 'output_tokens': 1, 'total_tokens': 57, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\n  "countries', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='": [\n    ', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{\n      "', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='name": "France', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='",\n      "', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='population": 67', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='413', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='000\n    },', additional_kwargs={}, response_metadata={}, id='run-156c3e40-82fb-49ff-8e41-9e998061be8c')}, 'run_id': '156c3e40-82fb-49ff-8e41-9e998061be8c', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['7b927055-bc1b-4b50-a34c-10d3cfcb3899']}
...


By Tags
​


caution
Tags are inherited by child components of a given runnable.
If you're using tags to filter, make sure that this is what you want.


chain 
=
 
(
model 
|
 JsonOutputParser
(
)
)
.
with_config
(
{
"tags"
:
 
[
"my_chain"
]
}
)
max_events 
=
 
0
async
 
for
 event 
in
 chain
.
astream_events
(
    
'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`'
,
    include_tags
=
[
"my_chain"
]
,
)
:
    
print
(
event
)
    max_events 
+=
 
1
    
if
 max_events 
>
 
10
:
        
# Truncate output
        
print
(
"..."
)
        
break


{'event': 'on_chain_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'RunnableSequence', 'tags': ['my_chain'], 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'metadata': {}, 'parent_ids': []}
{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}}, 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b', usage_metadata={'input_tokens': 56, 'output_tokens': 1, 'total_tokens': 57, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_parser_start', 'data': {}, 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'metadata': {}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_parser_stream', 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_chain_stream', 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': []}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\n  "countries', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='": [\n    ', additional_kwargs={}, response_metadata={}, id='run-8222e8a1-d978-4f30-87fc-b2dba838774b')}, 'run_id': '8222e8a1-d978-4f30-87fc-b2dba838774b', 'name': 'ChatAnthropic', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-sonnet-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_parser_stream', 'run_id': '75604c84-e1e6-494a-8b2a-950f45d932e8', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['58d1302e-36ce-4df7-a3cb-47cb73d57e44']}
{'event': 'on_chain_stream', 'run_id': '58d1302e-36ce-4df7-a3cb-47cb73d57e44', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': []}
...


Non-streaming components
​


Remember how some components don't stream well because they don't operate on 
input streams
?


While such components can break streaming of the final output when using 
astream
, 
astream_events
 will still yield streaming events from intermediate steps that support streaming!


# Function that does not support streaming.
# It operates on the finalizes inputs rather than
# operating on the input stream.
def
 
_extract_country_names
(
inputs
)
:
    
"""A function that does not operates on input streams and breaks streaming."""
    
if
 
not
 
isinstance
(
inputs
,
 
dict
)
:
        
return
 
""
    
if
 
"countries"
 
not
 
in
 inputs
:
        
return
 
""
    countries 
=
 inputs
[
"countries"
]
    
if
 
not
 
isinstance
(
countries
,
 
list
)
:
        
return
 
""
    country_names 
=
 
[
        country
.
get
(
"name"
)
 
for
 country 
in
 countries 
if
 
isinstance
(
country
,
 
dict
)
    
]
    
return
 country_names
chain 
=
 
(
    model 
|
 JsonOutputParser
(
)
 
|
 _extract_country_names
)
  
# This parser only works with OpenAI right now


As expected, the 
astream
 API doesn't work correctly because 
_extract_country_names
 doesn't operate on streams.


async
 
for
 chunk 
in
 chain
.
astream
(
    
"output a list of the countries france, spain and japan and their populations in JSON format. "
    
'Use a dict with an outer key of "countries" which contains a list of countries. '
    
"Each country should have the key `name` and `population`"
,
)
:
    
print
(
chunk
,
 flush
=
True
)


['France', 'Spain', 'Japan']


Now, let's confirm that with astream_events we're still seeing streaming output from the model and the parser.


num_events 
=
 
0
async
 
for
 event 
in
 chain
.
astream_events
(
    
"output a list of the countries france, spain and japan and their populations in JSON format. "
    
'Use a dict with an outer key of "countries" which contains a list of countries. '
    
"Each country should have the key `name` and `population`"
,
)
:
    kind 
=
 event
[
"event"
]
    
if
 kind 
==
 
"on_chat_model_stream"
:
        
print
(
            
f"Chat model chunk: 
{
repr
(
event
[
'data'
]
[
'chunk'
]
.
content
)
}
"
,
            flush
=
True
,
        
)
    
if
 kind 
==
 
"on_parser_stream"
:
        
print
(
f"Parser chunk: 
{
event
[
'data'
]
[
'chunk'
]
}
"
,
 flush
=
True
)
    num_events 
+=
 
1
    
if
 num_events 
>
 
30
:
        
# Truncate the output
        
print
(
"..."
)
        
break


Chat model chunk: ''
Chat model chunk: '{'
Parser chunk: {}
Chat model chunk: '\n  "countries'
Chat model chunk: '": [\n    '
Parser chunk: {'countries': []}
Chat model chunk: '{\n      "'
Parser chunk: {'countries': [{}]}
Chat model chunk: 'name": "France'
Parser chunk: {'countries': [{'name': 'France'}]}
Chat model chunk: '",\n      "'
Chat model chunk: 'population": 67'
Parser chunk: {'countries': [{'name': 'France', 'population': 67}]}
Chat model chunk: '413'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413}]}
Chat model chunk: '000\n    },'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}]}
Chat model chunk: '\n    {'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {}]}
Chat model chunk: '\n      "name":'
Chat model chunk: ' "Spain",'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain'}]}
Chat model chunk: '\n      "population":'
Chat model chunk: ' 47'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47}]}
Chat model chunk: '351'
Parser chunk: {'countries': [{'name': 'France', 'population': 67413000}, {'name': 'Spain', 'population': 47351}]}
...


Propagating Callbacks
​


caution
If you're using invoking runnables inside your tools, you need to propagate callbacks to the runnable; otherwise, no stream events will be generated.


note
When using 
RunnableLambdas
 or 
@chain
 decorator, callbacks are propagated automatically behind the scenes.


from
 langchain_core
.
runnables 
import
 RunnableLambda
from
 langchain_core
.
tools 
import
 tool
def
 
reverse_word
(
word
:
 
str
)
:
    
return
 word
[
:
:
-
1
]
reverse_word 
=
 RunnableLambda
(
reverse_word
)
@tool
def
 
bad_tool
(
word
:
 
str
)
:
    
"""Custom tool that doesn't propagate callbacks."""
    
return
 reverse_word
.
invoke
(
word
)
async
 
for
 event 
in
 bad_tool
.
astream_events
(
"hello"
)
:
    
print
(
event
)
API Reference:
RunnableLambda
 | 
tool


{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'bad_tool', 'tags': [], 'run_id': 'ea900472-a8f7-425d-b627-facdef936ee8', 'metadata': {}}
{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '77b01284-0515-48f4-8d7c-eb27c1882f86', 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '77b01284-0515-48f4-8d7c-eb27c1882f86', 'name': 'reverse_word', 'tags': [], 'metadata': {}}
{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'ea900472-a8f7-425d-b627-facdef936ee8', 'name': 'bad_tool', 'tags': [], 'metadata': {}}


Here's a re-implementation that does propagate callbacks correctly. You'll notice that now we're getting events from the 
reverse_word
 runnable as well.


@tool
def
 
correct_tool
(
word
:
 
str
,
 callbacks
)
:
    
"""A tool that correctly propagates callbacks."""
    
return
 reverse_word
.
invoke
(
word
,
 
{
"callbacks"
:
 callbacks
}
)
async
 
for
 event 
in
 correct_tool
.
astream_events
(
"hello"
)
:
    
print
(
event
)


{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'correct_tool', 'tags': [], 'run_id': 'd5ea83b9-9278-49cc-9f1d-aa302d671040', 'metadata': {}}
{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '44dafbf4-2f87-412b-ae0e-9f71713810df', 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '44dafbf4-2f87-412b-ae0e-9f71713810df', 'name': 'reverse_word', 'tags': [], 'metadata': {}}
{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'd5ea83b9-9278-49cc-9f1d-aa302d671040', 'name': 'correct_tool', 'tags': [], 'metadata': {}}


If you're invoking runnables from within Runnable Lambdas or 
@chains
, then callbacks will be passed automatically on your behalf.


from
 langchain_core
.
runnables 
import
 RunnableLambda
async
 
def
 
reverse_and_double
(
word
:
 
str
)
:
    
return
 
await
 reverse_word
.
ainvoke
(
word
)
 
*
 
2
reverse_and_double 
=
 RunnableLambda
(
reverse_and_double
)
await
 reverse_and_double
.
ainvoke
(
"1234"
)
async
 
for
 event 
in
 reverse_and_double
.
astream_events
(
"1234"
)
:
    
print
(
event
)
API Reference:
RunnableLambda


{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'metadata': {}}
{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '5cf26fc8-840b-4642-98ed-623dda28707a', 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '5cf26fc8-840b-4642-98ed-623dda28707a', 'name': 'reverse_word', 'tags': [], 'metadata': {}}
{'event': 'on_chain_stream', 'data': {'chunk': '43214321'}, 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '03b0e6a1-3e60-42fc-8373-1e7829198d80', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}


And with the 
@chain
 decorator:


from
 langchain_core
.
runnables 
import
 chain
@chain
async
 
def
 
reverse_and_double
(
word
:
 
str
)
:
    
return
 
await
 reverse_word
.
ainvoke
(
word
)
 
*
 
2
await
 reverse_and_double
.
ainvoke
(
"1234"
)
async
 
for
 event 
in
 reverse_and_double
.
astream_events
(
"1234"
)
:
    
print
(
event
)
API Reference:
chain


{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'metadata': {}}
{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '64fc99f0-5d7d-442b-b4f5-4537129f67d1', 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '64fc99f0-5d7d-442b-b4f5-4537129f67d1', 'name': 'reverse_word', 'tags': [], 'metadata': {}}
{'event': 'on_chain_stream', 'data': {'chunk': '43214321'}, 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}
{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '1bfcaedc-f4aa-4d8e-beee-9bba6ef17008', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}}


Next steps
​


Now you've learned some ways to stream both final outputs and internal steps with LangChain.


To learn more, check out the other how-to guides in this section, or the 
conceptual guide on Langchain Expression Language
.
Edit this page
Was this page helpful?
Previous
How to do query validation as part of SQL question-answering
Next
How to stream responses from an LLM
Using Stream
LLMs and Chat Models
Chains
Working with Input Streams
Non-streaming components
Using Stream Events
Event Reference
Chat Model
Chain
Filtering Events
Non-streaming components
Propagating Callbacks
Next steps


--- docs_how_to_streaming_llm.txt ---

How-to guides
How to stream responses from an LLM
On this page
How to stream responses from an LLM


All 
LLM
s implement the 
Runnable interface
, which comes with 
default
 implementations of standard runnable methods (i.e. 
ainvoke
, 
batch
, 
abatch
, 
stream
, 
astream
, 
astream_events
).


The 
default
 streaming implementations provide an
Iterator
 (or 
AsyncIterator
 for asynchronous streaming) that yields a single value: the final output from the underlying chat model provider.


The ability to stream the output token-by-token depends on whether the provider has implemented proper streaming support.


See which 
integrations support token-by-token streaming here
.


note
The 
default
 implementation does 
not
 provide support for token-by-token streaming, but it ensures that the model can be swapped in for any other model as it supports the same standard interface.


Sync stream
​


Below we use a 
|
 to help visualize the delimiter between tokens.


from
 langchain_openai 
import
 OpenAI
llm 
=
 OpenAI
(
model
=
"gpt-3.5-turbo-instruct"
,
 temperature
=
0
,
 max_tokens
=
512
)
for
 chunk 
in
 llm
.
stream
(
"Write me a 1 verse song about sparkling water."
)
:
    
print
(
chunk
,
 end
=
"|"
,
 flush
=
True
)
API Reference:
OpenAI


|Spark|ling| water|,| oh| so clear|
|Bubbles dancing|,| without| fear|
|Refreshing| taste|,| a| pure| delight|
|Spark|ling| water|,| my| thirst|'s| delight||


Async streaming
​


Let's see how to stream in an async setting using 
astream
.


from
 langchain_openai 
import
 OpenAI
llm 
=
 OpenAI
(
model
=
"gpt-3.5-turbo-instruct"
,
 temperature
=
0
,
 max_tokens
=
512
)
async
 
for
 chunk 
in
 llm
.
astream
(
"Write me a 1 verse song about sparkling water."
)
:
    
print
(
chunk
,
 end
=
"|"
,
 flush
=
True
)
API Reference:
OpenAI


|Spark|ling| water|,| oh| so clear|
|Bubbles dancing|,| without| fear|
|Refreshing| taste|,| a| pure| delight|
|Spark|ling| water|,| my| thirst|'s| delight||


Async event streaming
​


LLMs also support the standard 
astream events
 method.


tip
astream_events
 is most useful when implementing streaming in a larger LLM application that contains multiple steps (e.g., an application that involves an 
agent
).


from
 langchain_openai 
import
 OpenAI
llm 
=
 OpenAI
(
model
=
"gpt-3.5-turbo-instruct"
,
 temperature
=
0
,
 max_tokens
=
512
)
idx 
=
 
0
async
 
for
 event 
in
 llm
.
astream_events
(
    
"Write me a 1 verse song about goldfish on the moon"
,
 version
=
"v1"
)
:
    idx 
+=
 
1
    
if
 idx 
>=
 
5
:
  
# Truncate the output
        
print
(
"...Truncated"
)
        
break
    
print
(
event
)
API Reference:
OpenAI
Edit this page
Was this page helpful?
Previous
How to stream runnables
Next
How to use a time-weighted vector store retriever
Sync stream
Async streaming
Async event streaming


--- docs_how_to_structured_output.txt ---

How-to guides
How to return structured data from a model
On this page
How to return structured data from a model


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


Function/tool calling




It is often useful to have a model return output that matches a specific 
schema
. One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.


The 
.with_structured_output()
 method
​




Supported models
You can find a 
list of models that support this method here
.


This is the easiest and most reliable way to get structured outputs. 
with_structured_output()
 is implemented for 
models that provide native APIs for structuring outputs
, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.


This method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or 
messages
 it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, 
JSON Schema
 or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.


As an example, let's get a model to generate a joke and separate the setup from the punchline:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Pydantic class
​


If we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.


from
 typing 
import
 Optional
from
 pydantic 
import
 BaseModel
,
 Field
# Pydantic
class
 
Joke
(
BaseModel
)
:
    
"""Joke to tell user."""
    setup
:
 
str
 
=
 Field
(
description
=
"The setup of the joke"
)
    punchline
:
 
str
 
=
 Field
(
description
=
"The punchline to the joke"
)
    rating
:
 Optional
[
int
]
 
=
 Field
(
        default
=
None
,
 description
=
"How funny the joke is, from 1 to 10"
    
)
structured_llm 
=
 llm
.
with_structured_output
(
Joke
)
structured_llm
.
invoke
(
"Tell me a joke about cats"
)


Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)


tip
Beyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time 
with_structured_output
 is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.


TypedDict or JSON Schema
​


If you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special 
Annotated
 syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is 
not
 filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.


Requirements


Core: 
langchain-core>=0.2.26


Typing extensions: It is highly recommended to import 
Annotated
 and 
TypedDict
 from 
typing_extensions
 instead of 
typing
 to ensure consistent behavior across Python versions.




from
 typing 
import
 Optional
from
 typing_extensions 
import
 Annotated
,
 TypedDict
# TypedDict
class
 
Joke
(
TypedDict
)
:
    
"""Joke to tell user."""
    setup
:
 Annotated
[
str
,
 
.
.
.
,
 
"The setup of the joke"
]
    
# Alternatively, we could have specified setup as:
    
# setup: str                    # no default, no description
    
# setup: Annotated[str, ...]    # no default, no description
    
# setup: Annotated[str, "foo"]  # default, no description
    punchline
:
 Annotated
[
str
,
 
.
.
.
,
 
"The punchline of the joke"
]
    rating
:
 Annotated
[
Optional
[
int
]
,
 
None
,
 
"How funny the joke is, from 1 to 10"
]
structured_llm 
=
 llm
.
with_structured_output
(
Joke
)
structured_llm
.
invoke
(
"Tell me a joke about cats"
)


{'setup': 'Why was the cat sitting on the computer?',
 'punchline': 'Because it wanted to keep an eye on the mouse!',
 'rating': 7}


Equivalently, we can pass in a 
JSON Schema
 dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.


json_schema 
=
 
{
    
"title"
:
 
"joke"
,
    
"description"
:
 
"Joke to tell user."
,
    
"type"
:
 
"object"
,
    
"properties"
:
 
{
        
"setup"
:
 
{
            
"type"
:
 
"string"
,
            
"description"
:
 
"The setup of the joke"
,
        
}
,
        
"punchline"
:
 
{
            
"type"
:
 
"string"
,
            
"description"
:
 
"The punchline to the joke"
,
        
}
,
        
"rating"
:
 
{
            
"type"
:
 
"integer"
,
            
"description"
:
 
"How funny the joke is, from 1 to 10"
,
            
"default"
:
 
None
,
        
}
,
    
}
,
    
"required"
:
 
[
"setup"
,
 
"punchline"
]
,
}
structured_llm 
=
 llm
.
with_structured_output
(
json_schema
)
structured_llm
.
invoke
(
"Tell me a joke about cats"
)


{'setup': 'Why was the cat sitting on the computer?',
 'punchline': 'Because it wanted to keep an eye on the mouse!',
 'rating': 7}


Choosing between multiple schemas
​


The simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute.


Using Pydantic
​


from
 typing 
import
 Union
class
 
Joke
(
BaseModel
)
:
    
"""Joke to tell user."""
    setup
:
 
str
 
=
 Field
(
description
=
"The setup of the joke"
)
    punchline
:
 
str
 
=
 Field
(
description
=
"The punchline to the joke"
)
    rating
:
 Optional
[
int
]
 
=
 Field
(
        default
=
None
,
 description
=
"How funny the joke is, from 1 to 10"
    
)
class
 
ConversationalResponse
(
BaseModel
)
:
    
"""Respond in a conversational manner. Be kind and helpful."""
    response
:
 
str
 
=
 Field
(
description
=
"A conversational response to the user's query"
)
class
 
FinalResponse
(
BaseModel
)
:
    final_output
:
 Union
[
Joke
,
 ConversationalResponse
]
structured_llm 
=
 llm
.
with_structured_output
(
FinalResponse
)
structured_llm
.
invoke
(
"Tell me a joke about cats"
)


FinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))


structured_llm
.
invoke
(
"How are you today?"
)


FinalResponse(final_output=ConversationalResponse(response="I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!"))


Using TypedDict
​


from
 typing 
import
 Optional
,
 Union
from
 typing_extensions 
import
 Annotated
,
 TypedDict
class
 
Joke
(
TypedDict
)
:
    
"""Joke to tell user."""
    setup
:
 Annotated
[
str
,
 
.
.
.
,
 
"The setup of the joke"
]
    punchline
:
 Annotated
[
str
,
 
.
.
.
,
 
"The punchline of the joke"
]
    rating
:
 Annotated
[
Optional
[
int
]
,
 
None
,
 
"How funny the joke is, from 1 to 10"
]
class
 
ConversationalResponse
(
TypedDict
)
:
    
"""Respond in a conversational manner. Be kind and helpful."""
    response
:
 Annotated
[
str
,
 
.
.
.
,
 
"A conversational response to the user's query"
]
class
 
FinalResponse
(
TypedDict
)
:
    final_output
:
 Union
[
Joke
,
 ConversationalResponse
]
structured_llm 
=
 llm
.
with_structured_output
(
FinalResponse
)
structured_llm
.
invoke
(
"Tell me a joke about cats"
)


{'final_output': {'setup': 'Why was the cat sitting on the computer?',
  'punchline': 'Because it wanted to keep an eye on the mouse!',
  'rating': 7}}


structured_llm
.
invoke
(
"How are you today?"
)


{'final_output': {'response': "I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with whatever you need!"}}


Responses shall be identical to the ones shown in the Pydantic example.


Alternatively, you can use tool calling directly to allow the model to choose between options, if your 
chosen model supports it
. This involves a bit more parsing and setup but in some instances leads to better performance because you don't have to use nested schemas. See 
this how-to guide
 for more details.


Streaming
​


We can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or  JSON Schema dict).


info
Note that what's yielded is already aggregated chunks, not deltas.


from
 typing_extensions 
import
 Annotated
,
 TypedDict
# TypedDict
class
 
Joke
(
TypedDict
)
:
    
"""Joke to tell user."""
    setup
:
 Annotated
[
str
,
 
.
.
.
,
 
"The setup of the joke"
]
    punchline
:
 Annotated
[
str
,
 
.
.
.
,
 
"The punchline of the joke"
]
    rating
:
 Annotated
[
Optional
[
int
]
,
 
None
,
 
"How funny the joke is, from 1 to 10"
]
structured_llm 
=
 llm
.
with_structured_output
(
Joke
)
for
 chunk 
in
 structured_llm
.
stream
(
"Tell me a joke about cats"
)
:
    
print
(
chunk
)


{}
{'setup': ''}
{'setup': 'Why'}
{'setup': 'Why was'}
{'setup': 'Why was the'}
{'setup': 'Why was the cat'}
{'setup': 'Why was the cat sitting'}
{'setup': 'Why was the cat sitting on'}
{'setup': 'Why was the cat sitting on the'}
{'setup': 'Why was the cat sitting on the computer'}
{'setup': 'Why was the cat sitting on the computer?'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': ''}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}
{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}


Few-shot prompting
​


For more complex schemas it's very useful to add few-shot examples to the prompt. This can be done in a few ways.


The simplest and most universal way is to add examples to a system message in the prompt:


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
system 
=
 
"""You are a hilarious comedian. Your specialty is knock-knock jokes. \
Return a joke which has the setup (the response to "Who's there?") and the final punchline (the response to "<setup> who?").
Here are some examples of jokes:
example_user: Tell me a joke about planes
example_assistant: {{"setup": "Why don't planes ever get tired?", "punchline": "Because they have rest wings!", "rating": 2}}
example_user: Tell me another joke about planes
example_assistant: {{"setup": "Cargo", "punchline": "Cargo 'vroom vroom', but planes go 'zoom zoom'!", "rating": 10}}
example_user: Now about caterpillars
example_assistant: {{"setup": "Caterpillar", "punchline": "Caterpillar really slow, but watch me turn into a butterfly and steal the show!", "rating": 5}}"""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
[
(
"system"
,
 system
)
,
 
(
"human"
,
 
"{input}"
)
]
)
few_shot_structured_llm 
=
 prompt 
|
 structured_llm
few_shot_structured_llm
.
invoke
(
"what's something funny about woodpeckers"
)
API Reference:
ChatPromptTemplate


{'setup': 'Woodpecker',
 'punchline': "Woodpecker you a joke, but I'm afraid it might be too 'hole-some'!",
 'rating': 7}


When the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you're using makes use of tool calling in its API reference.


from
 langchain_core
.
messages 
import
 AIMessage
,
 HumanMessage
,
 ToolMessage
examples 
=
 
[
    HumanMessage
(
"Tell me a joke about planes"
,
 name
=
"example_user"
)
,
    AIMessage
(
        
""
,
        name
=
"example_assistant"
,
        tool_calls
=
[
            
{
                
"name"
:
 
"joke"
,
                
"args"
:
 
{
                    
"setup"
:
 
"Why don't planes ever get tired?"
,
                    
"punchline"
:
 
"Because they have rest wings!"
,
                    
"rating"
:
 
2
,
                
}
,
                
"id"
:
 
"1"
,
            
}
        
]
,
    
)
,
    
# Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.
    ToolMessage
(
""
,
 tool_call_id
=
"1"
)
,
    
# Some models also expect an AIMessage to follow any ToolMessages,
    
# so you may need to add an AIMessage here.
    HumanMessage
(
"Tell me another joke about planes"
,
 name
=
"example_user"
)
,
    AIMessage
(
        
""
,
        name
=
"example_assistant"
,
        tool_calls
=
[
            
{
                
"name"
:
 
"joke"
,
                
"args"
:
 
{
                    
"setup"
:
 
"Cargo"
,
                    
"punchline"
:
 
"Cargo 'vroom vroom', but planes go 'zoom zoom'!"
,
                    
"rating"
:
 
10
,
                
}
,
                
"id"
:
 
"2"
,
            
}
        
]
,
    
)
,
    ToolMessage
(
""
,
 tool_call_id
=
"2"
)
,
    HumanMessage
(
"Now about caterpillars"
,
 name
=
"example_user"
)
,
    AIMessage
(
        
""
,
        tool_calls
=
[
            
{
                
"name"
:
 
"joke"
,
                
"args"
:
 
{
                    
"setup"
:
 
"Caterpillar"
,
                    
"punchline"
:
 
"Caterpillar really slow, but watch me turn into a butterfly and steal the show!"
,
                    
"rating"
:
 
5
,
                
}
,
                
"id"
:
 
"3"
,
            
}
        
]
,
    
)
,
    ToolMessage
(
""
,
 tool_call_id
=
"3"
)
,
]
system 
=
 
"""You are a hilarious comedian. Your specialty is knock-knock jokes. \
Return a joke which has the setup (the response to "Who's there?") \
and the final punchline (the response to "<setup> who?")."""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 system
)
,
 
(
"placeholder"
,
 
"{examples}"
)
,
 
(
"human"
,
 
"{input}"
)
]
)
few_shot_structured_llm 
=
 prompt 
|
 structured_llm
few_shot_structured_llm
.
invoke
(
{
"input"
:
 
"crocodiles"
,
 
"examples"
:
 examples
}
)
API Reference:
AIMessage
 | 
HumanMessage
 | 
ToolMessage


{'setup': 'Crocodile',
 'punchline': 'Crocodile be seeing you later, alligator!',
 'rating': 6}


For more on few shot prompting when using tool calling, see 
here
.


(Advanced) Specifying the method for structuring outputs
​


For models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the 
method=
 argument.


JSON mode
If using JSON mode you'll have to still specify the desired schema in the model prompt. The schema you pass to 
with_structured_output
 will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.
To see if the model you're using supports JSON mode, check its entry in the 
API reference
.


structured_llm 
=
 llm
.
with_structured_output
(
None
,
 method
=
"json_mode"
)
structured_llm
.
invoke
(
    
"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys"
)


{'setup': 'Why was the cat sitting on the computer?',
 'punchline': 'Because it wanted to keep an eye on the mouse!'}


(Advanced) Raw outputs
​


LLMs aren't perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing 
include_raw=True
. This changes the output format to contain the raw message output, the 
parsed
 value (if successful), and any resulting errors:


structured_llm 
=
 llm
.
with_structured_output
(
Joke
,
 include_raw
=
True
)
structured_llm
.
invoke
(
"Tell me a joke about cats"
)


{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'function': {'arguments': '{"setup":"Why was the cat sitting on the computer?","punchline":"Because it wanted to keep an eye on the mouse!","rating":7}', 'name': 'Joke'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 93, 'total_tokens': 126}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None}, id='run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0', tool_calls=[{'name': 'Joke', 'args': {'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}, 'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 33, 'total_tokens': 126}),
 'parsed': {'setup': 'Why was the cat sitting on the computer?',
  'punchline': 'Because it wanted to keep an eye on the mouse!',
  'rating': 7},
 'parsing_error': None}


Prompting and parsing model outputs directly
​


Not all models support 
.with_structured_output()
, since not all models have tool calling or JSON mode support. For such models you'll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.


Using 
PydanticOutputParser
​


The following example uses the built-in 
PydanticOutputParser
 to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding 
format_instructions
 directly to the prompt from a method on the parser:


from
 typing 
import
 List
from
 langchain_core
.
output_parsers 
import
 PydanticOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 pydantic 
import
 BaseModel
,
 Field
class
 
Person
(
BaseModel
)
:
    
"""Information about a person."""
    name
:
 
str
 
=
 Field
(
.
.
.
,
 description
=
"The name of the person"
)
    height_in_meters
:
 
float
 
=
 Field
(
        
.
.
.
,
 description
=
"The height of the person expressed in meters."
    
)
class
 
People
(
BaseModel
)
:
    
"""Identifying information about all people in a text."""
    people
:
 List
[
Person
]
# Set up a parser
parser 
=
 PydanticOutputParser
(
pydantic_object
=
People
)
# Prompt
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"Answer the user query. Wrap the output in `json` tags\n{format_instructions}"
,
        
)
,
        
(
"human"
,
 
"{query}"
)
,
    
]
)
.
partial
(
format_instructions
=
parser
.
get_format_instructions
(
)
)
API Reference:
PydanticOutputParser
 | 
ChatPromptTemplate


Let’s take a look at what information is sent to the model:


query 
=
 
"Anna is 23 years old and she is 6 feet tall"
print
(
prompt
.
invoke
(
{
"query"
:
 query
}
)
.
to_string
(
)
)


System: Answer the user query. Wrap the output in `json` tags
The output should be formatted as a JSON instance that conforms to the JSON schema below.
As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.
Here is the output schema:
\`\`\`
{"description": "Identifying information about all people in a text.", "properties": {"people": {"title": "People", "type": "array", "items": {"$ref": "#/definitions/Person"}}}, "required": ["people"], "definitions": {"Person": {"title": "Person", "description": "Information about a person.", "type": "object", "properties": {"name": {"title": "Name", "description": "The name of the person", "type": "string"}, "height_in_meters": {"title": "Height In Meters", "description": "The height of the person expressed in meters.", "type": "number"}}, "required": ["name", "height_in_meters"]}}}
\`\`\`
Human: Anna is 23 years old and she is 6 feet tall


And now let's invoke it:


chain 
=
 prompt 
|
 llm 
|
 parser
chain
.
invoke
(
{
"query"
:
 query
}
)


People(people=[Person(name='Anna', height_in_meters=1.8288)])


For a deeper dive into using output parsers with prompting techniques for structured output, see 
this guide
.


Custom Parsing
​


You can also create a custom prompt and parser with 
LangChain Expression Language (LCEL)
, using a plain function to parse the output from the model:


import
 json
import
 re
from
 typing 
import
 List
from
 langchain_core
.
messages 
import
 AIMessage
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 pydantic 
import
 BaseModel
,
 Field
class
 
Person
(
BaseModel
)
:
    
"""Information about a person."""
    name
:
 
str
 
=
 Field
(
.
.
.
,
 description
=
"The name of the person"
)
    height_in_meters
:
 
float
 
=
 Field
(
        
.
.
.
,
 description
=
"The height of the person expressed in meters."
    
)
class
 
People
(
BaseModel
)
:
    
"""Identifying information about all people in a text."""
    people
:
 List
[
Person
]
# Prompt
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"Answer the user query. Output your answer as JSON that  "
            
"matches the given schema: \`\`\`json\n{schema}\n\`\`\`. "
            
"Make sure to wrap the answer in \`\`\`json and \`\`\` tags"
,
        
)
,
        
(
"human"
,
 
"{query}"
)
,
    
]
)
.
partial
(
schema
=
People
.
schema
(
)
)
# Custom parser
def
 
extract_json
(
message
:
 AIMessage
)
 
-
>
 List
[
dict
]
:
    
"""Extracts JSON content from a string where JSON is embedded between \`\`\`json and \`\`\` tags.
    Parameters:
        text (str): The text containing the JSON content.
    Returns:
        list: A list of extracted JSON strings.
    """
    text 
=
 message
.
content
    
# Define the regular expression pattern to match JSON blocks
    pattern 
=
 
r"\`\`\`json(.*?)\`\`\`"
    
# Find all non-overlapping matches of the pattern in the string
    matches 
=
 re
.
findall
(
pattern
,
 text
,
 re
.
DOTALL
)
    
# Return the list of matched JSON strings, stripping any leading or trailing whitespace
    
try
:
        
return
 
[
json
.
loads
(
match
.
strip
(
)
)
 
for
 
match
 
in
 matches
]
    
except
 Exception
:
        
raise
 ValueError
(
f"Failed to parse: 
{
message
}
"
)
API Reference:
AIMessage
 | 
ChatPromptTemplate


Here is the prompt sent to the model:


query 
=
 
"Anna is 23 years old and she is 6 feet tall"
print
(
prompt
.
format_prompt
(
query
=
query
)
.
to_string
(
)
)


System: Answer the user query. Output your answer as JSON that  matches the given schema: \`\`\`json
{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}
\`\`\`. Make sure to wrap the answer in \`\`\`json and \`\`\` tags
Human: Anna is 23 years old and she is 6 feet tall


And here's what it looks like when we invoke it:


chain 
=
 prompt 
|
 llm 
|
 extract_json
chain
.
invoke
(
{
"query"
:
 query
}
)


[{'people': [{'name': 'Anna', 'height_in_meters': 1.8288}]}]
Edit this page
Was this page helpful?
Previous
How to route between sub-chains
Next
How to summarize text through parallelization
The 
.with_structured_output()
 method
Pydantic class
TypedDict or JSON Schema
Choosing between multiple schemas
Streaming
Few-shot prompting
(Advanced) Specifying the method for structuring outputs
(Advanced) Raw outputs
Prompting and parsing model outputs directly
Using 
PydanticOutputParser
Custom Parsing


--- docs_how_to_summarize_map_reduce.txt ---

How-to guides
How to summarize text through parallelization
On this page
How to summarize text through parallelization


LLMs can summarize and otherwise distill desired information from text, including large volumes of text. In many cases, especially when the amount of text is large compared to the size of the model's context window, it can be helpful (or necessary) to break up the summarization task into smaller components.


Map-reduce represents one class of strategies for accomplishing this. The idea is to break the text into "sub-documents", and first map each sub-document to an individual summary using an LLM. Then, we reduce or consolidate those summaries into a single global summary.


Note that the map step is typically parallelized over the input documents. This strategy is especially effective when understanding of a sub-document does not rely on preceeding context. For example, when summarizing a corpus of many, shorter documents.


LangGraph
, built on top of 
langchain-core
, supports 
map-reduce
 workflows and is well-suited to this problem:




LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;


LangGraph's 
checkpointing
 supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.


The LangGraph implementation is straightforward to modify and extend.




Below, we demonstrate how to summarize text via a map-reduce strategy.


Load chat model
​


Let's first load a chat model:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Load documents
​


First we load in our documents. We will use 
WebBaseLoader
 to load a blog post, and split the documents into smaller sub-documents.


from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
from
 langchain_text_splitters 
import
 CharacterTextSplitter
text_splitter 
=
 CharacterTextSplitter
.
from_tiktoken_encoder
(
    chunk_size
=
1000
,
 chunk_overlap
=
0
)
loader 
=
 WebBaseLoader
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
)
docs 
=
 loader
.
load
(
)
split_docs 
=
 text_splitter
.
split_documents
(
docs
)
print
(
f"Generated 
{
len
(
split_docs
)
}
 documents."
)
API Reference:
WebBaseLoader
 | 
CharacterTextSplitter


Created a chunk of size 1003, which is longer than the specified 1000
``````output
Generated 14 documents.


Create graph
​


Map step
​


Let's first define the prompt associated with the map step, and associated it with the LLM via a 
chain
:


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
map_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"human"
,
 
"Write a concise summary of the following:\\n\\n{context}"
)
]
)
map_chain 
=
 map_prompt 
|
 llm 
|
 StrOutputParser
(
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate


Reduce step
​


We also define a chain that takes the document mapping results and reduces them into a single output.


reduce_template 
=
 
"""
The following is a set of summaries:
{docs}
Take these and distill it into a final, consolidated summary
of the main themes.
"""
reduce_prompt 
=
 ChatPromptTemplate
(
[
(
"human"
,
 reduce_template
)
]
)
reduce_chain 
=
 reduce_prompt 
|
 llm 
|
 StrOutputParser
(
)


Orchestration via LangGraph
​


Below we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.


Map-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model's context window size. Here we implement a recursive "collapsing" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.


We will need to install 
langgraph
:


pip install 
-
qU langgraph


import
 operator
from
 typing 
import
 Annotated
,
 List
,
 Literal
,
 TypedDict
from
 langchain
.
chains
.
combine_documents
.
reduce
 
import
 
(
    acollapse_docs
,
    split_list_of_docs
,
)
from
 langchain_core
.
documents 
import
 Document
from
 langgraph
.
constants 
import
 Send
from
 langgraph
.
graph 
import
 END
,
 START
,
 StateGraph
token_max 
=
 
1000
def
 
length_function
(
documents
:
 List
[
Document
]
)
 
-
>
 
int
:
    
"""Get number of tokens for input contents."""
    
return
 
sum
(
llm
.
get_num_tokens
(
doc
.
page_content
)
 
for
 doc 
in
 documents
)
# This will be the overall state of the main graph.
# It will contain the input document contents, corresponding
# summaries, and a final summary.
class
 
OverallState
(
TypedDict
)
:
    
# Notice here we use the operator.add
    
# This is because we want combine all the summaries we generate
    
# from individual nodes back into one list - this is essentially
    
# the "reduce" part
    contents
:
 List
[
str
]
    summaries
:
 Annotated
[
list
,
 operator
.
add
]
    collapsed_summaries
:
 List
[
Document
]
    final_summary
:
 
str
# This will be the state of the node that we will "map" all
# documents to in order to generate summaries
class
 
SummaryState
(
TypedDict
)
:
    content
:
 
str
# Here we generate a summary, given a document
async
 
def
 
generate_summary
(
state
:
 SummaryState
)
:
    response 
=
 
await
 map_chain
.
ainvoke
(
state
[
"content"
]
)
    
return
 
{
"summaries"
:
 
[
response
]
}
# Here we define the logic to map out over the documents
# We will use this an edge in the graph
def
 
map_summaries
(
state
:
 OverallState
)
:
    
# We will return a list of `Send` objects
    
# Each `Send` object consists of the name of a node in the graph
    
# as well as the state to send to that node
    
return
 
[
        Send
(
"generate_summary"
,
 
{
"content"
:
 content
}
)
 
for
 content 
in
 state
[
"contents"
]
    
]
def
 
collect_summaries
(
state
:
 OverallState
)
:
    
return
 
{
        
"collapsed_summaries"
:
 
[
Document
(
summary
)
 
for
 summary 
in
 state
[
"summaries"
]
]
    
}
# Add node to collapse summaries
async
 
def
 
collapse_summaries
(
state
:
 OverallState
)
:
    doc_lists 
=
 split_list_of_docs
(
        state
[
"collapsed_summaries"
]
,
 length_function
,
 token_max
    
)
    results 
=
 
[
]
    
for
 doc_list 
in
 doc_lists
:
        results
.
append
(
await
 acollapse_docs
(
doc_list
,
 reduce_chain
.
ainvoke
)
)
    
return
 
{
"collapsed_summaries"
:
 results
}
# This represents a conditional edge in the graph that determines
# if we should collapse the summaries or not
def
 
should_collapse
(
    state
:
 OverallState
,
)
 
-
>
 Literal
[
"collapse_summaries"
,
 
"generate_final_summary"
]
:
    num_tokens 
=
 length_function
(
state
[
"collapsed_summaries"
]
)
    
if
 num_tokens 
>
 token_max
:
        
return
 
"collapse_summaries"
    
else
:
        
return
 
"generate_final_summary"
# Here we will generate the final summary
async
 
def
 
generate_final_summary
(
state
:
 OverallState
)
:
    response 
=
 
await
 reduce_chain
.
ainvoke
(
state
[
"collapsed_summaries"
]
)
    
return
 
{
"final_summary"
:
 response
}
# Construct the graph
# Nodes:
graph 
=
 StateGraph
(
OverallState
)
graph
.
add_node
(
"generate_summary"
,
 generate_summary
)
  
# same as before
graph
.
add_node
(
"collect_summaries"
,
 collect_summaries
)
graph
.
add_node
(
"collapse_summaries"
,
 collapse_summaries
)
graph
.
add_node
(
"generate_final_summary"
,
 generate_final_summary
)
# Edges:
graph
.
add_conditional_edges
(
START
,
 map_summaries
,
 
[
"generate_summary"
]
)
graph
.
add_edge
(
"generate_summary"
,
 
"collect_summaries"
)
graph
.
add_conditional_edges
(
"collect_summaries"
,
 should_collapse
)
graph
.
add_conditional_edges
(
"collapse_summaries"
,
 should_collapse
)
graph
.
add_edge
(
"generate_final_summary"
,
 END
)
app 
=
 graph
.
compile
(
)
API Reference:
acollapse_docs
 | 
split_list_of_docs
 | 
Document
 | 
Send
 | 
StateGraph


LangGraph allows the graph structure to be plotted to help visualize its function:


from
 IPython
.
display 
import
 Image
Image
(
app
.
get_graph
(
)
.
draw_mermaid_png
(
)
)




Invoke graph
​


When running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.


Note that because we have a loop in the graph, it can be helpful to specify a 
recursion_limit
 on its execution. This will raise a specific error when the specified limit is exceeded.


async
 
for
 step 
in
 app
.
astream
(
    
{
"contents"
:
 
[
doc
.
page_content 
for
 doc 
in
 split_docs
]
}
,
    
{
"recursion_limit"
:
 
10
}
,
)
:
    
print
(
list
(
step
.
keys
(
)
)
)


['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['collect_summaries']
['collapse_summaries']
['collapse_summaries']
['generate_final_summary']


print
(
step
)


{'generate_final_summary': {'final_summary': 'The consolidated summary of the main themes from the provided documents highlights the advancements and applications of large language models (LLMs) in artificial intelligence, particularly in autonomous agents and software development. Key themes include:\n\n1. **Integration of LLMs**: LLMs play a crucial role in enabling autonomous agents to perform complex tasks through advanced reasoning and decision-making techniques, such as Chain of Thought (CoT) and Tree of Thoughts.\n\n2. **Memory Management**: The categorization of memory into sensory, short-term, and long-term types parallels machine learning concepts, with short-term memory facilitating in-context learning and long-term memory enhanced by external storage solutions.\n\n3. **Tool Use and APIs**: Autonomous agents utilize external APIs to expand their capabilities, demonstrating adaptability and improved problem-solving skills.\n\n4. **Search Algorithms**: Various approximate nearest neighbor search algorithms, including Locality-Sensitive Hashing (LSH) and FAISS, are discussed for enhancing search efficiency in high-dimensional spaces.\n\n5. **Neuro-Symbolic Architectures**: The integration of neuro-symbolic systems, such as the MRKL framework, combines expert modules with LLMs to improve problem-solving, particularly in complex tasks.\n\n6. **Challenges and Innovations**: The documents address challenges like hallucination and inefficient planning in LLMs, alongside innovative methods such as Chain of Hindsight (CoH) and Algorithm Distillation (AD) for performance enhancement.\n\n7. **Software Development Practices**: The use of LLMs in software development is explored, particularly in creating structured applications like a Super Mario game using the model-view-controller (MVC) architecture, emphasizing task management, component organization, and documentation.\n\n8. **Limitations of LLMs**: Constraints such as finite context length and challenges in long-term planning are acknowledged, along with concerns regarding the reliability of natural language as an interface.\n\nOverall, the integration of LLMs and neuro-symbolic architectures signifies a significant evolution in AI, with ongoing research focused on enhancing planning, memory management, and problem-solving capabilities across various applications.'}}


Next steps
​


Check out the 
LangGraph documentation
 for detail on building with LangGraph, including 
this guide
 on the details of map-reduce in LangGraph.


See the summarization 
how-to guides
 for additional summarization strategies, including those designed for larger volumes of text.


See also 
this tutorial
 for more detail on summarization.
Edit this page
Was this page helpful?
Previous
How to return structured data from a model
Next
How to summarize text through iterative refinement
Load chat model
Load documents
Create graph
Map step
Reduce step
Orchestration via LangGraph
Invoke graph
Next steps


--- docs_how_to_summarize_refine.txt ---

How-to guides
How to summarize text through iterative refinement
On this page
How to summarize text through iterative refinement


LLMs can summarize and otherwise distill desired information from text, including large volumes of text. In many cases, especially when the amount of text is large compared to the size of the model's context window, it can be helpful (or necessary) to break up the summarization task into smaller components.


Iterative refinement represents one strategy for summarizing long texts. The strategy is as follows:




Split a text into smaller documents;


Summarize the first document;


Refine or update the result based on the next document;


Repeat through the sequence of documents until finished.




Note that this strategy is not parallelized. It is especially effective when understanding of a sub-document depends on prior context-- for instance, when summarizing a novel or body of text with an inherent sequence.


LangGraph
, built on top of 
langchain-core
, is well-suited to this problem:




LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;


LangGraph's 
checkpointing
 supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.


Because it is assembled from modular components, it is also simple to extend or modify (e.g., to incorporate 
tool calling
 or other behavior).




Below, we demonstrate how to summarize text via iterative refinement.


Load chat model
​


Let's first load a chat model:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Load documents
​


Next, we need some documents to summarize. Below, we generate some toy documents for illustrative purposes. See the document loader 
how-to guides
 and 
integration pages
 for additional sources of data. The 
summarization tutorial
 also includes an example summarizing a blog post.


from
 langchain_core
.
documents 
import
 Document
documents 
=
 
[
    Document
(
page_content
=
"Apples are red"
,
 metadata
=
{
"title"
:
 
"apple_book"
}
)
,
    Document
(
page_content
=
"Blueberries are blue"
,
 metadata
=
{
"title"
:
 
"blueberry_book"
}
)
,
    Document
(
page_content
=
"Bananas are yelow"
,
 metadata
=
{
"title"
:
 
"banana_book"
}
)
,
]
API Reference:
Document


Create graph
​


Below we show a LangGraph implementation of this process:




We generate a simple chain for the initial summary that plucks out the first document, formats it into a prompt and runs inference with our LLM.


We generate a second 
refine_summary_chain
 that operates on each successive document, refining the initial summary.




We will need to install 
langgraph
:


pip install 
-
qU langgraph


import
 operator
from
 typing 
import
 List
,
 Literal
,
 TypedDict
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnableConfig
from
 langgraph
.
constants 
import
 Send
from
 langgraph
.
graph 
import
 END
,
 START
,
 StateGraph
# Initial summary
summarize_prompt 
=
 ChatPromptTemplate
(
    
[
        
(
"human"
,
 
"Write a concise summary of the following: {context}"
)
,
    
]
)
initial_summary_chain 
=
 summarize_prompt 
|
 llm 
|
 StrOutputParser
(
)
# Refining the summary with new docs
refine_template 
=
 
"""
Produce a final summary.
Existing summary up to this point:
{existing_answer}
New context:
------------
{context}
------------
Given the new context, refine the original summary.
"""
refine_prompt 
=
 ChatPromptTemplate
(
[
(
"human"
,
 refine_template
)
]
)
refine_summary_chain 
=
 refine_prompt 
|
 llm 
|
 StrOutputParser
(
)
# We will define the state of the graph to hold the document
# contents and summary. We also include an index to keep track
# of our position in the sequence of documents.
class
 
State
(
TypedDict
)
:
    contents
:
 List
[
str
]
    index
:
 
int
    summary
:
 
str
# We define functions for each node, including a node that generates
# the initial summary:
async
 
def
 
generate_initial_summary
(
state
:
 State
,
 config
:
 RunnableConfig
)
:
    summary 
=
 
await
 initial_summary_chain
.
ainvoke
(
        state
[
"contents"
]
[
0
]
,
        config
,
    
)
    
return
 
{
"summary"
:
 summary
,
 
"index"
:
 
1
}
# And a node that refines the summary based on the next document
async
 
def
 
refine_summary
(
state
:
 State
,
 config
:
 RunnableConfig
)
:
    content 
=
 state
[
"contents"
]
[
state
[
"index"
]
]
    summary 
=
 
await
 refine_summary_chain
.
ainvoke
(
        
{
"existing_answer"
:
 state
[
"summary"
]
,
 
"context"
:
 content
}
,
        config
,
    
)
    
return
 
{
"summary"
:
 summary
,
 
"index"
:
 state
[
"index"
]
 
+
 
1
}
# Here we implement logic to either exit the application or refine
# the summary.
def
 
should_refine
(
state
:
 State
)
 
-
>
 Literal
[
"refine_summary"
,
 END
]
:
    
if
 state
[
"index"
]
 
>=
 
len
(
state
[
"contents"
]
)
:
        
return
 END
    
else
:
        
return
 
"refine_summary"
graph 
=
 StateGraph
(
State
)
graph
.
add_node
(
"generate_initial_summary"
,
 generate_initial_summary
)
graph
.
add_node
(
"refine_summary"
,
 refine_summary
)
graph
.
add_edge
(
START
,
 
"generate_initial_summary"
)
graph
.
add_conditional_edges
(
"generate_initial_summary"
,
 should_refine
)
graph
.
add_conditional_edges
(
"refine_summary"
,
 should_refine
)
app 
=
 graph
.
compile
(
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnableConfig
 | 
Send
 | 
StateGraph


LangGraph allows the graph structure to be plotted to help visualize its function:


from
 IPython
.
display 
import
 Image
Image
(
app
.
get_graph
(
)
.
draw_mermaid_png
(
)
)




Invoke graph
​


We can step through the execution as follows, printing out the summary as it is refined:


async
 
for
 step 
in
 app
.
astream
(
    
{
"contents"
:
 
[
doc
.
page_content 
for
 doc 
in
 documents
]
}
,
    stream_mode
=
"values"
,
)
:
    
if
 summary 
:=
 step
.
get
(
"summary"
)
:
        
print
(
summary
)


Apples are characterized by their red color.
Apples are characterized by their red color, while blueberries are known for their blue hue.
Apples are characterized by their red color, blueberries are known for their blue hue, and bananas are recognized for their yellow color.


The final 
step
 contains the summary as synthesized from the entire set of documents.


Next steps
​


Check out the summarization 
how-to guides
 for additional summarization strategies, including those designed for larger volumes of text.


See 
this tutorial
 for more detail on summarization.


See also the 
LangGraph documentation
 for detail on building with LangGraph.
Edit this page
Was this page helpful?
Previous
How to summarize text through parallelization
Next
How to summarize text in a single LLM call
Load chat model
Load documents
Create graph
Invoke graph
Next steps


--- docs_how_to_summarize_stuff.txt ---

How-to guides
How to summarize text in a single LLM call
On this page
How to summarize text in a single LLM call


LLMs can summarize and otherwise distill desired information from text, including large volumes of text. In many cases, especially for models with larger context windows, this can be adequately achieved via a single LLM call.


LangChain implements a simple 
pre-built chain
 that "stuffs" a prompt with the desired context for summarization and other purposes. In this guide we demonstrate how to use the chain.


Load chat model
​


Let's first load a 
chat model
:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Load documents
​


Next, we need some documents to summarize. Below, we generate some toy documents for illustrative purposes. See the document loader 
how-to guides
 and 
integration pages
 for additional sources of data. The 
summarization tutorial
 also includes an example summarizing a blog post.


from
 langchain_core
.
documents 
import
 Document
documents 
=
 
[
    Document
(
page_content
=
"Apples are red"
,
 metadata
=
{
"title"
:
 
"apple_book"
}
)
,
    Document
(
page_content
=
"Blueberries are blue"
,
 metadata
=
{
"title"
:
 
"blueberry_book"
}
)
,
    Document
(
page_content
=
"Bananas are yelow"
,
 metadata
=
{
"title"
:
 
"banana_book"
}
)
,
]
API Reference:
Document


Load chain
​


Below, we define a simple prompt and instantiate the chain with our chat model and documents:


from
 langchain
.
chains
.
combine_documents 
import
 create_stuff_documents_chain
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
prompt 
=
 ChatPromptTemplate
.
from_template
(
"Summarize this content: {context}"
)
chain 
=
 create_stuff_documents_chain
(
llm
,
 prompt
)
API Reference:
create_stuff_documents_chain
 | 
ChatPromptTemplate


Invoke chain
​


Because the chain is a 
Runnable
, it implements the usual methods for invocation:


result 
=
 chain
.
invoke
(
{
"context"
:
 documents
}
)
result


'The content describes the colors of three fruits: apples are red, blueberries are blue, and bananas are yellow.'


Streaming
​


Note that the chain also supports streaming of individual output tokens:


for
 chunk 
in
 chain
.
stream
(
{
"context"
:
 documents
}
)
:
    
print
(
chunk
,
 end
=
"|"
)


|The| content| describes| the| colors| of| three| fruits|:| apples| are| red|,| blueberries| are| blue|,| and| bananas| are| yellow|.||


Next steps
​


See the summarization 
how-to guides
 for additional summarization strategies, including those designed for larger volumes of text.


See also 
this tutorial
 for more detail on summarization.
Edit this page
Was this page helpful?
Previous
How to summarize text through iterative refinement
Next
How to use toolkits
Load chat model
Load documents
Load chain
Invoke chain
Streaming
Next steps


--- docs_how_to_time_weighted_vectorstore.txt ---

How-to guides
How to use a time-weighted vector store retriever
On this page
How to use a time-weighted vector store retriever


This 
retriever
 uses a combination of semantic 
similarity
 and a time decay.


The algorithm for scoring them is:


semantic_similarity + (1.0 - decay_rate) ^ hours_passed


Notably, 
hours_passed
 refers to the hours passed since the object in the retriever 
was last accessed
, not since it was created. This means that frequently accessed objects remain "fresh".


from
 datetime 
import
 datetime
,
 timedelta
import
 faiss
from
 langchain
.
retrievers 
import
 TimeWeightedVectorStoreRetriever
from
 langchain_community
.
docstore 
import
 InMemoryDocstore
from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_core
.
documents 
import
 Document
from
 langchain_openai 
import
 OpenAIEmbeddings
API Reference:
TimeWeightedVectorStoreRetriever
 | 
InMemoryDocstore
 | 
FAISS
 | 
Document
 | 
OpenAIEmbeddings


Low decay rate
​


A low 
decay rate
 (in this, to be extreme, we will set it close to 0) means memories will be "remembered" for longer. A 
decay rate
 of 0 means memories never be forgotten, making this retriever equivalent to the vector lookup.


# Define your embedding model
embeddings_model 
=
 OpenAIEmbeddings
(
)
# Initialize the vectorstore as empty
embedding_size 
=
 
1536
index 
=
 faiss
.
IndexFlatL2
(
embedding_size
)
vectorstore 
=
 FAISS
(
embeddings_model
,
 index
,
 InMemoryDocstore
(
{
}
)
,
 
{
}
)
retriever 
=
 TimeWeightedVectorStoreRetriever
(
    vectorstore
=
vectorstore
,
 decay_rate
=
0.0000000000000000000000001
,
 k
=
1
)


yesterday 
=
 datetime
.
now
(
)
 
-
 timedelta
(
days
=
1
)
retriever
.
add_documents
(
    
[
Document
(
page_content
=
"hello world"
,
 metadata
=
{
"last_accessed_at"
:
 yesterday
}
)
]
)
retriever
.
add_documents
(
[
Document
(
page_content
=
"hello foo"
)
]
)


['73679bc9-d425-49c2-9d74-de6356c73489']


# "Hello World" is returned first because it is most salient, and the decay rate is close to 0., meaning it's still recent enough
retriever
.
invoke
(
"hello world"
)


[Document(metadata={'last_accessed_at': datetime.datetime(2024, 10, 22, 16, 37, 40, 818583), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 37, 975074), 'buffer_idx': 0}, page_content='hello world')]


High decay rate
​


With a high 
decay rate
 (e.g., several 9's), the 
recency score
 quickly goes to 0! If you set this all the way to 1, 
recency
 is 0 for all objects, once again making this equivalent to a vector lookup.


# Define your embedding model
embeddings_model 
=
 OpenAIEmbeddings
(
)
# Initialize the vectorstore as empty
embedding_size 
=
 
1536
index 
=
 faiss
.
IndexFlatL2
(
embedding_size
)
vectorstore 
=
 FAISS
(
embeddings_model
,
 index
,
 InMemoryDocstore
(
{
}
)
,
 
{
}
)
retriever 
=
 TimeWeightedVectorStoreRetriever
(
    vectorstore
=
vectorstore
,
 decay_rate
=
0.999
,
 k
=
1
)


yesterday 
=
 datetime
.
now
(
)
 
-
 timedelta
(
days
=
1
)
retriever
.
add_documents
(
    
[
Document
(
page_content
=
"hello world"
,
 metadata
=
{
"last_accessed_at"
:
 yesterday
}
)
]
)
retriever
.
add_documents
(
[
Document
(
page_content
=
"hello foo"
)
]
)


['379631f0-42c2-4773-8cc2-d36201e1e610']


# "Hello Foo" is returned first because "hello world" is mostly forgotten
retriever
.
invoke
(
"hello world"
)


[Document(metadata={'last_accessed_at': datetime.datetime(2024, 10, 22, 16, 37, 46, 553633), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 43, 927429), 'buffer_idx': 1}, page_content='hello foo')]


Virtual time
​


Using some utils in LangChain, you can mock out the time component.


from
 langchain_core
.
utils 
import
 mock_now
API Reference:
mock_now


# Notice the last access time is that date time
tomorrow 
=
 datetime
.
now
(
)
 
+
 timedelta
(
days
=
1
)
with
 mock_now
(
tomorrow
)
:
    
print
(
retriever
.
invoke
(
"hello world"
)
)


[Document(metadata={'last_accessed_at': MockDateTime(2024, 10, 23, 16, 38, 19, 66711), 'created_at': datetime.datetime(2024, 10, 22, 16, 37, 43, 599877), 'buffer_idx': 0}, page_content='hello world')]
Edit this page
Was this page helpful?
Previous
How to stream responses from an LLM
Next
How to return artifacts from a tool
Low decay rate
High decay rate
Virtual time


--- docs_how_to_tool_artifacts.txt ---

How-to guides
How to return artifacts from a tool
On this page
How to return artifacts from a tool


Prerequisites
This guide assumes familiarity with the following concepts:


ToolMessage


Tools


Function/tool calling




Tools
 are utilities that can be 
called by a model
, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.


The Tool and 
ToolMessage
 interfaces make it possible to distinguish between the parts of the tool output meant for the model (this is the ToolMessage.content) and those parts which are meant for use outside the model (ToolMessage.artifact).


Requires 
langchain-core >= 0.2.19
This functionality was added in 
langchain-core == 0.2.19
. Please make sure your package is up to date.


Defining the tool
​


If we want our tool to distinguish between message content and other artifacts, we need to specify 
response_format="content_and_artifact"
 when defining our tool and make sure that we return a tuple of (content, artifact):


%
pip install 
-
qU 
"langchain-core>=0.2.19"


import
 random
from
 typing 
import
 List
,
 Tuple
from
 langchain_core
.
tools 
import
 tool
@tool
(
response_format
=
"content_and_artifact"
)
def
 
generate_random_ints
(
min
:
 
int
,
 
max
:
 
int
,
 size
:
 
int
)
 
-
>
 Tuple
[
str
,
 List
[
int
]
]
:
    
"""Generate size random ints in the range [min, max]."""
    array 
=
 
[
random
.
randint
(
min
,
 
max
)
 
for
 _ 
in
 
range
(
size
)
]
    content 
=
 
f"Successfully generated array of 
{
size
}
 random ints in [
{
min
}
, 
{
max
}
]."
    
return
 content
,
 array
API Reference:
tool


Invoking the tool with ToolCall
​


If we directly invoke our tool with just the tool arguments, you'll notice that we only get back the content part of the Tool output:


generate_random_ints
.
invoke
(
{
"min"
:
 
0
,
 
"max"
:
 
9
,
 
"size"
:
 
10
}
)


'Successfully generated array of 10 random ints in [0, 9].'


In order to get back both the content and the artifact, we need to invoke our model with a ToolCall (which is just a dictionary with "name", "args", "id" and "type" keys), which has additional info needed to generate a ToolMessage like the tool call ID:


generate_random_ints
.
invoke
(
    
{
        
"name"
:
 
"generate_random_ints"
,
        
"args"
:
 
{
"min"
:
 
0
,
 
"max"
:
 
9
,
 
"size"
:
 
10
}
,
        
"id"
:
 
"123"
,
  
# required
        
"type"
:
 
"tool_call"
,
  
# required
    
}
)


ToolMessage(content='Successfully generated array of 10 random ints in [0, 9].', name='generate_random_ints', tool_call_id='123', artifact=[2, 8, 0, 6, 0, 0, 1, 5, 0, 0])


Using with a model
​


With a 
tool-calling model
, we can easily use a model to call our Tool and generate ToolMessages:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


llm_with_tools 
=
 llm
.
bind_tools
(
[
generate_random_ints
]
)
ai_msg 
=
 llm_with_tools
.
invoke
(
"generate 6 positive ints less than 25"
)
ai_msg
.
tool_calls


[{'name': 'generate_random_ints',
  'args': {'min': 1, 'max': 24, 'size': 6},
  'id': 'toolu_01EtALY3Wz1DVYhv1TLvZGvE',
  'type': 'tool_call'}]


generate_random_ints
.
invoke
(
ai_msg
.
tool_calls
[
0
]
)


ToolMessage(content='Successfully generated array of 6 random ints in [1, 24].', name='generate_random_ints', tool_call_id='toolu_01EtALY3Wz1DVYhv1TLvZGvE', artifact=[2, 20, 23, 8, 1, 15])


If we just pass in the tool call args, we'll only get back the content:


generate_random_ints
.
invoke
(
ai_msg
.
tool_calls
[
0
]
[
"args"
]
)


'Successfully generated array of 6 random ints in [1, 24].'


If we wanted to declaratively create a chain, we could do this:


from
 operator 
import
 attrgetter
chain 
=
 llm_with_tools 
|
 attrgetter
(
"tool_calls"
)
 
|
 generate_random_ints
.
map
(
)
chain
.
invoke
(
"give me a random number between 1 and 5"
)


[ToolMessage(content='Successfully generated array of 1 random ints in [1, 5].', name='generate_random_ints', tool_call_id='toolu_01FwYhnkwDPJPbKdGq4ng6uD', artifact=[5])]


Creating from BaseTool class
​


If you want to create a BaseTool object directly, instead of decorating a function with 
@tool
, you can do so like this:


from
 langchain_core
.
tools 
import
 BaseTool
class
 
GenerateRandomFloats
(
BaseTool
)
:
    name
:
 
str
 
=
 
"generate_random_floats"
    description
:
 
str
 
=
 
"Generate size random floats in the range [min, max]."
    response_format
:
 
str
 
=
 
"content_and_artifact"
    ndigits
:
 
int
 
=
 
2
    
def
 
_run
(
self
,
 
min
:
 
float
,
 
max
:
 
float
,
 size
:
 
int
)
 
-
>
 Tuple
[
str
,
 List
[
float
]
]
:
        range_ 
=
 
max
 
-
 
min
        array 
=
 
[
            
round
(
min
 
+
 
(
range_ 
*
 random
.
random
(
)
)
,
 ndigits
=
self
.
ndigits
)
            
for
 _ 
in
 
range
(
size
)
        
]
        content 
=
 
f"Generated 
{
size
}
 floats in [
{
min
}
, 
{
max
}
], rounded to 
{
self
.
ndigits
}
 decimals."
        
return
 content
,
 array
    
# Optionally define an equivalent async method
    
# async def _arun(self, min: float, max: float, size: int) -> Tuple[str, List[float]]:
    
#     ...
API Reference:
BaseTool


rand_gen 
=
 GenerateRandomFloats
(
ndigits
=
4
)
rand_gen
.
invoke
(
{
"min"
:
 
0.1
,
 
"max"
:
 
3.3333
,
 
"size"
:
 
3
}
)


'Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.'


rand_gen
.
invoke
(
    
{
        
"name"
:
 
"generate_random_floats"
,
        
"args"
:
 
{
"min"
:
 
0.1
,
 
"max"
:
 
3.3333
,
 
"size"
:
 
3
}
,
        
"id"
:
 
"123"
,
        
"type"
:
 
"tool_call"
,
    
}
)


ToolMessage(content='Generated 3 floats in [0.1, 3.3333], rounded to 4 decimals.', name='generate_random_floats', tool_call_id='123', artifact=[1.5789, 2.464, 2.2719])
Edit this page
Was this page helpful?
Previous
How to use a time-weighted vector store retriever
Next
How to use chat models to call tools
Defining the tool
Invoking the tool with ToolCall
Using with a model
Creating from BaseTool class


--- docs_how_to_tool_calling.txt ---

How-to guides
How to use chat models to call tools
On this page
How to use chat models to call tools


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


Tool calling


Tools


Output parsers




Tool calling
 allows a chat model to respond to a given prompt by "calling a tool".


Remember, while the name "tool calling" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.


Tool calling is a general technique that generates structured output from a model, and you can use it even when you don't intend to invoke any tools. An example use-case of that is 
extraction from unstructured text
.




If you want to see how to use the model-generated tool call to actually run a tool 
check out this guide
.


Supported models
Tool calling is not universal, but is supported by many popular LLM providers. You can find a 
list of all models that support tool calling here
.


LangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.
This guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.


Defining tool schemas
​


For a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it's arguments are. Chat models that support tool calling features implement a 
.bind_tools()
 method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain 
Tool objects
. Subsequent invocations of the model will pass in these tool schemas along with the prompt.


Python functions
​


Our tool schemas can be Python functions:


# The function name, type hints, and docstring are all part of the tool
# schema that's passed to the model. Defining good, descriptive schemas
# is an extension of prompt engineering and is an important part of
# getting models to perform well.
def
 
add
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Add two integers.
    Args:
        a: First integer
        b: Second integer
    """
    
return
 a 
+
 b
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two integers.
    Args:
        a: First integer
        b: Second integer
    """
    
return
 a 
*
 b


LangChain Tool
​


LangChain also implements a 
@tool
 decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide 
here
 for details.


Pydantic class
​


You can equivalently define the schemas without the accompanying functions using 
Pydantic
.


Note that all fields are 
required
 unless provided a default value.


from
 pydantic 
import
 BaseModel
,
 Field
class
 
add
(
BaseModel
)
:
    
"""Add two integers."""
    a
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"First integer"
)
    b
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"Second integer"
)
class
 
multiply
(
BaseModel
)
:
    
"""Multiply two integers."""
    a
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"First integer"
)
    b
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"Second integer"
)


TypedDict class
​


Requires 
langchain-core>=0.2.25


Or using TypedDicts and annotations:


from
 typing_extensions 
import
 Annotated
,
 TypedDict
class
 
add
(
TypedDict
)
:
    
"""Add two integers."""
    
# Annotations must have the type and can optionally include a default value and description (in that order).
    a
:
 Annotated
[
int
,
 
.
.
.
,
 
"First integer"
]
    b
:
 Annotated
[
int
,
 
.
.
.
,
 
"Second integer"
]
class
 
multiply
(
TypedDict
)
:
    
"""Multiply two integers."""
    a
:
 Annotated
[
int
,
 
.
.
.
,
 
"First integer"
]
    b
:
 Annotated
[
int
,
 
.
.
.
,
 
"Second integer"
]
tools 
=
 
[
add
,
 multiply
]


To actually bind those schemas to a chat model, we'll use the 
.bind_tools()
 method. This handles converting
the 
add
 and 
multiply
 schemas to the proper format for the model. The tool schema will then be passed it in each time the model is invoked.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


llm_with_tools 
=
 llm
.
bind_tools
(
tools
)
query 
=
 
"What is 3 * 12?"
llm_with_tools
.
invoke
(
query
)


AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_iXj4DiW1p7WLjTAQMRO0jxMs', 'function': {'arguments': '{"a":3,"b":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 80, 'total_tokens': 97}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_483d39d857', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-0b620986-3f62-4df7-9ba3-4595089f9ad4-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_iXj4DiW1p7WLjTAQMRO0jxMs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 80, 'output_tokens': 17, 'total_tokens': 97})


As we can see our LLM generated arguments to a tool! You can look at the docs for 
bind_tools()
 to learn about all the ways to customize how your LLM selects tools, as well as 
this guide on how to force the LLM to call a tool
 rather than letting it decide.


Tool calls
​


If tool calls are included in a LLM response, they are attached to the corresponding

message

or 
message chunk

as a list of 
tool call

objects in the 
.tool_calls
 attribute.


Note that chat models can call multiple tools at once.


A 
ToolCall
 is a typed dict that includes a
tool name, dict of argument values, and (optionally) an identifier. Messages with no
tool calls default to an empty list for this attribute.


query 
=
 
"What is 3 * 12? Also, what is 11 + 49?"
llm_with_tools
.
invoke
(
query
)
.
tool_calls


[{'name': 'multiply',
  'args': {'a': 3, 'b': 12},
  'id': 'call_1fyhJAbJHuKQe6n0PacubGsL',
  'type': 'tool_call'},
 {'name': 'add',
  'args': {'a': 11, 'b': 49},
  'id': 'call_fc2jVkKzwuPWyU7kS9qn1hyG',
  'type': 'tool_call'}]


The 
.tool_calls
 attribute should contain valid tool calls. Note that on occasion,
model providers may output malformed tool calls (e.g., arguments that are not
valid JSON). When parsing fails in these cases, instances
of 
InvalidToolCall

are populated in the 
.invalid_tool_calls
 attribute. An 
InvalidToolCall
 can have
a name, string arguments, identifier, and error message.


Parsing
​


If desired, 
output parsers
 can further process the output. For example, we can convert existing values populated on the 
.tool_calls
 to Pydantic objects using the

PydanticToolsParser
:


from
 langchain_core
.
output_parsers 
import
 PydanticToolsParser
from
 pydantic 
import
 BaseModel
,
 Field
class
 
add
(
BaseModel
)
:
    
"""Add two integers."""
    a
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"First integer"
)
    b
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"Second integer"
)
class
 
multiply
(
BaseModel
)
:
    
"""Multiply two integers."""
    a
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"First integer"
)
    b
:
 
int
 
=
 Field
(
.
.
.
,
 description
=
"Second integer"
)
chain 
=
 llm_with_tools 
|
 PydanticToolsParser
(
tools
=
[
add
,
 multiply
]
)
chain
.
invoke
(
query
)
API Reference:
PydanticToolsParser


[multiply(a=3, b=12), add(a=11, b=49)]


Next steps
​


Now you've learned how to bind tool schemas to a chat model and have the model call the tool.


Next, check out this guide on actually using the tool by invoking the function and passing the results back to the model:




Pass 
tool results back to model




You can also check out some more specific uses of tool calling:




Getting 
structured outputs
 from models


Few shot prompting 
with tools


Stream 
tool calls


Pass 
runtime values to tools


Edit this page
Was this page helpful?
Previous
How to return artifacts from a tool
Next
How to disable parallel tool calling
Defining tool schemas
Python functions
LangChain Tool
Pydantic class
TypedDict class
Tool calls
Parsing
Next steps


--- docs_how_to_tool_calling_parallel.txt ---

How-to guides
How to disable parallel tool calling
How to disable parallel tool calling


Provider-specific
This API is currently only supported by OpenAI and Anthropic.


OpenAI tool calling performs tool calling in parallel by default. That means that if we ask a question like "What is the weather in Tokyo, New York, and Chicago?" and we have a tool for getting the weather, it will call the tool 3 times in parallel. We can force it to call only a single tool once by using the 
parallel_tool_call
 parameter.


First let's set up our tools and model:


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
add
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Adds a and b."""
    
return
 a 
+
 b
@tool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiplies a and b."""
    
return
 a 
*
 b
tools 
=
 
[
add
,
 multiply
]
API Reference:
tool


import
 os
from
 getpass 
import
 getpass
from
 langchain
.
chat_models 
import
 init_chat_model
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)
llm 
=
 init_chat_model
(
"openai:gpt-4.1-mini"
)
API Reference:
init_chat_model


Now let's show a quick example of how disabling parallel tool calls work:


llm_with_tools 
=
 llm
.
bind_tools
(
tools
,
 parallel_tool_calls
=
False
)
llm_with_tools
.
invoke
(
"Please call the first tool two times"
)
.
tool_calls


[{'name': 'add',
  'args': {'a': 2, 'b': 2},
  'id': 'call_Hh4JOTCDM85Sm9Pr84VKrWu5'}]


As we can see, even though we explicitly told the model to call a tool twice, by disabling parallel tool calls the model was constrained to only calling one.
Edit this page
Was this page helpful?
Previous
How to use chat models to call tools
Next
How to force models to call a tool


--- docs_how_to_tool_choice.txt ---

How-to guides
How to force models to call a tool
How to force models to call a tool


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


LangChain Tools


How to use a model to call tools




In order to force our LLM to select a specific 
tool
, we can use the 
tool_choice
 parameter to ensure certain behavior. First, let's define our model and tools:


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
add
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Adds a and b."""
    
return
 a 
+
 b
@tool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiplies a and b."""
    
return
 a 
*
 b
tools 
=
 
[
add
,
 multiply
]
API Reference:
tool


For example, we can force our tool to call the multiply tool by using the following code:


llm_forced_to_multiply 
=
 llm
.
bind_tools
(
tools
,
 tool_choice
=
"multiply"
)
llm_forced_to_multiply
.
invoke
(
"what is 2 + 4"
)


AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9cViskmLvPnHjXk9tbVla5HA', 'function': {'arguments': '{"a":2,"b":4}', 'name': 'Multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 103, 'total_tokens': 112}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-095b827e-2bdd-43bb-8897-c843f4504883-0', tool_calls=[{'name': 'Multiply', 'args': {'a': 2, 'b': 4}, 'id': 'call_9cViskmLvPnHjXk9tbVla5HA'}], usage_metadata={'input_tokens': 103, 'output_tokens': 9, 'total_tokens': 112})


Even if we pass it something that doesn't require multiplcation - it will still call the tool!


We can also just force our tool to select at least one of our tools by passing in the "any" (or "required" which is OpenAI specific) keyword to the 
tool_choice
 parameter.


llm_forced_to_use_tool 
=
 llm
.
bind_tools
(
tools
,
 tool_choice
=
"any"
)
llm_forced_to_use_tool
.
invoke
(
"What day is today?"
)


AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mCSiJntCwHJUBfaHZVUB2D8W', 'function': {'arguments': '{"a":1,"b":2}', 'name': 'Add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 94, 'total_tokens': 109}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-28f75260-9900-4bed-8cd3-f1579abb65e5-0', tool_calls=[{'name': 'Add', 'args': {'a': 1, 'b': 2}, 'id': 'call_mCSiJntCwHJUBfaHZVUB2D8W'}], usage_metadata={'input_tokens': 94, 'output_tokens': 15, 'total_tokens': 109})
Edit this page
Was this page helpful?
Previous
How to disable parallel tool calling
Next
How to access the RunnableConfig from a tool


--- docs_how_to_tool_configure.txt ---

How-to guides
How to access the RunnableConfig from a tool
On this page
How to access the RunnableConfig from a tool


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Tools


Custom tools


LangChain Expression Language (LCEL)


Configuring runnable behavior




If you have a 
tool
 that calls 
chat models
, 
retrievers
, or other 
runnables
, you may want to access internal events from those runnables or configure them with additional properties. This guide shows you how to manually pass parameters properly so that you can do this using the 
astream_events()
 method.


Tools are 
runnables
, and you can treat them the same way as any other runnable at the interface level - you can call 
invoke()
, 
batch()
, and 
stream()
 on them as normal. However, when writing custom tools, you may want to invoke other runnables like chat models or retrievers. In order to properly trace and configure those sub-invocations, you'll need to manually access and pass in the tool's current 
RunnableConfig
 object. This guide show you some examples of how to do that.


Compatibility
This guide requires 
langchain-core>=0.2.16
.


Inferring by parameter type
​


To access reference the active config object from your custom tool, you'll need to add a parameter to your tool's signature typed as 
RunnableConfig
. When you invoke your tool, LangChain will inspect your tool's signature, look for a parameter typed as 
RunnableConfig
, and if it exists, populate that parameter with the correct value.


Note:
 The actual name of the parameter doesn't matter, only the typing.


To illustrate this, define a custom tool that takes a two parameters - one typed as a string, the other typed as 
RunnableConfig
:


%
pip install 
-
qU langchain_core


from
 langchain_core
.
runnables 
import
 RunnableConfig
from
 langchain_core
.
tools 
import
 tool
@tool
async
 
def
 
reverse_tool
(
text
:
 
str
,
 special_config_param
:
 RunnableConfig
)
 
-
>
 
str
:
    
"""A test tool that combines input text with a configurable parameter."""
    
return
 
(
text 
+
 special_config_param
[
"configurable"
]
[
"additional_field"
]
)
[
:
:
-
1
]
API Reference:
RunnableConfig
 | 
tool


Then, if we invoke the tool with a 
config
 containing a 
configurable
 field, we can see that 
additional_field
 is passed through correctly:


await
 reverse_tool
.
ainvoke
(
    
{
"text"
:
 
"abc"
}
,
 config
=
{
"configurable"
:
 
{
"additional_field"
:
 
"123"
}
}
)


'321cba'


Next steps
​


You've now seen how to configure and stream events from within a tool. Next, check out the following guides for more on using tools:




Stream events from child runs within a custom tool


Pass 
tool results back to a model




You can also check out some more specific uses of tool calling:




Building 
tool-using chains and agents


Getting 
structured outputs
 from models


Edit this page
Was this page helpful?
Previous
How to force models to call a tool
Next
How to pass tool outputs to chat models
Inferring by parameter type
Next steps


--- docs_how_to_tool_results_pass_to_model.txt ---

How-to guides
How to pass tool outputs to chat models
On this page
How to pass tool outputs to chat models


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Tools


Function/tool calling


Using chat models to call tools


Defining custom tools




Some models are capable of 
tool calling
 - generating arguments that conform to a specific user-provided schema. This guide will demonstrate how to use those tool calls to actually call a function and properly pass the results back to the model.






First, let's define our tools and our model:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
add
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Adds a and b."""
    
return
 a 
+
 b
@tool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiplies a and b."""
    
return
 a 
*
 b
tools 
=
 
[
add
,
 multiply
]
llm_with_tools 
=
 llm
.
bind_tools
(
tools
)
API Reference:
tool


Now, let's get the model to call a tool. We'll add it to a list of messages that we'll treat as conversation history:


from
 langchain_core
.
messages 
import
 HumanMessage
query 
=
 
"What is 3 * 12? Also, what is 11 + 49?"
messages 
=
 
[
HumanMessage
(
query
)
]
ai_msg 
=
 llm_with_tools
.
invoke
(
messages
)
print
(
ai_msg
.
tool_calls
)
messages
.
append
(
ai_msg
)
API Reference:
HumanMessage


[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_GPGPE943GORirhIAYnWv00rK', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_dm8o64ZrY3WFZHAvCh1bEJ6i', 'type': 'tool_call'}]


Next let's invoke the tool functions using the args the model populated!


Conveniently, if we invoke a LangChain 
Tool
 with a 
ToolCall
, we'll automatically get back a 
ToolMessage
 that can be fed back to the model:


Compatibility
This functionality was added in 
langchain-core == 0.2.19
. Please make sure your package is up to date.
If you are on earlier versions of 
langchain-core
, you will need to extract the 
args
 field from the tool and construct a 
ToolMessage
 manually.


for
 tool_call 
in
 ai_msg
.
tool_calls
:
    selected_tool 
=
 
{
"add"
:
 add
,
 
"multiply"
:
 multiply
}
[
tool_call
[
"name"
]
.
lower
(
)
]
    tool_msg 
=
 selected_tool
.
invoke
(
tool_call
)
    messages
.
append
(
tool_msg
)
messages


[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?'),
 AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_loT2pliJwJe3p7nkgXYF48A1', 'function': {'arguments': '{"a": 3, "b": 12}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_bG9tYZCXOeYDZf3W46TceoV4', 'function': {'arguments': '{"a": 11, "b": 49}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 87, 'total_tokens': 137}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e3db3c46-bf9e-478e-abc1-dc9a264f4afe-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_loT2pliJwJe3p7nkgXYF48A1', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 49}, 'id': 'call_bG9tYZCXOeYDZf3W46TceoV4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 87, 'output_tokens': 50, 'total_tokens': 137}),
 ToolMessage(content='36', name='multiply', tool_call_id='call_loT2pliJwJe3p7nkgXYF48A1'),
 ToolMessage(content='60', name='add', tool_call_id='call_bG9tYZCXOeYDZf3W46TceoV4')]


And finally, we'll invoke the model with the tool results. The model will use this information to generate a final answer to our original query:


llm_with_tools
.
invoke
(
messages
)


AIMessage(content='The result of \\(3 \\times 12\\) is 36, and the result of \\(11 + 49\\) is 60.', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 153, 'total_tokens': 184}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_661538dc1f', 'finish_reason': 'stop', 'logprobs': None}, id='run-87d1ef0a-1223-4bb3-9310-7b591789323d-0', usage_metadata={'input_tokens': 153, 'output_tokens': 31, 'total_tokens': 184})


Note that each 
ToolMessage
 must include a 
tool_call_id
 that matches an 
id
 in the original tool calls that the model generates. This helps the model match tool responses with tool calls.


Tool calling agents, like those in 
LangGraph
, use this basic flow to answer queries and solve tasks.


Related
​




LangGraph quickstart


Few shot prompting 
with tools


Stream 
tool calls


Pass 
runtime values to tools


Getting 
structured outputs
 from models


Edit this page
Was this page helpful?
Previous
How to access the RunnableConfig from a tool
Next
How to pass run time values to tools
Related


--- docs_how_to_tool_runtime.txt ---

How-to guides
How to pass run time values to tools
On this page
How to pass run time values to tools




📚
Prerequisites
Chat models
LangChain Tools
How to create tools
How to use a model to call tools


📦
Compatibility
The code in this guide requires
 
langchain-core>=0.2.21
. Please ensure you have the correct packages installed.


You may need to bind values to a 
tool
 that are only known at runtime. For example, the tool logic may require using the ID of the user who made the request.


Most of the time, such values should not be controlled by the LLM. In fact, allowing the LLM to control the user ID may lead to a security risk.


Instead, the LLM should only control the parameters of the tool that are meant to be controlled by the LLM, while other parameters (such as user ID) should be fixed by the application logic.


This how-to guide shows you how to prevent the model from generating certain tool arguments and injecting them in directly at runtime.


Using with LangGraph
If you're using LangGraph, please refer to 
this how-to guide

which shows how to create an agent that keeps track of a given user's favorite pets.


We can bind them to chat models as follows:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Hiding arguments from the model
​


We can use the InjectedToolArg annotation to mark certain parameters of our Tool, like 
user_id
 as being injected at runtime, meaning they shouldn't be generated by the model


from
 typing 
import
 List
from
 langchain_core
.
tools 
import
 InjectedToolArg
,
 tool
from
 typing_extensions 
import
 Annotated
user_to_pets 
=
 
{
}
@tool
(
parse_docstring
=
True
)
def
 
update_favorite_pets
(
    pets
:
 List
[
str
]
,
 user_id
:
 Annotated
[
str
,
 InjectedToolArg
]
)
 
-
>
 
None
:
    
"""Add the list of favorite pets.
    Args:
        pets: List of favorite pets to set.
        user_id: User's ID.
    """
    user_to_pets
[
user_id
]
 
=
 pets
@tool
(
parse_docstring
=
True
)
def
 
delete_favorite_pets
(
user_id
:
 Annotated
[
str
,
 InjectedToolArg
]
)
 
-
>
 
None
:
    
"""Delete the list of favorite pets.
    Args:
        user_id: User's ID.
    """
    
if
 user_id 
in
 user_to_pets
:
        
del
 user_to_pets
[
user_id
]
@tool
(
parse_docstring
=
True
)
def
 
list_favorite_pets
(
user_id
:
 Annotated
[
str
,
 InjectedToolArg
]
)
 
-
>
 
None
:
    
"""List favorite pets if any.
    Args:
        user_id: User's ID.
    """
    
return
 user_to_pets
.
get
(
user_id
,
 
[
]
)
API Reference:
InjectedToolArg
 | 
tool


If we look at the input schemas for these tools, we'll see that user_id is still listed:


update_favorite_pets
.
get_input_schema
(
)
.
schema
(
)


{'description': 'Add the list of favorite pets.',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'},
  'user_id': {'description': "User's ID.",
   'title': 'User Id',
   'type': 'string'}},
 'required': ['pets', 'user_id'],
 'title': 'update_favorite_petsSchema',
 'type': 'object'}


But if we look at the tool call schema, which is what is passed to the model for tool-calling, user_id has been removed:


update_favorite_pets
.
tool_call_schema
.
schema
(
)


{'description': 'Add the list of favorite pets.',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'}},
 'required': ['pets'],
 'title': 'update_favorite_pets',
 'type': 'object'}


So when we invoke our tool, we need to pass in user_id:


user_id 
=
 
"123"
update_favorite_pets
.
invoke
(
{
"pets"
:
 
[
"lizard"
,
 
"dog"
]
,
 
"user_id"
:
 user_id
}
)
print
(
user_to_pets
)
print
(
list_favorite_pets
.
invoke
(
{
"user_id"
:
 user_id
}
)
)


{'123': ['lizard', 'dog']}
['lizard', 'dog']


But when the model calls the tool, no user_id argument will be generated:


tools 
=
 
[
    update_favorite_pets
,
    delete_favorite_pets
,
    list_favorite_pets
,
]
llm_with_tools 
=
 llm
.
bind_tools
(
tools
)
ai_msg 
=
 llm_with_tools
.
invoke
(
"my favorite animals are cats and parrots"
)
ai_msg
.
tool_calls


[{'name': 'update_favorite_pets',
  'args': {'pets': ['cats', 'parrots']},
  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',
  'type': 'tool_call'}]


Injecting arguments at runtime
​


If we want to actually execute our tools using the model-generated tool call, we'll need to inject the user_id ourselves:


from
 copy 
import
 deepcopy
from
 langchain_core
.
runnables 
import
 chain
@chain
def
 
inject_user_id
(
ai_msg
)
:
    tool_calls 
=
 
[
]
    
for
 tool_call 
in
 ai_msg
.
tool_calls
:
        tool_call_copy 
=
 deepcopy
(
tool_call
)
        tool_call_copy
[
"args"
]
[
"user_id"
]
 
=
 user_id
        tool_calls
.
append
(
tool_call_copy
)
    
return
 tool_calls
inject_user_id
.
invoke
(
ai_msg
)
API Reference:
chain


[{'name': 'update_favorite_pets',
  'args': {'pets': ['cats', 'parrots'], 'user_id': '123'},
  'id': 'call_pZ6XVREGh1L0BBSsiGIf1xVm',
  'type': 'tool_call'}]


And now we can chain together our model, injection code, and the actual tools to create a tool-executing chain:


tool_map 
=
 
{
tool
.
name
:
 tool 
for
 tool 
in
 tools
}
@chain
def
 
tool_router
(
tool_call
)
:
    
return
 tool_map
[
tool_call
[
"name"
]
]
chain 
=
 llm_with_tools 
|
 inject_user_id 
|
 tool_router
.
map
(
)
chain
.
invoke
(
"my favorite animals are cats and parrots"
)


[ToolMessage(content='null', name='update_favorite_pets', tool_call_id='call_oYCD0THSedHTbwNAY3NW6uUj')]


Looking at the user_to_pets dict, we can see that it's been updated to include cats and parrots:


user_to_pets


{'123': ['cats', 'parrots']}


Other ways of annotating args
​


Here are a few other ways of annotating our tool args:


from
 langchain_core
.
tools 
import
 BaseTool
from
 pydantic 
import
 BaseModel
,
 Field
class
 
UpdateFavoritePetsSchema
(
BaseModel
)
:
    
"""Update list of favorite pets"""
    pets
:
 List
[
str
]
 
=
 Field
(
.
.
.
,
 description
=
"List of favorite pets to set."
)
    user_id
:
 Annotated
[
str
,
 InjectedToolArg
]
 
=
 Field
(
.
.
.
,
 description
=
"User's ID."
)
@tool
(
args_schema
=
UpdateFavoritePetsSchema
)
def
 
update_favorite_pets
(
pets
,
 user_id
)
:
    user_to_pets
[
user_id
]
 
=
 pets
update_favorite_pets
.
get_input_schema
(
)
.
schema
(
)
API Reference:
BaseTool


{'description': 'Update list of favorite pets',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'},
  'user_id': {'description': "User's ID.",
   'title': 'User Id',
   'type': 'string'}},
 'required': ['pets', 'user_id'],
 'title': 'UpdateFavoritePetsSchema',
 'type': 'object'}


update_favorite_pets
.
tool_call_schema
.
schema
(
)


{'description': 'Update list of favorite pets',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'}},
 'required': ['pets'],
 'title': 'update_favorite_pets',
 'type': 'object'}


from
 typing 
import
 Optional
,
 Type
class
 
UpdateFavoritePets
(
BaseTool
)
:
    name
:
 
str
 
=
 
"update_favorite_pets"
    description
:
 
str
 
=
 
"Update list of favorite pets"
    args_schema
:
 Optional
[
Type
[
BaseModel
]
]
 
=
 UpdateFavoritePetsSchema
    
def
 
_run
(
self
,
 pets
,
 user_id
)
:
        user_to_pets
[
user_id
]
 
=
 pets
UpdateFavoritePets
(
)
.
get_input_schema
(
)
.
schema
(
)


{'description': 'Update list of favorite pets',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'},
  'user_id': {'description': "User's ID.",
   'title': 'User Id',
   'type': 'string'}},
 'required': ['pets', 'user_id'],
 'title': 'UpdateFavoritePetsSchema',
 'type': 'object'}


UpdateFavoritePets
(
)
.
tool_call_schema
.
schema
(
)


{'description': 'Update list of favorite pets',
 'properties': {'pets': {'description': 'List of favorite pets to set.',
   'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'}},
 'required': ['pets'],
 'title': 'update_favorite_pets',
 'type': 'object'}


class
 
UpdateFavoritePets2
(
BaseTool
)
:
    name
:
 
str
 
=
 
"update_favorite_pets"
    description
:
 
str
 
=
 
"Update list of favorite pets"
    
def
 
_run
(
self
,
 pets
:
 List
[
str
]
,
 user_id
:
 Annotated
[
str
,
 InjectedToolArg
]
)
 
-
>
 
None
:
        user_to_pets
[
user_id
]
 
=
 pets
UpdateFavoritePets2
(
)
.
get_input_schema
(
)
.
schema
(
)


{'description': 'Use the tool.\n\nAdd run_manager: Optional[CallbackManagerForToolRun] = None\nto child implementations to enable tracing.',
 'properties': {'pets': {'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'},
  'user_id': {'title': 'User Id', 'type': 'string'}},
 'required': ['pets', 'user_id'],
 'title': 'update_favorite_petsSchema',
 'type': 'object'}


UpdateFavoritePets2
(
)
.
tool_call_schema
.
schema
(
)


{'description': 'Update list of favorite pets',
 'properties': {'pets': {'items': {'type': 'string'},
   'title': 'Pets',
   'type': 'array'}},
 'required': ['pets'],
 'title': 'update_favorite_pets',
 'type': 'object'}
Edit this page
Was this page helpful?
Previous
How to pass tool outputs to chat models
Next
How to stream events from a tool
Hiding arguments from the model
Injecting arguments at runtime
Other ways of annotating args


--- docs_how_to_tool_stream_events.txt ---

How-to guides
How to stream events from a tool
On this page
How to stream events from a tool


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Tools


Custom tools


Using stream events


Accessing RunnableConfig within a custom tool




If you have 
tools
 that call 
chat models
, 
retrievers
, or other 
runnables
, you may want to access internal events from those runnables or configure them with additional properties. This guide shows you how to manually pass parameters properly so that you can do this using the 
astream_events()
 method.


Compatibility
LangChain cannot automatically propagate configuration, including callbacks necessary for 
astream_events()
, to child runnables if you are running 
async
 code in 
python&lt;=3.10
. This is a common reason why you may fail to see events being emitted from custom runnables or tools.
If you are running python<=3.10, you will need to manually propagate the 
RunnableConfig
 object to the child runnable in async environments. For an example of how to manually propagate the config, see the implementation of the 
bar
 RunnableLambda below.
If you are running python>=3.11, the 
RunnableConfig
 will automatically propagate to child runnables in async environment. However, it is still a good idea to propagate the 
RunnableConfig
 manually if your code may run in older Python versions.
This guide also requires 
langchain-core>=0.2.16
.


Say you have a custom tool that calls a chain that condenses its input by prompting a chat model to return only 10 words, then reversing the output. First, define it in a naive way:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
tools 
import
 tool
@tool
async
 
def
 
special_summarization_tool
(
long_text
:
 
str
)
 
-
>
 
str
:
    
"""A tool that summarizes input text using advanced techniques."""
    prompt 
=
 ChatPromptTemplate
.
from_template
(
        
"You are an expert writer. Summarize the following text in 10 words or less:\n\n{long_text}"
    
)
    
def
 
reverse
(
x
:
 
str
)
:
        
return
 x
[
:
:
-
1
]
    chain 
=
 prompt 
|
 model 
|
 StrOutputParser
(
)
 
|
 reverse
    summary 
=
 
await
 chain
.
ainvoke
(
{
"long_text"
:
 long_text
}
)
    
return
 summary
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
tool


Invoking the tool directly works just fine:


LONG_TEXT 
=
 
"""
NARRATOR:
(Black screen with text; The sound of buzzing bees can be heard)
According to all known laws of aviation, there is no way a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway because bees don't care what humans think is impossible.
BARRY BENSON:
(Barry is picking out a shirt)
Yellow, black. Yellow, black. Yellow, black. Yellow, black. Ooh, black and yellow! Let's shake it up a little.
JANET BENSON:
Barry! Breakfast is ready!
BARRY:
Coming! Hang on a second.
"""
await
 special_summarization_tool
.
ainvoke
(
{
"long_text"
:
 LONG_TEXT
}
)


'.yad noitaudarg rof tiftuo sesoohc yrraB ;scisyhp seifed eeB'


But if you wanted to access the raw output from the chat model rather than the full tool, you might try to use the 
astream_events()
 method and look for an 
on_chat_model_end
 event. Here's what happens:


stream 
=
 special_summarization_tool
.
astream_events
(
{
"long_text"
:
 LONG_TEXT
}
)
async
 
for
 event 
in
 stream
:
    
if
 event
[
"event"
]
 
==
 
"on_chat_model_end"
:
        
# Never triggers in python<=3.10!
        
print
(
event
)


You'll notice (unless you're running through this guide in 
python>=3.11
) that there are no chat model events emitted from the child run!


This is because the example above does not pass the tool's config object into the internal chain. To fix this, redefine your tool to take a special parameter typed as 
RunnableConfig
 (see 
this guide
 for more details). You'll also need to pass that parameter through into the internal chain when executing it:


from
 langchain_core
.
runnables 
import
 RunnableConfig
@tool
async
 
def
 
special_summarization_tool_with_config
(
    long_text
:
 
str
,
 config
:
 RunnableConfig
)
 
-
>
 
str
:
    
"""A tool that summarizes input text using advanced techniques."""
    prompt 
=
 ChatPromptTemplate
.
from_template
(
        
"You are an expert writer. Summarize the following text in 10 words or less:\n\n{long_text}"
    
)
    
def
 
reverse
(
x
:
 
str
)
:
        
return
 x
[
:
:
-
1
]
    chain 
=
 prompt 
|
 model 
|
 StrOutputParser
(
)
 
|
 reverse
    
# Pass the "config" object as an argument to any executed runnables
    summary 
=
 
await
 chain
.
ainvoke
(
{
"long_text"
:
 long_text
}
,
 config
=
config
)
    
return
 summary
API Reference:
RunnableConfig


And now try the same 
astream_events()
 call as before with your new tool:


stream 
=
 special_summarization_tool_with_config
.
astream_events
(
{
"long_text"
:
 LONG_TEXT
}
)
async
 
for
 event 
in
 stream
:
    
if
 event
[
"event"
]
 
==
 
"on_chat_model_end"
:
        
print
(
event
)


{'event': 'on_chat_model_end', 'data': {'output': AIMessage(content='Bee defies physics; Barry chooses outfit for graduation day.', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-337ac14e-8da8-4c6d-a69f-1573f93b651e', usage_metadata={'input_tokens': 182, 'output_tokens': 19, 'total_tokens': 201, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}), 'input': {'messages': [[HumanMessage(content="You are an expert writer. Summarize the following text in 10 words or less:\n\n\nNARRATOR:\n(Black screen with text; The sound of buzzing bees can be heard)\nAccording to all known laws of aviation, there is no way a bee should be able to fly. Its wings are too small to get its fat little body off the ground. The bee, of course, flies anyway because bees don't care what humans think is impossible.\nBARRY BENSON:\n(Barry is picking out a shirt)\nYellow, black. Yellow, black. Yellow, black. Yellow, black. Ooh, black and yellow! Let's shake it up a little.\nJANET BENSON:\nBarry! Breakfast is ready!\nBARRY:\nComing! Hang on a second.\n", additional_kwargs={}, response_metadata={})]]}}, 'run_id': '337ac14e-8da8-4c6d-a69f-1573f93b651e', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['225beaa6-af73-4c91-b2d3-1afbbb88d53e']}


Awesome! This time there's an event emitted.


For streaming, 
astream_events()
 automatically calls internal runnables in a chain with streaming enabled if possible, so if you wanted to a stream of tokens as they are generated from the chat model, you could simply filter to look for 
on_chat_model_stream
 events with no other changes:


stream 
=
 special_summarization_tool_with_config
.
astream_events
(
{
"long_text"
:
 LONG_TEXT
}
)
async
 
for
 event 
in
 stream
:
    
if
 event
[
"event"
]
 
==
 
"on_chat_model_stream"
:
        
print
(
event
)


{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', usage_metadata={'input_tokens': 182, 'output_tokens': 2, 'total_tokens': 184, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='Bee', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' defies physics;', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' Barry chooses outfit for', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' graduation day.', additional_kwargs={}, response_metadata={}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5')}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}
{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run-f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', usage_metadata={'input_tokens': 0, 'output_tokens': 17, 'total_tokens': 17, 'input_token_details': {}})}, 'run_id': 'f5e049f7-4e98-4236-87ab-8cd1ce85a2d5', 'name': 'ChatAnthropic', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-5-sonnet-20240620', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'parent_ids': ['51858043-b301-4b76-8abb-56218e405283']}


Next steps
​


You've now seen how to stream events from within a tool. Next, check out the following guides for more on using tools:




Pass 
runtime values to tools


Pass 
tool results back to a model


Dispatch custom callback events




You can also check out some more specific uses of tool calling:




Building 
tool-using chains and agents


Getting 
structured outputs
 from models


Edit this page
Was this page helpful?
Previous
How to pass run time values to tools
Next
How to stream tool calls
Next steps


--- docs_how_to_tool_streaming.txt ---

How-to guides
How to stream tool calls
How to stream tool calls


When 
tools
 are called in a streaming context,

message chunks

will be populated with 
tool call chunk

objects in a list via the 
.tool_call_chunks
 attribute. A 
ToolCallChunk
 includes
optional string fields for the tool 
name
, 
args
, and 
id
, and includes an optional
integer field 
index
 that can be used to join chunks together. Fields are optional
because portions of a tool call may be streamed across different chunks (e.g., a chunk
that includes a substring of the arguments may have null values for the tool name and id).


Because message chunks inherit from their parent message class, an

AIMessageChunk

with tool call chunks will also include 
.tool_calls
 and 
.invalid_tool_calls
 fields.
These fields are parsed best-effort from the message's tool call chunks.


Note that not all providers currently support streaming for tool calls. Before we start let's define our tools and our model.


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
add
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Adds a and b."""
    
return
 a 
+
 b
@tool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiplies a and b."""
    
return
 a 
*
 b
tools 
=
 
[
add
,
 multiply
]
API Reference:
tool


import
 os
from
 getpass 
import
 getpass
from
 langchain_openai 
import
 ChatOpenAI
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
llm_with_tools 
=
 llm
.
bind_tools
(
tools
)
API Reference:
ChatOpenAI


Now let's define our query and stream our output:


query 
=
 
"What is 3 * 12? Also, what is 11 + 49?"
async
 
for
 chunk 
in
 llm_with_tools
.
astream
(
query
)
:
    
print
(
chunk
.
tool_call_chunks
)


[]
[{'name': 'Multiply', 'args': '', 'id': 'call_3aQwTP9CYlFxwOvQZPHDu6wL', 'index': 0}]
[{'name': None, 'args': '{"a"', 'id': None, 'index': 0}]
[{'name': None, 'args': ': 3, ', 'id': None, 'index': 0}]
[{'name': None, 'args': '"b": 1', 'id': None, 'index': 0}]
[{'name': None, 'args': '2}', 'id': None, 'index': 0}]
[{'name': 'Add', 'args': '', 'id': 'call_SQUoSsJz2p9Kx2x73GOgN1ja', 'index': 1}]
[{'name': None, 'args': '{"a"', 'id': None, 'index': 1}]
[{'name': None, 'args': ': 11,', 'id': None, 'index': 1}]
[{'name': None, 'args': ' "b": ', 'id': None, 'index': 1}]
[{'name': None, 'args': '49}', 'id': None, 'index': 1}]
[]


Note that adding message chunks will merge their corresponding tool call chunks. This is the principle by which LangChain's various 
tool output parsers
 support streaming.


For example, below we accumulate tool call chunks:


first 
=
 
True
async
 
for
 chunk 
in
 llm_with_tools
.
astream
(
query
)
:
    
if
 first
:
        gathered 
=
 chunk
        first 
=
 
False
    
else
:
        gathered 
=
 gathered 
+
 chunk
    
print
(
gathered
.
tool_call_chunks
)


[]
[{'name': 'Multiply', 'args': '', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a"', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, ', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 1', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{"a"', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{"a": 11,', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": ', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": 49}', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]
[{'name': 'Multiply', 'args': '{"a": 3, "b": 12}', 'id': 'call_AkL3dVeCjjiqvjv8ckLxL3gP', 'index': 0}, {'name': 'Add', 'args': '{"a": 11, "b": 49}', 'id': 'call_b4iMiB3chGNGqbt5SjqqD2Wh', 'index': 1}]


print
(
type
(
gathered
.
tool_call_chunks
[
0
]
[
"args"
]
)
)


<class 'str'>


And below we accumulate tool calls to demonstrate partial parsing:


first 
=
 
True
async
 
for
 chunk 
in
 llm_with_tools
.
astream
(
query
)
:
    
if
 first
:
        gathered 
=
 chunk
        first 
=
 
False
    
else
:
        gathered 
=
 gathered 
+
 chunk
    
print
(
gathered
.
tool_calls
)


[]
[]
[{'name': 'Multiply', 'args': {}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]
[{'name': 'Multiply', 'args': {'a': 3}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 1}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]
[{'name': 'Multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_4p0D4tHVXSiae9Mu0e8jlI1m'}, {'name': 'Add', 'args': {'a': 11, 'b': 49}, 'id': 'call_54Hx3DGjZitFlEjgMe1DYonh'}]


print
(
type
(
gathered
.
tool_calls
[
0
]
[
"args"
]
)
)


<class 'dict'>
Edit this page
Was this page helpful?
Previous
How to stream events from a tool
Next
How to convert tools to OpenAI Functions


--- docs_how_to_toolkits.txt ---

How-to guides
How to use toolkits
How to use toolkits


Toolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.


All Toolkits expose a 
get_tools
 method which returns a list of tools.
You can therefore do:


# Initialize a toolkit
toolkit 
=
 ExampleTookit
(
.
.
.
)
# Get list of tools
tools 
=
 toolkit
.
get_tools
(
)
# Create agent
agent 
=
 create_agent_method
(
llm
,
 tools
,
 prompt
)
Edit this page
Was this page helpful?
Previous
How to summarize text in a single LLM call
Next
How to add ad-hoc tool calling capability to LLMs and Chat Models


--- docs_how_to_tools_as_openai_functions.txt ---

How-to guides
How to convert tools to OpenAI Functions
How to convert tools to OpenAI Functions


This notebook goes over how to use LangChain 
tools
 as OpenAI functions.


%
pip install 
-
qU langchain
-
community langchain
-
openai


from
 langchain_community
.
tools 
import
 MoveFileTool
from
 langchain_core
.
messages 
import
 HumanMessage
from
 langchain_core
.
utils
.
function_calling 
import
 convert_to_openai_function
from
 langchain_openai 
import
 ChatOpenAI
API Reference:
MoveFileTool
 | 
HumanMessage
 | 
convert_to_openai_function
 | 
ChatOpenAI


model 
=
 ChatOpenAI
(
model
=
"gpt-3.5-turbo"
)


tools 
=
 
[
MoveFileTool
(
)
]
functions 
=
 
[
convert_to_openai_function
(
t
)
 
for
 t 
in
 tools
]


functions
[
0
]


{'name': 'move_file',
 'description': 'Move or rename a file from one location to another',
 'parameters': {'type': 'object',
  'properties': {'source_path': {'description': 'Path of the file to move',
    'type': 'string'},
   'destination_path': {'description': 'New path for the moved file',
    'type': 'string'}},
  'required': ['source_path', 'destination_path']}}


message 
=
 model
.
invoke
(
    
[
HumanMessage
(
content
=
"move file foo to bar"
)
]
,
 functions
=
functions
)


message


AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\n  "source_path": "foo",\n  "destination_path": "bar"\n}', 'name': 'move_file'}})


message
.
additional_kwargs
[
"function_call"
]


{'name': 'move_file',
 'arguments': '{\n  "source_path": "foo",\n  "destination_path": "bar"\n}'}


With OpenAI chat models we can also automatically bind and convert function-like objects with 
bind_functions


model_with_functions 
=
 model
.
bind_functions
(
tools
)
model_with_functions
.
invoke
(
[
HumanMessage
(
content
=
"move file foo to bar"
)
]
)


AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\n  "source_path": "foo",\n  "destination_path": "bar"\n}', 'name': 'move_file'}})


Or we can use the update OpenAI API that uses 
tools
 and 
tool_choice
 instead of 
functions
 and 
function_call
 by using 
ChatOpenAI.bind_tools
:


model_with_tools 
=
 model
.
bind_tools
(
tools
)
model_with_tools
.
invoke
(
[
HumanMessage
(
content
=
"move file foo to bar"
)
]
)


AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_btkY3xV71cEVAOHnNa5qwo44', 'function': {'arguments': '{\n  "source_path": "foo",\n  "destination_path": "bar"\n}', 'name': 'move_file'}, 'type': 'function'}]})
Edit this page
Was this page helpful?
Previous
How to stream tool calls
Next
How to handle tool errors


--- docs_how_to_tools_builtin.txt ---

How-to guides
How to use built-in tools and toolkits
On this page
How to use built-in tools and toolkits


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Tools


LangChain Toolkits




Tools
​


LangChain has a large collection of 3rd party tools. Please visit 
Tool Integrations
 for a list of the available tools.


important
When using 3rd party tools, make sure that you understand how the tool works, what permissions
it has. Read over its documentation and check if anything is required from you
from a security point of view. Please see our 
security

guidelines for more information.


Let's try out the 
Wikipedia integration
.


!pip install 
-
qU langchain
-
community wikipedia


from
 langchain_community
.
tools 
import
 WikipediaQueryRun
from
 langchain_community
.
utilities 
import
 WikipediaAPIWrapper
api_wrapper 
=
 WikipediaAPIWrapper
(
top_k_results
=
1
,
 doc_content_chars_max
=
100
)
tool 
=
 WikipediaQueryRun
(
api_wrapper
=
api_wrapper
)
print
(
tool
.
invoke
(
{
"query"
:
 
"langchain"
}
)
)
API Reference:
WikipediaQueryRun
 | 
WikipediaAPIWrapper


Page: LangChain
Summary: LangChain is a framework designed to simplify the creation of applications


The tool has the following defaults associated with it:


print
(
f"Name: 
{
tool
.
name
}
"
)
print
(
f"Description: 
{
tool
.
description
}
"
)
print
(
f"args schema: 
{
tool
.
args
}
"
)
print
(
f"returns directly?: 
{
tool
.
return_direct
}
"
)


Name: wikipedia
Description: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.
args schema: {'query': {'description': 'query to look up on wikipedia', 'title': 'Query', 'type': 'string'}}
returns directly?: False


Customizing Default Tools
​


We can also modify the built in name, description, and JSON schema of the arguments.


When defining the JSON schema of the arguments, it is important that the inputs remain the same as the function, so you shouldn't change that. But you can define custom descriptions for each input easily.


from
 langchain_community
.
tools 
import
 WikipediaQueryRun
from
 langchain_community
.
utilities 
import
 WikipediaAPIWrapper
from
 pydantic 
import
 BaseModel
,
 Field
class
 
WikiInputs
(
BaseModel
)
:
    
"""Inputs to the wikipedia tool."""
    query
:
 
str
 
=
 Field
(
        description
=
"query to look up in Wikipedia, should be 3 or less words"
    
)
tool 
=
 WikipediaQueryRun
(
    name
=
"wiki-tool"
,
    description
=
"look up things in wikipedia"
,
    args_schema
=
WikiInputs
,
    api_wrapper
=
api_wrapper
,
    return_direct
=
True
,
)
print
(
tool
.
run
(
"langchain"
)
)
API Reference:
WikipediaQueryRun
 | 
WikipediaAPIWrapper


Page: LangChain
Summary: LangChain is a framework designed to simplify the creation of applications


print
(
f"Name: 
{
tool
.
name
}
"
)
print
(
f"Description: 
{
tool
.
description
}
"
)
print
(
f"args schema: 
{
tool
.
args
}
"
)
print
(
f"returns directly?: 
{
tool
.
return_direct
}
"
)


Name: wiki-tool
Description: look up things in wikipedia
args schema: {'query': {'description': 'query to look up in Wikipedia, should be 3 or less words', 'title': 'Query', 'type': 'string'}}
returns directly?: True


How to use built-in toolkits
​


Toolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.


All Toolkits expose a 
get_tools
 method which returns a list of tools.


You're usually meant to use them this way:


# Initialize a toolkit
toolkit 
=
 ExampleTookit
(
.
.
.
)
# Get list of tools
tools 
=
 toolkit
.
get_tools
(
)
Edit this page
Was this page helpful?
Previous
How to handle multiple queries when doing query analysis
Next
How to pass through arguments from one step to the next
Tools
Customizing Default Tools
How to use built-in toolkits


--- docs_how_to_tools_chain.txt ---

How-to guides
How to use tools in a chain
On this page
How to use tools in a chain


In this guide, we will go over the basic ways to create Chains and Agents that call 
Tools
. Tools can be just about anything — APIs, functions, databases, etc. Tools allow us to extend the capabilities of a model beyond just outputting text/messages. The key to using models with tools is correctly prompting a model and parsing its response so that it chooses the right tools and provides the right inputs for them.


Setup
​


We'll need to install the following packages for this guide:


%
pip install 
-
-
upgrade 
-
-
quiet langchain


If you'd like to trace your runs in 
LangSmith
 uncomment and set the following environment variables:


import
 getpass
import
 os
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


Create a tool
​


First, we need to create a tool to call. For this example, we will create a custom tool from a function. For more information on creating custom tools, please see 
this guide
.


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
multiply
(
first_int
:
 
int
,
 second_int
:
 
int
)
 
-
>
 
int
:
    
"""Multiply two integers together."""
    
return
 first_int 
*
 second_int
API Reference:
tool


print
(
multiply
.
name
)
print
(
multiply
.
description
)
print
(
multiply
.
args
)


multiply
Multiply two integers together.
{'first_int': {'title': 'First Int', 'type': 'integer'}, 'second_int': {'title': 'Second Int', 'type': 'integer'}}


multiply
.
invoke
(
{
"first_int"
:
 
4
,
 
"second_int"
:
 
5
}
)


20


Chains
​


If we know that we only need to use a tool a fixed number of times, we can create a chain for doing so. Let's create a simple chain that just multiplies user-specified numbers.




Tool/function calling
​


One of the most reliable ways to use tools with LLMs is with 
tool calling
 APIs (also sometimes called function calling). This only works with models that explicitly support tool calling. You can see which models support tool calling 
here
, and learn more about how to use tool calling in 
this guide
.


First we'll define our model and tools. We'll start with just a single tool, 
multiply
.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


We'll use 
bind_tools
 to pass the definition of our tool in as part of each call to the model, so that the model can invoke the tool when appropriate:


llm_with_tools 
=
 llm
.
bind_tools
(
[
multiply
]
)


When the model invokes the tool, this will show up in the 
AIMessage.tool_calls
 attribute of the output:


msg 
=
 llm_with_tools
.
invoke
(
"whats 5 times forty two"
)
msg
.
tool_calls


[{'name': 'multiply',
  'args': {'first_int': 5, 'second_int': 42},
  'id': 'call_8QIg4QVFVAEeC1orWAgB2036',
  'type': 'tool_call'}]


Check out the 
LangSmith trace here
.


Invoking the tool
​


Great! We're able to generate tool invocations. But what if we want to actually call the tool? To do so we'll need to pass the generated tool args to our tool. As a simple example we'll just extract the arguments of the first tool_call:


from
 operator 
import
 itemgetter
chain 
=
 llm_with_tools 
|
 
(
lambda
 x
:
 x
.
tool_calls
[
0
]
[
"args"
]
)
 
|
 multiply
chain
.
invoke
(
"What's four times 23"
)


92


Check out the 
LangSmith trace here
.


Agents
​


Chains are great when we know the specific sequence of tool usage needed for any user input. But for certain use cases, how many times we use tools depends on the input. In these cases, we want to let the model itself decide how many times to use tools and in what order. 
Agents
 let us do just this.


We'll demonstrate a simple example using a LangGraph agent. See 
this tutorial
 for more detail.




!pip install 
-
qU langgraph


from
 langgraph
.
prebuilt 
import
 create_react_agent
API Reference:
create_react_agent


Agents are also great because they make it easy to use multiple tools.


@tool
def
 
add
(
first_int
:
 
int
,
 second_int
:
 
int
)
 
-
>
 
int
:
    
"Add two integers."
    
return
 first_int 
+
 second_int
@tool
def
 
exponentiate
(
base
:
 
int
,
 exponent
:
 
int
)
 
-
>
 
int
:
    
"Exponentiate the base to the exponent power."
    
return
 base
**
exponent
tools 
=
 
[
multiply
,
 add
,
 exponentiate
]


# Construct the tool calling agent
agent 
=
 create_react_agent
(
llm
,
 tools
)


With an agent, we can ask questions that require arbitrarily-many uses of our tools:


# Use the agent
query 
=
 
(
    
"Take 3 to the fifth power and multiply that by the sum of twelve and "
    
"three, then square the whole result."
)
input_message 
=
 
{
"role"
:
 
"user"
,
 
"content"
:
 query
}
for
 step 
in
 agent
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 stream_mode
=
"values"
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result.
==================================[1m Ai Message [0m==================================
Tool Calls:
  exponentiate (call_EHGS8gnEVNCJQ9rVOk11KCQH)
 Call ID: call_EHGS8gnEVNCJQ9rVOk11KCQH
  Args:
    base: 3
    exponent: 5
  add (call_s2cxOrXEKqI6z7LWbMUG6s8c)
 Call ID: call_s2cxOrXEKqI6z7LWbMUG6s8c
  Args:
    first_int: 12
    second_int: 3
=================================[1m Tool Message [0m=================================
Name: add
15
==================================[1m Ai Message [0m==================================
Tool Calls:
  multiply (call_25v5JEfDWuKNgmVoGBan0d7J)
 Call ID: call_25v5JEfDWuKNgmVoGBan0d7J
  Args:
    first_int: 243
    second_int: 15
=================================[1m Tool Message [0m=================================
Name: multiply
3645
==================================[1m Ai Message [0m==================================
Tool Calls:
  exponentiate (call_x1yKEeBPrFYmCp2z5Kn8705r)
 Call ID: call_x1yKEeBPrFYmCp2z5Kn8705r
  Args:
    base: 3645
    exponent: 2
=================================[1m Tool Message [0m=================================
Name: exponentiate
13286025
==================================[1m Ai Message [0m==================================
The final result of taking 3 to the fifth power, multiplying it by the sum of twelve and three, and then squaring the whole result is **13,286,025**.


Check out the 
LangSmith trace here
.
Edit this page
Was this page helpful?
Previous
How-to guides
Next
How to use a vectorstore as a retriever
Setup
Create a tool
Chains
Tool/function calling
Invoking the tool
Agents


--- docs_how_to_tools_error.txt ---

How-to guides
How to handle tool errors
On this page
How to handle tool errors


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


LangChain Tools


How to use a model to call tools




Calling tools
 with an LLM is generally more reliable than pure prompting, but it isn't perfect. The model may try to call a tool that doesn't exist or fail to return arguments that match the requested schema. Strategies like keeping schemas simple, reducing the number of tools you pass at once, and having good names and descriptions can help mitigate this risk, but aren't foolproof.


This guide covers some ways to build error handling into your chains to mitigate these failure modes.


Setup
​


We'll need to install the following packages:


%
pip install 
-
-
upgrade 
-
-
quiet langchain
-
core langchain
-
openai


If you'd like to trace your runs in 
LangSmith
 uncomment and set the following environment variables:


import
 getpass
import
 os
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


Chain
​


Suppose we have the following (dummy) tool and tool-calling chain. We'll make our tool intentionally convoluted to try and trip up the model.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


# Define tool
from
 langchain_core
.
tools 
import
 tool
@tool
def
 
complex_tool
(
int_arg
:
 
int
,
 float_arg
:
 
float
,
 dict_arg
:
 
dict
)
 
-
>
 
int
:
    
"""Do something complex with a complex tool."""
    
return
 int_arg 
*
 float_arg
llm_with_tools 
=
 llm
.
bind_tools
(
    
[
complex_tool
]
,
)
# Define chain
chain 
=
 llm_with_tools 
|
 
(
lambda
 msg
:
 msg
.
tool_calls
[
0
]
[
"args"
]
)
 
|
 complex_tool
API Reference:
tool


We can see that when we try to invoke this chain with even a fairly explicit input, the model fails to correctly call the tool (it forgets the 
dict_arg
 argument).


chain
.
invoke
(
    
"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg"
)


---------------------------------------------------------------------------
``````output
ValidationError                           Traceback (most recent call last)
``````output
Cell In[5], line 1
----> 1 chain.invoke(
      2     "use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg"
      3 )
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2998, in RunnableSequence.invoke(self, input, config, **kwargs)
   2996             input = context.run(step.invoke, input, config, **kwargs)
   2997         else:
-> 2998             input = context.run(step.invoke, input, config)
   2999 # finish the root run
   3000 except BaseException as e:
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:456, in BaseTool.invoke(self, input, config, **kwargs)
    449 def invoke(
    450     self,
    451     input: Union[str, Dict, ToolCall],
    452     config: Optional[RunnableConfig] = None,
    453     **kwargs: Any,
    454 ) -> Any:
    455     tool_input, kwargs = _prep_run_args(input, config, **kwargs)
--> 456     return self.run(tool_input, **kwargs)
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:659, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)
    657 if error_to_raise:
    658     run_manager.on_tool_error(error_to_raise)
--> 659     raise error_to_raise
    660 output = _format_output(content, artifact, tool_call_id, self.name, status)
    661 run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:622, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)
    620 context = copy_context()
    621 context.run(_set_config_context, child_config)
--> 622 tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input)
    623 if signature(self._run).parameters.get("run_manager"):
    624     tool_kwargs["run_manager"] = run_manager
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:545, in BaseTool._to_args_and_kwargs(self, tool_input)
    544 def _to_args_and_kwargs(self, tool_input: Union[str, Dict]) -> Tuple[Tuple, Dict]:
--> 545     tool_input = self._parse_input(tool_input)
    546     # For backwards compatibility, if run_input is a string,
    547     # pass as a positional argument.
    548     if isinstance(tool_input, str):
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/langchain_core/tools/base.py:487, in BaseTool._parse_input(self, tool_input)
    485 if input_args is not None:
    486     if issubclass(input_args, BaseModel):
--> 487         result = input_args.model_validate(tool_input)
    488         result_dict = result.model_dump()
    489     elif issubclass(input_args, BaseModelV1):
``````output
File ~/langchain/.venv/lib/python3.11/site-packages/pydantic/main.py:568, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)
    566 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
    567 __tracebackhide__ = True
--> 568 return cls.__pydantic_validator__.validate_python(
    569     obj, strict=strict, from_attributes=from_attributes, context=context
    570 )
``````output
ValidationError: 1 validation error for complex_toolSchema
dict_arg
  Field required [type=missing, input_value={'int_arg': 5, 'float_arg': 2.1}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.8/v/missing


Try/except tool call
​


The simplest way to more gracefully handle errors is to try/except the tool-calling step and return a helpful message on errors:


from
 typing 
import
 Any
from
 langchain_core
.
runnables 
import
 Runnable
,
 RunnableConfig
def
 
try_except_tool
(
tool_args
:
 
dict
,
 config
:
 RunnableConfig
)
 
-
>
 Runnable
:
    
try
:
        complex_tool
.
invoke
(
tool_args
,
 config
=
config
)
    
except
 Exception 
as
 e
:
        
return
 
f"Calling tool with arguments:\n\n
{
tool_args
}
\n\nraised the following error:\n\n
{
type
(
e
)
}
: 
{
e
}
"
chain 
=
 llm_with_tools 
|
 
(
lambda
 msg
:
 msg
.
tool_calls
[
0
]
[
"args"
]
)
 
|
 try_except_tool
print
(
    chain
.
invoke
(
        
"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg"
    
)
)
API Reference:
Runnable
 | 
RunnableConfig


Calling tool with arguments:
{'int_arg': 5, 'float_arg': 2.1}
raised the following error:
<class 'pydantic_core._pydantic_core.ValidationError'>: 1 validation error for complex_toolSchema
dict_arg
  Field required [type=missing, input_value={'int_arg': 5, 'float_arg': 2.1}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.8/v/missing


Fallbacks
​


We can also try to fallback to a better model in the event of a tool invocation error. In this case we'll fall back to an identical chain that uses 
gpt-4-1106-preview
 instead of 
gpt-3.5-turbo
.


chain 
=
 llm_with_tools 
|
 
(
lambda
 msg
:
 msg
.
tool_calls
[
0
]
[
"args"
]
)
 
|
 complex_tool
better_model 
=
 ChatOpenAI
(
model
=
"gpt-4-1106-preview"
,
 temperature
=
0
)
.
bind_tools
(
    
[
complex_tool
]
,
 tool_choice
=
"complex_tool"
)
better_chain 
=
 better_model 
|
 
(
lambda
 msg
:
 msg
.
tool_calls
[
0
]
[
"args"
]
)
 
|
 complex_tool
chain_with_fallback 
=
 chain
.
with_fallbacks
(
[
better_chain
]
)
chain_with_fallback
.
invoke
(
    
"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg"
)


10.5


Looking at the 
LangSmith trace
 for this chain run, we can see that the first chain call fails as expected and it's the fallback that succeeds.


Retry with exception
​


To take things one step further, we can try to automatically re-run the chain with the exception passed in, so that the model may be able to correct its behavior:


from
 langchain_core
.
messages 
import
 AIMessage
,
 HumanMessage
,
 ToolCall
,
 ToolMessage
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
class
 
CustomToolException
(
Exception
)
:
    
"""Custom LangChain tool exception."""
    
def
 
__init__
(
self
,
 tool_call
:
 ToolCall
,
 exception
:
 Exception
)
 
-
>
 
None
:
        
super
(
)
.
__init__
(
)
        self
.
tool_call 
=
 tool_call
        self
.
exception 
=
 exception
def
 
tool_custom_exception
(
msg
:
 AIMessage
,
 config
:
 RunnableConfig
)
 
-
>
 Runnable
:
    
try
:
        
return
 complex_tool
.
invoke
(
msg
.
tool_calls
[
0
]
[
"args"
]
,
 config
=
config
)
    
except
 Exception 
as
 e
:
        
raise
 CustomToolException
(
msg
.
tool_calls
[
0
]
,
 e
)
def
 
exception_to_messages
(
inputs
:
 
dict
)
 
-
>
 
dict
:
    exception 
=
 inputs
.
pop
(
"exception"
)
    
# Add historical messages to the original input, so the model knows that it made a mistake with the last tool call.
    messages 
=
 
[
        AIMessage
(
content
=
""
,
 tool_calls
=
[
exception
.
tool_call
]
)
,
        ToolMessage
(
            tool_call_id
=
exception
.
tool_call
[
"id"
]
,
 content
=
str
(
exception
.
exception
)
        
)
,
        HumanMessage
(
            content
=
"The last tool call raised an exception. Try calling the tool again with corrected arguments. Do not repeat mistakes."
        
)
,
    
]
    inputs
[
"last_output"
]
 
=
 messages
    
return
 inputs
# We add a last_output MessagesPlaceholder to our prompt which if not passed in doesn't
# affect the prompt at all, but gives us the option to insert an arbitrary list of Messages
# into the prompt if needed. We'll use this on retries to insert the error message.
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"human"
,
 
"{input}"
)
,
 
(
"placeholder"
,
 
"{last_output}"
)
]
)
chain 
=
 prompt 
|
 llm_with_tools 
|
 tool_custom_exception
# If the initial chain call fails, we rerun it withe the exception passed in as a message.
self_correcting_chain 
=
 chain
.
with_fallbacks
(
    
[
exception_to_messages 
|
 chain
]
,
 exception_key
=
"exception"
)
API Reference:
AIMessage
 | 
HumanMessage
 | 
ToolCall
 | 
ToolMessage
 | 
ChatPromptTemplate


self_correcting_chain
.
invoke
(
    
{
        
"input"
:
 
"use complex tool. the args are 5, 2.1, empty dictionary. don't forget dict_arg"
    
}
)


10.5


And our chain succeeds! Looking at the 
LangSmith trace
, we can see that indeed our initial chain still fails, and it's only on retrying that the chain succeeds.


Next steps
​


Now you've seen some strategies how to handle tool calling errors. Next, you can learn more about how to use tools:




Few shot prompting 
with tools


Stream 
tool calls


Pass 
runtime values to tools




You can also check out some more specific uses of tool calling:




Getting 
structured outputs
 from models


Edit this page
Was this page helpful?
Previous
How to convert tools to OpenAI Functions
Next
How to use few-shot prompting with tool calling
Setup
Chain
Try/except tool call
Fallbacks
Retry with exception
Next steps


--- docs_how_to_tools_few_shot.txt ---

How-to guides
How to use few-shot prompting with tool calling
How to use few-shot prompting with tool calling


For more complex tool use it's very useful to add 
few-shot examples
 to the prompt. We can do this by adding 
AIMessage
s with 
ToolCall
s and corresponding 
ToolMessage
s to our prompt.


First let's define our tools and model.


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
add
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Adds a and b."""
    
return
 a 
+
 b
@tool
def
 
multiply
(
a
:
 
int
,
 b
:
 
int
)
 
-
>
 
int
:
    
"""Multiplies a and b."""
    
return
 a 
*
 b
tools 
=
 
[
add
,
 multiply
]
API Reference:
tool


import
 os
from
 getpass 
import
 getpass
from
 langchain_openai 
import
 ChatOpenAI
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
llm_with_tools 
=
 llm
.
bind_tools
(
tools
)
API Reference:
ChatOpenAI


Let's run our model where we can notice that even with some special instructions our model can get tripped up by order of operations.


llm_with_tools
.
invoke
(
    
"Whats 119 times 8 minus 20. Don't do any math yourself, only use tools for math. Respect order of operations"
)
.
tool_calls


[{'name': 'Multiply',
  'args': {'a': 119, 'b': 8},
  'id': 'call_T88XN6ECucTgbXXkyDeC2CQj'},
 {'name': 'Add',
  'args': {'a': 952, 'b': -20},
  'id': 'call_licdlmGsRqzup8rhqJSb1yZ4'}]


The model shouldn't be trying to add anything yet, since it technically can't know the results of 119 * 8 yet.


By adding a prompt with some examples we can correct this behavior:


from
 langchain_core
.
messages 
import
 AIMessage
,
 HumanMessage
,
 ToolMessage
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
examples 
=
 
[
    HumanMessage
(
        
"What's the product of 317253 and 128472 plus four"
,
 name
=
"example_user"
    
)
,
    AIMessage
(
        
""
,
        name
=
"example_assistant"
,
        tool_calls
=
[
            
{
"name"
:
 
"Multiply"
,
 
"args"
:
 
{
"x"
:
 
317253
,
 
"y"
:
 
128472
}
,
 
"id"
:
 
"1"
}
        
]
,
    
)
,
    ToolMessage
(
"16505054784"
,
 tool_call_id
=
"1"
)
,
    AIMessage
(
        
""
,
        name
=
"example_assistant"
,
        tool_calls
=
[
{
"name"
:
 
"Add"
,
 
"args"
:
 
{
"x"
:
 
16505054784
,
 
"y"
:
 
4
}
,
 
"id"
:
 
"2"
}
]
,
    
)
,
    ToolMessage
(
"16505054788"
,
 tool_call_id
=
"2"
)
,
    AIMessage
(
        
"The product of 317253 and 128472 plus four is 16505054788"
,
        name
=
"example_assistant"
,
    
)
,
]
system 
=
 
"""You are bad at math but are an expert at using a calculator. 
Use past tool usage as an example of how to correctly use the tools."""
few_shot_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 system
)
,
        
*
examples
,
        
(
"human"
,
 
"{query}"
)
,
    
]
)
chain 
=
 
{
"query"
:
 RunnablePassthrough
(
)
}
 
|
 few_shot_prompt 
|
 llm_with_tools
chain
.
invoke
(
"Whats 119 times 8 minus 20"
)
.
tool_calls
API Reference:
AIMessage
 | 
HumanMessage
 | 
ToolMessage
 | 
ChatPromptTemplate
 | 
RunnablePassthrough


[{'name': 'Multiply',
  'args': {'a': 119, 'b': 8},
  'id': 'call_9MvuwQqg7dlJupJcoTWiEsDo'}]


And we get the correct output this time.


Here's what the 
LangSmith trace
 looks like.
Edit this page
Was this page helpful?
Previous
How to handle tool errors
Next
How to add a human-in-the-loop for tools


--- docs_how_to_tools_human.txt ---

How-to guides
How to add a human-in-the-loop for tools
On this page
How to add a human-in-the-loop for tools


There are certain tools that we don't trust a model to execute on its own. One thing we can do in such situations is require human approval before the tool is invoked.


info
This how-to guide shows a simple way to add human-in-the-loop for code running in a jupyter notebook or in a terminal.
To build a production application, you will need to do more work to keep track of application state appropriately.
We recommend using 
langgraph
 for powering such a capability. For more details, please see this 
guide
.


Setup
​


We'll need to install the following packages:


%
pip install 
-
-
upgrade 
-
-
quiet langchain


And set these environment variables:


import
 getpass
import
 os
# If you'd like to use LangSmith, uncomment the below:
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


Chain
​


Let's create a few simple (dummy) tools and a tool-calling chain:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


from
 typing 
import
 Dict
,
 List
from
 langchain_core
.
messages 
import
 AIMessage
from
 langchain_core
.
runnables 
import
 Runnable
,
 RunnablePassthrough
from
 langchain_core
.
tools 
import
 tool
@tool
def
 
count_emails
(
last_n_days
:
 
int
)
 
-
>
 
int
:
    
"""Dummy function to count number of e-mails. Returns 2 * last_n_days."""
    
return
 last_n_days 
*
 
2
@tool
def
 
send_email
(
message
:
 
str
,
 recipient
:
 
str
)
 
-
>
 
str
:
    
"""Dummy function for sending an e-mail."""
    
return
 
f"Successfully sent email to 
{
recipient
}
."
tools 
=
 
[
count_emails
,
 send_email
]
llm_with_tools 
=
 llm
.
bind_tools
(
tools
)
def
 
call_tools
(
msg
:
 AIMessage
)
 
-
>
 List
[
Dict
]
:
    
"""Simple sequential tool calling helper."""
    tool_map 
=
 
{
tool
.
name
:
 tool 
for
 tool 
in
 tools
}
    tool_calls 
=
 msg
.
tool_calls
.
copy
(
)
    
for
 tool_call 
in
 tool_calls
:
        tool_call
[
"output"
]
 
=
 tool_map
[
tool_call
[
"name"
]
]
.
invoke
(
tool_call
[
"args"
]
)
    
return
 tool_calls
chain 
=
 llm_with_tools 
|
 call_tools
chain
.
invoke
(
"how many emails did i get in the last 5 days?"
)
API Reference:
AIMessage
 | 
Runnable
 | 
RunnablePassthrough
 | 
tool


[{'name': 'count_emails',
  'args': {'last_n_days': 5},
  'id': 'toolu_01QYZdJ4yPiqsdeENWHqioFW',
  'output': 10}]


Adding human approval
​


Let's add a step in the chain that will ask a person to approve or reject the tool call request.


On rejection, the step will raise an exception which will stop execution of the rest of the chain.


import
 json
class
 
NotApproved
(
Exception
)
:
    
"""Custom exception."""
def
 
human_approval
(
msg
:
 AIMessage
)
 
-
>
 AIMessage
:
    
"""Responsible for passing through its input or raising an exception.
    Args:
        msg: output from the chat model
    Returns:
        msg: original output from the msg
    """
    tool_strs 
=
 
"\n\n"
.
join
(
        json
.
dumps
(
tool_call
,
 indent
=
2
)
 
for
 tool_call 
in
 msg
.
tool_calls
    
)
    input_msg 
=
 
(
        
f"Do you approve of the following tool invocations\n\n
{
tool_strs
}
\n\n"
        
"Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no.\n >>>"
    
)
    resp 
=
 
input
(
input_msg
)
    
if
 resp
.
lower
(
)
 
not
 
in
 
(
"yes"
,
 
"y"
)
:
        
raise
 NotApproved
(
f"Tool invocations not approved:\n\n
{
tool_strs
}
"
)
    
return
 msg


chain 
=
 llm_with_tools 
|
 human_approval 
|
 call_tools
chain
.
invoke
(
"how many emails did i get in the last 5 days?"
)


Do you approve of the following tool invocations
{
  "name": "count_emails",
  "args": {
    "last_n_days": 5
  },
  "id": "toolu_01WbD8XeMoQaRFtsZezfsHor"
}
Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no.
 >>> yes


[{'name': 'count_emails',
  'args': {'last_n_days': 5},
  'id': 'toolu_01WbD8XeMoQaRFtsZezfsHor',
  'output': 10}]


try
:
    chain
.
invoke
(
"Send sally@gmail.com an email saying 'What's up homie'"
)
except
 NotApproved 
as
 e
:
    
print
(
)
    
print
(
e
)


Do you approve of the following tool invocations
{
  "name": "send_email",
  "args": {
    "recipient": "sally@gmail.com",
    "message": "What's up homie"
  },
  "id": "toolu_014XccHFzBiVcc9GV1harV9U"
}
Anything except 'Y'/'Yes' (case-insensitive) will be treated as a no.
 >>> no
``````output
Tool invocations not approved:
{
  "name": "send_email",
  "args": {
    "recipient": "sally@gmail.com",
    "message": "What's up homie"
  },
  "id": "toolu_014XccHFzBiVcc9GV1harV9U"
}
Edit this page
Was this page helpful?
Previous
How to use few-shot prompting with tool calling
Next
How to bind model-specific tools
Setup
Chain
Adding human approval


--- docs_how_to_tools_model_specific.txt ---

How-to guides
How to bind model-specific tools
How to bind model-specific tools


Providers adopt different conventions for formatting tool schemas.
For instance, OpenAI uses a format like this:




type
: The type of the tool. At the time of writing, this is always 
"function"
.


function
: An object containing tool parameters.


function.name
: The name of the schema to output.


function.description
: A high level description of the schema to output.


function.parameters
: The nested details of the schema you want to extract, formatted as a 
JSON schema
 dict.




We can bind this model-specific format directly to the model as well if preferred. Here's an example:


from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
)
model_with_tools 
=
 model
.
bind
(
    tools
=
[
        
{
            
"type"
:
 
"function"
,
            
"function"
:
 
{
                
"name"
:
 
"multiply"
,
                
"description"
:
 
"Multiply two integers together."
,
                
"parameters"
:
 
{
                    
"type"
:
 
"object"
,
                    
"properties"
:
 
{
                        
"a"
:
 
{
"type"
:
 
"number"
,
 
"description"
:
 
"First integer"
}
,
                        
"b"
:
 
{
"type"
:
 
"number"
,
 
"description"
:
 
"Second integer"
}
,
                    
}
,
                    
"required"
:
 
[
"a"
,
 
"b"
]
,
                
}
,
            
}
,
        
}
    
]
)
model_with_tools
.
invoke
(
"Whats 119 times 8?"
)
API Reference:
ChatOpenAI


AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mn4ELw1NbuE0DFYhIeK0GrPe', 'function': {'arguments': '{"a":119,"b":8}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 62, 'total_tokens': 79}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-353e8a9a-7125-4f94-8c68-4f3da4c21120-0', tool_calls=[{'name': 'multiply', 'args': {'a': 119, 'b': 8}, 'id': 'call_mn4ELw1NbuE0DFYhIeK0GrPe'}])


This is functionally equivalent to the 
bind_tools()
 method.
Edit this page
Was this page helpful?
Previous
How to add a human-in-the-loop for tools
Next
How to trim messages


--- docs_how_to_tools_prompting.txt ---

How-to guides
How to add ad-hoc tool calling capability to LLMs and Chat Models
On this page
How to add ad-hoc tool calling capability to LLMs and Chat Models


caution
Some models have been fine-tuned for tool calling and provide a dedicated API for tool calling. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling. Please see the 
how to use a chat model to call tools
 guide for more information.


Prerequisites
This guide assumes familiarity with the following concepts:


LangChain Tools


Function/tool calling


Chat models


LLMs




In this guide, we'll see how to add 
ad-hoc
 tool calling support to a chat model. This is an alternative method to invoke tools if you're using a model that does not natively support 
tool calling
.


We'll do this by simply writing a prompt that will get the model to invoke the appropriate tools. Here's a diagram of the logic:




Setup
​


We'll need to install the following packages:


%
pip install 
-
-
upgrade 
-
-
quiet langchain langchain
-
community


If you'd like to use LangSmith, uncomment the below:


import
 getpass
import
 os
# os.environ["LANGSMITH_TRACING"] = "true"
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()


You can select any of the given models for this how-to guide. Keep in mind that most of these models already 
support native tool calling
, so using the prompting strategy shown here doesn't make sense for these models, and instead you should follow the 
how to use a chat model to call tools
 guide.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4"
,
 model_provider
=
"openai"
)


To illustrate the idea, we'll use 
phi3
 via Ollama, which does 
NOT
 have native support for tool calling. If you'd like to use 
Ollama
 as well follow 
these instructions
.


from
 langchain_community
.
llms 
import
 Ollama
model 
=
 Ollama
(
model
=
"phi3"
)
API Reference:
Ollama


Create a tool
​


First, let's create an 
add
 and 
multiply
 tools. For more information on creating custom tools, please see 
this guide
.


from
 langchain_core
.
tools 
import
 tool
@tool
def
 
multiply
(
x
:
 
float
,
 y
:
 
float
)
 
-
>
 
float
:
    
"""Multiply two numbers together."""
    
return
 x 
*
 y
@tool
def
 
add
(
x
:
 
int
,
 y
:
 
int
)
 
-
>
 
int
:
    
"Add two numbers."
    
return
 x 
+
 y
tools 
=
 
[
multiply
,
 add
]
# Let's inspect the tools
for
 t 
in
 tools
:
    
print
(
"--"
)
    
print
(
t
.
name
)
    
print
(
t
.
description
)
    
print
(
t
.
args
)
API Reference:
tool


--
multiply
Multiply two numbers together.
{'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}
--
add
Add two numbers.
{'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}}


multiply
.
invoke
(
{
"x"
:
 
4
,
 
"y"
:
 
5
}
)


20.0


Creating our prompt
​


We'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form 
{"name": "...", "arguments": {...}}
.


from
 langchain_core
.
output_parsers 
import
 JsonOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
tools 
import
 render_text_description
rendered_tools 
=
 render_text_description
(
tools
)
print
(
rendered_tools
)
API Reference:
JsonOutputParser
 | 
ChatPromptTemplate
 | 
render_text_description


multiply(x: float, y: float) -> float - Multiply two numbers together.
add(x: int, y: int) -> int - Add two numbers.


system_prompt 
=
 
f"""\
You are an assistant that has access to the following set of tools. 
Here are the names and descriptions for each tool:
{
rendered_tools
}
Given the user input, return the name and input of the tool to use. 
Return your response as a JSON blob with 'name' and 'arguments' keys.
The `arguments` should be a dictionary, with keys corresponding 
to the argument names and the values corresponding to the requested values.
"""
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 system_prompt
)
,
 
(
"user"
,
 
"{input}"
)
]
)


chain 
=
 prompt 
|
 model
message 
=
 chain
.
invoke
(
{
"input"
:
 
"what's 3 plus 1132"
}
)
# Let's take a look at the output from the model
# if the model is an LLM (not a chat model), the output will be a string.
if
 
isinstance
(
message
,
 
str
)
:
    
print
(
message
)
else
:
  
# Otherwise it's a chat model
    
print
(
message
.
content
)


{
    "name": "add",
    "arguments": {
        "x": 3,
        "y": 1132
    }
}


Adding an output parser
​


We'll use the 
JsonOutputParser
 for parsing our models output to JSON.


from
 langchain_core
.
output_parsers 
import
 JsonOutputParser
chain 
=
 prompt 
|
 model 
|
 JsonOutputParser
(
)
chain
.
invoke
(
{
"input"
:
 
"what's thirteen times 4"
}
)
API Reference:
JsonOutputParser


{'name': 'multiply', 'arguments': {'x': 13.0, 'y': 4.0}}


important
🎉 Amazing! 🎉 We now instructed our model on how to 
request
 that a tool be invoked.
Now, let's create some logic to actually run the tool!


Invoking the tool 🏃
​


Now that the model can request that a tool be invoked, we need to write a function that can actually invoke
the tool.


The function will select the appropriate tool by name, and pass to it the arguments chosen by the model.


from
 typing 
import
 Any
,
 Dict
,
 Optional
,
 TypedDict
from
 langchain_core
.
runnables 
import
 RunnableConfig
class
 
ToolCallRequest
(
TypedDict
)
:
    
"""A typed dict that shows the inputs into the invoke_tool function."""
    name
:
 
str
    arguments
:
 Dict
[
str
,
 Any
]
def
 
invoke_tool
(
    tool_call_request
:
 ToolCallRequest
,
 config
:
 Optional
[
RunnableConfig
]
 
=
 
None
)
:
    
"""A function that we can use the perform a tool invocation.
    Args:
        tool_call_request: a dict that contains the keys name and arguments.
            The name must match the name of a tool that exists.
            The arguments are the arguments to that tool.
        config: This is configuration information that LangChain uses that contains
            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.
    Returns:
        output from the requested tool
    """
    tool_name_to_tool 
=
 
{
tool
.
name
:
 tool 
for
 tool 
in
 tools
}
    name 
=
 tool_call_request
[
"name"
]
    requested_tool 
=
 tool_name_to_tool
[
name
]
    
return
 requested_tool
.
invoke
(
tool_call_request
[
"arguments"
]
,
 config
=
config
)
API Reference:
RunnableConfig


Let's test this out 🧪!


invoke_tool
(
{
"name"
:
 
"multiply"
,
 
"arguments"
:
 
{
"x"
:
 
3
,
 
"y"
:
 
5
}
}
)


15.0


Let's put it together
​


Let's put it together into a chain that creates a calculator with add and multiplication capabilities.


chain 
=
 prompt 
|
 model 
|
 JsonOutputParser
(
)
 
|
 invoke_tool
chain
.
invoke
(
{
"input"
:
 
"what's thirteen times 4.14137281"
}
)


53.83784653


Returning tool inputs
​


It can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by 
RunnablePassthrough.assign
-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:


from
 langchain_core
.
runnables 
import
 RunnablePassthrough
chain 
=
 
(
    prompt 
|
 model 
|
 JsonOutputParser
(
)
 
|
 RunnablePassthrough
.
assign
(
output
=
invoke_tool
)
)
chain
.
invoke
(
{
"input"
:
 
"what's thirteen times 4.14137281"
}
)
API Reference:
RunnablePassthrough


{'name': 'multiply',
 'arguments': {'x': 13, 'y': 4.14137281},
 'output': 53.83784653}


What's next?
​


This how-to guide shows the "happy path" when the model correctly outputs all the required tool information.


In reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.


You will need to be prepared to add strategies to improve the output from the model; e.g.,




Provide few shot examples.


Add error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output).


Edit this page
Was this page helpful?
Previous
How to use toolkits
Next
Build an Agent with AgentExecutor (Legacy)
Setup
Create a tool
Creating our prompt
Adding an output parser
Invoking the tool 🏃
Let's put it together
Returning tool inputs
What's next?


--- docs_how_to_trim_messages.txt ---

How-to guides
How to trim messages
On this page
How to trim messages


Prerequisites
This guide assumes familiarity with the following concepts:


Messages


Chat models


Chaining


Chat history


The methods in this guide also require 
langchain-core>=0.2.9
.


All models have finite context windows, meaning there's a limit to how many 
tokens
 they can take as input. If you have very long messages or a chain/agent that accumulates a long message history, you'll need to manage the length of the messages you're passing in to the model.


trim_messages
 can be used to reduce the size of a chat history to a specified token count or specified message count.


If passing the trimmed chat history back into a chat model directly, the trimmed chat history should satisfy the following properties:






The resulting chat history should be 
valid
. Usually this means that the following properties should be satisfied:




The chat history 
starts
 with either (1) a 
HumanMessage
 or (2) a 
SystemMessage
 followed by a 
HumanMessage
.


The chat history 
ends
 with either a 
HumanMessage
 or a 
ToolMessage
.


A 
ToolMessage
 can only appear after an 
AIMessage
 that involved a tool call.




This can be achieved by setting 
start_on="human"
 and 
ends_on=("human", "tool")
.






It includes recent messages and drops old messages in the chat history.
This can be achieved by setting 
strategy="last"
.






Usually, the new chat history should include the 
SystemMessage
 if it
was present in the original chat history since the 
SystemMessage
 includes
special instructions to the chat model. The 
SystemMessage
 is almost always
the first message in the history if present. This can be achieved by setting

include_system=True
.






Trimming based on token count
​


Here, we'll trim the chat history based on token count. The trimmed chat history will produce a 
valid
 chat history that includes the 
SystemMessage
.


To keep the most recent messages, we set 
strategy="last"
.  We'll also set 
include_system=True
 to include the 
SystemMessage
, and 
start_on="human"
 to make sure the resulting chat history is valid.


This is a good default configuration when using 
trim_messages
 based on token count. Remember to adjust 
token_counter
 and 
max_tokens
 for your use case.


Notice that for our 
token_counter
 we can pass in a function (more on that below) or a language model (since language models have a message token counting method). It makes sense to pass in a model when you're trimming your messages to fit into the context window of that specific model:


pip install 
-
qU langchain
-
openai


from
 langchain_core
.
messages 
import
 
(
    AIMessage
,
    HumanMessage
,
    SystemMessage
,
    ToolMessage
,
    trim_messages
,
)
from
 langchain_core
.
messages
.
utils 
import
 count_tokens_approximately
messages 
=
 
[
    SystemMessage
(
"you're a good assistant, you always respond with a joke."
)
,
    HumanMessage
(
"i wonder why it's called langchain"
)
,
    AIMessage
(
        
'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    
)
,
    HumanMessage
(
"and who is harrison chasing anyways"
)
,
    AIMessage
(
        
"Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"
    
)
,
    HumanMessage
(
"what do you call a speechless parrot"
)
,
]
trim_messages
(
    messages
,
    
# Keep the last <= n_count tokens of the messages.
    strategy
=
"last"
,
    
# Remember to adjust based on your model
    
# or else pass a custom token_counter
    token_counter
=
count_tokens_approximately
,
    
# Most chat models expect that chat history starts with either:
    
# (1) a HumanMessage or
    
# (2) a SystemMessage followed by a HumanMessage
    
# Remember to adjust based on the desired conversation
    
# length
    max_tokens
=
45
,
    
# Most chat models expect that chat history starts with either:
    
# (1) a HumanMessage or
    
# (2) a SystemMessage followed by a HumanMessage
    start_on
=
"human"
,
    
# Most chat models expect that chat history ends with either:
    
# (1) a HumanMessage or
    
# (2) a ToolMessage
    end_on
=
(
"human"
,
 
"tool"
)
,
    
# Usually, we want to keep the SystemMessage
    
# if it's present in the original history.
    
# The SystemMessage has special instructions for the model.
    include_system
=
True
,
    allow_partial
=
False
,
)
API Reference:
AIMessage
 | 
HumanMessage
 | 
SystemMessage
 | 
ToolMessage
 | 
trim_messages
 | 
count_tokens_approximately


[SystemMessage(content="you're a good assistant, you always respond with a joke.", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]


Trimming based on message count
​


Alternatively, we can trim the chat history based on 
message count
, by setting 
token_counter=len
. In this case, each message will count as a single token, and 
max_tokens
 will control
the maximum number of messages.


This is a good default configuration when using 
trim_messages
 based on message count. Remember to adjust 
max_tokens
 for your use case.


trim_messages
(
    messages
,
    
# Keep the last <= n_count tokens of the messages.
    strategy
=
"last"
,
    token_counter
=
len
,
    
# When token_counter=len, each message
    
# will be counted as a single token.
    
# Remember to adjust for your use case
    max_tokens
=
5
,
    
# Most chat models expect that chat history starts with either:
    
# (1) a HumanMessage or
    
# (2) a SystemMessage followed by a HumanMessage
    start_on
=
"human"
,
    
# Most chat models expect that chat history ends with either:
    
# (1) a HumanMessage or
    
# (2) a ToolMessage
    end_on
=
(
"human"
,
 
"tool"
)
,
    
# Usually, we want to keep the SystemMessage
    
# if it's present in the original history.
    
# The SystemMessage has special instructions for the model.
    include_system
=
True
,
)


[SystemMessage(content="you're a good assistant, you always respond with a joke.", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),
 AIMessage(content="Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]


Advanced Usage
​


You can use 
trim_messages
 as a building-block to create more complex processing logic.


If we want to allow splitting up the contents of a message we can specify 
allow_partial=True
:


trim_messages
(
    messages
,
    max_tokens
=
56
,
    strategy
=
"last"
,
    token_counter
=
count_tokens_approximately
,
    include_system
=
True
,
    allow_partial
=
True
,
)


[SystemMessage(content="you're a good assistant, you always respond with a joke.", additional_kwargs={}, response_metadata={}),
 AIMessage(content="\nWhy, he's probably chasing after the last cup of coffee in the office!", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]


By default, the 
SystemMessage
 will not be included, so you can drop it by either setting 
include_system=False
 or by dropping the 
include_system
 argument.


trim_messages
(
    messages
,
    max_tokens
=
45
,
    strategy
=
"last"
,
    token_counter
=
count_tokens_approximately
,
)


[AIMessage(content="Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]


We can perform the flipped operation of getting the 
first
 
max_tokens
 by specifying 
strategy="first"
:


trim_messages
(
    messages
,
    max_tokens
=
45
,
    strategy
=
"first"
,
    token_counter
=
count_tokens_approximately
,
)


[SystemMessage(content="you're a good assistant, you always respond with a joke.", additional_kwargs={}, response_metadata={}),
 HumanMessage(content="i wonder why it's called langchain", additional_kwargs={}, response_metadata={})]


Using 
ChatModel
 as a token counter
​


You can pass a ChatModel as a token-counter. This will use 
ChatModel.get_num_tokens_from_messages
. Let's demonstrate how to use it with OpenAI:


from
 langchain_openai 
import
 ChatOpenAI
trim_messages
(
    messages
,
    max_tokens
=
45
,
    strategy
=
"first"
,
    token_counter
=
ChatOpenAI
(
model
=
"gpt-4o"
)
,
)
API Reference:
ChatOpenAI


[SystemMessage(content="you're a good assistant, you always respond with a joke.", additional_kwargs={}, response_metadata={}),
 HumanMessage(content="i wonder why it's called langchain", additional_kwargs={}, response_metadata={})]


Writing a custom token counter
​


We can write a custom token counter function that takes in a list of messages and returns an int.


pip install 
-
qU tiktoken


from
 typing 
import
 List
import
 tiktoken
from
 langchain_core
.
messages 
import
 BaseMessage
,
 ToolMessage
def
 
str_token_counter
(
text
:
 
str
)
 
-
>
 
int
:
    enc 
=
 tiktoken
.
get_encoding
(
"o200k_base"
)
    
return
 
len
(
enc
.
encode
(
text
)
)
def
 
tiktoken_counter
(
messages
:
 List
[
BaseMessage
]
)
 
-
>
 
int
:
    
"""Approximately reproduce https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
    For simplicity only supports str Message.contents.
    """
    num_tokens 
=
 
3
  
# every reply is primed with <|start|>assistant<|message|>
    tokens_per_message 
=
 
3
    tokens_per_name 
=
 
1
    
for
 msg 
in
 messages
:
        
if
 
isinstance
(
msg
,
 HumanMessage
)
:
            role 
=
 
"user"
        
elif
 
isinstance
(
msg
,
 AIMessage
)
:
            role 
=
 
"assistant"
        
elif
 
isinstance
(
msg
,
 ToolMessage
)
:
            role 
=
 
"tool"
        
elif
 
isinstance
(
msg
,
 SystemMessage
)
:
            role 
=
 
"system"
        
else
:
            
raise
 ValueError
(
f"Unsupported messages type 
{
msg
.
__class__
}
"
)
        num_tokens 
+=
 
(
            tokens_per_message
            
+
 str_token_counter
(
role
)
            
+
 str_token_counter
(
msg
.
content
)
        
)
        
if
 msg
.
name
:
            num_tokens 
+=
 tokens_per_name 
+
 str_token_counter
(
msg
.
name
)
    
return
 num_tokens
trim_messages
(
    messages
,
    token_counter
=
tiktoken_counter
,
    
# Keep the last <= n_count tokens of the messages.
    strategy
=
"last"
,
    
# When token_counter=len, each message
    
# will be counted as a single token.
    
# Remember to adjust for your use case
    max_tokens
=
45
,
    
# Most chat models expect that chat history starts with either:
    
# (1) a HumanMessage or
    
# (2) a SystemMessage followed by a HumanMessage
    start_on
=
"human"
,
    
# Most chat models expect that chat history ends with either:
    
# (1) a HumanMessage or
    
# (2) a ToolMessage
    end_on
=
(
"human"
,
 
"tool"
)
,
    
# Usually, we want to keep the SystemMessage
    
# if it's present in the original history.
    
# The SystemMessage has special instructions for the model.
    include_system
=
True
,
)
API Reference:
BaseMessage
 | 
ToolMessage


[SystemMessage(content="you're a good assistant, you always respond with a joke.", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]


Chaining
​


trim_messages
 can be used imperatively (like above) or declaratively, making it easy to compose with other components in a chain


llm 
=
 ChatOpenAI
(
model
=
"gpt-4o"
)
# Notice we don't pass in messages. This creates
# a RunnableLambda that takes messages as input
trimmer 
=
 trim_messages
(
    token_counter
=
llm
,
    
# Keep the last <= n_count tokens of the messages.
    strategy
=
"last"
,
    
# When token_counter=len, each message
    
# will be counted as a single token.
    
# Remember to adjust for your use case
    max_tokens
=
45
,
    
# Most chat models expect that chat history starts with either:
    
# (1) a HumanMessage or
    
# (2) a SystemMessage followed by a HumanMessage
    start_on
=
"human"
,
    
# Most chat models expect that chat history ends with either:
    
# (1) a HumanMessage or
    
# (2) a ToolMessage
    end_on
=
(
"human"
,
 
"tool"
)
,
    
# Usually, we want to keep the SystemMessage
    
# if it's present in the original history.
    
# The SystemMessage has special instructions for the model.
    include_system
=
True
,
)
chain 
=
 trimmer 
|
 llm
chain
.
invoke
(
messages
)


AIMessage(content='A "polly-no-wanna-cracker"!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 32, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90d33c15d4', 'finish_reason': 'stop', 'logprobs': None}, id='run-b1f8b63b-6bc2-4df4-b3b9-dfc4e3e675fe-0', usage_metadata={'input_tokens': 32, 'output_tokens': 11, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})


Looking at the LangSmith trace we can see that before the messages are passed to the model they are first trimmed: 
https://smith.langchain.com/public/65af12c4-c24d-4824-90f0-6547566e59bb/r


Looking at just the trimmer, we can see that it's a Runnable object that can be invoked like all Runnables:


trimmer
.
invoke
(
messages
)


[SystemMessage(content="you're a good assistant, you always respond with a joke.", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]


Using with ChatMessageHistory
​


Trimming messages is especially useful when 
working with chat histories
, which can get arbitrarily long:


from
 langchain_core
.
chat_history 
import
 InMemoryChatMessageHistory
from
 langchain_core
.
runnables
.
history 
import
 RunnableWithMessageHistory
chat_history 
=
 InMemoryChatMessageHistory
(
messages
=
messages
[
:
-
1
]
)
def
 
dummy_get_session_history
(
session_id
)
:
    
if
 session_id 
!=
 
"1"
:
        
return
 InMemoryChatMessageHistory
(
)
    
return
 chat_history
trimmer 
=
 trim_messages
(
    max_tokens
=
45
,
    strategy
=
"last"
,
    token_counter
=
llm
,
    
# Usually, we want to keep the SystemMessage
    
# if it's present in the original history.
    
# The SystemMessage has special instructions for the model.
    include_system
=
True
,
    
# Most chat models expect that chat history starts with either:
    
# (1) a HumanMessage or
    
# (2) a SystemMessage followed by a HumanMessage
    
# start_on="human" makes sure we produce a valid chat history
    start_on
=
"human"
,
)
chain 
=
 trimmer 
|
 llm
chain_with_history 
=
 RunnableWithMessageHistory
(
chain
,
 dummy_get_session_history
)
chain_with_history
.
invoke
(
    
[
HumanMessage
(
"what do you call a speechless parrot"
)
]
,
    config
=
{
"configurable"
:
 
{
"session_id"
:
 
"1"
}
}
,
)
API Reference:
InMemoryChatMessageHistory
 | 
RunnableWithMessageHistory


AIMessage(content='A "polygon"!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 32, 'total_tokens': 36, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_c17d3befe7', 'finish_reason': 'stop', 'logprobs': None}, id='run-71d9fce6-bb0c-4bb3-acc8-d5eaee6ae7bc-0', usage_metadata={'input_tokens': 32, 'output_tokens': 4, 'total_tokens': 36})


Looking at the LangSmith trace we can see that we retrieve all of our messages but before the messages are passed to the model they are trimmed to be just the system message and last human message: 
https://smith.langchain.com/public/17dd700b-9994-44ca-930c-116e00997315/r


API reference
​


For a complete description of all arguments head to the API reference: 
https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html
Edit this page
Was this page helpful?
Previous
How to bind model-specific tools
Next
How to create and query vector stores
Trimming based on token count
Trimming based on message count
Advanced Usage
Using 
ChatModel
 as a token counter
Writing a custom token counter
Chaining
Using with ChatMessageHistory
API reference


--- docs_how_to_vectorstore_retriever.txt ---

How-to guides
How to use a vectorstore as a retriever
On this page
How to use a vectorstore as a retriever


A vector store retriever is a 
retriever
 that uses a 
vector store
 to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever 
interface
.
It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store.


In this guide we will cover:




How to instantiate a retriever from a vectorstore;


How to specify the search type for the retriever;


How to specify additional search parameters, such as threshold scores and top-k.




Creating a retriever from a vectorstore
​


You can build a retriever from a vectorstore using its 
.as_retriever
 method. Let's walk through an example.


First we instantiate a vectorstore. We will use an in-memory 
FAISS
 vectorstore:


from
 langchain_community
.
document_loaders 
import
 TextLoader
from
 langchain_community
.
vectorstores 
import
 FAISS
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 CharacterTextSplitter
loader 
=
 TextLoader
(
"state_of_the_union.txt"
)
documents 
=
 loader
.
load
(
)
text_splitter 
=
 CharacterTextSplitter
(
chunk_size
=
1000
,
 chunk_overlap
=
0
)
texts 
=
 text_splitter
.
split_documents
(
documents
)
embeddings 
=
 OpenAIEmbeddings
(
)
vectorstore 
=
 FAISS
.
from_documents
(
texts
,
 embeddings
)
API Reference:
TextLoader
 | 
FAISS
 | 
OpenAIEmbeddings
 | 
CharacterTextSplitter


We can then instantiate a retriever:


retriever 
=
 vectorstore
.
as_retriever
(
)


This creates a retriever (specifically a 
VectorStoreRetriever
), which we can use in the usual way:


docs 
=
 retriever
.
invoke
(
"what did the president say about ketanji brown jackson?"
)


Maximum marginal relevance retrieval
​


By default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type.


This effectively specifies what method on the underlying vectorstore is used (e.g., 
similarity_search
, 
max_marginal_relevance_search
, etc.).


retriever 
=
 vectorstore
.
as_retriever
(
search_type
=
"mmr"
)


docs 
=
 retriever
.
invoke
(
"what did the president say about ketanji brown jackson?"
)


Passing search parameters
​


We can pass parameters to the underlying vectorstore's search methods using 
search_kwargs
.


Similarity score threshold retrieval
​


For example, we can set a similarity score threshold and only return documents with a score above that threshold.


retriever 
=
 vectorstore
.
as_retriever
(
    search_type
=
"similarity_score_threshold"
,
 search_kwargs
=
{
"score_threshold"
:
 
0.5
}
)


docs 
=
 retriever
.
invoke
(
"what did the president say about ketanji brown jackson?"
)


Specifying top k
​


We can also limit the number of documents 
k
 returned by the retriever.


retriever 
=
 vectorstore
.
as_retriever
(
search_kwargs
=
{
"k"
:
 
1
}
)


docs 
=
 retriever
.
invoke
(
"what did the president say about ketanji brown jackson?"
)
len
(
docs
)


1
Edit this page
Was this page helpful?
Previous
How to use tools in a chain
Next
How to add memory to chatbots
Creating a retriever from a vectorstore
Maximum marginal relevance retrieval
Passing search parameters
Similarity score threshold retrieval
Specifying top k


--- docs_how_to_vectorstores.txt ---

How-to guides
How to create and query vector stores
On this page
How to create and query vector stores


info
Head to 
Integrations
 for documentation on built-in integrations with 3rd-party vector stores.


One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding
vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are
'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search
for you.


Get started
​


This guide showcases basic functionality related to vector stores. A key part of working with vector stores is creating the vector to put in them,
which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the 
text embedding model interfaces
 before diving into this.


Before using the vectorstore at all, we need to load some data and initialize an embedding model.


We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.


import
 os
import
 getpass
os
.
environ
[
'OPENAI_API_KEY'
]
 
=
 getpass
.
getpass
(
'OpenAI API Key:'
)


from
 langchain_community
.
document_loaders 
import
 TextLoader
from
 langchain_openai 
import
 OpenAIEmbeddings
from
 langchain_text_splitters 
import
 CharacterTextSplitter
# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents 
=
 TextLoader
(
'state_of_the_union.txt'
)
.
load
(
)
text_splitter 
=
 CharacterTextSplitter
(
chunk_size
=
1000
,
 chunk_overlap
=
0
)
documents 
=
 text_splitter
.
split_documents
(
raw_documents
)
API Reference:
TextLoader
 | 
OpenAIEmbeddings
 | 
CharacterTextSplitter




There are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.


Chroma
FAISS
Lance
This walkthrough uses the 
chroma
 vector database, which runs on your local machine as a library.
pip install langchain-chroma
from
 langchain_chroma 
import
 Chroma
db 
=
 Chroma
.
from_documents
(
documents
,
 OpenAIEmbeddings
(
)
)
This walkthrough uses the 
FAISS
 vector database, which makes use of the Facebook AI Similarity Search (FAISS) library.
pip install faiss-cpu
from
 langchain_community
.
vectorstores 
import
 FAISS
db 
=
 FAISS
.
from_documents
(
documents
,
 OpenAIEmbeddings
(
)
)
API Reference:
FAISS
This notebook shows how to use functionality related to the LanceDB vector database based on the Lance data format.
pip install lancedb
from
 langchain_community
.
vectorstores 
import
 LanceDB
import
 lancedb
db 
=
 lancedb
.
connect
(
"/tmp/lancedb"
)
table 
=
 db
.
create_table
(
    
"my_table"
,
    data
=
[
        
{
            
"vector"
:
 embeddings
.
embed_query
(
"Hello World"
)
,
            
"text"
:
 
"Hello World"
,
            
"id"
:
 
"1"
,
        
}
    
]
,
    mode
=
"overwrite"
,
)
db 
=
 LanceDB
.
from_documents
(
documents
,
 OpenAIEmbeddings
(
)
)
API Reference:
LanceDB


Similarity search
​


All vectorstores expose a 
similarity_search
 method.
This will take incoming documents, create an embedding of them, and then find all documents with the most similar embedding.


query 
=
 
"What did the president say about Ketanji Brown Jackson"
docs 
=
 db
.
similarity_search
(
query
)
print
(
docs
[
0
]
.
page_content
)


    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.
    Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.
    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.
    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.


Similarity search by vector
​


It is also possible to do a search for documents similar to a given embedding vector using 
similarity_search_by_vector
 which accepts an embedding vector as a parameter instead of a string.


embedding_vector 
=
 OpenAIEmbeddings
(
)
.
embed_query
(
query
)
docs 
=
 db
.
similarity_search_by_vector
(
embedding_vector
)
print
(
docs
[
0
]
.
page_content
)


    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.
    Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.
    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.
    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.


Async Operations
​


Vector stores are usually run as a separate service that requires some IO operations, and therefore they might be called asynchronously. That gives performance benefits as you don't waste time waiting for responses from external services. That might also be important if you work with an asynchronous framework, such as 
FastAPI
.


LangChain supports async operation on vector stores. All the methods might be called using their async counterparts, with the prefix 
a
, meaning 
async
.


docs 
=
 
await
 db
.
asimilarity_search
(
query
)
docs


[Document(page_content='Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'state_of_the_union.txt'}),
 Document(page_content='A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \n\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \n\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \n\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \n\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \n\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.', metadata={'source': 'state_of_the_union.txt'}),
 Document(page_content='And for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \n\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \n\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \n\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \n\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \n\nFirst, beat the opioid epidemic.', metadata={'source': 'state_of_the_union.txt'}),
 Document(page_content='Tonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \n\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \n\nThat ends on my watch. \n\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \n\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \n\nLet’s pass the Paycheck Fairness Act and paid leave.  \n\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \n\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges.', metadata={'source': 'state_of_the_union.txt'})]
Edit this page
Was this page helpful?
Previous
How to trim messages
Next
Conceptual guide
Get started
Similarity search
Similarity search by vector
Async Operations


--- docs_integrations_chat.txt ---

Components
Chat models
On this page
Chat models


Chat models
 are language models that use a sequence of 
messages
 as inputs and return messages as outputs (as opposed to using plain text). These are generally newer models.


info
If you'd like to write your own chat model, see 
this how-to
.
If you'd like to contribute an integration, see 
Contributing integrations
.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


model
.
invoke
(
"Hello, world!"
)


Featured Providers
​


info
While all these LangChain classes support the indicated advanced feature, you may have
to open the provider-specific documentation to learn which hosted models or backends support
the feature.




Provider
Tool calling
Structured output
JSON mode
Local
Multimodal
Package
ChatAnthropic
✅
✅
❌
❌
✅
langchain-anthropic
ChatMistralAI
✅
✅
❌
❌
❌
langchain-mistralai
ChatFireworks
✅
✅
✅
❌
❌
langchain-fireworks
AzureChatOpenAI
✅
✅
✅
❌
✅
langchain-openai
ChatOpenAI
✅
✅
✅
❌
✅
langchain-openai
ChatTogether
✅
✅
✅
❌
❌
langchain-together
ChatVertexAI
✅
✅
❌
❌
✅
langchain-google-vertexai
ChatGoogleGenerativeAI
✅
✅
❌
❌
✅
langchain-google-genai
ChatGroq
✅
✅
✅
❌
❌
langchain-groq
ChatCohere
✅
✅
❌
❌
❌
langchain-cohere
ChatBedrock
✅
✅
❌
❌
❌
langchain-aws
ChatHuggingFace
✅
✅
❌
✅
❌
langchain-huggingface
ChatNVIDIA
✅
✅
✅
✅
✅
langchain-nvidia-ai-endpoints
ChatOllama
✅
✅
✅
✅
❌
langchain-ollama
ChatLlamaCpp
✅
✅
❌
✅
❌
langchain-community
ChatAI21
✅
✅
❌
❌
❌
langchain-ai21
ChatUpstage
✅
✅
❌
❌
❌
langchain-upstage
ChatDatabricks
✅
✅
❌
❌
❌
databricks-langchain
ChatWatsonx
✅
✅
✅
❌
❌
langchain-ibm
ChatXAI
✅
✅
❌
❌
❌
langchain-xai
ChatPerplexity
❌
✅
✅
❌
✅
langchain-perplexity


All chat models
​


Name
Description
Abso
This will help you getting started with ChatAbso chat models. For det...
AI21 Labs
Overview
Alibaba Cloud PAI EAS
Alibaba Cloud PAI (Platform for AI) is a lightweight and cost-efficie...
Anthropic
This notebook provides a quick overview for getting started with Anth...
Anyscale
This notebook demonstrates the use of langchain.chat_models.ChatAnysc...
AzureAIChatCompletionsModel
This will help you getting started with AzureAIChatCompletionsModel c...
Azure OpenAI
This guide will help you get started with AzureOpenAI chat models. Fo...
Azure ML Endpoint
Azure Machine Learning is a platform used to build, train, and deploy...
Baichuan Chat
Baichuan chat models API by Baichuan Intelligent Technology. For more...
Baidu Qianfan
Baidu AI Cloud Qianfan Platform is a one-stop large model development...
AWS Bedrock
This doc will help you get started with AWS Bedrock chat models. Amaz...
Cerebras
This notebook provides a quick overview for getting started with Cere...
CloudflareWorkersAI
This will help you getting started with CloudflareWorkersAI chat mode...
Cohere
This notebook covers how to get started with Cohere chat models.
ContextualAI
This will help you getting started with Contextual AI's Grounded Lang...
Coze Chat
ChatCoze chat models API by coze.com. For more information, see https...
Dappier AI
Dappier: Powering AI with Dynamic, Real-Time Data Models
Databricks
Databricks Lakehouse Platform unifies data, analytics, and AI on one ...
DeepInfra
DeepInfra is a serverless inference as a service that provides access...
DeepSeek
This will help you getting started with DeepSeek's hosted chat models...
Eden AI
Eden AI is revolutionizing the AI landscape by uniting the best AI pr...
EverlyAI
EverlyAI allows you to run your ML models at scale in the cloud. It a...
Fireworks
This doc help you get started with Fireworks AI chat models. For deta...
ChatFriendli
Friendli enhances AI application performance and optimizes cost savin...
GigaChat
This notebook shows how to use LangChain with GigaChat.
Goodfire
This will help you getting started with Goodfire chat models. For det...
Google Gemini
Access Google's Generative AI models, including the Gemini family, di...
Google Cloud Vertex AI
This page provides a quick overview for getting started with VertexAI...
GPTRouter
GPTRouter is an open source LLM API Gateway that offers a universal A...
Groq
This will help you getting started with Groq chat models. For detaile...
ChatHuggingFace
This will help you getting started with langchainhuggingface chat mod...
IBM watsonx.ai
ChatWatsonx is a wrapper for IBM watsonx.ai foundation models.
JinaChat
This notebook covers how to get started with JinaChat chat models.
Kinetica
This notebook demonstrates how to use Kinetica to transform natural l...
Konko
Konko API is a fully managed Web API designed to help application dev...
LiteLLM
LiteLLM is a library that simplifies calling Anthropic, Azure, Huggin...
Llama 2 Chat
This notebook shows how to augment Llama-2 LLMs with the Llama2Chat w...
Llama API
This notebook shows how to use LangChain with LlamaAPI - a hosted ver...
LlamaEdge
LlamaEdge allows you to chat with LLMs of GGUF format both locally an...
Llama.cpp
llama.cpp python library is a simple Python bindings for @ggerganov
maritalk
Introduction
MiniMax
Minimax is a Chinese startup that provides LLM service for companies ...
MistralAI
This will help you getting started with Mistral chat models. For deta...
MLX
This notebook shows how to get started using MLX LLM's as chat models.
ModelScope
ModelScope (Home | GitHub) is built upon the notion of “Model-as-a-Se...
Moonshot
Moonshot is a Chinese startup that provides LLM service for companies...
Naver
This notebook provides a quick overview for getting started with Nave...
Netmind
This will help you getting started with Netmind chat models. For deta...
NVIDIA AI Endpoints
This will help you getting started with NVIDIA chat models. For detai...
ChatOCIModelDeployment
This will help you getting started with OCIModelDeployment chat model...
OCIGenAI
This notebook provides a quick overview for getting started with OCIG...
ChatOctoAI
OctoAI offers easy access to efficient compute and enables users to i...
Ollama
Ollama allows you to run open-source large language models, such as L...
OpenAI
This notebook provides a quick overview for getting started with Open...
Outlines
This will help you getting started with Outlines chat models. For det...
Perplexity
This page will help you get started with Perplexity chat models. For ...
Pipeshift
This will help you getting started with Pipeshift chat models. For de...
ChatPredictionGuard
Prediction Guard is a secure, scalable GenAI platform that safeguards...
PremAI
PremAI is an all-in-one platform that simplifies the creation of robu...
PromptLayer ChatOpenAI
This example showcases how to connect to PromptLayer to start recordi...
Qwen QwQ
This will help you getting started with QwQ chat models. For detailed...
Reka
This notebook provides a quick overview for getting started with Reka...
RunPod Chat Model
Get started with RunPod chat models.
SambaNovaCloud
This will help you getting started with SambaNovaCloud chat models. F...
SambaStudio
This will help you getting started with SambaStudio chat models. For ...
ChatSeekrFlow
Seekr provides AI-powered solutions for structured, explainable, and ...
Snowflake Cortex
Snowflake Cortex gives you instant access to industry-leading large l...
solar
Related
SparkLLM Chat
SparkLLM chat models API by iFlyTek. For more information, see iFlyTe...
Nebula (Symbl.ai)
Overview
Tencent Hunyuan
Tencent's hybrid model API (Hunyuan API)
Together
This page will help you get started with Together AI chat models. For...
Tongyi Qwen
Tongyi Qwen is a large language model developed by Alibaba's Damo Aca...
Upstage
This notebook covers how to get started with Upstage chat models.
vectara
Overview
vLLM Chat
vLLM can be deployed as a server that mimics the OpenAI API protocol....
Volc Enging Maas
This notebook provides you with a guide on how to get started with vo...
Chat Writer
This notebook provides a quick overview for getting started with Writ...
xAI
This page will help you get started with xAI chat models. For detaile...
Xinference
Xinference is a powerful and versatile library designed to serve LLMs,
YandexGPT
This notebook goes over how to use Langchain with YandexGPT chat mode...
ChatYI
This will help you getting started with Yi chat models. For detailed ...
Yuan2.0
This notebook shows how to use YUAN2 API in LangChain with the langch...
ZHIPU AI
This notebook shows how to use ZHIPU AI API in LangChain with the lan...
Edit this page
Was this page helpful?
Previous
Components
Next
Chat models
Featured Providers
All chat models


--- docs_integrations_providers.txt ---

Providers
On this page
Providers


info
If you'd like to write your own integration, see 
Extending LangChain
.
If you'd like to contribute an integration, see 
Contributing integrations
.


LangChain integrates with many providers.


Integration Packages
​


These providers have standalone 
langchain-{provider}
 packages for improved versioning, dependency management and testing.


Provider
Package
Downloads
Latest
JS
Google VertexAI
langchain-google-vertexai
✅
OpenAI
langchain-openai
✅
Google Community
langchain-google-community
❌
AWS
langchain-aws
✅
Anthropic
langchain-anthropic
✅
Google Generative AI
langchain-google-genai
✅
Chroma
langchain-chroma
❌
Ollama
langchain-ollama
✅
Cohere
langchain-cohere
✅
Pinecone
langchain-pinecone
✅
Postgres
langchain-postgres
❌
Groq
langchain-groq
✅
Huggingface
langchain-huggingface
❌
MistralAI
langchain-mistralai
✅
Fireworks
langchain-fireworks
❌
Ibm
langchain-ibm
✅
MongoDB
langchain-mongodb
✅
Deepseek
langchain-deepseek
✅
Milvus
langchain-milvus
❌
Nvidia AI Endpoints
langchain-nvidia-ai-endpoints
❌
Elasticsearch
langchain-elasticsearch
❌
Qdrant
langchain-qdrant
✅
Unstructured
langchain-unstructured
❌
Databricks
databricks-langchain
❌
DataStax Astra DB
langchain-astradb
❌
Together
langchain-together
❌
Sambanova
langchain-sambanova
❌
Neo4J
langchain-neo4j
❌
Graph RAG
langchain-graph-retriever
❌
XAI
langchain-xai
❌
Redis
langchain-redis
✅
Cerebras
langchain-cerebras
❌
Weaviate
langchain-weaviate
✅
Tavily
langchain-tavily
✅
Azure AI
langchain-azure-ai
✅
VoyageAI
langchain-voyageai
❌
Upstage
langchain-upstage
❌
Docling
langchain-docling
❌
VDMS
langchain-vdms
❌
Nomic
langchain-nomic
✅
Azure Dynamic Sessions
langchain-azure-dynamic-sessions
✅
Perplexity
langchain-perplexity
❌
Exa
langchain-exa
✅
Pymupdf4Llm
langchain-pymupdf4llm
❌
Litellm
langchain-litellm
❌
AI21
langchain-ai21
❌
Predictionguard
langchain-predictionguard
❌
Sqlserver
langchain-sqlserver
❌
Prompty
langchain-prompty
❌
Snowflake
langchain-snowflake
❌
LangFair
langfair
❌
Sema4
langchain-sema4
❌
Couchbase
langchain-couchbase
❌
Scrapegraph
langchain-scrapegraph
❌
Apify
langchain-apify
❌
Salesforce
langchain-salesforce
❌
Cloudflare
langchain-cloudflare
❌
Memgraph
langchain-memgraph
❌
Qwq
langchain-qwq
❌
Kuzu
langchain-kuzu
❌
LocalAI
langchain-localai
❌
Writer
langchain-writer
❌
Naver
langchain-naver
❌
Box
langchain-box
❌
Linkup
langchain-linkup
❌
Tableau
langchain-tableau
❌
ADS4GPTs
ads4gpts-langchain
❌
Contextual AI
langchain-contextual
❌
MariaDB
langchain-mariadb
❌
Hyperbrowser
langchain-hyperbrowser
❌
Taiga
langchain-taiga
❌
GOAT SDK
goat-sdk-adapter-langchain
❌
Galaxia Retriever
langchain-galaxia-retriever
❌
SAP HANA Cloud
langchain-hana
❌
Agentql
langchain-agentql
❌
Dappier
langchain-dappier
❌
RunPod
langchain-runpod
❌
Vectara
langchain-vectara
❌
Goodfire
langchain-goodfire
❌
Abso
langchain-abso
❌
CrateDB
langchain-cratedb
❌
Nimble
langchain-nimble
❌
YDB
langchain-ydb
❌
Permit
langchain-permit
❌
Valthera
langchain-valthera
❌
Payman Tool
langchain-payman-tool
❌
Jenkins
langchain-jenkins
❌
Prolog
langchain-prolog
❌
Naver (community-maintained)
langchain-naver-community
❌
Valyu
langchain-valyu
❌
Opengradient
langchain-opengradient
❌
Tilores
tilores-langchain
❌
SingleStore
langchain-singlestore
❌
Fmp Data
langchain-fmp-data
❌
Pull Md
langchain-pull-md
❌
Modelscope
langchain-modelscope
❌
Discord Shikenso
langchain-discord-shikenso
❌
Xinference
langchain-xinference
❌
FalkorDB
langchain-falkordb
❌
Pipeshift
langchain-pipeshift
❌
Cognee
langchain-cognee
❌
PowerScale RAG Connector
powerscale-rag-connector
❌
Oceanbase
langchain-oceanbase
❌
Deeplake
langchain-deeplake
❌
Zotero
langchain-zotero-retriever
❌
Oxylabs
langchain-oxylabs
❌
Lindorm Integration
langchain-lindorm-integration
❌
Netmind
langchain-netmind
❌
Gel
langchain-gel
❌
Aerospike
langchain-aerospike
❌
Brightdata
langchain-brightdata
❌


All Providers
​


Click 
here
 to see all providers. Or search for a
provider using the Search field in the top-right corner of the screen.
Edit this page
Was this page helpful?
Next
Anthropic
Integration Packages
All Providers


--- docs_integrations_vectorstores.txt ---

Components
Vector stores
On this page
Vector stores




A 
vector store
 stores 
embedded
 data and performs similarity search.


Select embedding model:




Select 
embeddings model
:
OpenAI
▾
OpenAI
Azure
Google
AWS
HuggingFace
Ollama
Cohere
MistralAI
Nomic
NVIDIA
Voyage AI
IBM watsonx
Fake
pip install -qU langchain-openai
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-large"
)


Select vector store:




Select 
vector store
:
In-memory
▾
In-memory
AstraDB
Chroma
FAISS
Milvus
MongoDB
PGVector
Pinecone
Qdrant
pip install -qU langchain-core
from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
vector_store 
=
 InMemoryVectorStore
(
embeddings
)


Vectorstore
Delete by ID
Filtering
Search by Vector
Search with score
Async
Passes Standard Tests
Multi Tenancy
IDs in add Documents
AstraDBVectorStore
✅
✅
✅
✅
✅
❌
❌
❌
Chroma
✅
✅
✅
✅
✅
❌
❌
❌
Clickhouse
✅
✅
❌
✅
❌
❌
❌
❌
CouchbaseVectorStore
✅
✅
❌
✅
✅
❌
❌
❌
DatabricksVectorSearch
✅
✅
✅
✅
✅
❌
❌
❌
ElasticsearchStore
✅
✅
✅
✅
✅
❌
❌
❌
FAISS
✅
✅
✅
✅
✅
❌
❌
❌
InMemoryVectorStore
✅
✅
❌
✅
✅
❌
❌
❌
Milvus
✅
✅
❌
✅
✅
❌
❌
❌
MongoDBAtlasVectorSearch
✅
✅
✅
✅
✅
❌
❌
❌
openGauss
✅
✅
✅
✅
❌
✅
❌
✅
PGVector
✅
✅
✅
✅
✅
❌
❌
❌
PineconeVectorStore
✅
✅
✅
❌
✅
❌
❌
❌
QdrantVectorStore
✅
✅
✅
✅
✅
❌
❌
❌
Redis
✅
✅
✅
✅
✅
❌
❌
❌
Weaviate
✅
✅
✅
✅
✅
❌
✅
❌
SQLServer
✅
✅
✅
✅
❌
❌
❌
❌


All Vectorstores
​


Name
Description
Activeloop Deep Lake
Activeloop Deep Lake as a Multi-Modal Vector Store that stores embedd...
Aerospike
Aerospike Vector Search (AVS) is an
Alibaba Cloud OpenSearch
Alibaba Cloud Opensearch is a one-stop platform to develop intelligen...
AnalyticDB
AnalyticDB for PostgreSQL is a massively parallel processing (MPP) da...
Annoy
Annoy (Approximate Nearest Neighbors Oh Yeah) is a C++ library with P...
Apache Doris
Apache Doris is a modern data warehouse for real-time analytics.
ApertureDB
ApertureDB is a database that stores, indexes, and manages multi-moda...
Astra DB Vector Store
This page provides a quickstart for using Astra DB as a Vector Store.
Atlas
Atlas is a platform by Nomic made for interacting with both small and...
AwaDB
AwaDB is an AI Native database for the search and storage of embeddin...
Azure Cosmos DB Mongo vCore
This notebook shows you how to leverage this integrated vector databa...
Azure Cosmos DB No SQL
This notebook shows you how to leverage this integrated vector databa...
Azure AI Search
Azure AI Search (formerly known as Azure Search and Azure Cognitive S...
Bagel
Bagel (Open Inference platform for AI), is like GitHub for AI data.
BagelDB
BagelDB (Open Vector Database for AI), is like GitHub for AI data.
Baidu Cloud ElasticSearch VectorSearch
Baidu Cloud VectorSearch is a fully managed, enterprise-level distrib...
Baidu VectorDB
Baidu VectorDB is a robust, enterprise-level distributed database ser...
Apache Cassandra
This page provides a quickstart for using Apache Cassandra® as a Vect...
Chroma
This notebook covers how to get started with the Chroma vector store.
Clarifai
Clarifai is an AI Platform that provides the full AI lifecycle rangin...
ClickHouse
ClickHouse is the fastest and most resource efficient open-source dat...
CloudflareVectorize
This notebook covers how to get started with the CloudflareVectorize ...
Couchbase
Couchbase is an award-winning distributed NoSQL cloud database that d...
DashVector
DashVector is a fully-managed vectorDB service that supports high-dim...
Databricks
Databricks Vector Search is a serverless similarity search engine tha...
DingoDB
DingoDB is a distributed multi-mode vector database, which combines t...
DocArray HnswSearch
DocArrayHnswSearch is a lightweight Document Index implementation pro...
DocArray InMemorySearch
DocArrayInMemorySearch is a document index provided by Docarray that ...
Amazon Document DB
Amazon DocumentDB (with MongoDB Compatibility) makes it easy to set u...
DuckDB
This notebook shows how to use DuckDB as a vector store.
China Mobile ECloud ElasticSearch VectorSearch
China Mobile ECloud VectorSearch is a fully managed, enterprise-level...
Elasticsearch
Elasticsearch is a distributed, RESTful search and analytics engine, ...
Epsilla
Epsilla is an open-source vector database that leverages the advanced...
Faiss
Facebook AI Similarity Search (FAISS) is a library for efficient simi...
Faiss (Async)
Facebook AI Similarity Search (Faiss) is a library for efficient simi...
FalkorDBVectorStore
FalkorDB is an open-source graph database with integrated support for...
Gel
An implementation of LangChain vectorstore abstraction using gel as t...
Google AlloyDB for PostgreSQL
AlloyDB is a fully managed relational database service that offers hi...
Google BigQuery Vector Search
Google Cloud BigQuery Vector Search lets you use GoogleSQL to do sema...
Google Cloud SQL for MySQL
Cloud SQL is a fully managed relational database service that offers ...
Google Cloud SQL for PostgreSQL
Cloud SQL is a fully managed relational database service that offers ...
Firestore
Firestore is a serverless document-oriented database that scales to m...
Google Memorystore for Redis
Google Memorystore for Redis is a fully-managed service that is power...
Google Spanner
Spanner is a highly scalable database that combines unlimited scalabi...
Google Vertex AI Feature Store
Google Cloud Vertex Feature Store streamlines your ML feature managem...
Google Vertex AI Vector Search
This notebook shows how to use functionality related to the Google Cl...
Hippo
Transwarp Hippo is an enterprise-level cloud-native distributed vecto...
Hologres
Hologres is a unified real-time data warehousing service developed by...
Infinispan
Infinispan is an open-source key-value data grid, it can work as sing...
Jaguar Vector Database
1. It is a distributed vector database
KDB.AI
KDB.AI is a powerful knowledge-based vector database and search engin...
Kinetica
Kinetica is a database with integrated support for vector similarity ...
LanceDB
LanceDB is an open-source database for vector-search built with persi...
Lantern
Lantern is an open-source vector similarity search for Postgres
Lindorm
This notebook covers how to get started with the Lindorm vector store.
LLMRails
LLMRails is a API platform for building GenAI applications. It provid...
ManticoreSearch VectorStore
ManticoreSearch is an open-source search engine that offers fast, sca...
MariaDB
LangChain's MariaDB integration (langchain-mariadb) provides vector c...
Marqo
This notebook shows how to use functionality related to the Marqo vec...
Meilisearch
Meilisearch is an open-source, lightning-fast, and hyper relevant sea...
Amazon MemoryDB
Vector Search introduction and langchain integration guide.
Milvus
Milvus is a database that stores, indexes, and manages massive embedd...
Momento Vector Index (MVI)
MVI: the most productive, easiest to use, serverless vector index for...
MongoDB Atlas
This notebook covers how to MongoDB Atlas vector search in LangChain,...
MyScale
MyScale is a cloud-based database optimized for AI applications and s...
Neo4j Vector Index
Neo4j is an open-source graph database with integrated support for ve...
NucliaDB
You can use a local NucliaDB instance or use Nuclia Cloud.
Oceanbase
This notebook covers how to get started with the Oceanbase vector sto...
openGauss
This notebook covers how to get started with the openGauss VectorStor...
OpenSearch
OpenSearch is a scalable, flexible, and extensible open-source softwa...
Oracle AI Vector Search: Vector Store
Oracle AI Vector Search is designed for Artificial Intelligence (AI) ...
Pathway
Pathway is an open data processing framework. It allows you to easily...
Postgres Embedding
Postgres Embedding is an open-source vector similarity search for Pos...
PGVecto.rs
This notebook shows how to use functionality related to the Postgres ...
PGVector
An implementation of LangChain vectorstore abstraction using postgres...
Pinecone
Pinecone is a vector database with broad functionality.
Pinecone (sparse)
Pinecone is a vector database with broad functionality.
Qdrant
Qdrant (read: quadrant) is a vector similarity search engine. It prov...
Redis
This notebook covers how to get started with the Redis vector store.
Relyt
Relyt is a cloud native data warehousing service that is designed to ...
Rockset
Rockset is a real-time search and analytics database built for the cl...
SAP HANA Cloud Vector Engine
SAP HANA Cloud Vector Engine is a vector store fully integrated into ...
ScaNN
ScaNN (Scalable Nearest Neighbors) is a method for efficient vector s...
SemaDB
SemaDB from SemaFind is a no fuss vector similarity database for buil...
SingleStore
SingleStore is a robust, high-performance distributed SQL database so...
scikit-learn
scikit-learn is an open-source collection of machine learning algorit...
SQLiteVec
This notebook covers how to get started with the SQLiteVec vector sto...
SQLite-VSS
SQLite-VSS is an SQLite extension designed for vector search, emphasi...
SQLServer
Azure SQL provides a dedicated Vector data type that simplifies the c...
StarRocks
StarRocks is a High-Performance Analytical Database.
Supabase (Postgres)
Supabase is an open-source Firebase alternative. Supabase is built on...
SurrealDB
SurrealDB is an end-to-end cloud-native database designed for modern ...
Tablestore
Tablestore is a fully managed NoSQL cloud database service.
Tair
Tair is a cloud native in-memory database service developed by Alibab...
Tencent Cloud VectorDB
Tencent Cloud VectorDB is a fully managed, self-developed, enterprise...
ThirdAI NeuralDB
NeuralDB is a CPU-friendly and fine-tunable vector store developed by...
TiDB Vector
TiDB Cloud, is a comprehensive Database-as-a-Service (DBaaS) solution...
Tigris
Tigris is an open-source Serverless NoSQL Database and Search Platfor...
TileDB
TileDB is a powerful engine for indexing and querying dense and spars...
Timescale Vector (Postgres)
Timescale Vector is PostgreSQL++ vector database for AI applications.
Typesense
Typesense is an open-source, in-memory search engine, that you can ei...
Upstash Vector
Upstash Vector is a serverless vector database designed for working w...
USearch
USearch is a Smaller & Faster Single-File Vector Search Engine
Vald
Vald is a highly scalable distributed fast approximate nearest neighb...
VDMS
This notebook covers how to get started with VDMS as a vector store.
Vearch
Vearch is the vector search infrastructure for deeping learning and A...
Vectara
Vectara is the trusted AI Assistant and Agent platform which focuses ...
Vespa
Vespa is a fully featured search engine and vector database. It suppo...
viking DB
viking DB is a database that stores, indexes, and manages massive emb...
vlite
VLite is a simple and blazing fast vector database that allows you to...
Weaviate
This notebook covers how to get started with the Weaviate vector stor...
Xata
Xata is a serverless data platform, based on PostgreSQL. It provides ...
YDB
YDB is a versatile open source Distributed SQL Database that combines...
Yellowbrick
Yellowbrick is an elastic, massively parallel processing (MPP) SQL da...
Zep
Recall, understand, and extract data from chat histories. Power perso...
Zep Cloud
Recall, understand, and extract data from chat histories. Power perso...
Zilliz
Zilliz Cloud is a fully managed service on cloud for LF AI Milvus®,
Edit this page
Was this page helpful?
Previous
ZeroxPDFLoader
Next
Vector stores
All Vectorstores


--- docs_introduction.txt ---

Introduction
On this page
Introduction


LangChain
 is a framework for developing applications powered by large language models (LLMs).


LangChain simplifies every stage of the LLM application lifecycle:




Development
: Build your applications using LangChain's open-source 
components
 and 
third-party integrations
.
Use 
LangGraph
 to build stateful agents with first-class streaming and human-in-the-loop support.


Productionization
: Use 
LangSmith
 to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.


Deployment
: Turn your LangGraph applications into production-ready APIs and Assistants with 
LangGraph Platform
.








LangChain implements a standard interface for large language models and related
technologies, such as embedding models and vector stores, and integrates with
hundreds of providers. See the 
integrations
 page for
more.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


model
.
invoke
(
"Hello, world!"
)


note
These docs focus on the Python LangChain library. 
Head here
 for docs on the JavaScript LangChain library.


Architecture
​


The LangChain framework consists of multiple open-source libraries. Read more in the

Architecture
 page.




langchain-core
: Base abstractions for chat models and other components.


Integration packages
 (e.g. 
langchain-openai
, 
langchain-anthropic
, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.


langchain
: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.


langchain-community
: Third-party integrations that are community maintained.


langgraph
: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See 
LangGraph documentation
.




Guides
​


Tutorials
​


If you're looking to build something specific or are more of a hands-on learner, check out our 
tutorials section
.
This is the best place to get started.


These are the best ones to get started with:




Build a Simple LLM Application


Build a Chatbot


Build an Agent


Introduction to LangGraph




Explore the full list of LangChain tutorials 
here
, and check out other 
LangGraph tutorials here
. To learn more about LangGraph, check out our first LangChain Academy course, 
Introduction to LangGraph
, available 
here
.


How-to guides
​


Here
 you’ll find short answers to “How do I….?” types of questions.
These how-to guides don’t cover topics in depth – you’ll find that material in the 
Tutorials
 and the 
API Reference
.
However, these guides will help you quickly accomplish common tasks using 
chat models
,

vector stores
, and other common LangChain components.


Check out 
LangGraph-specific how-tos here
.


Conceptual guide
​


Introductions to all the key parts of LangChain you’ll need to know! 
Here
 you'll find high level explanations of all LangChain concepts.


For a deeper dive into LangGraph concepts, check out 
this page
.


Integrations
​


LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.
If you're looking to get up and running quickly with 
chat models
, 
vector stores
,
or other LangChain components from a specific provider, check out our growing list of 
integrations
.


API reference
​


Head to the reference section for full documentation of all classes and methods in the LangChain Python packages.


Ecosystem
​


🦜🛠️ LangSmith
​


Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.


🦜🕸️ LangGraph
​


Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.


Additional resources
​


Versions
​


See what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.


Security
​


Read up on 
security
 best practices to make sure you're developing safely with LangChain.


Contributing
​


Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.
Edit this page
Was this page helpful?
Next
Tutorials
Architecture
Guides
Tutorials
How-to guides
Conceptual guide
Integrations
API reference
Ecosystem
🦜🛠️ LangSmith
🦜🕸️ LangGraph
Additional resources
Versions
Security
Contributing


--- docs_security.txt ---

Security Policy
On this page
Security Policy


LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.


Best practices
​


When building such applications developers should remember to follow good security practices:




Limit Permissions
: Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), specifying proxy configurations to control external requests, etc. as appropriate for your application.


Anticipate Potential Misuse
: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it’s safest to assume that any LLM able to use those credentials may in fact delete data.


Defense in Depth
: No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It’s best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.




Risks of not doing so include, but are not limited to:




Data corruption or loss.


Unauthorized access to confidential information.


Compromised performance or availability of critical resources.




Example scenarios with mitigation strategies:




A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.


A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.


A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.




If you're building applications that access external resources like file systems, APIs
or databases, consider speaking with your company's security team to determine how to best
design and secure your applications.


Reporting OSS Vulnerabilities
​


LangChain is partnered with 
huntr by Protect AI
 to provide
a bounty program for our open source projects.


Please report security vulnerabilities associated with the LangChain
open source projects by visiting the following link:


https://huntr.com/bounties/disclose/


Before reporting a vulnerability, please review:




In-Scope Targets and Out-of-Scope Targets below.


The 
langchain-ai/langchain
 monorepo structure.


The 
Best practicies
 above to
understand what we consider to be a security vulnerability vs. developer
responsibility.




In-Scope Targets
​


The following packages and repositories are eligible for bug bounties:




langchain-core


langchain (see exceptions)


langchain-community (see exceptions)


langgraph


langserve




Out of Scope Targets
​


All out of scope targets defined by huntr as well as:




langchain-experimental
: This repository is for experimental code and is not
eligible for bug bounties (see 
package warning
), bug reports to it will be marked as interesting or waste of
time and published with no bounty attached.


tools
: Tools in either langchain or langchain-community are not eligible for bug
bounties. This includes the following directories




libs/langchain/langchain/tools


libs/community/langchain_community/tools


Please review the 
best practices

for more details, but generally tools interact with the real world. Developers are
expected to understand the security implications of their code and are responsible
for the security of their tools.






Code documented with security notices. This will be decided done on a case by
case basis, but likely will not be eligible for a bounty as the code is already
documented with guidelines for developers that should be followed for making their
application secure.


Any LangSmith related repositories or APIs (see 
Reporting LangSmith Vulnerabilities
).




Reporting LangSmith Vulnerabilities
​


Please report security vulnerabilities associated with LangSmith by email to 
security@langchain.dev
.




LangSmith site: 
https://smith.langchain.com


SDK client: 
https://github.com/langchain-ai/langsmith-sdk




Other Security Concerns
​


For any other security concerns, please contact us at 
security@langchain.dev
.
Edit this page
Was this page helpful?
Previous
Release policy
Best practices
Reporting OSS Vulnerabilities
In-Scope Targets
Out of Scope Targets
Reporting LangSmith Vulnerabilities
Other Security Concerns


--- docs_troubleshooting_errors.txt ---

Error reference


This page contains guides around resolving common errors you may find while building with LangChain.
Errors referenced below will have an 
lc_error_code
 property corresponding to one of the below codes when they are thrown in code.




INVALID_PROMPT_INPUT


INVALID_TOOL_RESULTS


MESSAGE_COERCION_FAILURE


MODEL_AUTHENTICATION


MODEL_NOT_FOUND


MODEL_RATE_LIMIT


OUTPUT_PARSING_FAILURE


Edit this page
Was this page helpful?


--- docs_tutorials.txt ---

Tutorials
On this page
Tutorials


New to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.


Get started
​


Familiarize yourself with LangChain's open-source components by building simple applications.


If you're looking to get started with 
chat models
, 
vector stores
,
or other LangChain components from a specific provider, check out our supported 
integrations
.




Chat models and prompts
: Build a simple LLM application with 
prompt templates
 and 
chat models
.


Semantic search
: Build a semantic search engine over a PDF with 
document loaders
, 
embedding models
, and 
vector stores
.


Classification
: Classify text into categories or labels using 
chat models
 with 
structured outputs
.


Extraction
: Extract structured data from text and other unstructured media using 
chat models
 and 
few-shot examples
.




Refer to the 
how-to guides
 for more detail on using all LangChain components.


Orchestration
​


Get started using 
LangGraph
 to assemble LangChain components into full-featured applications.




Chatbots
: Build a chatbot that incorporates memory.


Agents
: Build an agent that interacts with external tools.


Retrieval Augmented Generation (RAG) Part 1
: Build an application that uses your own documents to inform its responses.


Retrieval Augmented Generation (RAG) Part 2
: Build a RAG application that incorporates a memory of its user interactions and multi-step retrieval.


Question-Answering with SQL
: Build a question-answering system that executes SQL queries to inform its responses.


Summarization
: Generate summaries of (potentially long) texts.


Question-Answering with Graph Databases
: Build a question-answering system that queries a graph database to inform its responses.




LangSmith
​


LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.


LangSmith documentation is hosted on a separate site.
You can peruse 
LangSmith tutorials here
.


Evaluation
​


LangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:




Evaluate your LLM application


Edit this page
Was this page helpful?
Previous
Introduction
Next
Build a Question Answering application over a Graph Database
Get started
Orchestration
LangSmith
Evaluation


--- docs_tutorials_agents.txt ---

Tutorials
Build an Agent
On this page
Build an Agent


By themselves, language models can't take actions - they just output text.
A big use case for LangChain is creating 
agents
.

Agents
 are systems that use 
LLMs
 as reasoning engines to determine which actions to take and the inputs necessary to perform the action.
After executing actions, the results can be fed back into the LLM to determine whether more actions are needed, or whether it is okay to finish. This is often achieved via 
tool-calling
.


In this tutorial we will build an agent that can interact with a search engine. You will be able to ask this agent questions, watch it call the search tool, and have conversations with it.


End-to-end agent
​


The code snippet below represents a fully functional agent that uses an LLM to decide which tools to use. It is equipped with a generic search tool. It has conversational memory - meaning that it can be used as a multi-turn chatbot.


In the rest of the guide, we will walk through the individual components and what each part does - but if you want to just grab some code and get started, feel free to use this!


# Import relevant functionality
from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_community
.
tools
.
tavily_search 
import
 TavilySearchResults
from
 langchain_core
.
messages 
import
 HumanMessage
from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
prebuilt 
import
 create_react_agent
# Create the agent
memory 
=
 MemorySaver
(
)
model 
=
 ChatAnthropic
(
model_name
=
"claude-3-sonnet-20240229"
)
search 
=
 TavilySearchResults
(
max_results
=
2
)
tools 
=
 
[
search
]
agent_executor 
=
 create_react_agent
(
model
,
 tools
,
 checkpointer
=
memory
)
API Reference:
ChatAnthropic
 | 
TavilySearchResults
 | 
HumanMessage
 | 
MemorySaver
 | 
create_react_agent


# Use the agent
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc123"
}
}
for
 step 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
HumanMessage
(
content
=
"hi im bob! and i live in sf"
)
]
}
,
    config
,
    stream_mode
=
"values"
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
hi im bob! and i live in sf
==================================[1m Ai Message [0m==================================
Hello Bob! Since you didn't ask a specific question, I don't need to use any tools right now. I'm an AI assistant created by Anthropic to be helpful, honest, and harmless. Feel free to ask me anything and I'll do my best to provide a useful response or look up information using my capabilities.


for
 step 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
HumanMessage
(
content
=
"whats the weather where I live?"
)
]
}
,
    config
,
    stream_mode
=
"values"
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
whats the weather where I live?
==================================[1m Ai Message [0m==================================
[{'text': 'To get the current weather for your location in San Francisco, I can use the tavily_search_results_json tool:', 'type': 'text'}, {'id': 'toolu_01AKa2MErG1CU3zRiGsvpBud', 'input': {'query': 'san francisco weather'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
  tavily_search_results_json (toolu_01AKa2MErG1CU3zRiGsvpBud)
 Call ID: toolu_01AKa2MErG1CU3zRiGsvpBud
  Args:
    query: san francisco weather
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json
[{"url": "https://www.weatherapi.com/", "content": "{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1739994486, 'localtime': '2025-02-19 11:48'}, 'current': {'last_updated_epoch': 1739994300, 'last_updated': '2025-02-19 11:45', 'temp_c': 13.3, 'temp_f': 55.9, 'is_day': 1, 'condition': {'text': 'Light rain', 'icon': '//cdn.weatherapi.com/weather/64x64/day/296.png', 'code': 1183}, 'wind_mph': 5.8, 'wind_kph': 9.4, 'wind_degree': 195, 'wind_dir': 'SSW', 'pressure_mb': 1023.0, 'pressure_in': 30.2, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 87, 'cloud': 100, 'feelslike_c': 12.7, 'feelslike_f': 54.8, 'windchill_c': 9.1, 'windchill_f': 48.4, 'heatindex_c': 10.2, 'heatindex_f': 50.3, 'dewpoint_c': 9.8, 'dewpoint_f': 49.7, 'vis_km': 4.0, 'vis_miles': 2.0, 'uv': 1.4, 'gust_mph': 8.9, 'gust_kph': 14.4}}"}, {"url": "https://world-weather.info/forecast/usa/san_francisco/february-2025/", "content": "Weather in San Francisco in February 2025 (California) - Detailed Weather Forecast for a Month Weather World Weather in San Francisco Weather in San Francisco in February 2025 San Francisco Weather Forecast for February 2025, is based on previous years' statistical data. +59°+50° +59°+52° +59°+50° +61°+52° +59°+50° +61°+50° +61°+52° +63°+52° +61°+52° +61°+50° +61°+50° +61°+50° +59°+50° +59°+50° +61°+50° +61°+52° +59°+50° +59°+48° +57°+48° +59°+50° +59°+48° +59°+50° +57°+46° +61°+50° +61°+50° +59°+50° +59°+48° +59°+50° Extended weather forecast in San Francisco HourlyWeek10-Day14-Day30-DayYear Weather in large and nearby cities Weather in Washington, D.C.+41° Sacramento+55° Pleasanton+55° Redwood City+55° San Leandro+55° San Mateo+54° San Rafael+52° San Ramon+52° South San Francisco+54° Vallejo+50° Palo Alto+55° Pacifica+55° Berkeley+54° Castro Valley+55° Concord+52° Daly City+54° Noverd+52° Sign Hill+54° world's temperature today day day Temperature units"}]
==================================[1m Ai Message [0m==================================
The search results provide the current weather conditions and forecast for San Francisco. According to the data from WeatherAPI, the current temperature in San Francisco is around 55°F (13°C) with light rain and winds around 6 mph. The extended forecast shows temperatures ranging from the upper 40s to low 60s Fahrenheit over the next few weeks.
So in summary, it's a cool, rainy day currently in San Francisco where you live, Bob. Let me know if you need any other details about the weather there!


Setup
​


Jupyter Notebook
​


This guide (and most of the other guides in the documentation) uses 
Jupyter notebooks
 and assumes the reader is as well. Jupyter notebooks are perfect interactive environments for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc), and observing these cases is a great way to better understand building with LLMs.


This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See 
here
 for instructions on how to install.


Installation
​


To install LangChain run:


%
pip install 
-
U langchain
-
community langgraph langchain
-
anthropic tavily
-
python langgraph
-
checkpoint
-
sqlite


For more details, see our 
Installation guide
.


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with 
LangSmith
.


After you sign up at the link above, make sure to set your environment variables to start logging traces:


export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."


Or, if in a notebook, you can set them with:


import
 getpass
import
 os
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Tavily
​


We will be using 
Tavily
 (a search engine) as a tool.
In order to use it, you will need to get and set an API key:


export TAVILY_API_KEY="..."


Or, if in a notebook, you can set it with:


import
 getpass
import
 os
os
.
environ
[
"TAVILY_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Define tools
​


We first need to create the tools we want to use. Our main tool of choice will be 
Tavily
 - a search engine. We have a built-in tool in LangChain to easily use Tavily search engine as tool.


from
 langchain_community
.
tools
.
tavily_search 
import
 TavilySearchResults
search 
=
 TavilySearchResults
(
max_results
=
2
)
search_results 
=
 search
.
invoke
(
"what is the weather in SF"
)
print
(
search_results
)
# If we want, we can create other tools.
# Once we have all the tools we want, we can put them in a list that we will reference later.
tools 
=
 
[
search
]
API Reference:
TavilySearchResults


[{'url': 'https://www.weatherapi.com/', 'content': "{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1739993250, 'localtime': '2025-02-19 11:27'}, 'current': {'last_updated_epoch': 1739992500, 'last_updated': '2025-02-19 11:15', 'temp_c': 13.3, 'temp_f': 55.9, 'is_day': 1, 'condition': {'text': 'Light rain', 'icon': '//cdn.weatherapi.com/weather/64x64/day/296.png', 'code': 1183}, 'wind_mph': 5.8, 'wind_kph': 9.4, 'wind_degree': 195, 'wind_dir': 'SSW', 'pressure_mb': 1023.0, 'pressure_in': 30.2, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 87, 'cloud': 100, 'feelslike_c': 12.7, 'feelslike_f': 54.8, 'windchill_c': 9.1, 'windchill_f': 48.4, 'heatindex_c': 10.2, 'heatindex_f': 50.3, 'dewpoint_c': 9.8, 'dewpoint_f': 49.7, 'vis_km': 4.0, 'vis_miles': 2.0, 'uv': 1.4, 'gust_mph': 8.9, 'gust_kph': 14.4}}"}, {'url': 'https://weathershogun.com/weather/usa/ca/san-francisco/480/february/2025-02-19', 'content': 'San Francisco, California Weather: Wednesday, February 19, 2025. Cloudy weather, overcast skies with clouds. Day 61°. Night 43°.'}]


Using Language Models
​


Next, let's learn how to use a language model to call tools. LangChain supports many different language models that you can use interchangably - select the one you want to use below!




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4"
,
 model_provider
=
"openai"
)


You can call the language model by passing in a list of messages. By default, the response is a 
content
 string.


from
 langchain_core
.
messages 
import
 HumanMessage
response 
=
 model
.
invoke
(
[
HumanMessage
(
content
=
"hi!"
)
]
)
response
.
content
API Reference:
HumanMessage


'Hi there!'


We can now see what it is like to enable this model to do tool calling. In order to enable that we use 
.bind_tools
 to give the language model knowledge of these tools


model_with_tools 
=
 model
.
bind_tools
(
tools
)


We can now call the model. Let's first call it with a normal message, and see how it responds. We can look at both the 
content
 field as well as the 
tool_calls
 field.


response 
=
 model_with_tools
.
invoke
(
[
HumanMessage
(
content
=
"Hi!"
)
]
)
print
(
f"ContentString: 
{
response
.
content
}
"
)
print
(
f"ToolCalls: 
{
response
.
tool_calls
}
"
)


ContentString: Hello!
ToolCalls: []


Now, let's try calling it with some input that would expect a tool to be called.


response 
=
 model_with_tools
.
invoke
(
[
HumanMessage
(
content
=
"What's the weather in SF?"
)
]
)
print
(
f"ContentString: 
{
response
.
content
}
"
)
print
(
f"ToolCalls: 
{
response
.
tool_calls
}
"
)


ContentString: 
ToolCalls: [{'name': 'tavily_search_results_json', 'args': {'query': 'weather san francisco'}, 'id': 'toolu_01VTP7DUvSfgtYxsq9x4EwMp'}]


We can see that there's now no text content, but there is a tool call! It wants us to call the Tavily Search tool.


This isn't calling that tool yet - it's just telling us to. In order to actually call it, we'll want to create our agent.


Create the agent
​


Now that we have defined the tools and the LLM, we can create the agent. We will be using 
LangGraph
 to construct the agent.
Currently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.


Now, we can initialize the agent with the LLM and the tools.


Note that we are passing in the 
model
, not 
model_with_tools
. That is because 
create_react_agent
 will call 
.bind_tools
 for us under the hood.


from
 langgraph
.
prebuilt 
import
 create_react_agent
agent_executor 
=
 create_react_agent
(
model
,
 tools
)
API Reference:
create_react_agent


Run the agent
​


We can now run the agent with a few queries! Note that for now, these are all 
stateless
 queries (it won't remember previous interactions). Note that the agent will return the 
final
 state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs).


First up, let's see how it responds when there's no need to call a tool:


response 
=
 agent_executor
.
invoke
(
{
"messages"
:
 
[
HumanMessage
(
content
=
"hi!"
)
]
}
)
response
[
"messages"
]


[HumanMessage(content='hi!', id='a820fcc5-9b87-457a-9af0-f21768143ee3'),
 AIMessage(content='Hello!', response_metadata={'id': 'msg_01VbC493X1VEDyusgttiEr1z', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 264, 'output_tokens': 5}}, id='run-0e0ddae8-a85b-4bd6-947c-c36c857a4698-0', usage_metadata={'input_tokens': 264, 'output_tokens': 5, 'total_tokens': 269})]


In order to see exactly what is happening under the hood (and to make sure it's not calling a tool) we can take a look at the 
LangSmith trace


Let's now try it out on an example where it should be invoking the tool


response 
=
 agent_executor
.
invoke
(
    
{
"messages"
:
 
[
HumanMessage
(
content
=
"whats the weather in sf?"
)
]
}
)
response
[
"messages"
]


[HumanMessage(content='whats the weather in sf?', id='1d6c96bb-4ddb-415c-a579-a07d5264de0d'),
 AIMessage(content=[{'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}], response_metadata={'id': 'msg_0132wQUcEduJ8UKVVVqwJzM4', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'input_tokens': 269, 'output_tokens': 61}}, id='run-26d5e5e8-d4fd-46d2-a197-87b95b10e823-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'weather in san francisco'}, 'id': 'toolu_01Y5EK4bw2LqsQXeaUv8iueF'}], usage_metadata={'input_tokens': 269, 'output_tokens': 61, 'total_tokens': 330}),
 ToolMessage(content='[{"url": "https://www.weatherapi.com/", "content": "{\'location\': {\'name\': \'San Francisco\', \'region\': \'California\', \'country\': \'United States of America\', \'lat\': 37.78, \'lon\': -122.42, \'tz_id\': \'America/Los_Angeles\', \'localtime_epoch\': 1717238703, \'localtime\': \'2024-06-01 3:45\'}, \'current\': {\'last_updated_epoch\': 1717237800, \'last_updated\': \'2024-06-01 03:30\', \'temp_c\': 12.0, \'temp_f\': 53.6, \'is_day\': 0, \'condition\': {\'text\': \'Mist\', \'icon\': \'//cdn.weatherapi.com/weather/64x64/night/143.png\', \'code\': 1030}, \'wind_mph\': 5.6, \'wind_kph\': 9.0, \'wind_degree\': 310, \'wind_dir\': \'NW\', \'pressure_mb\': 1013.0, \'pressure_in\': 29.92, \'precip_mm\': 0.0, \'precip_in\': 0.0, \'humidity\': 88, \'cloud\': 100, \'feelslike_c\': 10.5, \'feelslike_f\': 50.8, \'windchill_c\': 9.3, \'windchill_f\': 48.7, \'heatindex_c\': 11.1, \'heatindex_f\': 51.9, \'dewpoint_c\': 8.8, \'dewpoint_f\': 47.8, \'vis_km\': 6.4, \'vis_miles\': 3.0, \'uv\': 1.0, \'gust_mph\': 12.5, \'gust_kph\': 20.1}}"}, {"url": "https://www.timeanddate.com/weather/usa/san-francisco/hourly", "content": "Sun & Moon. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 59 \\u00b0F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather."}]', name='tavily_search_results_json', id='37aa1fd9-b232-4a02-bd22-bc5b9b44a22c', tool_call_id='toolu_01Y5EK4bw2LqsQXeaUv8iueF'),
 AIMessage(content='Based on the search results, here is a summary of the current weather in San Francisco:\n\nThe weather in San Francisco is currently misty with a temperature of around 53°F (12°C). There is complete cloud cover and moderate winds from the northwest around 5-9 mph (9-14 km/h). Humidity is high at 88%. Visibility is around 3 miles (6.4 km). \n\nThe results provide an hourly forecast as well as current conditions from a couple different weather sources. Let me know if you need any additional details about the San Francisco weather!', response_metadata={'id': 'msg_01BRX9mrT19nBDdHYtR7wJ92', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 920, 'output_tokens': 132}}, id='run-d0325583-3ddc-4432-b2b2-d023eb97660f-0', usage_metadata={'input_tokens': 920, 'output_tokens': 132, 'total_tokens': 1052})]


We can check out the 
LangSmith trace
 to make sure it's calling the search tool effectively.


Streaming Messages
​


We've seen how the agent can be called with 
.invoke
 to get  a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.


for
 step 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
HumanMessage
(
content
=
"whats the weather in sf?"
)
]
}
,
    stream_mode
=
"values"
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
whats the weather in sf?
==================================[1m Ai Message [0m==================================
[{'text': 'Okay, let me look up the current weather for San Francisco using a search engine:', 'type': 'text'}, {'id': 'toolu_01H1brh5EZpZqtqHBxkosPtN', 'input': {'query': 'san francisco weather'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
  tavily_search_results_json (toolu_01H1brh5EZpZqtqHBxkosPtN)
 Call ID: toolu_01H1brh5EZpZqtqHBxkosPtN
  Args:
    query: san francisco weather
=================================[1m Tool Message [0m=================================
Name: tavily_search_results_json
[{"url": "https://www.weatherapi.com/", "content": "{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1739994486, 'localtime': '2025-02-19 11:48'}, 'current': {'last_updated_epoch': 1739994300, 'last_updated': '2025-02-19 11:45', 'temp_c': 13.3, 'temp_f': 55.9, 'is_day': 1, 'condition': {'text': 'Light rain', 'icon': '//cdn.weatherapi.com/weather/64x64/day/296.png', 'code': 1183}, 'wind_mph': 5.8, 'wind_kph': 9.4, 'wind_degree': 195, 'wind_dir': 'SSW', 'pressure_mb': 1023.0, 'pressure_in': 30.2, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 87, 'cloud': 100, 'feelslike_c': 12.7, 'feelslike_f': 54.8, 'windchill_c': 9.1, 'windchill_f': 48.4, 'heatindex_c': 10.2, 'heatindex_f': 50.3, 'dewpoint_c': 9.8, 'dewpoint_f': 49.7, 'vis_km': 4.0, 'vis_miles': 2.0, 'uv': 1.4, 'gust_mph': 8.9, 'gust_kph': 14.4}}"}, {"url": "https://world-weather.info/forecast/usa/san_francisco/february-2025/", "content": "Weather in San Francisco in February 2025 (California) - Detailed Weather Forecast for a Month Weather World Weather in San Francisco Weather in San Francisco in February 2025 San Francisco Weather Forecast for February 2025, is based on previous years' statistical data. +59°+50° +59°+52° +59°+50° +61°+52° +59°+50° +61°+50° +61°+52° +63°+52° +61°+52° +61°+50° +61°+50° +61°+50° +59°+50° +59°+50° +61°+50° +61°+52° +59°+50° +59°+48° +57°+48° +59°+50° +59°+48° +59°+50° +57°+46° +61°+50° +61°+50° +59°+50° +59°+48° +59°+50° Extended weather forecast in San Francisco HourlyWeek10-Day14-Day30-DayYear Weather in large and nearby cities Weather in Washington, D.C.+41° Sacramento+55° Pleasanton+55° Redwood City+55° San Leandro+55° San Mateo+54° San Rafael+52° San Ramon+52° South San Francisco+54° Vallejo+50° Palo Alto+55° Pacifica+55° Berkeley+54° Castro Valley+55° Concord+52° Daly City+54° Noverd+52° Sign Hill+54° world's temperature today day day Temperature units"}]
==================================[1m Ai Message [0m==================================
The search results provide details on the current weather conditions and forecast for San Francisco. Some key details:
- It is lightly raining in San Francisco right now, with a temperature around 55°F/13°C. 
- The forecast for the rest of February 2025 shows daytime highs mostly in the upper 50s to low 60s F, with night lows in the upper 40s to low 50s F. 
- Typical weather includes some rain, clouds, cool temperatures and breezy conditions.
So in summary, as is common for San Francisco in late winter, it is currently cool with light rain showers, and similar mild, unsettled weather is expected over the next couple weeks. Layers and a light jacket would be advisable for being outdoors. Let me know if you need any other details!


Streaming tokens
​


In addition to streaming back messages, it is also useful to stream back tokens.
We can do this by specifying 
stream_mode="messages"
.


::: note


Below we use 
message.text()
, which requires 
langchain-core>=0.3.37
.


:::


for
 step
,
 metadata 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
HumanMessage
(
content
=
"whats the weather in sf?"
)
]
}
,
    stream_mode
=
"messages"
,
)
:
    
if
 metadata
[
"langgraph_node"
]
 
==
 
"agent"
 
and
 
(
text 
:=
 step
.
text
(
)
)
:
        
print
(
text
,
 end
=
"|"
)


Base|d on the weather| search| results, here| are the key details| about the weather in| San Francisco:|
- The current temperature| in| San Francisco is aroun|d 55|-|56|°F (13|°|C).| Light| rain is occurring with| |100|% clou|d cover. |
-| Winds| are aroun|d 5-9| mph from| the south|-southwest.|
- The| forecast| for| the rest| of February| 2025 |shows da|ytime highs mostly| in the upper| 50s to| low| 60s°|F,| with overnight lows| in| the upper| 40s to| low| 50s°|F.|
-| Overall|, typical| cool| an|d show|ery late| winter weather is| expected in San Francisco| for the remainder| of February,| with a| mix| of rain| and dry| periods|.| Temperatures will be| season|able| for| this| time of year.|
So| in summary, San| Francisco is| experiencing light| rain an|d cool| temperatures currently, but| the late| winter forecast| shows typical mil|d and show|ery conditions| pers|isting through the en|d of the| month.| Let| me know if you| need any other| details about| the weather in the| city!|


Adding in memory
​


As mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a 
thread_id
 when invoking the agent (so it knows which thread/conversation to resume from).


from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
memory 
=
 MemorySaver
(
)
API Reference:
MemorySaver


agent_executor 
=
 create_react_agent
(
model
,
 tools
,
 checkpointer
=
memory
)
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc123"
}
}


for
 chunk 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
HumanMessage
(
content
=
"hi im bob!"
)
]
}
,
 config
)
:
    
print
(
chunk
)
    
print
(
"----"
)


{'agent': {'messages': [AIMessage(content="Hello Bob! It's nice to meet you again.", response_metadata={'id': 'msg_013C1z2ZySagEFwmU1EsysR2', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 1162, 'output_tokens': 14}}, id='run-f878acfd-d195-44e8-9166-e2796317e3f8-0', usage_metadata={'input_tokens': 1162, 'output_tokens': 14, 'total_tokens': 1176})]}}
----


for
 chunk 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
HumanMessage
(
content
=
"whats my name?"
)
]
}
,
 config
)
:
    
print
(
chunk
)
    
print
(
"----"
)


{'agent': {'messages': [AIMessage(content='You mentioned your name is Bob when you introduced yourself earlier. So your name is Bob.', response_metadata={'id': 'msg_01WNwnRNGwGDRw6vRdivt6i1', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 1184, 'output_tokens': 21}}, id='run-f5c0b957-8878-405a-9d4b-a7cd38efe81f-0', usage_metadata={'input_tokens': 1184, 'output_tokens': 21, 'total_tokens': 1205})]}}
----


Example 
LangSmith trace


If you want to start a new conversation, all you have to do is change the 
thread_id
 used


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"xyz123"
}
}
for
 chunk 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
HumanMessage
(
content
=
"whats my name?"
)
]
}
,
 config
)
:
    
print
(
chunk
)
    
print
(
"----"
)


{'agent': {'messages': [AIMessage(content="I'm afraid I don't actually know your name. As an AI assistant without personal information about you, I don't have a specific name associated with our conversation.", response_metadata={'id': 'msg_01NoaXNNYZKSoBncPcLkdcbo', 'model': 'claude-3-sonnet-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 267, 'output_tokens': 36}}, id='run-c9f7df3d-525a-4d8f-bbcf-a5b4a5d2e4b0-0', usage_metadata={'input_tokens': 267, 'output_tokens': 36, 'total_tokens': 303})]}}
----


Conclusion
​


That's a wrap! In this quick start we covered how to create a simple agent.
We've then shown how to stream back a response - not only with the intermediate steps, but also tokens!
We've also added in memory so you can have a conversation with them.
Agents are a complex topic with lots to learn!


For more information on Agents, please check out the 
LangGraph
 documentation. This has it's own set of concepts, tutorials, and how-to guides.
Edit this page
Was this page helpful?
Previous
Build an Extraction Chain
Next
Tagging
End-to-end agent
Setup
Jupyter Notebook
Installation
LangSmith
Tavily
Define tools
Using Language Models
Create the agent
Run the agent
Streaming Messages
Streaming tokens
Adding in memory
Conclusion


--- docs_tutorials_chatbot.txt ---

Tutorials
Build a Chatbot
On this page
Build a Chatbot


note
This tutorial previously used the 
RunnableWithMessageHistory
 abstraction. You can access that version of the documentation in the 
v0.2 docs
.
As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of 
LangGraph persistence
 to incorporate 
memory
 into new LangChain applications.
If your code is already relying on 
RunnableWithMessageHistory
 or 
BaseChatMessageHistory
, you do 
not
 need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses 
RunnableWithMessageHistory
 will continue to work as expected.
Please see 
How to migrate to LangGraph Memory
 for more details.


Overview
​


We'll go over an example of how to design and implement an LLM-powered chatbot.
This chatbot will be able to have a conversation and remember previous interactions with a 
chat model
.


Note that this chatbot that we build will only use the language model to have a conversation.
There are several other related concepts that you may be looking for:




Conversational RAG
: Enable a chatbot experience over an external source of data


Agents
: Build a chatbot that can take actions




This tutorial will cover the basics which will be helpful for those two more advanced topics, but feel free to skip directly to there should you choose.


Setup
​


Jupyter Notebook
​


This guide (and most of the other guides in the documentation) uses 
Jupyter notebooks
 and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.


This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See 
here
 for instructions on how to install.


Installation
​


For this tutorial we will need 
langchain-core
 and 
langgraph
. This guide requires 
langgraph >= 0.2.28
.




Pip
Conda
pip install langchain-core langgraph>0.2.27
conda install langchain-core langgraph>0.2.27 -c conda-forge


For more details, see our 
Installation guide
.


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with 
LangSmith
.


After you sign up at the link above, make sure to set your environment variables to start logging traces:


export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."


Or, if in a notebook, you can set them with:


import
 getpass
import
 os
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Quickstart
​


First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably - select the one you want to use below!




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Let's first use the model directly. 
ChatModel
s are instances of LangChain "Runnables", which means they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the 
.invoke
 method.


from
 langchain_core
.
messages 
import
 HumanMessage
model
.
invoke
(
[
HumanMessage
(
content
=
"Hi! I'm Bob"
)
]
)
API Reference:
HumanMessage


AIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-5211544f-da9f-4325-8b8e-b3d92b2fc71a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})


The model on its own does not have any concept of state. For example, if you ask a followup question:


model
.
invoke
(
[
HumanMessage
(
content
=
"What's my name?"
)
]
)


AIMessage(content="I'm sorry, but I don't have access to personal information about users unless it has been shared with me in the course of our conversation. How can I assist you today?", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 11, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a2d13a18-7022-4784-b54f-f85c097d1075-0', usage_metadata={'input_tokens': 11, 'output_tokens': 34, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})


Let's take a look at the example 
LangSmith trace


We can see that it doesn't take the previous conversation turn into context, and cannot answer the question.
This makes for a terrible chatbot experience!


To get around this, we need to pass the entire 
conversation history
 into the model. Let's see what happens when we do that:


from
 langchain_core
.
messages 
import
 AIMessage
model
.
invoke
(
    
[
        HumanMessage
(
content
=
"Hi! I'm Bob"
)
,
        AIMessage
(
content
=
"Hello Bob! How can I assist you today?"
)
,
        HumanMessage
(
content
=
"What's my name?"
)
,
    
]
)
API Reference:
AIMessage


AIMessage(content='Your name is Bob! How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 33, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-34bcccb3-446e-42f2-b1de-52c09936c02c-0', usage_metadata={'input_tokens': 33, 'output_tokens': 14, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})


And now we can see that we get a good response!


This is the basic idea underpinning a chatbot's ability to interact conversationally.
So how do we best implement this?


Message persistence
​


LangGraph
 implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.


Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.


LangGraph comes with a simple in-memory checkpointer, which we use below. See its 
documentation
 for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).


from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
graph 
import
 START
,
 MessagesState
,
 StateGraph
# Define a new graph
workflow 
=
 StateGraph
(
state_schema
=
MessagesState
)
# Define the function that calls the model
def
 
call_model
(
state
:
 MessagesState
)
:
    response 
=
 model
.
invoke
(
state
[
"messages"
]
)
    
return
 
{
"messages"
:
 response
}
# Define the (single) node in the graph
workflow
.
add_edge
(
START
,
 
"model"
)
workflow
.
add_node
(
"model"
,
 call_model
)
# Add memory
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
checkpointer
=
memory
)
API Reference:
MemorySaver
 | 
StateGraph


We now need to create a 
config
 that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a 
thread_id
. This should look like:


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc123"
}
}


This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.


We can then invoke the application:


query 
=
 
"Hi! I'm Bob."
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
{
"messages"
:
 input_messages
}
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
  
# output contains all messages in state


==================================[1m Ai Message [0m==================================
Hi Bob! How can I assist you today?


query 
=
 
"What's my name?"
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
{
"messages"
:
 input_messages
}
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
Your name is Bob! How can I help you today, Bob?


Great! Our chatbot now remembers things about us. If we change the config to reference a different 
thread_id
, we can see that it starts the conversation fresh.


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc234"
}
}
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
{
"messages"
:
 input_messages
}
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
I'm sorry, but I don't have access to personal information about you unless you've shared it in this conversation. How can I assist you today?


However, we can always go back to the original conversation (since we are persisting it in a database)


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc123"
}
}
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
{
"messages"
:
 input_messages
}
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
Your name is Bob. What would you like to discuss today?


This is how we can support a chatbot having conversations with many users!


tip
For async support, update the 
call_model
 node to be an async function and use 
.ainvoke
 when invoking the application:
# Async function for node:
async
 
def
 
call_model
(
state
:
 MessagesState
)
:
    response 
=
 
await
 model
.
ainvoke
(
state
[
"messages"
]
)
    
return
 
{
"messages"
:
 response
}
# Define graph as before:
workflow 
=
 StateGraph
(
state_schema
=
MessagesState
)
workflow
.
add_edge
(
START
,
 
"model"
)
workflow
.
add_node
(
"model"
,
 call_model
)
app 
=
 workflow
.
compile
(
checkpointer
=
MemorySaver
(
)
)
# Async invocation:
output 
=
 
await
 app
.
ainvoke
(
{
"messages"
:
 input_messages
}
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


Right now, all we've done is add a simple persistence layer around the model. We can start to make the chatbot more complicated and personalized by adding in a prompt template.


Prompt templates
​


Prompt Templates
 help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.


To add in a system message, we will create a 
ChatPromptTemplate
. We will utilize 
MessagesPlaceholder
 to pass all the messages in.


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 MessagesPlaceholder
prompt_template 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"You talk like a pirate. Answer all questions to the best of your ability."
,
        
)
,
        MessagesPlaceholder
(
variable_name
=
"messages"
)
,
    
]
)
API Reference:
ChatPromptTemplate
 | 
MessagesPlaceholder


We can now update our application to incorporate this template:


workflow 
=
 StateGraph
(
state_schema
=
MessagesState
)
def
 
call_model
(
state
:
 MessagesState
)
:
    prompt 
=
 prompt_template
.
invoke
(
state
)
    response 
=
 model
.
invoke
(
prompt
)
    
return
 
{
"messages"
:
 response
}
workflow
.
add_edge
(
START
,
 
"model"
)
workflow
.
add_node
(
"model"
,
 call_model
)
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
checkpointer
=
memory
)


We invoke the application in the same way:


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc345"
}
}
query 
=
 
"Hi! I'm Jim."
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
{
"messages"
:
 input_messages
}
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
Ahoy there, Jim! What brings ye to these waters today? Be ye seekin' treasure, knowledge, or perhaps a good tale from the high seas? Arrr!


query 
=
 
"What is my name?"
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
{
"messages"
:
 input_messages
}
,
 config
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
Ye be called Jim, matey! A fine name fer a swashbuckler such as yerself! What else can I do fer ye? Arrr!


Awesome! Let's now make our prompt a little bit more complicated. Let's assume that the prompt template now looks something like this:


prompt_template 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"You are a helpful assistant. Answer all questions to the best of your ability in {language}."
,
        
)
,
        MessagesPlaceholder
(
variable_name
=
"messages"
)
,
    
]
)


Note that we have added a new 
language
 input to the prompt. Our application now has two parameters-- the input 
messages
 and 
language
. We should update our application's state to reflect this:


from
 typing 
import
 Sequence
from
 langchain_core
.
messages 
import
 BaseMessage
from
 langgraph
.
graph
.
message 
import
 add_messages
from
 typing_extensions 
import
 Annotated
,
 TypedDict
class
 
State
(
TypedDict
)
:
    messages
:
 Annotated
[
Sequence
[
BaseMessage
]
,
 add_messages
]
    language
:
 
str
workflow 
=
 StateGraph
(
state_schema
=
State
)
def
 
call_model
(
state
:
 State
)
:
    prompt 
=
 prompt_template
.
invoke
(
state
)
    response 
=
 model
.
invoke
(
prompt
)
    
return
 
{
"messages"
:
 
[
response
]
}
workflow
.
add_edge
(
START
,
 
"model"
)
workflow
.
add_node
(
"model"
,
 call_model
)
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
checkpointer
=
memory
)
API Reference:
BaseMessage
 | 
add_messages


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc456"
}
}
query 
=
 
"Hi! I'm Bob."
language 
=
 
"Spanish"
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
    
{
"messages"
:
 input_messages
,
 
"language"
:
 language
}
,
    config
,
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
¡Hola, Bob! ¿Cómo puedo ayudarte hoy?


Note that the entire state is persisted, so we can omit parameters like 
language
 if no changes are desired:


query 
=
 
"What is my name?"
input_messages 
=
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
    
{
"messages"
:
 input_messages
}
,
    config
,
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
Tu nombre es Bob. ¿Hay algo más en lo que pueda ayudarte?


To help you understand what's happening internally, check out 
this LangSmith trace
.


Managing Conversation History
​


One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.


Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.


We can do this by adding a simple step in front of the prompt that modifies the 
messages
 key appropriately, and then wrap that new chain in the Message History class.


LangChain comes with a few built-in helpers for 
managing a list of messages
. In this case we'll use the 
trim_messages
 helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:


from
 langchain_core
.
messages 
import
 SystemMessage
,
 trim_messages
trimmer 
=
 trim_messages
(
    max_tokens
=
65
,
    strategy
=
"last"
,
    token_counter
=
model
,
    include_system
=
True
,
    allow_partial
=
False
,
    start_on
=
"human"
,
)
messages 
=
 
[
    SystemMessage
(
content
=
"you're a good assistant"
)
,
    HumanMessage
(
content
=
"hi! I'm bob"
)
,
    AIMessage
(
content
=
"hi!"
)
,
    HumanMessage
(
content
=
"I like vanilla ice cream"
)
,
    AIMessage
(
content
=
"nice"
)
,
    HumanMessage
(
content
=
"whats 2 + 2"
)
,
    AIMessage
(
content
=
"4"
)
,
    HumanMessage
(
content
=
"thanks"
)
,
    AIMessage
(
content
=
"no problem!"
)
,
    HumanMessage
(
content
=
"having fun?"
)
,
    AIMessage
(
content
=
"yes!"
)
,
]
trimmer
.
invoke
(
messages
)
API Reference:
SystemMessage
 | 
trim_messages


[SystemMessage(content="you're a good assistant", additional_kwargs={}, response_metadata={}),
 HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),
 AIMessage(content='4', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),
 AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),
 AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]


To  use it in our chain, we just need to run the trimmer before we pass the 
messages
 input to our prompt.


workflow 
=
 StateGraph
(
state_schema
=
State
)
def
 
call_model
(
state
:
 State
)
:
    trimmed_messages 
=
 trimmer
.
invoke
(
state
[
"messages"
]
)
    prompt 
=
 prompt_template
.
invoke
(
        
{
"messages"
:
 trimmed_messages
,
 
"language"
:
 state
[
"language"
]
}
    
)
    response 
=
 model
.
invoke
(
prompt
)
    
return
 
{
"messages"
:
 
[
response
]
}
workflow
.
add_edge
(
START
,
 
"model"
)
workflow
.
add_node
(
"model"
,
 call_model
)
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
checkpointer
=
memory
)


Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history:


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc567"
}
}
query 
=
 
"What is my name?"
language 
=
 
"English"
input_messages 
=
 messages 
+
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
    
{
"messages"
:
 input_messages
,
 
"language"
:
 language
}
,
    config
,
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
I don't know your name. You haven't told me yet!


But if we ask about information that is within the last few messages, it remembers:


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc678"
}
}
query 
=
 
"What math problem did I ask?"
language 
=
 
"English"
input_messages 
=
 messages 
+
 
[
HumanMessage
(
query
)
]
output 
=
 app
.
invoke
(
    
{
"messages"
:
 input_messages
,
 
"language"
:
 language
}
,
    config
,
)
output
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


==================================[1m Ai Message [0m==================================
You asked what 2 + 2 equals.


If you take a look at LangSmith, you can see exactly what is happening under the hood in the 
LangSmith trace
.


Streaming
​


Now we've got a functioning chatbot. However, one 
really
 important UX consideration for chatbot applications is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.


It's actually super easy to do this!


By default, 
.stream
 in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting 
stream_mode="messages"
 allows us to stream output tokens instead:


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc789"
}
}
query 
=
 
"Hi I'm Todd, please tell me a joke."
language 
=
 
"English"
input_messages 
=
 
[
HumanMessage
(
query
)
]
for
 chunk
,
 metadata 
in
 app
.
stream
(
    
{
"messages"
:
 input_messages
,
 
"language"
:
 language
}
,
    config
,
    stream_mode
=
"messages"
,
)
:
    
if
 
isinstance
(
chunk
,
 AIMessage
)
:
  
# Filter to just model responses
        
print
(
chunk
.
content
,
 end
=
"|"
)


|Hi| Todd|!| Here|’s| a| joke| for| you|:
|Why| don|’t| skeleton|s| fight| each| other|?
|Because| they| don|’t| have| the| guts|!||


Next Steps
​


Now that you understand the basics of how to create a chatbot in LangChain, some more advanced tutorials you may be interested in are:




Conversational RAG
: Enable a chatbot experience over an external source of data


Agents
: Build a chatbot that can take actions




If you want to dive deeper on specifics, some things worth checking out are:




Streaming
: streaming is 
crucial
 for chat applications


How to add message history
: for a deeper dive into all things related to message history


How to manage large message history
: more techniques for managing a large chat history


LangGraph main docs
: for more detail on building with LangGraph


Edit this page
Was this page helpful?
Previous
Build a simple LLM application with chat models and prompt templates
Next
Build a Retrieval Augmented Generation (RAG) App: Part 2
Overview
Setup
Jupyter Notebook
Installation
LangSmith
Quickstart
Message persistence
Prompt templates
Managing Conversation History
Streaming
Next Steps


--- docs_tutorials_classification.txt ---

Tutorials
Tagging
On this page


Classify Text into Labels


Tagging means labeling a document with classes such as:




Sentiment


Language


Style (formal, informal etc.)


Covered topics


Political tendency






Overview
​


Tagging has a few components:




function
: Like 
extraction
, tagging uses 
functions
 to specify how the model should tag a document


schema
: defines how we want to tag the document




Quickstart
​


Let's see a very straightforward example of how we can use OpenAI tool calling for tagging in LangChain. We'll use the 
with_structured_output
 method supported by OpenAI models.


pip install 
-
-
upgrade 
-
-
quiet langchain
-
core


We'll need to load a 
chat model
:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Let's specify a Pydantic model with a few properties and their expected type in our schema.


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
from
 pydantic 
import
 BaseModel
,
 Field
tagging_prompt 
=
 ChatPromptTemplate
.
from_template
(
    
"""
Extract the desired information from the following passage.
Only extract the properties mentioned in the 'Classification' function.
Passage:
{input}
"""
)
class
 
Classification
(
BaseModel
)
:
    sentiment
:
 
str
 
=
 Field
(
description
=
"The sentiment of the text"
)
    aggressiveness
:
 
int
 
=
 Field
(
        description
=
"How aggressive the text is on a scale from 1 to 10"
    
)
    language
:
 
str
 
=
 Field
(
description
=
"The language the text is written in"
)
# Structured LLM
structured_llm 
=
 llm
.
with_structured_output
(
Classification
)
API Reference:
ChatPromptTemplate
 | 
ChatOpenAI


inp 
=
 
"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!"
prompt 
=
 tagging_prompt
.
invoke
(
{
"input"
:
 inp
}
)
response 
=
 structured_llm
.
invoke
(
prompt
)
response


Classification(sentiment='positive', aggressiveness=1, language='Spanish')


If we want dictionary output, we can just call 
.model_dump()


inp 
=
 
"Estoy muy enojado con vos! Te voy a dar tu merecido!"
prompt 
=
 tagging_prompt
.
invoke
(
{
"input"
:
 inp
}
)
response 
=
 structured_llm
.
invoke
(
prompt
)
response
.
model_dump
(
)


{'sentiment': 'enojado', 'aggressiveness': 8, 'language': 'es'}


As we can see in the examples, it correctly interprets what we want.


The results vary so that we may get, for example, sentiments in different languages ('positive', 'enojado' etc.).


We will see how to control these results in the next section.


Finer control
​


Careful schema definition gives us more control over the model's output.


Specifically, we can define:




Possible values for each property


Description to make sure that the model understands the property


Required properties to be returned




Let's redeclare our Pydantic model to control for each of the previously mentioned aspects using enums:


class
 
Classification
(
BaseModel
)
:
    sentiment
:
 
str
 
=
 Field
(
.
.
.
,
 enum
=
[
"happy"
,
 
"neutral"
,
 
"sad"
]
)
    aggressiveness
:
 
int
 
=
 Field
(
        
.
.
.
,
        description
=
"describes how aggressive the statement is, the higher the number the more aggressive"
,
        enum
=
[
1
,
 
2
,
 
3
,
 
4
,
 
5
]
,
    
)
    language
:
 
str
 
=
 Field
(
        
.
.
.
,
 enum
=
[
"spanish"
,
 
"english"
,
 
"french"
,
 
"german"
,
 
"italian"
]
    
)


tagging_prompt 
=
 ChatPromptTemplate
.
from_template
(
    
"""
Extract the desired information from the following passage.
Only extract the properties mentioned in the 'Classification' function.
Passage:
{input}
"""
)
llm 
=
 ChatOpenAI
(
temperature
=
0
,
 model
=
"gpt-4o-mini"
)
.
with_structured_output
(
    Classification
)


Now the answers will be restricted in a way we expect!


inp 
=
 
"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!"
prompt 
=
 tagging_prompt
.
invoke
(
{
"input"
:
 inp
}
)
llm
.
invoke
(
prompt
)


Classification(sentiment='positive', aggressiveness=1, language='Spanish')


inp 
=
 
"Estoy muy enojado con vos! Te voy a dar tu merecido!"
prompt 
=
 tagging_prompt
.
invoke
(
{
"input"
:
 inp
}
)
llm
.
invoke
(
prompt
)


Classification(sentiment='enojado', aggressiveness=8, language='es')


inp 
=
 
"Weather is ok here, I can go outside without much more than a coat"
prompt 
=
 tagging_prompt
.
invoke
(
{
"input"
:
 inp
}
)
llm
.
invoke
(
prompt
)


Classification(sentiment='neutral', aggressiveness=1, language='English')


The 
LangSmith trace
 lets us peek under the hood:




Going deeper
​




You can use the 
metadata tagger
 document transformer to extract metadata from a LangChain 
Document
.


This covers the same basic functionality as the tagging chain, only applied to a LangChain 
Document
.


Edit this page
Was this page helpful?
Previous
Build an Agent
Next
Build a Retrieval Augmented Generation (RAG) App: Part 1
Overview
Quickstart
Finer control
Going deeper


--- docs_tutorials_extraction.txt ---

Tutorials
Build an Extraction Chain
On this page
Build an Extraction Chain


In this tutorial, we will use 
tool-calling
 features of 
chat models
 to extract structured information from unstructured text. We will also demonstrate how to use 
few-shot prompting
 in this context to improve performance.


important
This tutorial requires 
langchain-core>=0.3.20
 and will only work with models that support 
tool calling
.


Setup
​


Jupyter Notebook
​


This and other tutorials are perhaps most conveniently run in a 
Jupyter notebooks
. Going through guides in an interactive environment is a great way to better understand them. See 
here
 for instructions on how to install.


Installation
​


To install LangChain run:




Pip
Conda
pip install --upgrade langchain-core
conda install langchain-core -c conda-forge


For more details, see our 
Installation guide
.


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with 
LangSmith
.


After you sign up at the link above, make sure to set your environment variables to start logging traces:


export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."


Or, if in a notebook, you can set them with:


import
 getpass
import
 os
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


The Schema
​


First, we need to describe what information we want to extract from the text.


We'll use Pydantic to define an example schema  to extract personal information.


from
 typing 
import
 Optional
from
 pydantic 
import
 BaseModel
,
 Field
class
 
Person
(
BaseModel
)
:
    
"""Information about a person."""
    
# ^ Doc-string for the entity Person.
    
# This doc-string is sent to the LLM as the description of the schema Person,
    
# and it can help to improve extraction results.
    
# Note that:
    
# 1. Each field is an `optional` -- this allows the model to decline to extract it!
    
# 2. Each field has a `description` -- this description is used by the LLM.
    
# Having a good description can help improve extraction results.
    name
:
 Optional
[
str
]
 
=
 Field
(
default
=
None
,
 description
=
"The name of the person"
)
    hair_color
:
 Optional
[
str
]
 
=
 Field
(
        default
=
None
,
 description
=
"The color of the person's hair if known"
    
)
    height_in_meters
:
 Optional
[
str
]
 
=
 Field
(
        default
=
None
,
 description
=
"Height measured in meters"
    
)


There are two best practices when defining schema:




Document the 
attributes
 and the 
schema
 itself: This information is sent to the LLM and is used to improve the quality of information extraction.


Do not force the LLM to make up information! Above we used 
Optional
 for the attributes allowing the LLM to output 
None
 if it doesn't know the answer.




important
For best performance, document the schema well and make sure the model isn't force to return results if there's no information to be extracted in the text.


The Extractor
​


Let's create an information extractor using the schema we defined above.


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 MessagesPlaceholder
# Define a custom prompt to provide instructions and any additional context.
# 1) You can add examples into the prompt template to improve extraction quality
# 2) Introduce additional parameters to take context into account (e.g., include metadata
#    about the document from which the text was extracted.)
prompt_template 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"You are an expert extraction algorithm. "
            
"Only extract relevant information from the text. "
            
"If you do not know the value of an attribute asked to extract, "
            
"return null for the attribute's value."
,
        
)
,
        
# Please see the how-to about improving performance with
        
# reference examples.
        
# MessagesPlaceholder('examples'),
        
(
"human"
,
 
"{text}"
)
,
    
]
)
API Reference:
ChatPromptTemplate
 | 
MessagesPlaceholder


We need to use a model that supports function/tool calling.


Please review 
the documentation
 for all models that can be used with this API.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


structured_llm 
=
 llm
.
with_structured_output
(
schema
=
Person
)


Let's test it out:


text 
=
 
"Alan Smith is 6 feet tall and has blond hair."
prompt 
=
 prompt_template
.
invoke
(
{
"text"
:
 text
}
)
structured_llm
.
invoke
(
prompt
)


Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')


important
Extraction is Generative 🤯
LLMs are generative models, so they can do some pretty cool things like correctly extract the height of the person in meters
even though it was provided in feet!


We can see the LangSmith trace 
here
. Note that the 
chat model portion of the trace
 reveals the exact sequence of messages sent to the model, tools invoked, and other metadata.


Multiple Entities
​


In 
most cases
, you should be extracting a list of entities rather than a single entity.


This can be easily achieved using pydantic by nesting models inside one another.


from
 typing 
import
 List
,
 Optional
from
 pydantic 
import
 BaseModel
,
 Field
class
 
Person
(
BaseModel
)
:
    
"""Information about a person."""
    
# ^ Doc-string for the entity Person.
    
# This doc-string is sent to the LLM as the description of the schema Person,
    
# and it can help to improve extraction results.
    
# Note that:
    
# 1. Each field is an `optional` -- this allows the model to decline to extract it!
    
# 2. Each field has a `description` -- this description is used by the LLM.
    
# Having a good description can help improve extraction results.
    name
:
 Optional
[
str
]
 
=
 Field
(
default
=
None
,
 description
=
"The name of the person"
)
    hair_color
:
 Optional
[
str
]
 
=
 Field
(
        default
=
None
,
 description
=
"The color of the person's hair if known"
    
)
    height_in_meters
:
 Optional
[
str
]
 
=
 Field
(
        default
=
None
,
 description
=
"Height measured in meters"
    
)
class
 
Data
(
BaseModel
)
:
    
"""Extracted data about people."""
    
# Creates a model so that we can extract multiple entities.
    people
:
 List
[
Person
]


important
Extraction results might not be perfect here. Read on to see how to use 
Reference Examples
 to improve the quality of extraction, and check out our extraction 
how-to
 guides for more detail.


structured_llm 
=
 llm
.
with_structured_output
(
schema
=
Data
)
text 
=
 
"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me."
prompt 
=
 prompt_template
.
invoke
(
{
"text"
:
 text
}
)
structured_llm
.
invoke
(
prompt
)


Data(people=[Person(name='Jeff', hair_color='black', height_in_meters='1.83'), Person(name='Anna', hair_color='black', height_in_meters=None)])


tip
When the schema accommodates the extraction of 
multiple entities
, it also allows the model to extract 
no entities
 if no relevant information
is in the text by providing an empty list.
This is usually a 
good
 thing! It allows specifying 
required
 attributes on an entity without necessarily forcing the model to detect this entity.


We can see the LangSmith trace 
here
.


Reference examples
​


The behavior of LLM applications can be steered using 
few-shot prompting
. For 
chat models
, this can take the form of a sequence of pairs of input and response messages demonstrating desired behaviors.


For example, we can convey the meaning of a symbol with alternating 
user
 and 
assistant
 
messages
:


messages 
=
 
[
    
{
"role"
:
 
"user"
,
 
"content"
:
 
"2 🦜 2"
}
,
    
{
"role"
:
 
"assistant"
,
 
"content"
:
 
"4"
}
,
    
{
"role"
:
 
"user"
,
 
"content"
:
 
"2 🦜 3"
}
,
    
{
"role"
:
 
"assistant"
,
 
"content"
:
 
"5"
}
,
    
{
"role"
:
 
"user"
,
 
"content"
:
 
"3 🦜 4"
}
,
]
response 
=
 llm
.
invoke
(
messages
)
print
(
response
.
content
)


7


Structured output
 often uses 
tool calling
 under-the-hood. This typically involves the generation of 
AI messages
 containing tool calls, as well as 
tool messages
 containing the results of tool calls. What should a sequence of messages look like in this case?


Different 
chat model providers
 impose different requirements for valid message sequences. Some will accept a (repeating) message sequence of the form:




User message


AI message with tool call


Tool message with result




Others require a final AI message containing some sort of response.


LangChain includes a utility function 
tool_example_to_messages
 that will generate a valid sequence for most model providers. It simplifies the generation of structured few-shot examples by just requiring Pydantic representations of the corresponding tool calls.


Let's try this out. We can convert pairs of input strings and desired Pydantic objects to a sequence of messages that can be provided to a chat model. Under the hood, LangChain will format the tool calls to each provider's required format.


Note: this version of 
tool_example_to_messages
 requires 
langchain-core>=0.3.20
.


from
 langchain_core
.
utils
.
function_calling 
import
 tool_example_to_messages
examples 
=
 
[
    
(
        
"The ocean is vast and blue. It's more than 20,000 feet deep."
,
        Data
(
people
=
[
]
)
,
    
)
,
    
(
        
"Fiona traveled far from France to Spain."
,
        Data
(
people
=
[
Person
(
name
=
"Fiona"
,
 height_in_meters
=
None
,
 hair_color
=
None
)
]
)
,
    
)
,
]
messages 
=
 
[
]
for
 txt
,
 tool_call 
in
 examples
:
    
if
 tool_call
.
people
:
        
# This final message is optional for some providers
        ai_response 
=
 
"Detected people."
    
else
:
        ai_response 
=
 
"Detected no people."
    messages
.
extend
(
tool_example_to_messages
(
txt
,
 
[
tool_call
]
,
 ai_response
=
ai_response
)
)
API Reference:
tool_example_to_messages


Inspecting the result, we see these two example pairs generated eight messages:


for
 message 
in
 messages
:
    message
.
pretty_print
(
)


================================[1m Human Message [0m=================================
The ocean is vast and blue. It's more than 20,000 feet deep.
==================================[1m Ai Message [0m==================================
Tool Calls:
  Data (d8f2e054-7fb9-417f-b28f-0447a775b2c3)
 Call ID: d8f2e054-7fb9-417f-b28f-0447a775b2c3
  Args:
    people: []
=================================[1m Tool Message [0m=================================
You have correctly called this tool.
==================================[1m Ai Message [0m==================================
Detected no people.
================================[1m Human Message [0m=================================
Fiona traveled far from France to Spain.
==================================[1m Ai Message [0m==================================
Tool Calls:
  Data (0178939e-a4b1-4d2a-a93e-b87f665cdfd6)
 Call ID: 0178939e-a4b1-4d2a-a93e-b87f665cdfd6
  Args:
    people: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]
=================================[1m Tool Message [0m=================================
You have correctly called this tool.
==================================[1m Ai Message [0m==================================
Detected people.


Let's compare performance with and without these messages. For example, let's pass a message for which we intend no people to be extracted:


message_no_extraction 
=
 
{
    
"role"
:
 
"user"
,
    
"content"
:
 
"The solar system is large, but earth has only 1 moon."
,
}
structured_llm 
=
 llm
.
with_structured_output
(
schema
=
Data
)
structured_llm
.
invoke
(
[
message_no_extraction
]
)


Data(people=[Person(name='Earth', hair_color='None', height_in_meters='0.00')])


In this example, the model is liable to erroneously generate records of people.


Because our few-shot examples contain examples of "negatives", we encourage the model to behave correctly in this case:


structured_llm
.
invoke
(
messages 
+
 
[
message_no_extraction
]
)


Data(people=[])


tip
The 
LangSmith
 trace for the run reveals the exact sequence of messages sent to the chat model, tool calls generated, latency, token counts, and other metadata.


See 
this guide
 for more detail on extraction workflows with reference examples, including how to incorporate prompt templates and customize the generation of example messages.


Next steps
​


Now that you understand the basics of extraction with LangChain, you're ready to proceed to the rest of the how-to guides:




Add Examples
: More detail on using 
reference examples
 to improve performance.


Handle Long Text
: What should you do if the text does not fit into the context window of the LLM?


Use a Parsing Approach
: Use a prompt based approach to extract with models that do not support 
tool/function calling
.


Edit this page
Was this page helpful?
Previous
Build a Retrieval Augmented Generation (RAG) App: Part 2
Next
Build an Agent
Setup
Jupyter Notebook
Installation
LangSmith
The Schema
The Extractor
Multiple Entities
Reference examples
Next steps


--- docs_tutorials_graph.txt ---

Tutorials
Build a Question Answering application over a Graph Database
On this page
Build a Question Answering application over a Graph Database


In this guide we'll go over the basic ways to create a Q&A chain over a graph database. These systems will allow us to ask a question about the data in a graph database and get back a natural language answer. First, we will show a simple out-of-the-box option and then implement a more sophisticated version with LangGraph.


⚠️ Security note ⚠️
​


Building Q&A systems of graph databases requires executing model-generated graph queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, 
see here
.


Architecture
​


At a high-level, the steps of most graph chains are:




Convert question to a graph database query
: Model converts user input to a graph database query (e.g. Cypher).


Execute graph database query
: Execute the graph database query.


Answer the question
: Model responds to user input using the query results.






Setup
​


First, get required packages and set environment variables.
In this example, we will be using Neo4j graph database.


%
pip install 
-
-
upgrade 
-
-
quiet langchain langchain
-
neo4j langchain
-
openai langgraph


We default to OpenAI models in this guide.


import
 getpass
import
 os
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter your OpenAI API key: "
)
# Uncomment the below to use LangSmith. Not required.
# os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
# os.environ["LANGSMITH_TRACING"] = "true"


Enter your OpenAI API key:  ········


Next, we need to define Neo4j credentials.
Follow 
these installation steps
 to set up a Neo4j database.


os
.
environ
[
"NEO4J_URI"
]
 
=
 
"bolt://localhost:7687"
os
.
environ
[
"NEO4J_USERNAME"
]
 
=
 
"neo4j"
os
.
environ
[
"NEO4J_PASSWORD"
]
 
=
 
"password"


The below example will create a connection with a Neo4j database and will populate it with example data about movies and their actors.


from
 langchain_neo4j 
import
 Neo4jGraph
graph 
=
 Neo4jGraph
(
)
# Import movie information
movies_query 
=
 
"""
LOAD CSV WITH HEADERS FROM 
'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'
AS row
MERGE (m:Movie {id:row.movieId})
SET m.released = date(row.released),
    m.title = row.title,
    m.imdbRating = toFloat(row.imdbRating)
FOREACH (director in split(row.director, '|') | 
    MERGE (p:Person {name:trim(director)})
    MERGE (p)-[:DIRECTED]->(m))
FOREACH (actor in split(row.actors, '|') | 
    MERGE (p:Person {name:trim(actor)})
    MERGE (p)-[:ACTED_IN]->(m))
FOREACH (genre in split(row.genres, '|') | 
    MERGE (g:Genre {name:trim(genre)})
    MERGE (m)-[:IN_GENRE]->(g))
"""
graph
.
query
(
movies_query
)
API Reference:
Neo4jGraph


[]


Graph schema
​


In order for an LLM to be able to generate a Cypher statement, it needs information about the graph schema. When you instantiate a graph object, it retrieves the information about the graph schema. If you later make any changes to the graph, you can run the 
refresh_schema
 method to refresh the schema information.


graph
.
refresh_schema
(
)
print
(
graph
.
schema
)


Node properties:
Person {name: STRING}
Movie {id: STRING, released: DATE, title: STRING, imdbRating: FLOAT}
Genre {name: STRING}
Chunk {id: STRING, embedding: LIST, text: STRING, question: STRING, query: STRING}
Relationship properties:
The relationships:
(:Person)-[:DIRECTED]->(:Movie)
(:Person)-[:ACTED_IN]->(:Movie)
(:Movie)-[:IN_GENRE]->(:Genre)


For more involved schema information, you can use 
enhanced_schema
 option.


enhanced_graph 
=
 Neo4jGraph
(
enhanced_schema
=
True
)
print
(
enhanced_graph
.
schema
)


Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: "CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships"
``````output
Node properties:
- **Person**
  - `name`: STRING Example: "John Lasseter"
- **Movie**
  - `id`: STRING Example: "1"
  - `released`: DATE Min: 1964-12-16, Max: 1996-09-15
  - `title`: STRING Example: "Toy Story"
  - `imdbRating`: FLOAT Min: 2.4, Max: 9.3
- **Genre**
  - `name`: STRING Example: "Adventure"
- **Chunk**
  - `id`: STRING Available options: ['d66006059fd78d63f3df90cc1059639a', '0e3dcb4502853979d12357690a95ec17', 'c438c6bcdcf8e4fab227f29f8e7ff204', '97fe701ec38057594464beaa2df0710e', 'b54f9286e684373498c4504b4edd9910', '5b50a72c3a4954b0ff7a0421be4f99b9', 'fb28d41771e717255f0d8f6c799ede32', '58e6f14dd2e6c6702cf333f2335c499c']
  - `text`: STRING Available options: ['How many artists are there?', 'Which actors played in the movie Casino?', 'How many movies has Tom Hanks acted in?', "List all the genres of the movie Schindler's List", 'Which actors have worked in movies from both the c', 'Which directors have made movies with at least thr', 'Identify movies where directors also played a role', 'Find the actor with the highest number of movies i']
  - `question`: STRING Available options: ['How many artists are there?', 'Which actors played in the movie Casino?', 'How many movies has Tom Hanks acted in?', "List all the genres of the movie Schindler's List", 'Which actors have worked in movies from both the c', 'Which directors have made movies with at least thr', 'Identify movies where directors also played a role', 'Find the actor with the highest number of movies i']
  - `query`: STRING Available options: ['MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN coun', "MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a)", "MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->", "MATCH (m:Movie {title: 'Schindler's List'})-[:IN_G", 'MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]', 'MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_I', 'MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACT', 'MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.na']
Relationship properties:
The relationships:
(:Person)-[:DIRECTED]->(:Movie)
(:Person)-[:ACTED_IN]->(:Movie)
(:Movie)-[:IN_GENRE]->(:Genre)


The 
enhanced_schema
 option enriches property information by including details such as minimum and maximum values for floats and dates, as well as example values for string properties. This additional context helps guide the LLM toward generating more accurate and effective queries.


Great! We've got a graph database that we can query. Now let's try hooking it up to an LLM.


GraphQACypherChain
​


Let's use a simple out-of-the-box chain that takes a question, turns it into a Cypher query, executes the query, and uses the result to answer the original question.




LangChain comes with a built-in chain for this workflow that is designed to work with Neo4j: 
GraphCypherQAChain


from
 langchain_neo4j 
import
 GraphCypherQAChain
from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o"
,
 temperature
=
0
)
chain 
=
 GraphCypherQAChain
.
from_llm
(
    graph
=
enhanced_graph
,
 llm
=
llm
,
 verbose
=
True
,
 allow_dangerous_requests
=
True
)
response 
=
 chain
.
invoke
(
{
"query"
:
 
"What was the cast of the Casino?"
}
)
response
API Reference:
GraphCypherQAChain
 | 
ChatOpenAI


[1m> Entering new GraphCypherQAChain chain...[0m
Generated Cypher:
[32;1m[1;3mcypher
MATCH (p:Person)-[:ACTED_IN]->(m:Movie {title: "Casino"})
RETURN p.name
[0m
Full Context:
[32;1m[1;3m[{'p.name': 'Robert De Niro'}, {'p.name': 'Joe Pesci'}, {'p.name': 'Sharon Stone'}, {'p.name': 'James Woods'}][0m
[1m> Finished chain.[0m


{'query': 'What was the cast of the Casino?',
 'result': 'Robert De Niro, Joe Pesci, Sharon Stone, and James Woods were the cast of Casino.'}


Advanced implementation with LangGraph
​


While the GraphCypherQAChain is effective for quick demonstrations, it may face challenges in production environments. Transitioning to LangGraph can enhance the workflow, but implementing natural language to query flows in production remains a complex task. Nevertheless, there are several strategies to significantly improve accuracy and reliability, which we will explore next.


Here is the visualized LangGraph flow we will implement:




We will begin by defining the Input, Output, and Overall state of the LangGraph application.


from
 operator 
import
 add
from
 typing 
import
 Annotated
,
 List
from
 typing_extensions 
import
 TypedDict
class
 
InputState
(
TypedDict
)
:
    question
:
 
str
class
 
OverallState
(
TypedDict
)
:
    question
:
 
str
    next_action
:
 
str
    cypher_statement
:
 
str
    cypher_errors
:
 List
[
str
]
    database_records
:
 List
[
dict
]
    steps
:
 Annotated
[
List
[
str
]
,
 add
]
class
 
OutputState
(
TypedDict
)
:
    answer
:
 
str
    steps
:
 List
[
str
]
    cypher_statement
:
 
str


The first step is a simple 
guardrails
 step, where we validate whether the question pertains to movies or their cast. If it doesn't, we notify the user that we cannot answer any other questions. Otherwise, we move on to the Cypher generation step.


from
 typing 
import
 Literal
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 pydantic 
import
 BaseModel
,
 Field
guardrails_system 
=
 
"""
As an intelligent assistant, your primary objective is to decide whether a given question is related to movies or not. 
If the question is related to movies, output "movie". Otherwise, output "end".
To make this decision, assess the content of the question and determine if it refers to any movie, actor, director, film industry, 
or related topics. Provide only the specified output: "movie" or "end".
"""
guardrails_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            guardrails_system
,
        
)
,
        
(
            
"human"
,
            
(
"{question}"
)
,
        
)
,
    
]
)
class
 
GuardrailsOutput
(
BaseModel
)
:
    decision
:
 Literal
[
"movie"
,
 
"end"
]
 
=
 Field
(
        description
=
"Decision on whether the question is related to movies"
    
)
guardrails_chain 
=
 guardrails_prompt 
|
 llm
.
with_structured_output
(
GuardrailsOutput
)
def
 
guardrails
(
state
:
 InputState
)
 
-
>
 OverallState
:
    
"""
    Decides if the question is related to movies or not.
    """
    guardrails_output 
=
 guardrails_chain
.
invoke
(
{
"question"
:
 state
.
get
(
"question"
)
}
)
    database_records 
=
 
None
    
if
 guardrails_output
.
decision 
==
 
"end"
:
        database_records 
=
 
"This questions is not about movies or their cast. Therefore I cannot answer this question."
    
return
 
{
        
"next_action"
:
 guardrails_output
.
decision
,
        
"database_records"
:
 database_records
,
        
"steps"
:
 
[
"guardrail"
]
,
    
}
API Reference:
ChatPromptTemplate


Few-shot prompting
​


Converting natural language into accurate queries is challenging. One way to enhance this process is by providing relevant few-shot examples to guide the LLM in query generation. To achieve this, we will use the 
SemanticSimilarityExampleSelector
 to dynamically select the most relevant examples.


from
 langchain_core
.
example_selectors 
import
 SemanticSimilarityExampleSelector
from
 langchain_neo4j 
import
 Neo4jVector
from
 langchain_openai 
import
 OpenAIEmbeddings
examples 
=
 
[
    
{
        
"question"
:
 
"How many artists are there?"
,
        
"query"
:
 
"MATCH (a:Person)-[:ACTED_IN]->(:Movie) RETURN count(DISTINCT a)"
,
    
}
,
    
{
        
"question"
:
 
"Which actors played in the movie Casino?"
,
        
"query"
:
 
"MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a) RETURN a.name"
,
    
}
,
    
{
        
"question"
:
 
"How many movies has Tom Hanks acted in?"
,
        
"query"
:
 
"MATCH (a:Person {name: 'Tom Hanks'})-[:ACTED_IN]->(m:Movie) RETURN count(m)"
,
    
}
,
    
{
        
"question"
:
 
"List all the genres of the movie Schindler's List"
,
        
"query"
:
 
"MATCH (m:Movie {title: 'Schindler's List'})-[:IN_GENRE]->(g:Genre) RETURN g.name"
,
    
}
,
    
{
        
"question"
:
 
"Which actors have worked in movies from both the comedy and action genres?"
,
        
"query"
:
 
"MATCH (a:Person)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g1:Genre), (a)-[:ACTED_IN]->(:Movie)-[:IN_GENRE]->(g2:Genre) WHERE g1.name = 'Comedy' AND g2.name = 'Action' RETURN DISTINCT a.name"
,
    
}
,
    
{
        
"question"
:
 
"Which directors have made movies with at least three different actors named 'John'?"
,
        
"query"
:
 
"MATCH (d:Person)-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person) WHERE a.name STARTS WITH 'John' WITH d, COUNT(DISTINCT a) AS JohnsCount WHERE JohnsCount >= 3 RETURN d.name"
,
    
}
,
    
{
        
"question"
:
 
"Identify movies where directors also played a role in the film."
,
        
"query"
:
 
"MATCH (p:Person)-[:DIRECTED]->(m:Movie), (p)-[:ACTED_IN]->(m) RETURN m.title, p.name"
,
    
}
,
    
{
        
"question"
:
 
"Find the actor with the highest number of movies in the database."
,
        
"query"
:
 
"MATCH (a:Actor)-[:ACTED_IN]->(m:Movie) RETURN a.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1"
,
    
}
,
]
example_selector 
=
 SemanticSimilarityExampleSelector
.
from_examples
(
    examples
,
 OpenAIEmbeddings
(
)
,
 Neo4jVector
,
 k
=
5
,
 input_keys
=
[
"question"
]
)
API Reference:
SemanticSimilarityExampleSelector
 | 
Neo4jVector
 | 
OpenAIEmbeddings


Next, we implement the Cypher generation chain, also known as 
text2cypher
. The prompt includes an enhanced graph schema, dynamically selected few-shot examples, and the user’s question. This combination enables the generation of a Cypher query to retrieve relevant information from the database.


from
 langchain_core
.
output_parsers 
import
 StrOutputParser
text2cypher_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
(
                
"Given an input question, convert it to a Cypher query. No pre-amble."
                
"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!"
            
)
,
        
)
,
        
(
            
"human"
,
            
(
                
"""You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.
Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!
Here is the schema information
{schema}
Below are a number of examples of questions and their corresponding Cypher queries.
{fewshot_examples}
User input: {question}
Cypher query:"""
            
)
,
        
)
,
    
]
)
text2cypher_chain 
=
 text2cypher_prompt 
|
 llm 
|
 StrOutputParser
(
)
def
 
generate_cypher
(
state
:
 OverallState
)
 
-
>
 OverallState
:
    
"""
    Generates a cypher statement based on the provided schema and user input
    """
    NL 
=
 
"\n"
    fewshot_examples 
=
 
(
NL 
*
 
2
)
.
join
(
        
[
            
f"Question: 
{
el
[
'question'
]
}
{
NL
}
Cypher:
{
el
[
'query'
]
}
"
            
for
 el 
in
 example_selector
.
select_examples
(
                
{
"question"
:
 state
.
get
(
"question"
)
}
            
)
        
]
    
)
    generated_cypher 
=
 text2cypher_chain
.
invoke
(
        
{
            
"question"
:
 state
.
get
(
"question"
)
,
            
"fewshot_examples"
:
 fewshot_examples
,
            
"schema"
:
 enhanced_graph
.
schema
,
        
}
    
)
    
return
 
{
"cypher_statement"
:
 generated_cypher
,
 
"steps"
:
 
[
"generate_cypher"
]
}
API Reference:
StrOutputParser


Query validation
​


The next step is to validate the generated Cypher statement and ensuring that all property values are accurate. While numbers and dates typically don’t require validation, strings such as movie titles or people’s names do. In this example, we’ll use a basic 
CONTAINS
 clause for validation, though more advanced mapping and validation techniques can be implemented if needed.


First, we will create a chain that detects any errors in the Cypher statement and extracts the property values it references.


from
 typing 
import
 List
,
 Optional
validate_cypher_system 
=
 
"""
You are a Cypher expert reviewing a statement written by a junior developer.
"""
validate_cypher_user 
=
 
"""You must check the following:
* Are there any syntax errors in the Cypher statement?
* Are there any missing or undefined variables in the Cypher statement?
* Are any node labels missing from the schema?
* Are any relationship types missing from the schema?
* Are any of the properties not included in the schema?
* Does the Cypher statement include enough information to answer the question?
Examples of good errors:
* Label (:Foo) does not exist, did you mean (:Bar)?
* Property bar does not exist for label Foo, did you mean baz?
* Relationship FOO does not exist, did you mean FOO_BAR?
Schema:
{schema}
The question is:
{question}
The Cypher statement is:
{cypher}
Make sure you don't make any mistakes!"""
validate_cypher_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            validate_cypher_system
,
        
)
,
        
(
            
"human"
,
            
(
validate_cypher_user
)
,
        
)
,
    
]
)
class
 
Property
(
BaseModel
)
:
    
"""
    Represents a filter condition based on a specific node property in a graph in a Cypher statement.
    """
    node_label
:
 
str
 
=
 Field
(
        description
=
"The label of the node to which this property belongs."
    
)
    property_key
:
 
str
 
=
 Field
(
description
=
"The key of the property being filtered."
)
    property_value
:
 
str
 
=
 Field
(
        description
=
"The value that the property is being matched against."
    
)
class
 
ValidateCypherOutput
(
BaseModel
)
:
    
"""
    Represents the validation result of a Cypher query's output,
    including any errors and applied filters.
    """
    errors
:
 Optional
[
List
[
str
]
]
 
=
 Field
(
        description
=
"A list of syntax or semantical errors in the Cypher statement. Always explain the discrepancy between schema and Cypher statement"
    
)
    filters
:
 Optional
[
List
[
Property
]
]
 
=
 Field
(
        description
=
"A list of property-based filters applied in the Cypher statement."
    
)
validate_cypher_chain 
=
 validate_cypher_prompt 
|
 llm
.
with_structured_output
(
    ValidateCypherOutput
)


LLMs often struggle with correctly determining relationship directions in generated Cypher statements. Since we have access to the schema, we can deterministically correct these directions using the 
CypherQueryCorrector
.


Note: The 
CypherQueryCorrector
 is an experimental feature and doesn't support all the newest Cypher syntax.


from
 langchain_neo4j
.
chains
.
graph_qa
.
cypher_utils 
import
 CypherQueryCorrector
,
 Schema
# Cypher query corrector is experimental
corrector_schema 
=
 
[
    Schema
(
el
[
"start"
]
,
 el
[
"type"
]
,
 el
[
"end"
]
)
    
for
 el 
in
 enhanced_graph
.
structured_schema
.
get
(
"relationships"
)
]
cypher_query_corrector 
=
 CypherQueryCorrector
(
corrector_schema
)
API Reference:
CypherQueryCorrector
 | 
Schema


Now we can implement the Cypher validation step. First, we use the 
EXPLAIN
 method to detect any syntax errors. Next, we leverage the LLM to identify potential issues and extract the properties used for filtering. For string properties, we validate them against the database using a simple 
CONTAINS
 clause.


Based on the validation results, the process can take the following paths:




If value mapping fails, we end the conversation and inform the user that we couldn't identify a specific property value (e.g., a person or movie title).


If errors are found, we route the query for correction.


If no issues are detected, we proceed to the Cypher execution step.




from
 neo4j
.
exceptions 
import
 CypherSyntaxError
def
 
validate_cypher
(
state
:
 OverallState
)
 
-
>
 OverallState
:
    
"""
    Validates the Cypher statements and maps any property values to the database.
    """
    errors 
=
 
[
]
    mapping_errors 
=
 
[
]
    
# Check for syntax errors
    
try
:
        enhanced_graph
.
query
(
f"EXPLAIN 
{
state
.
get
(
'cypher_statement'
)
}
"
)
    
except
 CypherSyntaxError 
as
 e
:
        errors
.
append
(
e
.
message
)
    
# Experimental feature for correcting relationship directions
    corrected_cypher 
=
 cypher_query_corrector
(
state
.
get
(
"cypher_statement"
)
)
    
if
 
not
 corrected_cypher
:
        errors
.
append
(
"The generated Cypher statement doesn't fit the graph schema"
)
    
if
 
not
 corrected_cypher 
==
 state
.
get
(
"cypher_statement"
)
:
        
print
(
"Relationship direction was corrected"
)
    
# Use LLM to find additional potential errors and get the mapping for values
    llm_output 
=
 validate_cypher_chain
.
invoke
(
        
{
            
"question"
:
 state
.
get
(
"question"
)
,
            
"schema"
:
 enhanced_graph
.
schema
,
            
"cypher"
:
 state
.
get
(
"cypher_statement"
)
,
        
}
    
)
    
if
 llm_output
.
errors
:
        errors
.
extend
(
llm_output
.
errors
)
    
if
 llm_output
.
filters
:
        
for
 
filter
 
in
 llm_output
.
filters
:
            
# Do mapping only for string values
            
if
 
(
                
not
 
[
                    prop
                    
for
 prop 
in
 enhanced_graph
.
structured_schema
[
"node_props"
]
[
                        
filter
.
node_label
                    
]
                    
if
 prop
[
"property"
]
 
==
 
filter
.
property_key
                
]
[
0
]
[
"type"
]
                
==
 
"STRING"
            
)
:
                
continue
            mapping 
=
 enhanced_graph
.
query
(
                
f"MATCH (n:
{
filter
.
node_label
}
) WHERE toLower(n.`
{
filter
.
property_key
}
`) = toLower($value) RETURN 'yes' LIMIT 1"
,
                
{
"value"
:
 
filter
.
property_value
}
,
            
)
            
if
 
not
 mapping
:
                
print
(
                    
f"Missing value mapping for 
{
filter
.
node_label
}
 on property 
{
filter
.
property_key
}
 with value 
{
filter
.
property_value
}
"
                
)
                mapping_errors
.
append
(
                    
f"Missing value mapping for 
{
filter
.
node_label
}
 on property 
{
filter
.
property_key
}
 with value 
{
filter
.
property_value
}
"
                
)
    
if
 mapping_errors
:
        next_action 
=
 
"end"
    
elif
 errors
:
        next_action 
=
 
"correct_cypher"
    
else
:
        next_action 
=
 
"execute_cypher"
    
return
 
{
        
"next_action"
:
 next_action
,
        
"cypher_statement"
:
 corrected_cypher
,
        
"cypher_errors"
:
 errors
,
        
"steps"
:
 
[
"validate_cypher"
]
,
    
}


The Cypher correction step takes the existing Cypher statement, any identified errors, and the original question to generate a corrected version of the query.


correct_cypher_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
(
                
"You are a Cypher expert reviewing a statement written by a junior developer. "
                
"You need to correct the Cypher statement based on the provided errors. No pre-amble."
                
"Do not wrap the response in any backticks or anything else. Respond with a Cypher statement only!"
            
)
,
        
)
,
        
(
            
"human"
,
            
(
                
"""Check for invalid syntax or semantics and return a corrected Cypher statement.
Schema:
{schema}
Note: Do not include any explanations or apologies in your responses.
Do not wrap the response in any backticks or anything else.
Respond with a Cypher statement only!
Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.
The question is:
{question}
The Cypher statement is:
{cypher}
The errors are:
{errors}
Corrected Cypher statement: """
            
)
,
        
)
,
    
]
)
correct_cypher_chain 
=
 correct_cypher_prompt 
|
 llm 
|
 StrOutputParser
(
)
def
 
correct_cypher
(
state
:
 OverallState
)
 
-
>
 OverallState
:
    
"""
    Correct the Cypher statement based on the provided errors.
    """
    corrected_cypher 
=
 correct_cypher_chain
.
invoke
(
        
{
            
"question"
:
 state
.
get
(
"question"
)
,
            
"errors"
:
 state
.
get
(
"cypher_errors"
)
,
            
"cypher"
:
 state
.
get
(
"cypher_statement"
)
,
            
"schema"
:
 enhanced_graph
.
schema
,
        
}
    
)
    
return
 
{
        
"next_action"
:
 
"validate_cypher"
,
        
"cypher_statement"
:
 corrected_cypher
,
        
"steps"
:
 
[
"correct_cypher"
]
,
    
}


We need to add a step that executes the given Cypher statement. If no results are returned, we should explicitly handle this scenario, as leaving the context empty can sometimes lead to LLM hallucinations.


no_results 
=
 
"I couldn't find any relevant information in the database"
def
 
execute_cypher
(
state
:
 OverallState
)
 
-
>
 OverallState
:
    
"""
    Executes the given Cypher statement.
    """
    records 
=
 enhanced_graph
.
query
(
state
.
get
(
"cypher_statement"
)
)
    
return
 
{
        
"database_records"
:
 records 
if
 records 
else
 no_results
,
        
"next_action"
:
 
"end"
,
        
"steps"
:
 
[
"execute_cypher"
]
,
    
}


The final step is to generate the answer. This involves combining the initial question with the database output to produce a relevant response.


generate_final_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
            
"system"
,
            
"You are a helpful assistant"
,
        
)
,
        
(
            
"human"
,
            
(
                
"""Use the following results retrieved from a database to provide
a succinct, definitive answer to the user's question.
Respond as if you are answering the question directly.
Results: {results}
Question: {question}"""
            
)
,
        
)
,
    
]
)
generate_final_chain 
=
 generate_final_prompt 
|
 llm 
|
 StrOutputParser
(
)
def
 
generate_final_answer
(
state
:
 OverallState
)
 
-
>
 OutputState
:
    
"""
    Decides if the question is related to movies.
    """
    final_answer 
=
 generate_final_chain
.
invoke
(
        
{
"question"
:
 state
.
get
(
"question"
)
,
 
"results"
:
 state
.
get
(
"database_records"
)
}
    
)
    
return
 
{
"answer"
:
 final_answer
,
 
"steps"
:
 
[
"generate_final_answer"
]
}


Next, we will implement the LangGraph workflow, starting with defining the conditional edge functions.


def
 
guardrails_condition
(
    state
:
 OverallState
,
)
 
-
>
 Literal
[
"generate_cypher"
,
 
"generate_final_answer"
]
:
    
if
 state
.
get
(
"next_action"
)
 
==
 
"end"
:
        
return
 
"generate_final_answer"
    
elif
 state
.
get
(
"next_action"
)
 
==
 
"movie"
:
        
return
 
"generate_cypher"
def
 
validate_cypher_condition
(
    state
:
 OverallState
,
)
 
-
>
 Literal
[
"generate_final_answer"
,
 
"correct_cypher"
,
 
"execute_cypher"
]
:
    
if
 state
.
get
(
"next_action"
)
 
==
 
"end"
:
        
return
 
"generate_final_answer"
    
elif
 state
.
get
(
"next_action"
)
 
==
 
"correct_cypher"
:
        
return
 
"correct_cypher"
    
elif
 state
.
get
(
"next_action"
)
 
==
 
"execute_cypher"
:
        
return
 
"execute_cypher"


Let's put it all together now.


from
 IPython
.
display 
import
 Image
,
 display
from
 langgraph
.
graph 
import
 END
,
 START
,
 StateGraph
langgraph 
=
 StateGraph
(
OverallState
,
 
input
=
InputState
,
 output
=
OutputState
)
langgraph
.
add_node
(
guardrails
)
langgraph
.
add_node
(
generate_cypher
)
langgraph
.
add_node
(
validate_cypher
)
langgraph
.
add_node
(
correct_cypher
)
langgraph
.
add_node
(
execute_cypher
)
langgraph
.
add_node
(
generate_final_answer
)
langgraph
.
add_edge
(
START
,
 
"guardrails"
)
langgraph
.
add_conditional_edges
(
    
"guardrails"
,
    guardrails_condition
,
)
langgraph
.
add_edge
(
"generate_cypher"
,
 
"validate_cypher"
)
langgraph
.
add_conditional_edges
(
    
"validate_cypher"
,
    validate_cypher_condition
,
)
langgraph
.
add_edge
(
"execute_cypher"
,
 
"generate_final_answer"
)
langgraph
.
add_edge
(
"correct_cypher"
,
 
"validate_cypher"
)
langgraph
.
add_edge
(
"generate_final_answer"
,
 END
)
langgraph 
=
 langgraph
.
compile
(
)
# View
display
(
Image
(
langgraph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)
API Reference:
StateGraph




We can now test the application by asking an irrelevant question.


langgraph
.
invoke
(
{
"question"
:
 
"What's the weather in Spain?"
}
)


{'answer': "I'm sorry, but I cannot provide current weather information. Please check a reliable weather website or app for the latest updates on the weather in Spain.",
 'steps': ['guardrail', 'generate_final_answer']}


Let's now ask something relevant about the movies.


langgraph
.
invoke
(
{
"question"
:
 
"What was the cast of the Casino?"
}
)


{'answer': 'The cast of "Casino" includes Robert De Niro, Joe Pesci, Sharon Stone, and James Woods.',
 'steps': ['guardrail',
  'generate_cypher',
  'validate_cypher',
  'execute_cypher',
  'generate_final_answer'],
 'cypher_statement': "MATCH (m:Movie {title: 'Casino'})<-[:ACTED_IN]-(a:Person) RETURN a.name"}


Next steps
​


For other graph techniques like this and more check out:




Semantic layer
: Techniques for implementing semantic layers.


Constructing graphs
: Techniques for constructing knowledge graphs.


Edit this page
Was this page helpful?
Previous
Tutorials
Next
Tutorials
⚠️ Security note ⚠️
Architecture
Setup
Graph schema
GraphQACypherChain
Advanced implementation with LangGraph
Few-shot prompting
Query validation
Next steps


--- docs_tutorials_llm_chain.txt ---

Tutorials
Build a simple LLM application with chat models and prompt templates
On this page
Build a simple LLM application with chat models and prompt templates


In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!


After reading this tutorial, you'll have a high level overview of:






Using 
language models






Using 
prompt templates






Debugging and tracing your application using 
LangSmith






Let's dive in!


Setup
​


Jupyter Notebook
​


This and other tutorials are perhaps most conveniently run in a 
Jupyter notebooks
. Going through guides in an interactive environment is a great way to better understand them. See 
here
 for instructions on how to install.


Installation
​


To install LangChain run:




Pip
Conda
pip install langchain
conda install langchain -c conda-forge


For more details, see our 
Installation guide
.


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with 
LangSmith
.


After you sign up at the link above, make sure to set your environment variables to start logging traces:


export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
export LANGSMITH_PROJECT="default" # or any other project name


Or, if in a notebook, you can set them with:


import
 getpass
import
 os
try
:
    
# load environment variables from .env file (requires `python-dotenv`)
    
from
 dotenv 
import
 load_dotenv
    load_dotenv
(
)
except
 ImportError
:
    
pass
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
if
 
"LANGSMITH_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
        prompt
=
"Enter your LangSmith API key (optional): "
    
)
if
 
"LANGSMITH_PROJECT"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"LANGSMITH_PROJECT"
]
 
=
 getpass
.
getpass
(
        prompt
=
'Enter your LangSmith Project Name (default = "default"): '
    
)
    
if
 
not
 os
.
environ
.
get
(
"LANGSMITH_PROJECT"
)
:
        os
.
environ
[
"LANGSMITH_PROJECT"
]
 
=
 
"default"
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
        prompt
=
"Enter your OpenAI API key (required if using OpenAI): "
    
)


Using Language Models
​


First up, let's learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to 
supported integrations
.








Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Let's first use the model directly. 
ChatModels
 are instances of LangChain 
Runnables
, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of 
messages
 to the 
.invoke
 method.


from
 langchain_core
.
messages 
import
 HumanMessage
,
 SystemMessage
messages 
=
 
[
    SystemMessage
(
"Translate the following from English into Italian"
)
,
    HumanMessage
(
"hi!"
)
,
]
model
.
invoke
(
messages
)
API Reference:
HumanMessage
 | 
SystemMessage


AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})


tip
If we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the 
LangSmith trace
. The LangSmith trace reports 
token
 usage information, latency, 
standard model parameters
 (such as temperature), and other information.


Note that ChatModels receive 
message
 objects as input and generate message objects as output. In addition to text content, message objects convey conversational 
roles
 and hold important data, such as 
tool calls
 and token usage counts.


LangChain also supports chat model inputs via strings or 
OpenAI format
. The following are equivalent:


model
.
invoke
(
"Hello"
)
model
.
invoke
(
[
{
"role"
:
 
"user"
,
 
"content"
:
 
"Hello"
}
]
)
model
.
invoke
(
[
HumanMessage
(
"Hello"
)
]
)


Streaming
​


Because chat models are 
Runnables
, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:


for
 token 
in
 model
.
stream
(
messages
)
:
    
print
(
token
.
content
,
 end
=
"|"
)


|C|iao|!||


You can find more details on streaming chat model outputs in 
this guide
.


Prompt Templates
​


Right now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.


Prompt templates
 are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.


Let's create a prompt template here. It will take in two user variables:




language
: The language to translate text into


text
: The text to translate




from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
system_template 
=
 
"Translate the following from English into {language}"
prompt_template 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 system_template
)
,
 
(
"user"
,
 
"{text}"
)
]
)
API Reference:
ChatPromptTemplate


Note that 
ChatPromptTemplate
 supports multiple 
message roles
 in a single template. We format the 
language
 parameter into the system message, and the user 
text
 into a user message.


The input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself


prompt 
=
 prompt_template
.
invoke
(
{
"language"
:
 
"Italian"
,
 
"text"
:
 
"hi!"
}
)
prompt


ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])


We can see that it returns a 
ChatPromptValue
 that consists of two messages. If we want to access the messages directly we do:


prompt
.
to_messages
(
)


[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]


Finally, we can invoke the chat model on the formatted prompt:


response 
=
 model
.
invoke
(
prompt
)
print
(
response
.
content
)


Ciao!


tip
Message 
content
 can contain both text and 
content blocks
 with additional structure. See 
this guide
 for more information.


If we take a look at the 
LangSmith trace
, we can see exactly what prompt the chat model receives, along with 
token
 usage information, latency, 
standard model parameters
 (such as temperature), and other information.


Conclusion
​


That's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.


This just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!


For further reading on the core concepts of LangChain, we've got detailed 
Conceptual Guides
.


If you have more specific questions on these concepts, check out the following sections of the how-to guides:




Chat models


Prompt templates




And the LangSmith docs:




LangSmith


Edit this page
Was this page helpful?
Previous
Tutorials
Next
Build a Chatbot
Setup
Jupyter Notebook
Installation
LangSmith
Using Language Models
Streaming
Prompt Templates
Conclusion


--- docs_tutorials_qa_chat_history.txt ---

Tutorials
Build a Retrieval Augmented Generation (RAG) App: Part 2
On this page
Build a Retrieval Augmented Generation (RAG) App: Part 2


In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of "memory" of past questions and answers, and some logic for incorporating those into its current thinking.


This is the second part of a multi-part tutorial:




Part 1
 introduces RAG and walks through a minimal implementation.


Part 2
 (this guide) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.




Here we focus on 
adding logic for incorporating historical messages.
 This involves the management of a 
chat history
.


We will cover two approaches:




Chains
, in which we execute at most one retrieval step;


Agents
, in which we give an LLM discretion to execute multiple retrieval steps.




note
The methods presented here leverage 
tool-calling
 capabilities in modern 
chat models
. See 
this page
 for a table of models supporting tool calling features.


For the external knowledge source, we will use the same 
LLM Powered Autonomous Agents
 blog post by Lilian Weng from the 
Part 1
 of the RAG tutorial.


Setup
​


Components
​


We will need to select three components from LangChain's suite of integrations.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)




Select 
embeddings model
:
OpenAI
▾
OpenAI
Azure
Google
AWS
HuggingFace
Ollama
Cohere
MistralAI
Nomic
NVIDIA
Voyage AI
IBM watsonx
Fake
pip install -qU langchain-openai
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-large"
)




Select 
vector store
:
In-memory
▾
In-memory
AstraDB
Chroma
FAISS
Milvus
MongoDB
PGVector
Pinecone
Qdrant
pip install -qU langchain-core
from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
vector_store 
=
 InMemoryVectorStore
(
embeddings
)


Dependencies
​


In addition, we'll use the following packages:


%
%
capture 
-
-
no
-
stderr
%
pip install 
-
-
upgrade 
-
-
quiet langgraph langchain
-
community beautifulsoup4


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with 
LangSmith
.


Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:


os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
if
 
not
 os
.
environ
.
get
(
"LANGSMITH_API_KEY"
)
:
    os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Chains
​


Let's first revisit the vector store we built in 
Part 1
, which indexes an 
LLM Powered Autonomous Agents
 blog post by Lilian Weng.


import
 bs4
from
 langchain 
import
 hub
from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
from
 langchain_core
.
documents 
import
 Document
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
from
 typing_extensions 
import
 List
,
 TypedDict
# Load and chunk contents of the blog
loader 
=
 WebBaseLoader
(
    web_paths
=
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
,
)
,
    bs_kwargs
=
dict
(
        parse_only
=
bs4
.
SoupStrainer
(
            class_
=
(
"post-content"
,
 
"post-title"
,
 
"post-header"
)
        
)
    
)
,
)
docs 
=
 loader
.
load
(
)
text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
1000
,
 chunk_overlap
=
200
)
all_splits 
=
 text_splitter
.
split_documents
(
docs
)
API Reference:
hub
 | 
WebBaseLoader
 | 
Document
 | 
RecursiveCharacterTextSplitter


# Index chunks
_ 
=
 vector_store
.
add_documents
(
documents
=
all_splits
)


In the 
Part 1
 of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of 
messages
. In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via 
tool messages
. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have




User input as a 
HumanMessage
;


Vector store query as an 
AIMessage
 with tool calls;


Retrieved documents as a 
ToolMessage
;


Final response as a 
AIMessage
.




This model for state is so versatile that LangGraph offers a built-in version for convenience:


from
 langgraph
.
graph 
import
 MessagesState
,
 StateGraph
graph_builder 
=
 StateGraph
(
MessagesState
)
API Reference:
StateGraph


Leveraging 
tool-calling
 to interact with a retrieval step has another benefit, which is that the query for the retrieval is generated by our model. This is especially important in a conversational setting, where user queries may require contextualization based on the chat history. For instance, consider the following exchange:




Human: "What is Task Decomposition?"


AI: "Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable for an agent or model."


Human: "What are common ways of doing it?"




In this scenario, a model could generate a query such as 
"common approaches to task decomposition"
. Tool-calling facilitates this naturally. As in the 
query analysis
 section of the RAG tutorial, this allows a model to rewrite user queries into more effective search queries. It also provides support for direct responses that do not involve a retrieval step (e.g., in response to a generic greeting from the user).


Let's turn our retrieval step into a 
tool
:


from
 langchain_core
.
tools 
import
 tool
@tool
(
response_format
=
"content_and_artifact"
)
def
 
retrieve
(
query
:
 
str
)
:
    
"""Retrieve information related to a query."""
    retrieved_docs 
=
 vector_store
.
similarity_search
(
query
,
 k
=
2
)
    serialized 
=
 
"\n\n"
.
join
(
        
(
f"Source: 
{
doc
.
metadata
}
\n"
 
f"Content: 
{
doc
.
page_content
}
"
)
        
for
 doc 
in
 retrieved_docs
    
)
    
return
 serialized
,
 retrieved_docs
API Reference:
tool


See 
this guide
 for more detail on creating tools.


Our graph will consist of three nodes:




A node that fields the user input, either generating a query for the retriever or responding directly;


A node for the retriever tool that executes the retrieval step;


A node that generates the final response using the retrieved context.




We build them below. Note that we leverage another pre-built LangGraph component, 
ToolNode
, that executes the tool and adds the result as a 
ToolMessage
 to the state.


from
 langchain_core
.
messages 
import
 SystemMessage
from
 langgraph
.
prebuilt 
import
 ToolNode
# Step 1: Generate an AIMessage that may include a tool-call to be sent.
def
 
query_or_respond
(
state
:
 MessagesState
)
:
    
"""Generate tool call for retrieval or respond."""
    llm_with_tools 
=
 llm
.
bind_tools
(
[
retrieve
]
)
    response 
=
 llm_with_tools
.
invoke
(
state
[
"messages"
]
)
    
# MessagesState appends messages to state instead of overwriting
    
return
 
{
"messages"
:
 
[
response
]
}
# Step 2: Execute the retrieval.
tools 
=
 ToolNode
(
[
retrieve
]
)
# Step 3: Generate a response using the retrieved content.
def
 
generate
(
state
:
 MessagesState
)
:
    
"""Generate answer."""
    
# Get generated ToolMessages
    recent_tool_messages 
=
 
[
]
    
for
 message 
in
 
reversed
(
state
[
"messages"
]
)
:
        
if
 message
.
type
 
==
 
"tool"
:
            recent_tool_messages
.
append
(
message
)
        
else
:
            
break
    tool_messages 
=
 recent_tool_messages
[
:
:
-
1
]
    
# Format into prompt
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
content 
for
 doc 
in
 tool_messages
)
    system_message_content 
=
 
(
        
"You are an assistant for question-answering tasks. "
        
"Use the following pieces of retrieved context to answer "
        
"the question. If you don't know the answer, say that you "
        
"don't know. Use three sentences maximum and keep the "
        
"answer concise."
        
"\n\n"
        
f"
{
docs_content
}
"
    
)
    conversation_messages 
=
 
[
        message
        
for
 message 
in
 state
[
"messages"
]
        
if
 message
.
type
 
in
 
(
"human"
,
 
"system"
)
        
or
 
(
message
.
type
 
==
 
"ai"
 
and
 
not
 message
.
tool_calls
)
    
]
    prompt 
=
 
[
SystemMessage
(
system_message_content
)
]
 
+
 conversation_messages
    
# Run
    response 
=
 llm
.
invoke
(
prompt
)
    
return
 
{
"messages"
:
 
[
response
]
}
API Reference:
SystemMessage
 | 
ToolNode


Finally, we compile our application into a single 
graph
 object. In this case, we are just connecting the steps into a sequence. We also allow the first 
query_or_respond
 step to "short-circuit" and respond directly to the user if it does not generate a tool call. This allows our application to support conversational experiences-- e.g., responding to generic greetings that may not require a retrieval step


from
 langgraph
.
graph 
import
 END
from
 langgraph
.
prebuilt 
import
 ToolNode
,
 tools_condition
graph_builder
.
add_node
(
query_or_respond
)
graph_builder
.
add_node
(
tools
)
graph_builder
.
add_node
(
generate
)
graph_builder
.
set_entry_point
(
"query_or_respond"
)
graph_builder
.
add_conditional_edges
(
    
"query_or_respond"
,
    tools_condition
,
    
{
END
:
 END
,
 
"tools"
:
 
"tools"
}
,
)
graph_builder
.
add_edge
(
"tools"
,
 
"generate"
)
graph_builder
.
add_edge
(
"generate"
,
 END
)
graph 
=
 graph_builder
.
compile
(
)
API Reference:
ToolNode
 | 
tools_condition


from
 IPython
.
display 
import
 Image
,
 display
display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




Let's test our application.


Note that it responds appropriately to messages that do not require an additional retrieval step:


input_message 
=
 
"Hello"
for
 step 
in
 graph
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 input_message
}
]
}
,
    stream_mode
=
"values"
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
Hello
==================================[1m Ai Message [0m==================================
Hello! How can I assist you today?


And when executing a search, we can stream the steps to observe the query generation, retrieval, and answer generation:


input_message 
=
 
"What is Task Decomposition?"
for
 step 
in
 graph
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 input_message
}
]
}
,
    stream_mode
=
"values"
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
What is Task Decomposition?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_dLjB3rkMoxZZxwUGXi33UBeh)
 Call ID: call_dLjB3rkMoxZZxwUGXi33UBeh
  Args:
    query: Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
==================================[1m Ai Message [0m==================================
Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps. It often involves techniques like Chain of Thought (CoT), which encourages models to think step by step, enhancing performance on complex tasks. This approach allows for a clearer understanding of the task and aids in structuring the problem-solving process.


Check out the LangSmith trace 
here
.


Stateful management of chat history
​


note
This section of the tutorial previously used the 
RunnableWithMessageHistory
 abstraction. You can access that version of the documentation in the 
v0.2 docs
.
As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of 
LangGraph persistence
 to incorporate 
memory
 into new LangChain applications.
If your code is already relying on 
RunnableWithMessageHistory
 or 
BaseChatMessageHistory
, you do 
not
 need to make any changes. We do not plan on deprecating this functionality in the near future as it works for simple chat applications and any code that uses 
RunnableWithMessageHistory
 will continue to work as expected.
Please see 
How to migrate to LangGraph Memory
 for more details.


In production, the Q&A application will usually persist the chat history into a database, and be able to read and update it appropriately.


LangGraph
 implements a built-in 
persistence layer
, making it ideal for chat applications that support multiple conversational turns.


To manage multiple conversational turns and threads, all we have to do is specify a 
checkpointer
 when compiling our application. Because the nodes in our graph are appending messages to the state, we will retain a consistent chat history across invocations.


LangGraph comes with a simple in-memory checkpointer, which we use below. See its 
documentation
 for more detail, including how to use different persistence backends (e.g., SQLite or Postgres).


For a detailed walkthrough of how to manage message history, head to the 
How to add message history (memory)
 guide.


from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
memory 
=
 MemorySaver
(
)
graph 
=
 graph_builder
.
compile
(
checkpointer
=
memory
)
# Specify an ID for the thread
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"abc123"
}
}
API Reference:
MemorySaver


We can now invoke similar to before:


input_message 
=
 
"What is Task Decomposition?"
for
 step 
in
 graph
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 input_message
}
]
}
,
    stream_mode
=
"values"
,
    config
=
config
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
What is Task Decomposition?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_JZb6GLD812bW2mQsJ5EJQDnN)
 Call ID: call_JZb6GLD812bW2mQsJ5EJQDnN
  Args:
    query: Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
==================================[1m Ai Message [0m==================================
Task Decomposition is a technique used to break down complicated tasks into smaller, manageable steps. It involves using methods like Chain of Thought (CoT) prompting, which encourages the model to think step by step, enhancing performance on complex tasks. This process helps to clarify the model's reasoning and makes it easier to tackle difficult problems.


input_message 
=
 
"Can you look up some common ways of doing it?"
for
 step 
in
 graph
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 input_message
}
]
}
,
    stream_mode
=
"values"
,
    config
=
config
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
Can you look up some common ways of doing it?
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_kjRI4Y5cJOiB73yvd7dmb6ux)
 Call ID: call_kjRI4Y5cJOiB73yvd7dmb6ux
  Args:
    query: common methods of task decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
==================================[1m Ai Message [0m==================================
Common ways of performing Task Decomposition include: (1) using Large Language Models (LLMs) with simple prompts like "Steps for XYZ" or "What are the subgoals for achieving XYZ?", (2) employing task-specific instructions such as "Write a story outline" for specific tasks, and (3) incorporating human inputs to guide the decomposition process.


Note that the query generated by the model in the second question incorporates the conversational context.


The 
LangSmith
 trace is particularly informative here, as we can see exactly what messages are visible to our chat model at each step.


Agents
​


Agents
 leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the retrieval process. Although their behavior is less predictable than the above "chain", they are able to execute multiple retrieval steps in service of a query, or iterate on a single search.


Below we assemble a minimal RAG agent. Using LangGraph's 
pre-built ReAct agent constructor
, we can do this in one line.


tip
Check out LangGraph's 
Agentic RAG
 tutorial for more advanced formulations.


from
 langgraph
.
prebuilt 
import
 create_react_agent
agent_executor 
=
 create_react_agent
(
llm
,
 
[
retrieve
]
,
 checkpointer
=
memory
)
API Reference:
create_react_agent


Let's inspect the graph:


display
(
Image
(
agent_executor
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




The key difference from our earlier implementation is that instead of a final generation step that ends the run, here the tool invocation loops back to the original LLM call. The model can then either answer the question using the retrieved context, or generate another tool call to obtain more information.


Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:


config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"def234"
}
}
input_message 
=
 
(
    
"What is the standard method for Task Decomposition?\n\n"
    
"Once you get the answer, look up common extensions of that method."
)
for
 event 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 input_message
}
]
}
,
    stream_mode
=
"values"
,
    config
=
config
,
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
What is the standard method for Task Decomposition?
Once you get the answer, look up common extensions of that method.
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_Y3YaIzL71B83Cjqa8d2G0O8N)
 Call ID: call_Y3YaIzL71B83Cjqa8d2G0O8N
  Args:
    query: standard method for Task Decomposition
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
==================================[1m Ai Message [0m==================================
Tool Calls:
  retrieve (call_2JntP1x4XQMWwgVpYurE12ff)
 Call ID: call_2JntP1x4XQMWwgVpYurE12ff
  Args:
    query: common extensions of Task Decomposition methods
=================================[1m Tool Message [0m=================================
Name: retrieve
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Fig. 1. Overview of a LLM-powered autonomous agent system.
Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.
==================================[1m Ai Message [0m==================================
The standard method for task decomposition involves using techniques such as Chain of Thought (CoT), where a model is instructed to "think step by step" to break down complex tasks into smaller, more manageable components. This approach enhances model performance by allowing for more thorough reasoning and planning. Task decomposition can be accomplished through various means, including:
1. Simple prompting (e.g., asking for steps to achieve a goal).
2. Task-specific instructions (e.g., asking for a story outline).
3. Human inputs to guide the decomposition process.
### Common Extensions of Task Decomposition Methods:
1. **Tree of Thoughts**: This extension builds on CoT by not only decomposing the problem into thought steps but also generating multiple thoughts at each step, creating a tree structure. The search process can employ breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.
These extensions aim to enhance reasoning capabilities and improve the effectiveness of task decomposition in various contexts.


Note that the agent:




Generates a query to search for a standard method for task decomposition;


Receiving the answer, generates a second query to search for common extensions of it;


Having received all necessary context, answers the question.




We can see the full sequence of steps, along with latency and other metadata, in the 
LangSmith trace
.


Next steps
​


We've covered the steps to build a basic conversational Q&A application:




We used chains to build a predictable application that generates at most one query per user input;


We used agents to build an application that can iterate on a sequence of queries.




To explore different types of retrievers and retrieval strategies, visit the 
retrievers
 section of the how-to guides.


For a detailed walkthrough of LangChain's conversation memory abstractions, visit the 
How to add message history (memory)
 guide.


To learn more about agents, check out the 
conceptual guide
 and LangGraph 
agent architectures
 page.
Edit this page
Was this page helpful?
Previous
Build a Chatbot
Next
Build an Extraction Chain
Setup
Components
Dependencies
LangSmith
Chains
Stateful management of chat history
Agents
Next steps


--- docs_tutorials_rag.txt ---

Tutorials
Build a Retrieval Augmented Generation (RAG) App: Part 1
On this page
Build a Retrieval Augmented Generation (RAG) App: Part 1


One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or 
RAG
.


This is a multi-part tutorial:




Part 1
 (this guide) introduces RAG and walks through a minimal implementation.


Part 2
 extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.




This tutorial will show how to build a simple Q&A application
over a text data source. Along the way we’ll go over a typical Q&A
architecture and highlight additional resources for more advanced Q&A techniques. We’ll also see
how LangSmith can help us trace and understand our application.
LangSmith will become increasingly helpful as our application grows in
complexity.


If you're already familiar with basic retrieval, you might also be interested in
this 
high-level overview of different retrieval techniques
.


Note
: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing 
question/answering over SQL data
.


Overview
​


A typical RAG application has two main components:


Indexing
: a pipeline for ingesting data from a source and indexing it. 
This usually happens offline.


Retrieval and generation
: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.


Note: the indexing portion of this tutorial will largely follow the 
semantic search tutorial
.


The most common full sequence from raw data to answer looks like:


Indexing
​




Load
: First we need to load our data. This is done with 
Document Loaders
.


Split
: 
Text splitters
 break large 
Documents
 into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.


Store
: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a 
VectorStore
 and 
Embeddings
 model.






Retrieval and generation
​




Retrieve
: Given a user input, relevant splits are retrieved from storage using a 
Retriever
.


Generate
: A 
ChatModel
 / 
LLM
 produces an answer using a prompt that includes both the question with the retrieved data






Once we've indexed our data, we will use 
LangGraph
 as our orchestration framework to implement the retrieval and generation steps.


Setup
​


Jupyter Notebook
​


This and other tutorials are perhaps most conveniently run in a 
Jupyter notebooks
. Going through guides in an interactive environment is a great way to better understand them. See 
here
 for instructions on how to install.


Installation
​


This tutorial requires these langchain dependencies:




Pip
Conda
%
pip install 
-
-
quiet 
-
-
upgrade langchain
-
text
-
splitters langchain
-
community langgraph
conda install langchain-text-splitters langchain-community langgraph -c conda-forge


For more details, see our 
Installation guide
.


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with 
LangSmith
.


After you sign up at the link above, make sure to set your environment variables to start logging traces:


export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."


Or, if in a notebook, you can set them with:


import
 getpass
import
 os
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Components
​


We will need to select three components from LangChain's suite of integrations.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)




Select 
embeddings model
:
OpenAI
▾
OpenAI
Azure
Google
AWS
HuggingFace
Ollama
Cohere
MistralAI
Nomic
NVIDIA
Voyage AI
IBM watsonx
Fake
pip install -qU langchain-openai
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-large"
)




Select 
vector store
:
In-memory
▾
In-memory
AstraDB
Chroma
FAISS
Milvus
MongoDB
PGVector
Pinecone
Qdrant
pip install -qU langchain-core
from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
vector_store 
=
 InMemoryVectorStore
(
embeddings
)


Preview
​


In this guide we’ll build an app that answers questions about the website's content. The specific website we will use is the 
LLM Powered Autonomous
Agents
 blog post
by Lilian Weng, which allows us to ask questions about the contents of
the post.


We can create a simple indexing pipeline and RAG chain to do this in ~50
lines of code.


import
 bs4
from
 langchain 
import
 hub
from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
from
 langchain_core
.
documents 
import
 Document
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
from
 langgraph
.
graph 
import
 START
,
 StateGraph
from
 typing_extensions 
import
 List
,
 TypedDict
# Load and chunk contents of the blog
loader 
=
 WebBaseLoader
(
    web_paths
=
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
,
)
,
    bs_kwargs
=
dict
(
        parse_only
=
bs4
.
SoupStrainer
(
            class_
=
(
"post-content"
,
 
"post-title"
,
 
"post-header"
)
        
)
    
)
,
)
docs 
=
 loader
.
load
(
)
text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
1000
,
 chunk_overlap
=
200
)
all_splits 
=
 text_splitter
.
split_documents
(
docs
)
# Index chunks
_ 
=
 vector_store
.
add_documents
(
documents
=
all_splits
)
# Define prompt for question-answering
prompt 
=
 hub
.
pull
(
"rlm/rag-prompt"
)
# Define state for application
class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 
str
# Define application steps
def
 
retrieve
(
state
:
 State
)
:
    retrieved_docs 
=
 vector_store
.
similarity_search
(
state
[
"question"
]
)
    
return
 
{
"context"
:
 retrieved_docs
}
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    response 
=
 llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
.
content
}
# Compile application and test
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)
API Reference:
hub
 | 
WebBaseLoader
 | 
Document
 | 
RecursiveCharacterTextSplitter
 | 
StateGraph


response 
=
 graph
.
invoke
(
{
"question"
:
 
"What is Task Decomposition?"
}
)
print
(
response
[
"answer"
]
)


Task Decomposition is the process of breaking down a complicated task into smaller, manageable steps to facilitate easier execution and understanding. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) guide models to think step-by-step, allowing them to explore multiple reasoning possibilities. This method enhances performance on complex tasks and provides insight into the model's thinking process.


Check out the 
LangSmith
trace
.


Detailed walkthrough
​


Let’s go through the above code step-by-step to really understand what’s
going on.


1. Indexing
​


note
This section is an abbreviated version of the content in the 
semantic search tutorial
.
If you're comfortable with 
document loaders
, 
embeddings
, and 
vector stores
,
feel free to skip to the next section on 
retrieval and generation
.


Loading documents
​


We need to first load the blog post contents. We can use

DocumentLoaders

for this, which are objects that load in data from a source and return a
list of

Document

objects.


In this case we’ll use the

WebBaseLoader
,
which uses 
urllib
 to load HTML from web URLs and 
BeautifulSoup
 to
parse it to text. We can customize the HTML -> text parsing by passing
in parameters into the 
BeautifulSoup
 parser via 
bs_kwargs
 (see

BeautifulSoup
docs
).
In this case only HTML tags with class “post-content”, “post-title”, or
“post-header” are relevant, so we’ll remove all others.


import
 bs4
from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
# Only keep post title, headers, and content from the full HTML.
bs4_strainer 
=
 bs4
.
SoupStrainer
(
class_
=
(
"post-title"
,
 
"post-header"
,
 
"post-content"
)
)
loader 
=
 WebBaseLoader
(
    web_paths
=
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
,
)
,
    bs_kwargs
=
{
"parse_only"
:
 bs4_strainer
}
,
)
docs 
=
 loader
.
load
(
)
assert
 
len
(
docs
)
 
==
 
1
print
(
f"Total characters: 
{
len
(
docs
[
0
]
.
page_content
)
}
"
)
API Reference:
WebBaseLoader


Total characters: 43131


print
(
docs
[
0
]
.
page_content
[
:
500
]
)


      LLM Powered Autonomous Agents
    
Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng
Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.
Agent System Overview#
In


Go deeper
​


DocumentLoader
: Object that loads data from a source as list of 
Documents
.




Docs
:
Detailed documentation on how to use 
DocumentLoaders
.


Integrations
: 160+
integrations to choose from.


Interface
:
API reference for the base interface.




Splitting documents
​


Our loaded document is over 42k characters which is too long to fit
into the context window of many models. Even for those models that could
fit the full post in their context window, models can struggle to find
information in very long inputs.


To handle this we’ll split the 
Document
 into chunks for embedding and
vector storage. This should help us retrieve only the most relevant parts
of the blog post at run time.


As in the 
semantic search tutorial
, we use a

RecursiveCharacterTextSplitter
,
which will recursively split the document using common separators like
new lines until each chunk is the appropriate size. This is the
recommended text splitter for generic text use cases.


from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
text_splitter 
=
 RecursiveCharacterTextSplitter
(
    chunk_size
=
1000
,
  
# chunk size (characters)
    chunk_overlap
=
200
,
  
# chunk overlap (characters)
    add_start_index
=
True
,
  
# track index in original document
)
all_splits 
=
 text_splitter
.
split_documents
(
docs
)
print
(
f"Split blog post into 
{
len
(
all_splits
)
}
 sub-documents."
)
API Reference:
RecursiveCharacterTextSplitter


Split blog post into 66 sub-documents.


Go deeper
​


TextSplitter
: Object that splits a list of 
Document
s into smaller
chunks. Subclass of 
DocumentTransformer
s.




Learn more about splitting text using different methods by reading the 
how-to docs


Code (py or js)


Scientific papers


Interface
: API reference for the base interface.




DocumentTransformer
: Object that performs a transformation on a list
of 
Document
 objects.




Docs
: Detailed documentation on how to use 
DocumentTransformers


Integrations


Interface
: API reference for the base interface.




Storing documents
​


Now we need to index our 66 text chunks so that we can search over them
at runtime. Following the 
semantic search tutorial
,
our approach is to 
embed
 the contents of each document split and insert these embeddings
into a 
vector store
. Given an input query, we can then use
vector search to retrieve relevant documents.


We can embed and store all of our document splits in a single command
using the vector store and embeddings model selected at the 
start of the tutorial
.


document_ids 
=
 vector_store
.
add_documents
(
documents
=
all_splits
)
print
(
document_ids
[
:
3
]
)


['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']


Go deeper
​


Embeddings
: Wrapper around a text embedding model, used for converting
text to embeddings.




Docs
: Detailed documentation on how to use embeddings.


Integrations
: 30+ integrations to choose from.


Interface
: API reference for the base interface.




VectorStore
: Wrapper around a vector database, used for storing and
querying embeddings.




Docs
: Detailed documentation on how to use vector stores.


Integrations
: 40+ integrations to choose from.


Interface
: API reference for the base interface.




This completes the 
Indexing
 portion of the pipeline. At this point
we have a query-able vector store containing the chunked contents of our
blog post. Given a user question, we should ideally be able to return
the snippets of the blog post that answer the question.


2. Retrieval and Generation
​


Now let’s write the actual application logic. We want to create a simple
application that takes a user question, searches for documents relevant
to that question, passes the retrieved documents and initial question to
a model, and returns an answer.


For generation, we will use the chat model selected at the 
start of the tutorial
.


We’ll use a prompt for RAG that is checked into the LangChain prompt hub
(
here
).


from
 langchain 
import
 hub
prompt 
=
 hub
.
pull
(
"rlm/rag-prompt"
)
example_messages 
=
 prompt
.
invoke
(
    
{
"context"
:
 
"(context goes here)"
,
 
"question"
:
 
"(question goes here)"
}
)
.
to_messages
(
)
assert
 
len
(
example_messages
)
 
==
 
1
print
(
example_messages
[
0
]
.
content
)
API Reference:
hub


You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Question: (question goes here) 
Context: (context goes here) 
Answer:


We'll use 
LangGraph
 to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:




We can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.


We get streamlined deployments via 
LangGraph Platform
.


LangSmith will automatically trace the steps of our application together.


We can easily add key features to our application, including 
persistence
 and 
human-in-the-loop approval
, with minimal code changes.




To use LangGraph, we need to define three things:




The state of our application;


The nodes of our application (i.e., application steps);


The "control flow" of our application (e.g., the ordering of the steps).




State:
​


The 
state
 of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a 
TypedDict
, but can also be a 
Pydantic BaseModel
.


For a simple RAG application, we can just keep track of the input question, retrieved context, and generated answer:


from
 langchain_core
.
documents 
import
 Document
from
 typing_extensions 
import
 List
,
 TypedDict
class
 
State
(
TypedDict
)
:
    question
:
 
str
    context
:
 List
[
Document
]
    answer
:
 
str
API Reference:
Document


Nodes (application steps)
​


Let's start with a simple sequence of two steps: retrieval and generation.


def
 
retrieve
(
state
:
 State
)
:
    retrieved_docs 
=
 vector_store
.
similarity_search
(
state
[
"question"
]
)
    
return
 
{
"context"
:
 retrieved_docs
}
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    response 
=
 llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
.
content
}


Our retrieval step simply runs a similarity search using the input question, and the generation step formats the retrieved context and original question into a prompt for the chat model.


Control flow
​


Finally, we compile our application into a single 
graph
 object. In this case, we are just connecting the retrieval and generation steps into a single sequence.


from
 langgraph
.
graph 
import
 START
,
 StateGraph
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"retrieve"
)
graph 
=
 graph_builder
.
compile
(
)
API Reference:
StateGraph


LangGraph also comes with built-in utilities for visualizing the control flow of your application:


from
 IPython
.
display 
import
 Image
,
 display
display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




Do I need to use LangGraph?
LangGraph is not required to build a RAG application. Indeed, we can implement the same application logic through invocations of the individual components:
question 
=
 
"..."
retrieved_docs 
=
 vector_store
.
similarity_search
(
question
)
docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 retrieved_docs
)
prompt 
=
 prompt
.
invoke
(
{
"question"
:
 question
,
 
"context"
:
 docs_content
}
)
answer 
=
 llm
.
invoke
(
prompt
)
The benefits of LangGraph include:


Support for multiple invocation modes: this logic would need to be rewritten if we wanted to stream output tokens, or stream the results of individual steps;


Automatic support for tracing via 
LangSmith
 and deployments via 
LangGraph Platform
;


Support for persistence, human-in-the-loop, and other features.


Many use-cases demand RAG in a conversational experience, such that a user can receive context-informed answers via a stateful conversation. As we will see in 
Part 2
 of the tutorial, LangGraph's management and persistence of state simplifies these applications enormously.


Usage
​


Let's test our application! LangGraph supports multiple invocation modes, including sync, async, and streaming.


Invoke:


result 
=
 graph
.
invoke
(
{
"question"
:
 
"What is Task Decomposition?"
}
)
print
(
f'Context: 
{
result
[
"context"
]
}
\n\n'
)
print
(
f'Answer: 
{
result
[
"answer"
]
}
'
)


Context: [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content="(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.")]
Answer: Task decomposition is a technique used to break down complex tasks into smaller, manageable steps, allowing for more efficient problem-solving. This can be achieved through methods like chain of thought prompting or the tree of thoughts approach, which explores multiple reasoning possibilities at each step. It can be initiated through simple prompts, task-specific instructions, or human inputs.


Stream steps:


for
 step 
in
 graph
.
stream
(
    
{
"question"
:
 
"What is Task Decomposition?"
}
,
 stream_mode
=
"updates"
)
:
    
print
(
f"
{
step
}
\n\n----------------\n"
)


{'retrieve': {'context': [Document(id='a42dc78b-8f76-472a-9e25-180508af74f3', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='c0e45887-d0b0-483d-821a-bb5d8316d51d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.'), Document(id='4cc7f318-35f5-440f-a4a4-145b5f0b918d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}, page_content='Resources:\n1. Internet access for searches and information gathering.\n2. Long Term memory management.\n3. GPT-3.5 powered Agents for delegation of simple tasks.\n4. File output.\n\nPerformance Evaluation:\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n2. Constructively self-criticize your big-picture behavior constantly.\n3. Reflect on past decisions and strategies to refine your approach.\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='f621ade4-9b0d-471f-a522-44eb5feeba0c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373}, page_content="(3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.")]}}
----------------
{'generate': {'answer': 'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This technique, often enhanced by methods like Chain of Thought (CoT) or Tree of Thoughts, allows models to reason through tasks systematically and improves performance by clarifying the thought process. It can be achieved through simple prompts, task-specific instructions, or human inputs.'}}
----------------


Stream 
tokens
:


for
 message
,
 metadata 
in
 graph
.
stream
(
    
{
"question"
:
 
"What is Task Decomposition?"
}
,
 stream_mode
=
"messages"
)
:
    
print
(
message
.
content
,
 end
=
"|"
)


|Task| decomposition| is| the| process| of| breaking| down| complex| tasks| into| smaller|,| more| manageable| steps|.| It| can| be| achieved| through| techniques| like| Chain| of| Thought| (|Co|T|)| prompting|,| which| encourages| the| model| to| think| step| by| step|,| or| through| more| structured| methods| like| the| Tree| of| Thoughts|.| This| approach| not| only| simplifies| task| execution| but| also| provides| insights| into| the| model|'s| reasoning| process|.||


tip
For async invocations, use:
result 
=
 
await
 graph
.
ainvoke
(
.
.
.
)
and
async
 
for
 step 
in
 graph
.
astream
(
.
.
.
)
:


Returning sources
​


Note that by storing the retrieved context in the state of the graph, we recover sources for the model's generated answer in the 
"context"
 field of the state. See 
this guide
 on returning sources for more detail.


Go deeper
​


Chat models
 take in a sequence of messages and return a message.




Docs


Integrations
: 25+ integrations to choose from.


Interface
: API reference for the base interface.




Customizing the prompt


As shown above, we can load prompts (e.g., 
this RAG
prompt
) from the prompt
hub. The prompt can also be easily customized. For example:


from
 langchain_core
.
prompts 
import
 PromptTemplate
template 
=
 
"""Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.
Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""
custom_rag_prompt 
=
 PromptTemplate
.
from_template
(
template
)
API Reference:
PromptTemplate


Query analysis
​


So far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:




In addition to semantic search, we can build in structured filters (e.g., "Find documents since the year 2020.");


The model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.




Query analysis
 employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let's add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later.


total_documents 
=
 
len
(
all_splits
)
third 
=
 total_documents 
//
 
3
for
 i
,
 document 
in
 
enumerate
(
all_splits
)
:
    
if
 i 
<
 third
:
        document
.
metadata
[
"section"
]
 
=
 
"beginning"
    
elif
 i 
<
 
2
 
*
 third
:
        document
.
metadata
[
"section"
]
 
=
 
"middle"
    
else
:
        document
.
metadata
[
"section"
]
 
=
 
"end"
all_splits
[
0
]
.
metadata


{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',
 'start_index': 8,
 'section': 'beginning'}


We will need to update the documents in our vector store. We will use a simple 
InMemoryVectorStore
 for this, as we will use some of its specific features (i.e., metadata filtering). Refer to the vector store 
integration documentation
 for relevant features of your chosen vector store.


from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
vector_store 
=
 InMemoryVectorStore
(
embeddings
)
_ 
=
 vector_store
.
add_documents
(
all_splits
)
API Reference:
InMemoryVectorStore


Let's next define a schema for our search query. We will use 
structured output
 for this purpose. Here we define a query as containing a string query and a document section (either "beginning", "middle", or "end"), but this can be defined however you like.


from
 typing 
import
 Literal
from
 typing_extensions 
import
 Annotated
class
 
Search
(
TypedDict
)
:
    
"""Search query."""
    query
:
 Annotated
[
str
,
 
.
.
.
,
 
"Search query to run."
]
    section
:
 Annotated
[
        Literal
[
"beginning"
,
 
"middle"
,
 
"end"
]
,
        
.
.
.
,
        
"Section to query."
,
    
]


Finally, we add a step to our LangGraph application to generate a query from the user's raw input:


class
 
State
(
TypedDict
)
:
    question
:
 
str
    query
:
 Search
    context
:
 List
[
Document
]
    answer
:
 
str
def
 
analyze_query
(
state
:
 State
)
:
    structured_llm 
=
 llm
.
with_structured_output
(
Search
)
    query 
=
 structured_llm
.
invoke
(
state
[
"question"
]
)
    
return
 
{
"query"
:
 query
}
def
 
retrieve
(
state
:
 State
)
:
    query 
=
 state
[
"query"
]
    retrieved_docs 
=
 vector_store
.
similarity_search
(
        query
[
"query"
]
,
        
filter
=
lambda
 doc
:
 doc
.
metadata
.
get
(
"section"
)
 
==
 query
[
"section"
]
,
    
)
    
return
 
{
"context"
:
 retrieved_docs
}
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    response 
=
 llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
.
content
}
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
analyze_query
,
 retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"analyze_query"
)
graph 
=
 graph_builder
.
compile
(
)


Full Code:
from
 typing 
import
 Literal
import
 bs4
from
 langchain 
import
 hub
from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
from
 langchain_core
.
documents 
import
 Document
from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
from
 langgraph
.
graph 
import
 START
,
 StateGraph
from
 typing_extensions 
import
 Annotated
,
 List
,
 TypedDict
# Load and chunk contents of the blog
loader 
=
 WebBaseLoader
(
    web_paths
=
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
,
)
,
    bs_kwargs
=
dict
(
        parse_only
=
bs4
.
SoupStrainer
(
            class_
=
(
"post-content"
,
 
"post-title"
,
 
"post-header"
)
        
)
    
)
,
)
docs 
=
 loader
.
load
(
)
text_splitter 
=
 RecursiveCharacterTextSplitter
(
chunk_size
=
1000
,
 chunk_overlap
=
200
)
all_splits 
=
 text_splitter
.
split_documents
(
docs
)
# Update metadata (illustration purposes)
total_documents 
=
 
len
(
all_splits
)
third 
=
 total_documents 
//
 
3
for
 i
,
 document 
in
 
enumerate
(
all_splits
)
:
    
if
 i 
<
 third
:
        document
.
metadata
[
"section"
]
 
=
 
"beginning"
    
elif
 i 
<
 
2
 
*
 third
:
        document
.
metadata
[
"section"
]
 
=
 
"middle"
    
else
:
        document
.
metadata
[
"section"
]
 
=
 
"end"
# Index chunks
vector_store 
=
 InMemoryVectorStore
(
embeddings
)
_ 
=
 vector_store
.
add_documents
(
all_splits
)
# Define schema for search
class
 
Search
(
TypedDict
)
:
    
"""Search query."""
    query
:
 Annotated
[
str
,
 
.
.
.
,
 
"Search query to run."
]
    section
:
 Annotated
[
        Literal
[
"beginning"
,
 
"middle"
,
 
"end"
]
,
        
.
.
.
,
        
"Section to query."
,
    
]
# Define prompt for question-answering
prompt 
=
 hub
.
pull
(
"rlm/rag-prompt"
)
# Define state for application
class
 
State
(
TypedDict
)
:
    question
:
 
str
    query
:
 Search
    context
:
 List
[
Document
]
    answer
:
 
str
def
 
analyze_query
(
state
:
 State
)
:
    structured_llm 
=
 llm
.
with_structured_output
(
Search
)
    query 
=
 structured_llm
.
invoke
(
state
[
"question"
]
)
    
return
 
{
"query"
:
 query
}
def
 
retrieve
(
state
:
 State
)
:
    query 
=
 state
[
"query"
]
    retrieved_docs 
=
 vector_store
.
similarity_search
(
        query
[
"query"
]
,
        
filter
=
lambda
 doc
:
 doc
.
metadata
.
get
(
"section"
)
 
==
 query
[
"section"
]
,
    
)
    
return
 
{
"context"
:
 retrieved_docs
}
def
 
generate
(
state
:
 State
)
:
    docs_content 
=
 
"\n\n"
.
join
(
doc
.
page_content 
for
 doc 
in
 state
[
"context"
]
)
    messages 
=
 prompt
.
invoke
(
{
"question"
:
 state
[
"question"
]
,
 
"context"
:
 docs_content
}
)
    response 
=
 llm
.
invoke
(
messages
)
    
return
 
{
"answer"
:
 response
.
content
}
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
[
analyze_query
,
 retrieve
,
 generate
]
)
graph_builder
.
add_edge
(
START
,
 
"analyze_query"
)
graph 
=
 graph_builder
.
compile
(
)
API Reference:
hub
 | 
WebBaseLoader
 | 
Document
 | 
InMemoryVectorStore
 | 
RecursiveCharacterTextSplitter
 | 
StateGraph


display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




We can test our implementation by specifically asking for context from the end of the post. Note that the model includes different information in its answer.


for
 step 
in
 graph
.
stream
(
    
{
"question"
:
 
"What does the end of the post say about Task Decomposition?"
}
,
    stream_mode
=
"updates"
,
)
:
    
print
(
f"
{
step
}
\n\n----------------\n"
)


{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}
----------------
{'retrieve': {'context': [Document(id='d6cef137-e1e8-4ddc-91dc-b62bd33c6020', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39221, 'section': 'end'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n\n\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'), Document(id='d1834ae1-eb6a-43d7-a023-08dfa5028799', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 39086, 'section': 'end'}, page_content='}\n]\nChallenges#\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='ca7f06e4-2c2e-4788-9a81-2418d82213d9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 32942, 'section': 'end'}, page_content='}\n]\nThen after these clarification, the agent moved into the code writing mode with a different system message.\nSystem message:'), Document(id='1fcc2736-30f4-4ef6-90f2-c64af92118cb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 35127, 'section': 'end'}, page_content='"content": "You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\n\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\n\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code\'s language, and CODE is the code:\\n\\nFILENAME\\n\`\`\`LANG\\nCODE\\n\`\`\`\\n\\nYou will start with the \\"entrypoint\\" file, then go to the ones that are imported by that file, and so on.\\nPlease')]}}
----------------
{'generate': {'answer': 'The end of the post highlights that task decomposition faces challenges in long-term planning and adapting to unexpected errors. LLMs struggle with adjusting their plans, making them less robust compared to humans who learn from trial and error. This indicates a limitation in effectively exploring the solution space and handling complex tasks.'}}
----------------


In both the streamed steps and the 
LangSmith trace
, we can now observe the structured query that was fed into the retrieval step.


Query Analysis is a rich problem with a wide range of approaches. Refer to the 
how-to guides
 for more examples.


Next steps
​


We've covered the steps to build a basic Q&A app over data:




Loading data with a 
Document Loader


Chunking the indexed data with a 
Text Splitter
 to make it more easily usable by a model


Embedding the data
 and storing the data in a 
vectorstore


Retrieving
 the previously stored chunks in response to incoming questions


Generating an answer using the retrieved chunks as context.




In 
Part 2
 of the tutorial, we will extend the implementation here to accommodate conversation-style interactions and multi-step retrieval processes.


Further reading:




Return sources
: Learn how to return source documents


Streaming
: Learn how to stream outputs and intermediate steps


Add chat history
: Learn how to add chat history to your app


Retrieval conceptual guide
: A high-level overview of specific retrieval techniques


Edit this page
Was this page helpful?
Previous
Tagging
Next
Build a semantic search engine
Overview
Indexing
Retrieval and generation
Setup
Jupyter Notebook
Installation
LangSmith
Components
Preview
Detailed walkthrough
1. Indexing
Loading documents
Splitting documents
Storing documents
2. Retrieval and Generation
Query analysis
Next steps


--- docs_tutorials_retrievers.txt ---

Tutorials
Build a semantic search engine
On this page
Build a semantic search engine


This tutorial will familiarize you with LangChain's 
document loader
, 
embedding
, and 
vector store
 abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or 
RAG
 (see our RAG tutorial 
here
).


Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.


Concepts
​


This guide focuses on retrieval of text data. We will cover the following concepts:




Documents and document loaders;


Text splitters;


Embeddings;


Vector stores and retrievers.




Setup
​


Jupyter Notebook
​


This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See 
here
 for instructions on how to install.


Installation
​


This tutorial requires the 
langchain-community
 and 
pypdf
 packages:




Pip
Conda
pip install langchain-community pypdf
conda install langchain-community pypdf -c conda-forge


For more details, see our 
Installation guide
.


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with 
LangSmith
.


After you sign up at the link above, make sure to set your environment variables to start logging traces:


export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."


Or, if in a notebook, you can set them with:


import
 getpass
import
 os
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Documents and Document Loaders
​


LangChain implements a 
Document
 abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:




page_content
: a string representing the content;


metadata
: a dict containing arbitrary metadata;


id
: (optional) a string identifier for the document.




The 
metadata
 attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual 
Document
 object often represents a chunk of a larger document.


We can generate sample documents when desired:


from
 langchain_core
.
documents 
import
 Document
documents 
=
 
[
    Document
(
        page_content
=
"Dogs are great companions, known for their loyalty and friendliness."
,
        metadata
=
{
"source"
:
 
"mammal-pets-doc"
}
,
    
)
,
    Document
(
        page_content
=
"Cats are independent pets that often enjoy their own space."
,
        metadata
=
{
"source"
:
 
"mammal-pets-doc"
}
,
    
)
,
]
API Reference:
Document


However, the LangChain ecosystem implements 
document loaders
 that 
integrate with hundreds of common sources
. This makes it easy to incorporate data from these sources into your AI application.


Loading documents
​


Let's load a PDF into a sequence of 
Document
 objects. There is a sample PDF in the LangChain repo 
here
 -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for 
available PDF document loaders
. Let's select 
PyPDFLoader
, which is fairly lightweight.


from
 langchain_community
.
document_loaders 
import
 PyPDFLoader
file_path 
=
 
"../example_data/nke-10k-2023.pdf"
loader 
=
 PyPDFLoader
(
file_path
)
docs 
=
 loader
.
load
(
)
print
(
len
(
docs
)
)
API Reference:
PyPDFLoader


107


tip
See 
this guide
 for more detail on PDF document loaders.


PyPDFLoader
 loads one 
Document
 object per PDF page. For each, we can easily access:




The string content of the page;


Metadata containing the file name and page number.




print
(
f"
{
docs
[
0
]
.
page_content
[
:
200]
}
\n"
)
print
(
docs
[
0
]
.
metadata
)


Table of Contents
UNITED STATES
SECURITIES AND EXCHANGE COMMISSION
Washington, D.C. 20549
FORM 10-K
(Mark One)
☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934
FO
{'source': '../example_data/nke-10k-2023.pdf', 'page': 0}


Splitting
​


For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve 
Document
 objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not "washed out" by surrounding text.


We can use 
text splitters
 for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters
with 200 characters of overlap between chunks. The overlap helps
mitigate the possibility of separating a statement from important
context related to it. We use the

RecursiveCharacterTextSplitter
,
which will recursively split the document using common separators like
new lines until each chunk is the appropriate size. This is the
recommended text splitter for generic text use cases.


We set 
add_start_index=True
 so that the character index where each
split Document starts within the initial Document is preserved as
metadata attribute “start_index”.


See 
this guide
 for more detail about working with PDFs, including how to extract text from specific sections and images.


from
 langchain_text_splitters 
import
 RecursiveCharacterTextSplitter
text_splitter 
=
 RecursiveCharacterTextSplitter
(
    chunk_size
=
1000
,
 chunk_overlap
=
200
,
 add_start_index
=
True
)
all_splits 
=
 text_splitter
.
split_documents
(
docs
)
len
(
all_splits
)
API Reference:
RecursiveCharacterTextSplitter


514


Embeddings
​


Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can 
embed
 it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.


LangChain supports embeddings from 
dozens of providers
. These models specify how text should be converted into a numeric vector. Let's select a model:




Select 
embeddings model
:
OpenAI
▾
OpenAI
Azure
Google
AWS
HuggingFace
Ollama
Cohere
MistralAI
Nomic
NVIDIA
Voyage AI
IBM watsonx
Fake
pip install -qU langchain-openai
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-large"
)


vector_1 
=
 embeddings
.
embed_query
(
all_splits
[
0
]
.
page_content
)
vector_2 
=
 embeddings
.
embed_query
(
all_splits
[
1
]
.
page_content
)
assert
 
len
(
vector_1
)
 
==
 
len
(
vector_2
)
print
(
f"Generated vectors of length 
{
len
(
vector_1
)
}
\n"
)
print
(
vector_1
[
:
10
]
)


Generated vectors of length 1536
[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]


Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.


Vector stores
​


LangChain 
VectorStore
 objects contain methods for adding text and 
Document
 objects to the store, and querying them using various similarity metrics. They are often initialized with 
embedding
 models, which determine how text data is translated to numeric vectors.


LangChain includes a suite of 
integrations
 with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as 
Postgres
) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:




Select 
vector store
:
In-memory
▾
In-memory
AstraDB
Chroma
FAISS
Milvus
MongoDB
PGVector
Pinecone
Qdrant
pip install -qU langchain-core
from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
vector_store 
=
 InMemoryVectorStore
(
embeddings
)


Having instantiated our vector store, we can now index the documents.


ids 
=
 vector_store
.
add_documents
(
documents
=
all_splits
)


Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific 
integration
 for more detail.


Once we've instantiated a 
VectorStore
 that contains documents, we can query it. 
VectorStore
 includes methods for querying:




Synchronously and asynchronously;


By string query and by vector;


With and without returning similarity scores;


By similarity and 
maximum marginal relevance
 (to balance similarity with query to diversity in retrieved results).




The methods will generally include a list of 
Document
 objects in their outputs.


Usage
​


Embeddings typically represent text as a "dense" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.


Return documents based on similarity to a string query:


results 
=
 vector_store
.
similarity_search
(
    
"How many distribution centers does Nike have in the US?"
)
print
(
results
[
0
]
)


page_content='direct to consumer operations sell products through the following number of retail stores in the United States:
U.S. RETAIL STORES NUMBER
NIKE Brand factory stores 213 
NIKE Brand in-line stores (including employee-only stores) 74 
Converse stores (including factory stores) 82 
TOTAL 369 
In the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.
2023 FORM 10-K 2' metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}


Async query:


results 
=
 
await
 vector_store
.
asimilarity_search
(
"When was Nike incorporated?"
)
print
(
results
[
0
]
)


page_content='Table of Contents
PART I
ITEM 1. BUSINESS
GENERAL
NIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"
"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.
Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is
the largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores
and sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}


Return scores:


# Note that providers implement different scores; the score here
# is a distance metric that varies inversely with similarity.
results 
=
 vector_store
.
similarity_search_with_score
(
"What was Nike's revenue in 2023?"
)
doc
,
 score 
=
 results
[
0
]
print
(
f"Score: 
{
score
}
\n"
)
print
(
doc
)


Score: 0.23699893057346344
page_content='Table of Contents
FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS
The following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:
FISCAL 2023 COMPARED TO FISCAL 2022
•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.
The increase was due to higher revenues in North America, Europe, Middle East & Africa ("EMEA"), APLA and Greater China, which contributed approximately 7, 6,
2 and 1 percentage points to NIKE, Inc. Revenues, respectively.
•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This
increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale
equivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}


Return documents based on similarity to an embedded query:


embedding 
=
 embeddings
.
embed_query
(
"How were Nike's margins impacted in 2023?"
)
results 
=
 vector_store
.
similarity_search_by_vector
(
embedding
)
print
(
results
[
0
]
)


page_content='Table of Contents
GROSS MARGIN
FISCAL 2023 COMPARED TO FISCAL 2022
For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to
43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:
*Wholesale equivalent
The decrease in gross margin for fiscal 2023 was primarily due to:
•Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as
product mix;
•Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in
the prior period resulting from lower available inventory supply;
•Unfavorable changes in net foreign currency exchange rates, including hedges; and
•Lower off-price margin, on a wholesale equivalent basis.
This was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}


Learn more:




API reference


How-to guide


Integration-specific docs




Retrievers
​


LangChain 
VectorStore
 objects do not subclass 
Runnable
. LangChain 
Retrievers
 are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous 
invoke
 and 
batch
 operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).


We can create a simple version of this ourselves, without subclassing 
Retriever
. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the 
similarity_search
 method:


from
 typing 
import
 List
from
 langchain_core
.
documents 
import
 Document
from
 langchain_core
.
runnables 
import
 chain
@chain
def
 
retriever
(
query
:
 
str
)
 
-
>
 List
[
Document
]
:
    
return
 vector_store
.
similarity_search
(
query
,
 k
=
1
)
retriever
.
batch
(
    
[
        
"How many distribution centers does Nike have in the US?"
,
        
"When was Nike incorporated?"
,
    
]
,
)
API Reference:
Document
 | 
chain


[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213 \nNIKE Brand in-line stores (including employee-only stores) 74 \nConverse stores (including factory stores) 82 \nTOTAL 369 \nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2')],
 [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales')]]


Vectorstores implement an 
as_retriever
 method that will generate a Retriever, specifically a 
VectorStoreRetriever
. These retrievers include specific 
search_type
 and 
search_kwargs
 attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:


retriever 
=
 vector_store
.
as_retriever
(
    search_type
=
"similarity"
,
    search_kwargs
=
{
"k"
:
 
1
}
,
)
retriever
.
batch
(
    
[
        
"How many distribution centers does Nike have in the US?"
,
        
"When was Nike incorporated?"
,
    
]
,
)


[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213 \nNIKE Brand in-line stores (including employee-only stores) 74 \nConverse stores (including factory stores) 82 \nTOTAL 369 \nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2')],
 [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales')]]


VectorStoreRetriever
 supports search types of 
"similarity"
 (default), 
"mmr"
 (maximum marginal relevance, described above), and 
"similarity_score_threshold"
. We can use the latter to threshold documents output by the retriever by similarity score.


Retrievers can easily be incorporated into more complex applications, such as 
retrieval-augmented generation (RAG)
 applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the 
RAG tutorial
 tutorial.


Learn more:
​


Retrieval strategies can be rich and complex. For example:




We can 
infer hard rules and filters
 from a query (e.g., "using documents published after 2020");


We can 
return documents that are linked
 to the retrieved context in some way (e.g., via some document taxonomy);


We can generate 
multiple embeddings
 for each unit of context;


We can 
ensemble results
 from multiple retrievers;


We can assign weights to documents, e.g., to weigh 
recent documents
 higher.




The 
retrievers
 section of the how-to guides covers these and other built-in retrieval strategies.


It is also straightforward to extend the 
BaseRetriever
 class in order to implement custom retrievers. See our how-to guide 
here
.


Next steps
​


You've now seen how to build a semantic search engine over a PDF document.


For more on document loaders:




Conceptual guide


How-to guides


Available integrations




For more on embeddings:




Conceptual guide


How-to guides


Available integrations




For more on vector stores:




Conceptual guide


How-to guides


Available integrations




For more on RAG, see:




Build a Retrieval Augmented Generation (RAG) App


Related how-to guides


Edit this page
Was this page helpful?
Previous
Build a Retrieval Augmented Generation (RAG) App: Part 1
Next
Build a Question/Answering system over SQL data
Concepts
Setup
Jupyter Notebook
Installation
LangSmith
Documents and Document Loaders
Loading documents
Splitting
Embeddings
Vector stores
Usage
Retrievers
Learn more:
Next steps


--- docs_tutorials_sql_qa.txt ---

Tutorials
Build a Question/Answering system over SQL data
On this page
Build a Question/Answering system over SQL data


Prerequisites
This guide assumes familiarity with the following concepts:


Chat models


Tools


Agents


LangGraph




Enabling a LLM system to query structured data can be qualitatively different from unstructured text data. Whereas in the latter it is common to generate text that can be searched against a vector database, the approach for structured data is often for the LLM to write and execute queries in a DSL, such as SQL. In this guide we'll go over the basic ways to create a Q&A system over tabular data in databases. We will cover implementations using both 
chains
 and 
agents
. These systems will allow us to ask a question about the data in a database and get back a natural language answer. The main difference between the two is that our agent can query the database in a loop as many times as it needs to answer the question.


⚠️ Security note ⚠️
​


Building Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your chain/agent's needs. This will mitigate though not eliminate the risks of building a model-driven system. For more on general security best practices, 
see here
.


Architecture
​


At a high-level, the steps of these systems are:




Convert question to SQL query
: Model converts user input to a SQL query.


Execute SQL query
: Execute the query.


Answer the question
: Model responds to user input using the query results.




Note that querying data in CSVs can follow a similar approach. See our 
how-to guide
 on question-answering over CSV data for more detail.




Setup
​


First, get required packages and set environment variables:


%
%
capture 
-
-
no
-
stderr
%
pip install 
-
-
upgrade 
-
-
quiet langchain
-
community langgraph


# Comment out the below to opt-out of using LangSmith in this notebook. Not required.
if
 
not
 os
.
environ
.
get
(
"LANGSMITH_API_KEY"
)
:
    os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)
    os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"


Sample data
​


The below example will use a SQLite connection with the Chinook database, which is a sample database that represents a digital media store. Follow 
these installation steps
 to create 
Chinook.db
 in the same directory as this notebook. You can also download and build the database via the command line:


curl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db


Now, 
Chinook.db
 is in our directory and we can interface with it using the SQLAlchemy-driven 
SQLDatabase
 class:


from
 langchain_community
.
utilities 
import
 SQLDatabase
db 
=
 SQLDatabase
.
from_uri
(
"sqlite:///Chinook.db"
)
print
(
db
.
dialect
)
print
(
db
.
get_usable_table_names
(
)
)
db
.
run
(
"SELECT * FROM Artist LIMIT 10;"
)
API Reference:
SQLDatabase


sqlite
['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']


"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]"


Great! We've got a SQL database that we can query. Now let's try hooking it up to an LLM.


Chains
​


Chains are compositions of predictable steps. In 
LangGraph
, we can represent a chain via simple sequence of nodes. Let's create a sequence of steps that, given a question, does the following:




converts the question into a SQL query;


executes the query;


uses the result to answer the original question.




There are scenarios not supported by this arrangement. For example, this system will execute a SQL query for any user input-- even "hello". Importantly, as we'll see below, some questions require more than one query to answer. We will address these scenarios in the Agents section.


Application state
​


The LangGraph 
state
 of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a 
TypedDict
, but can also be a 
Pydantic BaseModel
.


For this application, we can just keep track of the input question, generated query, query result, and generated answer:


from
 typing_extensions 
import
 TypedDict
class
 
State
(
TypedDict
)
:
    question
:
 
str
    query
:
 
str
    result
:
 
str
    answer
:
 
str


Now we just need functions that operate on this state and populate its contents.


Convert question to SQL query
​


The first step is to take the user input and convert it to a SQL query. To reliably obtain SQL queries (absent markdown formatting and explanations or clarifications), we will make use of LangChain's 
structured output
 abstraction.


Let's select a chat model for our application:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Let's provide some instructions for our model:


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
system_message 
=
 
"""
Given an input question, create a syntactically correct {dialect} query to
run to help find the answer. Unless the user specifies in his question a
specific number of examples they wish to obtain, always limit your query to
at most {top_k} results. You can order the results by a relevant column to
return the most interesting examples in the database.
Never query for all the columns from a specific table, only ask for a the
few relevant columns given the question.
Pay attention to use only the column names that you can see in the schema
description. Be careful to not query for columns that do not exist. Also,
pay attention to which column is in which table.
Only use the following tables:
{table_info}
"""
user_prompt 
=
 
"Question: {input}"
query_prompt_template 
=
 ChatPromptTemplate
(
    
[
(
"system"
,
 system_message
)
,
 
(
"user"
,
 user_prompt
)
]
)
for
 message 
in
 query_prompt_template
.
messages
:
    message
.
pretty_print
(
)
API Reference:
ChatPromptTemplate


================================[1m System Message [0m================================
Given an input question, create a syntactically correct [33;1m[1;3m{dialect}[0m query to
run to help find the answer. Unless the user specifies in his question a
specific number of examples they wish to obtain, always limit your query to
at most [33;1m[1;3m{top_k}[0m results. You can order the results by a relevant column to
return the most interesting examples in the database.
Never query for all the columns from a specific table, only ask for a the
few relevant columns given the question.
Pay attention to use only the column names that you can see in the schema
description. Be careful to not query for columns that do not exist. Also,
pay attention to which column is in which table.
Only use the following tables:
[33;1m[1;3m{table_info}[0m
================================[1m Human Message [0m=================================
Question: [33;1m[1;3m{input}[0m


The prompt includes several parameters we will need to populate, such as the SQL dialect and table schemas. LangChain's 
SQLDatabase
 object includes methods to help with this. Our 
write_query
 step will just populate these parameters and prompt a model to generate the SQL query:


from
 typing_extensions 
import
 Annotated
class
 
QueryOutput
(
TypedDict
)
:
    
"""Generated SQL query."""
    query
:
 Annotated
[
str
,
 
.
.
.
,
 
"Syntactically valid SQL query."
]
def
 
write_query
(
state
:
 State
)
:
    
"""Generate SQL query to fetch information."""
    prompt 
=
 query_prompt_template
.
invoke
(
        
{
            
"dialect"
:
 db
.
dialect
,
            
"top_k"
:
 
10
,
            
"table_info"
:
 db
.
get_table_info
(
)
,
            
"input"
:
 state
[
"question"
]
,
        
}
    
)
    structured_llm 
=
 llm
.
with_structured_output
(
QueryOutput
)
    result 
=
 structured_llm
.
invoke
(
prompt
)
    
return
 
{
"query"
:
 result
[
"query"
]
}


Let's test it out:


write_query
(
{
"question"
:
 
"How many Employees are there?"
}
)


{'query': 'SELECT COUNT(*) as employee_count FROM Employee;'}


Execute query
​


This is the most dangerous part of creating a SQL chain.
 Consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution (see below).


To execute the query, we will load a tool from 
langchain-community
. Our 
execute_query
 node will just wrap this tool:


from
 langchain_community
.
tools
.
sql_database
.
tool 
import
 QuerySQLDatabaseTool
def
 
execute_query
(
state
:
 State
)
:
    
"""Execute SQL query."""
    execute_query_tool 
=
 QuerySQLDatabaseTool
(
db
=
db
)
    
return
 
{
"result"
:
 execute_query_tool
.
invoke
(
state
[
"query"
]
)
}
API Reference:
QuerySQLDatabaseTool


Testing this step:


execute_query
(
{
"query"
:
 
"SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;"
}
)


{'result': '[(8,)]'}


Generate answer
​


Finally, our last step generates an answer to the question given the information pulled from the database:


def
 
generate_answer
(
state
:
 State
)
:
    
"""Answer question using retrieved information as context."""
    prompt 
=
 
(
        
"Given the following user question, corresponding SQL query, "
        
"and SQL result, answer the user question.\n\n"
        
f'Question: 
{
state
[
"question"
]
}
\n'
        
f'SQL Query: 
{
state
[
"query"
]
}
\n'
        
f'SQL Result: 
{
state
[
"result"
]
}
'
    
)
    response 
=
 llm
.
invoke
(
prompt
)
    
return
 
{
"answer"
:
 response
.
content
}


Orchestrating with LangGraph
​


Finally, we compile our application into a single 
graph
 object. In this case, we are just connecting the three steps into a single sequence.


from
 langgraph
.
graph 
import
 START
,
 StateGraph
graph_builder 
=
 StateGraph
(
State
)
.
add_sequence
(
    
[
write_query
,
 execute_query
,
 generate_answer
]
)
graph_builder
.
add_edge
(
START
,
 
"write_query"
)
graph 
=
 graph_builder
.
compile
(
)
API Reference:
StateGraph


LangGraph also comes with built-in utilities for visualizing the control flow of your application:


from
 IPython
.
display 
import
 Image
,
 display
display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




Let's test our application! Note that we can stream the results of individual steps:


for
 step 
in
 graph
.
stream
(
    
{
"question"
:
 
"How many employees are there?"
}
,
 stream_mode
=
"updates"
)
:
    
print
(
step
)


{'write_query': {'query': 'SELECT COUNT(*) as employee_count FROM Employee;'}}
{'execute_query': {'result': '[(8,)]'}}
{'generate_answer': {'answer': 'There are 8 employees in total.'}}


Check out the 
LangSmith trace
.


Human-in-the-loop
​


LangGraph supports a number of features that can be useful for this workflow. One of them is 
human-in-the-loop
: we can interrupt our application before sensitive steps (such as the execution of a SQL query) for human review. This is enabled by LangGraph's 
persistence
 layer, which saves run progress to your storage of choice. Below, we specify storage in-memory:


from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
memory 
=
 MemorySaver
(
)
graph 
=
 graph_builder
.
compile
(
checkpointer
=
memory
,
 interrupt_before
=
[
"execute_query"
]
)
# Now that we're using persistence, we need to specify a thread ID
# so that we can continue the run after review.
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"1"
}
}
API Reference:
MemorySaver


display
(
Image
(
graph
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
)




Let's repeat the same run, adding in a simple yes/no approval step:


for
 step 
in
 graph
.
stream
(
    
{
"question"
:
 
"How many employees are there?"
}
,
    config
,
    stream_mode
=
"updates"
,
)
:
    
print
(
step
)
try
:
    user_approval 
=
 
input
(
"Do you want to go to execute query? (yes/no): "
)
except
 Exception
:
    user_approval 
=
 
"no"
if
 user_approval
.
lower
(
)
 
==
 
"yes"
:
    
# If approved, continue the graph execution
    
for
 step 
in
 graph
.
stream
(
None
,
 config
,
 stream_mode
=
"updates"
)
:
        
print
(
step
)
else
:
    
print
(
"Operation cancelled by user."
)


{'write_query': {'query': 'SELECT COUNT(EmployeeId) AS EmployeeCount FROM Employee;'}}
{'__interrupt__': ()}
``````output
Do you want to go to execute query? (yes/no):  yes
``````output
{'execute_query': {'result': '[(8,)]'}}
{'generate_answer': {'answer': 'There are 8 employees.'}}


See 
this
 LangGraph guide for more detail and examples.


Next steps
​


For more complex query-generation, we may want to create few-shot prompts or add query-checking steps. For advanced techniques like this and more check out:




Prompting strategies
: Advanced prompt engineering techniques.


Query checking
: Add query validation and error handling.


Large databases
: Techniques for working with large databases.




Agents
​


Agents
 leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allows you to offload additional discretion over the query generation and execution process. Although their behavior is less predictable than the above "chain", they feature some advantages:




They can query the database as many times as needed to answer the user question.


They can recover from errors by running a generated query, catching the traceback and regenerating it correctly.


They can answer questions based on the databases' schema as well as on the databases' content (like describing a specific table).




Below we assemble a minimal SQL agent. We will equip it with a set of tools using LangChain's 
SQLDatabaseToolkit
. Using LangGraph's 
pre-built ReAct agent constructor
, we can do this in one line.


tip
Check out LangGraph's 
SQL Agent Tutorial
 for a more advanced formulation of a SQL agent.


The 
SQLDatabaseToolkit
 includes tools that can:




Create and execute queries


Check query syntax


Retrieve table descriptions


... and more




from
 langchain_community
.
agent_toolkits 
import
 SQLDatabaseToolkit
toolkit 
=
 SQLDatabaseToolkit
(
db
=
db
,
 llm
=
llm
)
tools 
=
 toolkit
.
get_tools
(
)
tools
API Reference:
SQLDatabaseToolkit


[QuerySQLDatabaseTool(description="Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>),
 InfoSQLDatabaseTool(description='Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>),
 ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>),
 QuerySQLCheckerTool(description='Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x10d5f9120>, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['dialect', 'query'], input_types={}, partial_variables={}, template='\n{query}\nDouble check the {dialect} query above for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\n\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n\nOutput the final SQL query only.\n\nSQL Query: '), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x119315480>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x119317550>, root_client=<openai.OpenAI object at 0x10d5f8df0>, root_async_client=<openai.AsyncOpenAI object at 0x1193154e0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))]


System Prompt
​


We will also want to load a system prompt for our agent. This will consist of instructions for how to behave. Note that the prompt below has several parameters, which we assign below.


system_message 
=
 
"""
You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct {dialect} query to run,
then look at the results of the query and return the answer. Unless the user
specifies a specific number of examples they wish to obtain, always limit your
query to at most {top_k} results.
You can order the results by a relevant column to return the most interesting
examples in the database. Never query for all the columns from a specific table,
only ask for the relevant columns given the question.
You MUST double check your query before executing it. If you get an error while
executing a query, rewrite the query and try again.
DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the
database.
To start you should ALWAYS look at the tables in the database to see what you
can query. Do NOT skip this step.
Then you should query the schema of the most relevant tables.
"""
.
format
(
    dialect
=
"SQLite"
,
    top_k
=
5
,
)


Initializing agent
​


We will use a prebuilt 
LangGraph
 agent to build our agent


from
 langchain_core
.
messages 
import
 HumanMessage
from
 langgraph
.
prebuilt 
import
 create_react_agent
agent_executor 
=
 create_react_agent
(
llm
,
 tools
,
 prompt
=
system_message
)
API Reference:
HumanMessage
 | 
create_react_agent


Consider how the agent responds to the below question:


question 
=
 
"Which country's customers spent the most?"
for
 step 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 question
}
]
}
,
    stream_mode
=
"values"
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
Which country's customers spent the most?
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_list_tables (call_tFp7HYD6sAAmCShgeqkVZH6Q)
 Call ID: call_tFp7HYD6sAAmCShgeqkVZH6Q
  Args:
=================================[1m Tool Message [0m=================================
Name: sql_db_list_tables
Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_schema (call_KJZ1Jx6JazyDdJa0uH1UeiOz)
 Call ID: call_KJZ1Jx6JazyDdJa0uH1UeiOz
  Args:
    table_names: Customer, Invoice
=================================[1m Tool Message [0m=================================
Name: sql_db_schema
CREATE TABLE "Customer" (
	"CustomerId" INTEGER NOT NULL, 
	"FirstName" NVARCHAR(40) NOT NULL, 
	"LastName" NVARCHAR(20) NOT NULL, 
	"Company" NVARCHAR(80), 
	"Address" NVARCHAR(70), 
	"City" NVARCHAR(40), 
	"State" NVARCHAR(40), 
	"Country" NVARCHAR(40), 
	"PostalCode" NVARCHAR(10), 
	"Phone" NVARCHAR(24), 
	"Fax" NVARCHAR(24), 
	"Email" NVARCHAR(60) NOT NULL, 
	"SupportRepId" INTEGER, 
	PRIMARY KEY ("CustomerId"), 
	FOREIGN KEY("SupportRepId") REFERENCES "Employee" ("EmployeeId")
)
/*
3 rows from Customer table:
CustomerId	FirstName	LastName	Company	Address	City	State	Country	PostalCode	Phone	Fax	Email	SupportRepId
1	Luís	Gonçalves	Embraer - Empresa Brasileira de Aeronáutica S.A.	Av. Brigadeiro Faria Lima, 2170	São José dos Campos	SP	Brazil	12227-000	+55 (12) 3923-5555	+55 (12) 3923-5566	luisg@embraer.com.br	3
2	Leonie	Köhler	None	Theodor-Heuss-Straße 34	Stuttgart	None	Germany	70174	+49 0711 2842222	None	leonekohler@surfeu.de	5
3	François	Tremblay	None	1498 rue Bélanger	Montréal	QC	Canada	H2G 1A7	+1 (514) 721-4711	None	ftremblay@gmail.com	3
*/
CREATE TABLE "Invoice" (
	"InvoiceId" INTEGER NOT NULL, 
	"CustomerId" INTEGER NOT NULL, 
	"InvoiceDate" DATETIME NOT NULL, 
	"BillingAddress" NVARCHAR(70), 
	"BillingCity" NVARCHAR(40), 
	"BillingState" NVARCHAR(40), 
	"BillingCountry" NVARCHAR(40), 
	"BillingPostalCode" NVARCHAR(10), 
	"Total" NUMERIC(10, 2) NOT NULL, 
	PRIMARY KEY ("InvoiceId"), 
	FOREIGN KEY("CustomerId") REFERENCES "Customer" ("CustomerId")
)
/*
3 rows from Invoice table:
InvoiceId	CustomerId	InvoiceDate	BillingAddress	BillingCity	BillingState	BillingCountry	BillingPostalCode	Total
1	2	2021-01-01 00:00:00	Theodor-Heuss-Straße 34	Stuttgart	None	Germany	70174	1.98
2	4	2021-01-02 00:00:00	Ullevålsveien 14	Oslo	None	Norway	0171	3.96
3	8	2021-01-03 00:00:00	Grétrystraat 63	Brussels	None	Belgium	1000	5.94
*/
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_query_checker (call_AQuTGbgH63u4gPgyV723yrjX)
 Call ID: call_AQuTGbgH63u4gPgyV723yrjX
  Args:
    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;
=================================[1m Tool Message [0m=================================
Name: sql_db_query_checker
\`\`\`sql
SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;
\`\`\`
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_query (call_B88EwU44nwwpQL5M9nlcemSU)
 Call ID: call_B88EwU44nwwpQL5M9nlcemSU
  Args:
    query: SELECT c.Country, SUM(i.Total) as TotalSpent FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId GROUP BY c.Country ORDER BY TotalSpent DESC LIMIT 1;
=================================[1m Tool Message [0m=================================
Name: sql_db_query
[('USA', 523.06)]
==================================[1m Ai Message [0m==================================
The country whose customers spent the most is the USA, with a total spending of 523.06.


You can also use the 
LangSmith trace
 to visualize these steps and associated metadata.


Note that the agent executes multiple queries until it has the information it needs:




List available tables;


Retrieves the schema for three tables;


Queries multiple of the tables via a join operation.




The agent is then able to use the result of the final query to generate an answer to the original question.


The agent can similarly handle qualitative questions:


question 
=
 
"Describe the playlisttrack table"
for
 step 
in
 agent_executor
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 question
}
]
}
,
    stream_mode
=
"values"
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
Describe the playlisttrack table
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_list_tables (call_fMF8eTmX5TJDJjc3Mhdg52TI)
 Call ID: call_fMF8eTmX5TJDJjc3Mhdg52TI
  Args:
=================================[1m Tool Message [0m=================================
Name: sql_db_list_tables
Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_schema (call_W8Vkk4NEodkAAIg8nexAszUH)
 Call ID: call_W8Vkk4NEodkAAIg8nexAszUH
  Args:
    table_names: PlaylistTrack
=================================[1m Tool Message [0m=================================
Name: sql_db_schema
CREATE TABLE "PlaylistTrack" (
	"PlaylistId" INTEGER NOT NULL, 
	"TrackId" INTEGER NOT NULL, 
	PRIMARY KEY ("PlaylistId", "TrackId"), 
	FOREIGN KEY("TrackId") REFERENCES "Track" ("TrackId"), 
	FOREIGN KEY("PlaylistId") REFERENCES "Playlist" ("PlaylistId")
)
/*
3 rows from PlaylistTrack table:
PlaylistId	TrackId
1	3402
1	3389
1	3390
*/
==================================[1m Ai Message [0m==================================
The `PlaylistTrack` table is designed to associate tracks with playlists. It has the following structure:
- **PlaylistId**: An integer that serves as a foreign key referencing the `Playlist` table. It is part of the composite primary key.
- **TrackId**: An integer that serves as a foreign key referencing the `Track` table. It is also part of the composite primary key.
The primary key for this table is a composite key consisting of both `PlaylistId` and `TrackId`, ensuring that each track can be uniquely associated with a playlist. The table enforces referential integrity by linking to the `Track` and `Playlist` tables through foreign keys.


Dealing with high-cardinality columns
​


In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.


We can achieve this by creating a vector store with all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.


First we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:


import
 ast
import
 re
def
 
query_as_list
(
db
,
 query
)
:
    res 
=
 db
.
run
(
query
)
    res 
=
 
[
el 
for
 sub 
in
 ast
.
literal_eval
(
res
)
 
for
 el 
in
 sub 
if
 el
]
    res 
=
 
[
re
.
sub
(
r"\b\d+\b"
,
 
""
,
 string
)
.
strip
(
)
 
for
 string 
in
 res
]
    
return
 
list
(
set
(
res
)
)
artists 
=
 query_as_list
(
db
,
 
"SELECT Name FROM Artist"
)
albums 
=
 query_as_list
(
db
,
 
"SELECT Title FROM Album"
)
albums
[
:
5
]


['In Through The Out Door',
 'Transmission',
 'Battlestar Galactica (Classic), Season',
 'A Copland Celebration, Vol. I',
 'Quiet Songs']


Using this function, we can create a 
retriever tool
 that the agent can execute at its discretion.


Let's select an 
embeddings model
 and 
vector store
 for this step:


Select an embedding model
:




Select 
embeddings model
:
OpenAI
▾
OpenAI
Azure
Google
AWS
HuggingFace
Ollama
Cohere
MistralAI
Nomic
NVIDIA
Voyage AI
IBM watsonx
Fake
pip install -qU langchain-openai
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain_openai 
import
 OpenAIEmbeddings
embeddings 
=
 OpenAIEmbeddings
(
model
=
"text-embedding-3-large"
)


Select a vector store
:




Select 
vector store
:
In-memory
▾
In-memory
AstraDB
Chroma
FAISS
Milvus
MongoDB
PGVector
Pinecone
Qdrant
pip install -qU langchain-core
from
 langchain_core
.
vectorstores 
import
 InMemoryVectorStore
vector_store 
=
 InMemoryVectorStore
(
embeddings
)


We can now construct a retrieval tool that can search over relevant proper nouns in the database:


from
 langchain
.
agents
.
agent_toolkits 
import
 create_retriever_tool
_ 
=
 vector_store
.
add_texts
(
artists 
+
 albums
)
retriever 
=
 vector_store
.
as_retriever
(
search_kwargs
=
{
"k"
:
 
5
}
)
description 
=
 
(
    
"Use to look up values to filter on. Input is an approximate spelling "
    
"of the proper noun, output is valid proper nouns. Use the noun most "
    
"similar to the search."
)
retriever_tool 
=
 create_retriever_tool
(
    retriever
,
    name
=
"search_proper_nouns"
,
    description
=
description
,
)
API Reference:
create_retriever_tool


Let's try it out:


print
(
retriever_tool
.
invoke
(
"Alice Chains"
)
)


Alice In Chains
Alanis Morissette
Pearl Jam
Pearl Jam
Audioslave


This way, if the agent determines it needs to write a filter based on an artist along the lines of "Alice Chains", it can first use the retriever tool to observe relevant values of a column.


Putting this together:


# Add to system message
suffix 
=
 
(
    
"If you need to filter on a proper noun like a Name, you must ALWAYS first look up "
    
"the filter value using the 'search_proper_nouns' tool! Do not try to "
    
"guess at the proper name - use this function to find similar ones."
)
system 
=
 
f"
{
system_message
}
\n\n
{
suffix
}
"
tools
.
append
(
retriever_tool
)
agent 
=
 create_react_agent
(
llm
,
 tools
,
 prompt
=
system
)


question 
=
 
"How many albums does alis in chain have?"
for
 step 
in
 agent
.
stream
(
    
{
"messages"
:
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 question
}
]
}
,
    stream_mode
=
"values"
,
)
:
    step
[
"messages"
]
[
-
1
]
.
pretty_print
(
)


================================[1m Human Message [0m=================================
How many albums does alis in chain have?
==================================[1m Ai Message [0m==================================
Tool Calls:
  search_proper_nouns (call_8ryjsRPLAr79mM3Qvnq6gTOH)
 Call ID: call_8ryjsRPLAr79mM3Qvnq6gTOH
  Args:
    query: alis in chain
=================================[1m Tool Message [0m=================================
Name: search_proper_nouns
Alice In Chains
Aisha Duo
Xis
Da Lama Ao Caos
A-Sides
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_list_tables (call_NJjtCpU89MBMplssjn1z0xzq)
 Call ID: call_NJjtCpU89MBMplssjn1z0xzq
  Args:
  search_proper_nouns (call_1BfrueC9koSIyi4OfMu2Ao8q)
 Call ID: call_1BfrueC9koSIyi4OfMu2Ao8q
  Args:
    query: Alice In Chains
=================================[1m Tool Message [0m=================================
Name: search_proper_nouns
Alice In Chains
Pearl Jam
Pearl Jam
Foo Fighters
Soundgarden
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_schema (call_Kn09w9jd9swcNzIZ1b5MlKID)
 Call ID: call_Kn09w9jd9swcNzIZ1b5MlKID
  Args:
    table_names: Album, Artist
=================================[1m Tool Message [0m=================================
Name: sql_db_schema
CREATE TABLE "Album" (
	"AlbumId" INTEGER NOT NULL, 
	"Title" NVARCHAR(160) NOT NULL, 
	"ArtistId" INTEGER NOT NULL, 
	PRIMARY KEY ("AlbumId"), 
	FOREIGN KEY("ArtistId") REFERENCES "Artist" ("ArtistId")
)
/*
3 rows from Album table:
AlbumId	Title	ArtistId
1	For Those About To Rock We Salute You	1
2	Balls to the Wall	2
3	Restless and Wild	2
*/
CREATE TABLE "Artist" (
	"ArtistId" INTEGER NOT NULL, 
	"Name" NVARCHAR(120), 
	PRIMARY KEY ("ArtistId")
)
/*
3 rows from Artist table:
ArtistId	Name
1	AC/DC
2	Accept
3	Aerosmith
*/
==================================[1m Ai Message [0m==================================
Tool Calls:
  sql_db_query (call_WkHRiPcBoGN9bc58MIupRHKP)
 Call ID: call_WkHRiPcBoGN9bc58MIupRHKP
  Args:
    query: SELECT COUNT(*) FROM Album WHERE ArtistId = (SELECT ArtistId FROM Artist WHERE Name = 'Alice In Chains')
=================================[1m Tool Message [0m=================================
Name: sql_db_query
[(1,)]
==================================[1m Ai Message [0m==================================
Alice In Chains has released 1 album in the database.


As we can see, both in the streamed steps and in the 
LangSmith trace
, the agent used the 
search_proper_nouns
 tool in order to check how to correctly query the database for this specific artist.
Edit this page
Was this page helpful?
Previous
Build a semantic search engine
Next
Summarize Text
⚠️ Security note ⚠️
Architecture
Setup
Sample data
Chains
Application state
Convert question to SQL query
Execute query
Generate answer
Orchestrating with LangGraph
Human-in-the-loop
Next steps
Agents
System Prompt
Initializing agent
Dealing with high-cardinality columns


--- docs_tutorials_summarization.txt ---

Tutorials
Summarize Text
On this page
Summarize Text


info
This tutorial demonstrates text summarization using built-in chains and 
LangGraph
.
A 
previous version
 of this page showcased the legacy chains 
StuffDocumentsChain
, 
MapReduceDocumentsChain
, and 
RefineDocumentsChain
. See 
here
 for information on using those abstractions and a comparison with the methods demonstrated in this tutorial.


Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content.


LLMs are a great tool for this given their proficiency in understanding and synthesizing text.


In the context of 
retrieval-augmented generation
, summarizing text can help distill the information in a large number of retrieved documents to provide context for a LLM.


In this walkthrough we'll go over how to summarize content from multiple documents using LLMs.




Concepts
​


Concepts we will cover are:






Using 
language models
.






Using 
document loaders
, specifically the 
WebBaseLoader
 to load content from an HTML webpage.






Two ways to summarize or otherwise combine documents.




Stuff
, which simply concatenates documents into a prompt;


Map-reduce
, for larger sets of documents. This splits documents into batches, summarizes those, and then summarizes the summaries.








Shorter, targeted guides on these strategies and others, including 
iterative refinement
, can be found in the 
how-to guides
.


Setup
​


Jupyter Notebook
​


This guide (and most of the other guides in the documentation) uses 
Jupyter notebooks
 and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.


This and other tutorials are perhaps most conveniently run in a Jupyter notebook. See 
here
 for instructions on how to install.


Installation
​


To install LangChain run:




Pip
Conda
pip install langchain
conda install langchain -c conda-forge


For more details, see our 
Installation guide
.


LangSmith
​


Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with 
LangSmith
.


After you sign up at the link above, make sure to set your environment variables to start logging traces:


export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."


Or, if in a notebook, you can set them with:


import
 getpass
import
 os
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"
os
.
environ
[
"LANGSMITH_API_KEY"
]
 
=
 getpass
.
getpass
(
)


Overview
​


A central question for building a summarizer is how to pass your documents into the LLM's context window. Two common approaches for this are:






Stuff
: Simply "stuff" all your documents into a single prompt. This is the simplest approach (see 
here
 for more on the 
create_stuff_documents_chain
 constructor, which is used for this method).






Map-reduce
: Summarize each document on its own in a "map" step and then "reduce" the summaries into a final summary (see 
here
 for more on the 
MapReduceDocumentsChain
, which is used for this method).






Note that map-reduce is especially effective when understanding of a sub-document does not rely on preceding context. For example, when summarizing a corpus of many, shorter documents. In other cases, such as summarizing a novel or body of text with an inherent sequence, 
iterative refinement
 may be more effective.




Setup
​


First set environment variables and install packages:


%
pip install 
-
-
upgrade 
-
-
quiet tiktoken langchain langgraph beautifulsoup4 langchain
-
community
# Set env var OPENAI_API_KEY or load from a .env file
# import dotenv
# dotenv.load_dotenv()


import
 os
os
.
environ
[
"LANGSMITH_TRACING"
]
 
=
 
"true"


First we load in our documents. We will use 
WebBaseLoader
 to load a blog post:


from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
loader 
=
 WebBaseLoader
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
)
docs 
=
 loader
.
load
(
)
API Reference:
WebBaseLoader


Let's next select a LLM:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Stuff: summarize in a single LLM call
​


We can use 
create_stuff_documents_chain
, especially if using larger context window models such as:




128k token OpenAI 
gpt-4o


200k token Anthropic 
claude-3-5-sonnet-20240620




The chain will take a list of documents, insert them all into a prompt, and pass that prompt to an LLM:


from
 langchain
.
chains
.
combine_documents 
import
 create_stuff_documents_chain
from
 langchain
.
chains
.
llm 
import
 LLMChain
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
# Define prompt
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 
"Write a concise summary of the following:\\n\\n{context}"
)
]
)
# Instantiate chain
chain 
=
 create_stuff_documents_chain
(
llm
,
 prompt
)
# Invoke chain
result 
=
 chain
.
invoke
(
{
"context"
:
 docs
}
)
print
(
result
)
API Reference:
create_stuff_documents_chain
 | 
LLMChain
 | 
ChatPromptTemplate


The article "LLM Powered Autonomous Agents" by Lilian Weng discusses the development and capabilities of autonomous agents powered by large language models (LLMs). It outlines a system architecture that includes three main components: Planning, Memory, and Tool Use. 
1. **Planning** involves task decomposition, where complex tasks are broken down into manageable subgoals, and self-reflection, allowing agents to learn from past actions to improve future performance. Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are highlighted for enhancing reasoning and planning.
2. **Memory** is categorized into short-term and long-term memory, with mechanisms for fast retrieval using Maximum Inner Product Search (MIPS) algorithms. This allows agents to retain and recall information effectively.
3. **Tool Use** enables agents to interact with external APIs and tools, enhancing their capabilities beyond the limitations of their training data. Examples include MRKL systems and frameworks like HuggingGPT, which facilitate task planning and execution.
The article also addresses challenges such as finite context length, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes with case studies demonstrating the practical applications of these concepts in scientific discovery and interactive simulations. Overall, the article emphasizes the potential of LLMs as powerful problem solvers in autonomous agent systems.


Streaming
​


Note that we can also stream the result token-by-token:


for
 token 
in
 chain
.
stream
(
{
"context"
:
 docs
}
)
:
    
print
(
token
,
 end
=
"|"
)


|The| article| "|LL|M| Powered| Autonomous| Agents|"| by| Lil|ian| W|eng| discusses| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| outlines| a| system| architecture| that| includes| three| main| components|:| Planning|,| Memory|,| and| Tool| Use|.| 
|1|.| **|Planning|**| involves| task| decomposition|,| where| complex| tasks| are| broken| down| into| manageable| sub|go|als|,| and| self|-ref|lection|,| allowing| agents| to| learn| from| past| actions| to| improve| future| performance|.| Techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|)| are| highlighted| for| enhancing| reasoning| and| planning|.
|2|.| **|Memory|**| is| categorized| into| short|-term| and| long|-term| memory|,| with| mechanisms| for| fast| retrieval| using| Maximum| Inner| Product| Search| (|M|IPS|)| algorithms|.| This| allows| agents| to| retain| and| recall| information| effectively|.
|3|.| **|Tool| Use|**| emphasizes| the| integration| of| external| APIs| and| tools| to| extend| the| capabilities| of| L|LM|s|,| enabling| them| to| perform| tasks| beyond| their| inherent| limitations|.| Examples| include| MR|KL| systems| and| frameworks| like| Hug|ging|GPT|,| which| facilitate| task| planning| and| execution|.
|The| article| also| addresses| challenges| such| as| finite| context| length|,| difficulties| in| long|-term| planning|,| and| the| reliability| of| natural| language| interfaces|.| It| concludes| with| case| studies| demonstrating| the| practical| applications| of| L|LM|-powered| agents| in| scientific| discovery| and| interactive| simulations|.| Overall|,| the| piece| illustrates| the| potential| of| L|LM|s| as| general| problem| sol|vers| and| their| evolving| role| in| autonomous| systems|.||


Go deeper
​




You can easily customize the prompt.


You can easily try different LLMs, (e.g., 
Claude
) via the 
llm
 parameter.




Map-Reduce: summarize long texts via parallelization
​


Let's unpack the map reduce approach. For this, we'll first map each document to an individual summary using an LLM. Then we'll reduce or consolidate those summaries into a single global summary.


Note that the map step is typically parallelized over the input documents.


LangGraph
, built on top of 
langchain-core
, supports 
map-reduce
 workflows and is well-suited to this problem:




LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;


LangGraph's 
checkpointing
 supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.


The LangGraph implementation is straightforward to modify and extend, as we will see below.




Map
​


Let's first define the prompt associated with the map step. We can use the same summarization prompt as in the 
stuff
 approach, above:


from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
map_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"system"
,
 
"Write a concise summary of the following:\\n\\n{context}"
)
]
)
API Reference:
ChatPromptTemplate


We can also use the Prompt Hub to store and fetch prompts.


This will work with your 
LangSmith API key
.


For example, see the map prompt 
here
.


from
 langchain 
import
 hub
map_prompt 
=
 hub
.
pull
(
"rlm/map-prompt"
)
API Reference:
hub


Reduce
​


We also define a prompt that takes the document mapping results and reduces them into a single output.


# Also available via the hub: `hub.pull("rlm/reduce-prompt")`
reduce_template 
=
 
"""
The following is a set of summaries:
{docs}
Take these and distill it into a final, consolidated summary
of the main themes.
"""
reduce_prompt 
=
 ChatPromptTemplate
(
[
(
"human"
,
 reduce_template
)
]
)


Orchestration via LangGraph
​


Below we implement a simple application that maps the summarization step on a list of documents, then reduces them using the above prompts.


Map-reduce flows are particularly useful when texts are long compared to the context window of a LLM. For long texts, we need a mechanism that ensures that the context to be summarized in the reduce step does not exceed a model's context window size. Here we implement a recursive "collapsing" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.


First we chunk the blog post into smaller "sub documents" to be mapped:


from
 langchain_text_splitters 
import
 CharacterTextSplitter
text_splitter 
=
 CharacterTextSplitter
.
from_tiktoken_encoder
(
    chunk_size
=
1000
,
 chunk_overlap
=
0
)
split_docs 
=
 text_splitter
.
split_documents
(
docs
)
print
(
f"Generated 
{
len
(
split_docs
)
}
 documents."
)
API Reference:
CharacterTextSplitter


Created a chunk of size 1003, which is longer than the specified 1000
``````output
Generated 14 documents.


Next, we define our graph. Note that we define an artificially low maximum token length of 1,000 tokens to illustrate the "collapsing" step.


import
 operator
from
 typing 
import
 Annotated
,
 List
,
 Literal
,
 TypedDict
from
 langchain
.
chains
.
combine_documents
.
reduce
 
import
 
(
    acollapse_docs
,
    split_list_of_docs
,
)
from
 langchain_core
.
documents 
import
 Document
from
 langgraph
.
constants 
import
 Send
from
 langgraph
.
graph 
import
 END
,
 START
,
 StateGraph
token_max 
=
 
1000
def
 
length_function
(
documents
:
 List
[
Document
]
)
 
-
>
 
int
:
    
"""Get number of tokens for input contents."""
    
return
 
sum
(
llm
.
get_num_tokens
(
doc
.
page_content
)
 
for
 doc 
in
 documents
)
# This will be the overall state of the main graph.
# It will contain the input document contents, corresponding
# summaries, and a final summary.
class
 
OverallState
(
TypedDict
)
:
    
# Notice here we use the operator.add
    
# This is because we want combine all the summaries we generate
    
# from individual nodes back into one list - this is essentially
    
# the "reduce" part
    contents
:
 List
[
str
]
    summaries
:
 Annotated
[
list
,
 operator
.
add
]
    collapsed_summaries
:
 List
[
Document
]
    final_summary
:
 
str
# This will be the state of the node that we will "map" all
# documents to in order to generate summaries
class
 
SummaryState
(
TypedDict
)
:
    content
:
 
str
# Here we generate a summary, given a document
async
 
def
 
generate_summary
(
state
:
 SummaryState
)
:
    prompt 
=
 map_prompt
.
invoke
(
state
[
"content"
]
)
    response 
=
 
await
 llm
.
ainvoke
(
prompt
)
    
return
 
{
"summaries"
:
 
[
response
.
content
]
}
# Here we define the logic to map out over the documents
# We will use this an edge in the graph
def
 
map_summaries
(
state
:
 OverallState
)
:
    
# We will return a list of `Send` objects
    
# Each `Send` object consists of the name of a node in the graph
    
# as well as the state to send to that node
    
return
 
[
        Send
(
"generate_summary"
,
 
{
"content"
:
 content
}
)
 
for
 content 
in
 state
[
"contents"
]
    
]
def
 
collect_summaries
(
state
:
 OverallState
)
:
    
return
 
{
        
"collapsed_summaries"
:
 
[
Document
(
summary
)
 
for
 summary 
in
 state
[
"summaries"
]
]
    
}
async
 
def
 
_reduce
(
input
:
 
dict
)
 
-
>
 
str
:
    prompt 
=
 reduce_prompt
.
invoke
(
input
)
    response 
=
 
await
 llm
.
ainvoke
(
prompt
)
    
return
 response
.
content
# Add node to collapse summaries
async
 
def
 
collapse_summaries
(
state
:
 OverallState
)
:
    doc_lists 
=
 split_list_of_docs
(
        state
[
"collapsed_summaries"
]
,
 length_function
,
 token_max
    
)
    results 
=
 
[
]
    
for
 doc_list 
in
 doc_lists
:
        results
.
append
(
await
 acollapse_docs
(
doc_list
,
 _reduce
)
)
    
return
 
{
"collapsed_summaries"
:
 results
}
# This represents a conditional edge in the graph that determines
# if we should collapse the summaries or not
def
 
should_collapse
(
    state
:
 OverallState
,
)
 
-
>
 Literal
[
"collapse_summaries"
,
 
"generate_final_summary"
]
:
    num_tokens 
=
 length_function
(
state
[
"collapsed_summaries"
]
)
    
if
 num_tokens 
>
 token_max
:
        
return
 
"collapse_summaries"
    
else
:
        
return
 
"generate_final_summary"
# Here we will generate the final summary
async
 
def
 
generate_final_summary
(
state
:
 OverallState
)
:
    response 
=
 
await
 _reduce
(
state
[
"collapsed_summaries"
]
)
    
return
 
{
"final_summary"
:
 response
}
# Construct the graph
# Nodes:
graph 
=
 StateGraph
(
OverallState
)
graph
.
add_node
(
"generate_summary"
,
 generate_summary
)
  
# same as before
graph
.
add_node
(
"collect_summaries"
,
 collect_summaries
)
graph
.
add_node
(
"collapse_summaries"
,
 collapse_summaries
)
graph
.
add_node
(
"generate_final_summary"
,
 generate_final_summary
)
# Edges:
graph
.
add_conditional_edges
(
START
,
 map_summaries
,
 
[
"generate_summary"
]
)
graph
.
add_edge
(
"generate_summary"
,
 
"collect_summaries"
)
graph
.
add_conditional_edges
(
"collect_summaries"
,
 should_collapse
)
graph
.
add_conditional_edges
(
"collapse_summaries"
,
 should_collapse
)
graph
.
add_edge
(
"generate_final_summary"
,
 END
)
app 
=
 graph
.
compile
(
)
API Reference:
acollapse_docs
 | 
split_list_of_docs
 | 
Document
 | 
Send
 | 
StateGraph


LangGraph allows the graph structure to be plotted to help visualize its function:


from
 IPython
.
display 
import
 Image
Image
(
app
.
get_graph
(
)
.
draw_mermaid_png
(
)
)




When running the application, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.


Note that because we have a loop in the graph, it can be helpful to specify a 
recursion_limit
 on its execution. This will raise a specific error when the specified limit is exceeded.


async
 
for
 step 
in
 app
.
astream
(
    
{
"contents"
:
 
[
doc
.
page_content 
for
 doc 
in
 split_docs
]
}
,
    
{
"recursion_limit"
:
 
10
}
,
)
:
    
print
(
list
(
step
.
keys
(
)
)
)


['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['collect_summaries']
['collapse_summaries']
['collapse_summaries']
['generate_final_summary']


print
(
step
)


{'generate_final_summary': {'final_summary': 'The consolidated summary of the main themes from the provided documents is as follows:\n\n1. **Integration of Large Language Models (LLMs) in Autonomous Agents**: The documents explore the evolving role of LLMs in autonomous systems, emphasizing their enhanced reasoning and acting capabilities through methodologies that incorporate structured planning, memory systems, and tool use.\n\n2. **Core Components of Autonomous Agents**:\n   - **Planning**: Techniques like task decomposition (e.g., Chain of Thought) and external classical planners are utilized to facilitate long-term planning by breaking down complex tasks.\n   - **Memory**: The memory system is divided into short-term (in-context learning) and long-term memory, with parallels drawn between human memory and machine learning to improve agent performance.\n   - **Tool Use**: Agents utilize external APIs and algorithms to enhance problem-solving abilities, exemplified by frameworks like HuggingGPT that manage task workflows.\n\n3. **Neuro-Symbolic Architectures**: The integration of MRKL (Modular Reasoning, Knowledge, and Language) systems combines neural and symbolic expert modules with LLMs, addressing challenges in tasks such as verbal math problem-solving.\n\n4. **Specialized Applications**: Case studies, such as ChemCrow and projects in anticancer drug discovery, demonstrate the advantages of LLMs augmented with expert tools in specialized domains.\n\n5. **Challenges and Limitations**: The documents highlight challenges such as hallucination in model outputs and the finite context length of LLMs, which affects their ability to incorporate historical information and perform self-reflection. Techniques like Chain of Hindsight and Algorithm Distillation are discussed to enhance model performance through iterative learning.\n\n6. **Structured Software Development**: A systematic approach to creating Python software projects is emphasized, focusing on defining core components, managing dependencies, and adhering to best practices for documentation.\n\nOverall, the integration of structured planning, memory systems, and advanced tool use aims to enhance the capabilities of LLM-powered autonomous agents while addressing the challenges and limitations these technologies face in real-world applications.'}}


In the corresponding 
LangSmith trace
 we can see the individual LLM calls, grouped under their respective nodes.


Go deeper
​


Customization




As shown above, you can customize the LLMs and prompts for map and reduce stages.




Real-world use-case




See 
this blog post
 case-study on analyzing user interactions (questions about LangChain documentation)!


The blog post and associated 
repo
 also introduce clustering as a means of summarization.


This opens up another path beyond the 
stuff
 or 
map-reduce
 approaches that is worth considering.






Next steps
​


We encourage you to check out the 
how-to guides
 for more detail on:




Other summarization strategies, such as 
iterative refinement


Built-in 
document loaders
 and 
text-splitters


Integrating various combine-document chains into a 
RAG application


Incorporating retrieval into a 
chatbot




and other concepts.
Edit this page
Was this page helpful?
Previous
Build a Question/Answering system over SQL data
Next
How-to guides
Concepts
Setup
Jupyter Notebook
Installation
LangSmith
Overview
Setup
Stuff: summarize in a single LLM call
Streaming
Go deeper
Map-Reduce: summarize long texts via parallelization
Map
Reduce
Orchestration via LangGraph
Go deeper
Next steps


--- docs_versions_migrating_chains_constitutional_chain.txt ---

Versions
Migrating from v0.0 chains
Migrating from ConstitutionalChain
On this page
Migrating from ConstitutionalChain


ConstitutionalChain
 allowed for a LLM to critique and revise generations based on 
principles
, structured as combinations of critique and revision requests. For example, a principle might include a request to identify harmful content, and a request to rewrite the content.


Constitutional AI principles
 are based on the 
Constitutional AI: Harmlessness from AI Feedback
 paper.


In 
ConstitutionalChain
, this structure of critique requests and associated revisions was formatted into a LLM prompt and parsed out of string responses. This is more naturally achieved via 
structured output
 features of chat models. We can construct a simple chain in 
LangGraph
 for this purpose. Some advantages of this approach include:




Leverage tool-calling capabilities of chat models that have been fine-tuned for this purpose;


Reduce parsing errors from extracting expression from a string LLM response;


Delegation of instructions to 
message roles
 (e.g., chat models can understand what a 
ToolMessage
 represents without the need for additional prompting);


Support for streaming, both of individual tokens and chain steps.




%
pip install 
-
-
upgrade 
-
-
quiet langchain
-
openai


import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


Legacy
​


Details
from
 langchain
.
chains 
import
 ConstitutionalChain
,
 LLMChain
from
 langchain
.
chains
.
constitutional_ai
.
models 
import
 ConstitutionalPrinciple
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_openai 
import
 OpenAI
llm 
=
 OpenAI
(
)
qa_prompt 
=
 PromptTemplate
(
    template
=
"Q: {question} A:"
,
    input_variables
=
[
"question"
]
,
)
qa_chain 
=
 LLMChain
(
llm
=
llm
,
 prompt
=
qa_prompt
)
constitutional_chain 
=
 ConstitutionalChain
.
from_llm
(
    llm
=
llm
,
    chain
=
qa_chain
,
    constitutional_principles
=
[
        ConstitutionalPrinciple
(
            critique_request
=
"Tell if this answer is good."
,
            revision_request
=
"Give a better answer."
,
        
)
    
]
,
    return_intermediate_steps
=
True
,
)
result 
=
 constitutional_chain
.
invoke
(
"What is the meaning of life?"
)
API Reference:
ConstitutionalChain
 | 
LLMChain
 | 
ConstitutionalPrinciple
 | 
PromptTemplate
 | 
OpenAI
result
{'question': 'What is the meaning of life?',
 'output': 'The meaning of life is a deeply personal and ever-evolving concept. It is a journey of self-discovery and growth, and can be different for each individual. Some may find meaning in relationships, others in achieving their goals, and some may never find a concrete answer. Ultimately, the meaning of life is what we make of it.',
 'initial_output': ' The meaning of life is a subjective concept that can vary from person to person. Some may believe that the purpose of life is to find happiness and fulfillment, while others may see it as a journey of self-discovery and personal growth. Ultimately, the meaning of life is something that each individual must determine for themselves.',
 'critiques_and_revisions': [('This answer is good in that it recognizes and acknowledges the subjective nature of the question and provides a valid and thoughtful response. However, it could have also mentioned that the meaning of life is a complex and deeply personal concept that can also change and evolve over time for each individual. Critique Needed.',
   'The meaning of life is a deeply personal and ever-evolving concept. It is a journey of self-discovery and growth, and can be different for each individual. Some may find meaning in relationships, others in achieving their goals, and some may never find a concrete answer. Ultimately, the meaning of life is what we make of it.')]}
Above, we've returned intermediate steps showing:


The original question;


The initial output;


Critiques and revisions;


The final output (matching a revision).




LangGraph
​


Details
Below, we use the 
.with_structured_output
 method to simultaneously generate (1) a judgment of whether a critique is needed, and (2) the critique. We surface all prompts involved for clarity and ease of customizability.
Note that we are also able to stream intermediate steps with this implementation, so we can monitor and if needed intervene during its execution.
from
 typing 
import
 List
,
 Optional
,
 Tuple
from
 langchain
.
chains
.
constitutional_ai
.
models 
import
 ConstitutionalPrinciple
from
 langchain
.
chains
.
constitutional_ai
.
prompts 
import
 
(
    CRITIQUE_PROMPT
,
    REVISION_PROMPT
,
)
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
from
 langgraph
.
graph 
import
 END
,
 START
,
 StateGraph
from
 typing_extensions 
import
 Annotated
,
 TypedDict
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
class
 
Critique
(
TypedDict
)
:
    
"""Generate a critique, if needed."""
    critique_needed
:
 Annotated
[
bool
,
 
.
.
.
,
 
"Whether or not a critique is needed."
]
    critique
:
 Annotated
[
str
,
 
.
.
.
,
 
"If needed, the critique."
]
critique_prompt 
=
 ChatPromptTemplate
.
from_template
(
    
"Critique this response according to the critique request. "
    
"If no critique is needed, specify that.\n\n"
    
"Query: {query}\n\n"
    
"Response: {response}\n\n"
    
"Critique request: {critique_request}"
)
revision_prompt 
=
 ChatPromptTemplate
.
from_template
(
    
"Revise this response according to the critique and reivsion request.\n\n"
    
"Query: {query}\n\n"
    
"Response: {response}\n\n"
    
"Critique request: {critique_request}\n\n"
    
"Critique: {critique}\n\n"
    
"If the critique does not identify anything worth changing, ignore the "
    
"revision request and return 'No revisions needed'. If the critique "
    
"does identify something worth changing, revise the response based on "
    
"the revision request.\n\n"
    
"Revision Request: {revision_request}"
)
chain 
=
 llm 
|
 StrOutputParser
(
)
critique_chain 
=
 critique_prompt 
|
 llm
.
with_structured_output
(
Critique
)
revision_chain 
=
 revision_prompt 
|
 llm 
|
 StrOutputParser
(
)
class
 
State
(
TypedDict
)
:
    query
:
 
str
    constitutional_principles
:
 List
[
ConstitutionalPrinciple
]
    initial_response
:
 
str
    critiques_and_revisions
:
 List
[
Tuple
[
str
,
 
str
]
]
    response
:
 
str
async
 
def
 
generate_response
(
state
:
 State
)
:
    
"""Generate initial response."""
    response 
=
 
await
 chain
.
ainvoke
(
state
[
"query"
]
)
    
return
 
{
"response"
:
 response
,
 
"initial_response"
:
 response
}
async
 
def
 
critique_and_revise
(
state
:
 State
)
:
    
"""Critique and revise response according to principles."""
    critiques_and_revisions 
=
 
[
]
    response 
=
 state
[
"initial_response"
]
    
for
 principle 
in
 state
[
"constitutional_principles"
]
:
        critique 
=
 
await
 critique_chain
.
ainvoke
(
            
{
                
"query"
:
 state
[
"query"
]
,
                
"response"
:
 response
,
                
"critique_request"
:
 principle
.
critique_request
,
            
}
        
)
        
if
 critique
[
"critique_needed"
]
:
            revision 
=
 
await
 revision_chain
.
ainvoke
(
                
{
                    
"query"
:
 state
[
"query"
]
,
                    
"response"
:
 response
,
                    
"critique_request"
:
 principle
.
critique_request
,
                    
"critique"
:
 critique
[
"critique"
]
,
                    
"revision_request"
:
 principle
.
revision_request
,
                
}
            
)
            response 
=
 revision
            critiques_and_revisions
.
append
(
(
critique
[
"critique"
]
,
 revision
)
)
        
else
:
            critiques_and_revisions
.
append
(
(
critique
[
"critique"
]
,
 
""
)
)
    
return
 
{
        
"critiques_and_revisions"
:
 critiques_and_revisions
,
        
"response"
:
 response
,
    
}
graph 
=
 StateGraph
(
State
)
graph
.
add_node
(
"generate_response"
,
 generate_response
)
graph
.
add_node
(
"critique_and_revise"
,
 critique_and_revise
)
graph
.
add_edge
(
START
,
 
"generate_response"
)
graph
.
add_edge
(
"generate_response"
,
 
"critique_and_revise"
)
graph
.
add_edge
(
"critique_and_revise"
,
 END
)
app 
=
 graph
.
compile
(
)
API Reference:
ConstitutionalPrinciple
 | 
CRITIQUE_PROMPT
 | 
REVISION_PROMPT
 | 
StrOutputParser
 | 
ChatPromptTemplate
 | 
ChatOpenAI
 | 
StateGraph
constitutional_principles 
=
 
[
    ConstitutionalPrinciple
(
        critique_request
=
"Tell if this answer is good."
,
        revision_request
=
"Give a better answer."
,
    
)
]
query 
=
 
"What is the meaning of life? Answer in 10 words or fewer."
async
 
for
 step 
in
 app
.
astream
(
    
{
"query"
:
 query
,
 
"constitutional_principles"
:
 constitutional_principles
}
,
    stream_mode
=
"values"
,
)
:
    subset 
=
 
[
"initial_response"
,
 
"critiques_and_revisions"
,
 
"response"
]
    
print
(
{
k
:
 v 
for
 k
,
 v 
in
 step
.
items
(
)
 
if
 k 
in
 subset
}
)
{}
{'initial_response': 'Finding purpose, connection, and joy in our experiences and relationships.', 'response': 'Finding purpose, connection, and joy in our experiences and relationships.'}
{'initial_response': 'Finding purpose, connection, and joy in our experiences and relationships.', 'critiques_and_revisions': [("The response exceeds the 10-word limit, providing a more elaborate answer than requested. A concise response, such as 'To seek purpose and joy in life,' would better align with the query.", 'To seek purpose and joy in life.')], 'response': 'To seek purpose and joy in life.'}


Next steps
​


See guides for generating structured output 
here
.


Check out the 
LangGraph documentation
 for detail on building with LangGraph.
Edit this page
Was this page helpful?
Previous
How to migrate from v0.0 chains
Next
Migrating from ConversationalChain
Legacy
LangGraph
Next steps


--- docs_versions_migrating_chains_conversation_chain.txt ---

Versions
Migrating from v0.0 chains
Migrating from ConversationalChain
On this page
Migrating from ConversationalChain


ConversationChain
 incorporated a memory of previous messages to sustain a stateful conversation.


Some advantages of switching to the Langgraph implementation are:




Innate support for threads/separate sessions. To make this work with 
ConversationChain
, you'd need to instantiate a separate memory class outside the chain.


More explicit parameters. 
ConversationChain
 contains a hidden default prompt, which can cause confusion.


Streaming support. 
ConversationChain
 only supports streaming via callbacks.




Langgraph's 
checkpointing
 system supports multiple threads or sessions, which can be specified via the 
"thread_id"
 key in its configuration parameters.


%
pip install 
-
-
upgrade 
-
-
quiet langchain langchain
-
openai


import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


Legacy
​


Details
from
 langchain
.
chains 
import
 ConversationChain
from
 langchain
.
memory 
import
 ConversationBufferMemory
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
template 
=
 
"""
You are a pirate. Answer the following questions as best you can.
Chat history: {history}
Question: {input}
"""
prompt 
=
 ChatPromptTemplate
.
from_template
(
template
)
memory 
=
 ConversationBufferMemory
(
)
chain 
=
 ConversationChain
(
    llm
=
ChatOpenAI
(
)
,
    memory
=
memory
,
    prompt
=
prompt
,
)
chain
(
{
"input"
:
 
"I'm Bob, how are you?"
}
)
API Reference:
ConversationChain
 | 
ConversationBufferMemory
 | 
ChatPromptTemplate
 | 
ChatOpenAI
{'input': "I'm Bob, how are you?",
 'history': '',
 'response': "Arrr matey, I be a pirate sailin' the high seas. What be yer business with me?"}
chain
(
{
"input"
:
 
"What is my name?"
}
)
{'input': 'What is my name?',
 'history': "Human: I'm Bob, how are you?\nAI: Arrr matey, I be a pirate sailin' the high seas. What be yer business with me?",
 'response': 'Your name be Bob, matey.'}


Langgraph
​


Details
import
 uuid
from
 langchain_openai 
import
 ChatOpenAI
from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
graph 
import
 START
,
 MessagesState
,
 StateGraph
model 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
# Define a new graph
workflow 
=
 StateGraph
(
state_schema
=
MessagesState
)
# Define the function that calls the model
def
 
call_model
(
state
:
 MessagesState
)
:
    response 
=
 model
.
invoke
(
state
[
"messages"
]
)
    
return
 
{
"messages"
:
 response
}
# Define the two nodes we will cycle between
workflow
.
add_edge
(
START
,
 
"model"
)
workflow
.
add_node
(
"model"
,
 call_model
)
# Add memory
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
checkpointer
=
memory
)
# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
thread_id 
=
 uuid
.
uuid4
(
)
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 thread_id
}
}
API Reference:
ChatOpenAI
 | 
MemorySaver
 | 
StateGraph
query 
=
 
"I'm Bob, how are you?"
input_messages 
=
 
[
    
{
        
"role"
:
 
"system"
,
        
"content"
:
 
"You are a pirate. Answer the following questions as best you can."
,
    
}
,
    
{
"role"
:
 
"user"
,
 
"content"
:
 query
}
,
]
for
 event 
in
 app
.
stream
(
{
"messages"
:
 input_messages
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
================================[1m Human Message [0m=================================
I'm Bob, how are you?
==================================[1m Ai Message [0m==================================
Ahoy, Bob! I be feelin' as lively as a ship in full sail! How be ye on this fine day?
query 
=
 
"What is my name?"
input_messages 
=
 
[
{
"role"
:
 
"user"
,
 
"content"
:
 query
}
]
for
 event 
in
 app
.
stream
(
{
"messages"
:
 input_messages
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
================================[1m Human Message [0m=================================
What is my name?
==================================[1m Ai Message [0m==================================
Ye be callin' yerself Bob, I reckon! A fine name for a swashbuckler like yerself!


Next steps
​


See 
this tutorial
 for a more end-to-end guide on building with 
RunnableWithMessageHistory
.


Check out the 
LCEL conceptual docs
 for more background information.
Edit this page
Was this page helpful?
Previous
Migrating from ConstitutionalChain
Next
Migrating from ConversationalRetrievalChain
Legacy
Langgraph
Next steps


--- docs_versions_migrating_chains_llm_chain.txt ---

Versions
Migrating from v0.0 chains
Migrating from LLMChain
On this page
Migrating from LLMChain


LLMChain
 combined a prompt template, LLM, and output parser into a class.


Some advantages of switching to the LCEL implementation are:




Clarity around contents and parameters. The legacy 
LLMChain
 contains a default output parser and other options.


Easier streaming. 
LLMChain
 only supports streaming via callbacks.


Easier access to raw message outputs if desired. 
LLMChain
 only exposes these via a parameter or via callback.




%
pip install 
-
-
upgrade 
-
-
quiet langchain
-
openai


import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


Legacy
​


Details
from
 langchain
.
chains 
import
 LLMChain
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"user"
,
 
"Tell me a {adjective} joke"
)
]
,
)
legacy_chain 
=
 LLMChain
(
llm
=
ChatOpenAI
(
)
,
 prompt
=
prompt
)
legacy_result 
=
 legacy_chain
(
{
"adjective"
:
 
"funny"
}
)
legacy_result
API Reference:
LLMChain
 | 
ChatPromptTemplate
 | 
ChatOpenAI
{'adjective': 'funny',
 'text': "Why couldn't the bicycle stand up by itself?\n\nBecause it was two tired!"}
Note that 
LLMChain
 by default returned a 
dict
 containing both the input and the output from 
StrOutputParser
, so to extract the output, you need to access the 
"text"
 key.
legacy_result
[
"text"
]
"Why couldn't the bicycle stand up by itself?\n\nBecause it was two tired!"


LCEL
​


Details
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
(
"user"
,
 
"Tell me a {adjective} joke"
)
]
,
)
chain 
=
 prompt 
|
 ChatOpenAI
(
)
 
|
 StrOutputParser
(
)
chain
.
invoke
(
{
"adjective"
:
 
"funny"
}
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
ChatOpenAI
'Why was the math book sad?\n\nBecause it had too many problems.'
If you'd like to mimic the 
dict
 packaging of input and output in 
LLMChain
, you can use a 
RunnablePassthrough.assign
 like:
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
outer_chain 
=
 RunnablePassthrough
(
)
.
assign
(
text
=
chain
)
outer_chain
.
invoke
(
{
"adjective"
:
 
"funny"
}
)
API Reference:
RunnablePassthrough
{'adjective': 'funny',
 'text': 'Why did the scarecrow win an award? Because he was outstanding in his field!'}


Next steps
​


See 
this tutorial
 for more detail on building with prompt templates, LLMs, and output parsers.


Check out the 
LCEL conceptual docs
 for more background information.
Edit this page
Was this page helpful?
Previous
Migrating from ConversationalRetrievalChain
Next
Migrating from LLMMathChain
Legacy
LCEL
Next steps


--- docs_versions_migrating_chains_llm_math_chain.txt ---

Versions
Migrating from v0.0 chains
Migrating from LLMMathChain
On this page
Migrating from LLMMathChain


LLMMathChain
 enabled the evaluation of mathematical expressions generated by a LLM. Instructions for generating the expressions were formatted into the prompt, and the expressions were parsed out of the string response before evaluation using the 
numexpr
 library.


This is more naturally achieved via 
tool calling
. We can equip a chat model with a simple calculator tool leveraging 
numexpr
 and construct a simple chain around it using 
LangGraph
. Some advantages of this approach include:




Leverage tool-calling capabilities of chat models that have been fine-tuned for this purpose;


Reduce parsing errors from extracting expression from a string LLM response;


Delegation of instructions to 
message roles
 (e.g., chat models can understand what a 
ToolMessage
 represents without the need for additional prompting);


Support for streaming, both of individual tokens and chain steps.




%
pip install 
-
-
upgrade 
-
-
quiet numexpr


import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


Legacy
​


Details
from
 langchain
.
chains 
import
 LLMMathChain
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
chain 
=
 LLMMathChain
.
from_llm
(
llm
)
chain
.
invoke
(
"What is 551368 divided by 82?"
)
API Reference:
LLMMathChain
 | 
ChatPromptTemplate
 | 
ChatOpenAI
{'question': 'What is 551368 divided by 82?', 'answer': 'Answer: 6724.0'}


LangGraph
​


Details
import
 math
from
 typing 
import
 Annotated
,
 Sequence
import
 numexpr
from
 langchain_core
.
messages 
import
 BaseMessage
from
 langchain_core
.
runnables 
import
 RunnableConfig
from
 langchain_core
.
tools 
import
 tool
from
 langchain_openai 
import
 ChatOpenAI
from
 langgraph
.
graph 
import
 END
,
 StateGraph
from
 langgraph
.
graph
.
message 
import
 add_messages
from
 langgraph
.
prebuilt
.
tool_node 
import
 ToolNode
from
 typing_extensions 
import
 TypedDict
@tool
def
 
calculator
(
expression
:
 
str
)
 
-
>
 
str
:
    
"""Calculate expression using Python's numexpr library.
    Expression should be a single line mathematical expression
    that solves the problem.
    Examples:
        "37593 * 67" for "37593 times 67"
        "37593**(1/5)" for "37593^(1/5)"
    """
    local_dict 
=
 
{
"pi"
:
 math
.
pi
,
 
"e"
:
 math
.
e
}
    
return
 
str
(
        numexpr
.
evaluate
(
            expression
.
strip
(
)
,
            global_dict
=
{
}
,
  
# restrict access to globals
            local_dict
=
local_dict
,
  
# add common mathematical functions
        
)
    
)
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
tools 
=
 
[
calculator
]
llm_with_tools 
=
 llm
.
bind_tools
(
tools
,
 tool_choice
=
"any"
)
class
 
ChainState
(
TypedDict
)
:
    
"""LangGraph state."""
    messages
:
 Annotated
[
Sequence
[
BaseMessage
]
,
 add_messages
]
async
 
def
 
acall_chain
(
state
:
 ChainState
,
 config
:
 RunnableConfig
)
:
    last_message 
=
 state
[
"messages"
]
[
-
1
]
    response 
=
 
await
 llm_with_tools
.
ainvoke
(
state
[
"messages"
]
,
 config
)
    
return
 
{
"messages"
:
 
[
response
]
}
async
 
def
 
acall_model
(
state
:
 ChainState
,
 config
:
 RunnableConfig
)
:
    response 
=
 
await
 llm
.
ainvoke
(
state
[
"messages"
]
,
 config
)
    
return
 
{
"messages"
:
 
[
response
]
}
graph_builder 
=
 StateGraph
(
ChainState
)
graph_builder
.
add_node
(
"call_tool"
,
 acall_chain
)
graph_builder
.
add_node
(
"execute_tool"
,
 ToolNode
(
tools
)
)
graph_builder
.
add_node
(
"call_model"
,
 acall_model
)
graph_builder
.
set_entry_point
(
"call_tool"
)
graph_builder
.
add_edge
(
"call_tool"
,
 
"execute_tool"
)
graph_builder
.
add_edge
(
"execute_tool"
,
 
"call_model"
)
graph_builder
.
add_edge
(
"call_model"
,
 END
)
chain 
=
 graph_builder
.
compile
(
)
API Reference:
BaseMessage
 | 
RunnableConfig
 | 
tool
 | 
ChatOpenAI
 | 
StateGraph
 | 
add_messages
 | 
ToolNode
# Visualize chain:
from
 IPython
.
display 
import
 Image
Image
(
chain
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
# Stream chain steps:
example_query 
=
 
"What is 551368 divided by 82"
events 
=
 chain
.
astream
(
    
{
"messages"
:
 
[
(
"user"
,
 example_query
)
]
}
,
    stream_mode
=
"values"
,
)
async
 
for
 event 
in
 events
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
================================[1m Human Message [0m=================================
What is 551368 divided by 82
==================================[1m Ai Message [0m==================================
Tool Calls:
  calculator (call_1ic3gjuII0Aq9vxlSYiwvjSb)
 Call ID: call_1ic3gjuII0Aq9vxlSYiwvjSb
  Args:
    expression: 551368 / 82
=================================[1m Tool Message [0m=================================
Name: calculator
6724.0
==================================[1m Ai Message [0m==================================
551368 divided by 82 equals 6724.


Next steps
​


See guides for building and working with tools 
here
.


Check out the 
LangGraph documentation
 for detail on building with LangGraph.
Edit this page
Was this page helpful?
Previous
Migrating from LLMChain
Next
Migrating from LLMRouterChain
Legacy
LangGraph
Next steps


--- docs_versions_migrating_chains_llm_router_chain.txt ---

Versions
Migrating from v0.0 chains
Migrating from LLMRouterChain
On this page
Migrating from LLMRouterChain


The 
LLMRouterChain
 routed an input query to one of multiple destinations-- that is, given an input query, it used a LLM to select from a list of destination chains, and passed its inputs to the selected chain.


LLMRouterChain
 does not support common 
chat model
 features, such as message roles and 
tool calling
. Under the hood, 
LLMRouterChain
 routes a query by instructing the LLM to generate JSON-formatted text, and parsing out the intended destination.


Consider an example from a 
MultiPromptChain
, which uses 
LLMRouterChain
. Below is an (example) default prompt:


from
 langchain
.
chains
.
router
.
multi_prompt 
import
 MULTI_PROMPT_ROUTER_TEMPLATE
destinations 
=
 
"""
animals: prompt for animal expert
vegetables: prompt for a vegetable expert
"""
router_template 
=
 MULTI_PROMPT_ROUTER_TEMPLATE
.
format
(
destinations
=
destinations
)
print
(
router_template
.
replace
(
"`"
,
 
"'"
)
)
  
# for rendering purposes


Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.
<< FORMATTING >>
Return a markdown code snippet with a JSON object formatted to look like:
'''json
{{
    "destination": string \ name of the prompt to use or "DEFAULT"
    "next_inputs": string \ a potentially modified version of the original input
}}
'''
REMEMBER: "destination" MUST be one of the candidate prompt names specified below OR it can be "DEFAULT" if the input is not well suited for any of the candidate prompts.
REMEMBER: "next_inputs" can just be the original input if you don't think any modifications are needed.
<< CANDIDATE PROMPTS >>
animals: prompt for animal expert
vegetables: prompt for a vegetable expert
<< INPUT >>
{input}
<< OUTPUT (must include '''json at the start of the response) >>
<< OUTPUT (must end with ''') >>


Most of the behavior is determined via a single natural language prompt. Chat models that support 
tool calling
 features confer a number of advantages for this task:




Supports chat prompt templates, including messages with 
system
 and other roles;


Tool-calling models are fine-tuned to generate structured output;


Support for runnable methods like streaming and async operations.




Now let's look at 
LLMRouterChain
 side-by-side with an LCEL implementation that uses tool-calling. Note that for this guide we will 
langchain-openai >= 0.1.20
:


%
pip install 
-
qU langchain
-
core langchain
-
openai


import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


Legacy
​


Details
from
 langchain
.
chains
.
router
.
llm_router 
import
 LLMRouterChain
,
 RouterOutputParser
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
router_prompt 
=
 PromptTemplate
(
    
# Note: here we use the prompt template from above. Generally this would need
    
# to be customized.
    template
=
router_template
,
    input_variables
=
[
"input"
]
,
    output_parser
=
RouterOutputParser
(
)
,
)
chain 
=
 LLMRouterChain
.
from_llm
(
llm
,
 router_prompt
)
API Reference:
LLMRouterChain
 | 
RouterOutputParser
 | 
PromptTemplate
 | 
ChatOpenAI
result 
=
 chain
.
invoke
(
{
"input"
:
 
"What color are carrots?"
}
)
print
(
result
[
"destination"
]
)
vegetables


LCEL
​


Details
from
 operator 
import
 itemgetter
from
 typing 
import
 Literal
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnablePassthrough
from
 langchain_openai 
import
 ChatOpenAI
from
 typing_extensions 
import
 TypedDict
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
route_system 
=
 
"Route the user's query to either the animal or vegetable expert."
route_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 route_system
)
,
        
(
"human"
,
 
"{input}"
)
,
    
]
)
# Define schema for output:
class
 
RouteQuery
(
TypedDict
)
:
    
"""Route query to destination expert."""
    destination
:
 Literal
[
"animal"
,
 
"vegetable"
]
# Instead of writing formatting instructions into the prompt, we
# leverage .with_structured_output to coerce the output into a simple
# schema.
chain 
=
 route_prompt 
|
 llm
.
with_structured_output
(
RouteQuery
)
API Reference:
ChatPromptTemplate
 | 
RunnablePassthrough
 | 
ChatOpenAI
result 
=
 chain
.
invoke
(
{
"input"
:
 
"What color are carrots?"
}
)
print
(
result
[
"destination"
]
)
vegetable


Next steps
​


See 
this tutorial
 for more detail on building with prompt templates, LLMs, and output parsers.


Check out the 
LCEL conceptual docs
 for more background information.
Edit this page
Was this page helpful?
Previous
Migrating from LLMMathChain
Next
Migrating from MapReduceDocumentsChain
Legacy
LCEL
Next steps


--- docs_versions_migrating_chains_map_reduce_chain.txt ---

Versions
Migrating from v0.0 chains
Migrating from MapReduceDocumentsChain
On this page
Migrating from MapReduceDocumentsChain


MapReduceDocumentsChain
 implements a map-reduce strategy over (potentially long) texts. The strategy is as follows:




Split a text into smaller documents;


Map a process onto the smaller documents;


Reduce or consolidate the results of the process into a final result.




Note that the map step is typically parallelized over the input documents.


A common process applied in this context is summarization, in which the map step summarizes individual documents, and the reduce step generates a summary of the summaries.


In the reduce step, 
MapReduceDocumentsChain
 supports a recursive "collapsing" of the summaries: the inputs would be partitioned based on a token limit, and summaries would be generated of the partitions. This step would be repeated until the total length of the summaries was within a desired limit, allowing for the summarization of arbitrary-length text. This is particularly useful for models with smaller context windows.


LangGraph supports 
map-reduce
 workflows, and confers a number of advantages for this problem:




LangGraph allows for individual steps (such as successive summarizations) to be streamed, allowing for greater control of execution;


LangGraph's 
checkpointing
 supports error recovery, extending with human-in-the-loop workflows, and easier incorporation into conversational applications.


The LangGraph implementation is easier to extend, as we will see below.




Below we will go through both 
MapReduceDocumentsChain
 and a corresponding LangGraph implementation, first on a simple example for illustrative purposes, and second on a longer example text to demonstrate the recursive reduce step.


Let's first load a chat model:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Basic example (short documents)
​


Let's use the following 3 documents for illustrative purposes.


from
 langchain_core
.
documents 
import
 Document
documents 
=
 
[
    Document
(
page_content
=
"Apples are red"
,
 metadata
=
{
"title"
:
 
"apple_book"
}
)
,
    Document
(
page_content
=
"Blueberries are blue"
,
 metadata
=
{
"title"
:
 
"blueberry_book"
}
)
,
    Document
(
page_content
=
"Bananas are yelow"
,
 metadata
=
{
"title"
:
 
"banana_book"
}
)
,
]
API Reference:
Document


Legacy
​


Details
Below we show an implementation with 
MapReduceDocumentsChain
. We define the prompt templates for the map and reduce steps, instantiate separate chains for these steps, and finally instantiate the 
MapReduceDocumentsChain
:
from
 langchain
.
chains 
import
 MapReduceDocumentsChain
,
 ReduceDocumentsChain
from
 langchain
.
chains
.
combine_documents
.
stuff 
import
 StuffDocumentsChain
from
 langchain
.
chains
.
llm 
import
 LLMChain
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_text_splitters 
import
 CharacterTextSplitter
# Map
map_template 
=
 
"Write a concise summary of the following: {docs}."
map_prompt 
=
 ChatPromptTemplate
(
[
(
"human"
,
 map_template
)
]
)
map_chain 
=
 LLMChain
(
llm
=
llm
,
 prompt
=
map_prompt
)
# Reduce
reduce_template 
=
 
"""
The following is a set of summaries:
{docs}
Take these and distill it into a final, consolidated summary
of the main themes.
"""
reduce_prompt 
=
 ChatPromptTemplate
(
[
(
"human"
,
 reduce_template
)
]
)
reduce_chain 
=
 LLMChain
(
llm
=
llm
,
 prompt
=
reduce_prompt
)
# Takes a list of documents, combines them into a single string, and passes this to an LLMChain
combine_documents_chain 
=
 StuffDocumentsChain
(
    llm_chain
=
reduce_chain
,
 document_variable_name
=
"docs"
)
# Combines and iteratively reduces the mapped documents
reduce_documents_chain 
=
 ReduceDocumentsChain
(
    
# This is final chain that is called.
    combine_documents_chain
=
combine_documents_chain
,
    
# If documents exceed context for `StuffDocumentsChain`
    collapse_documents_chain
=
combine_documents_chain
,
    
# The maximum number of tokens to group documents into.
    token_max
=
1000
,
)
# Combining documents by mapping a chain over them, then combining results
map_reduce_chain 
=
 MapReduceDocumentsChain
(
    
# Map chain
    llm_chain
=
map_chain
,
    
# Reduce chain
    reduce_documents_chain
=
reduce_documents_chain
,
    
# The variable name in the llm_chain to put the documents in
    document_variable_name
=
"docs"
,
    
# Return the results of the map steps in the output
    return_intermediate_steps
=
False
,
)
API Reference:
MapReduceDocumentsChain
 | 
ReduceDocumentsChain
 | 
StuffDocumentsChain
 | 
LLMChain
 | 
ChatPromptTemplate
 | 
CharacterTextSplitter
result 
=
 map_reduce_chain
.
invoke
(
documents
)
print
(
result
[
"output_text"
]
)
Fruits come in a variety of colors, with apples being red, blueberries being blue, and bananas being yellow.
In the 
LangSmith trace
 we observe four LLM calls: one summarizing each of the three input documents, and one summarizing the summaries.


LangGraph
​


Below we show a LangGraph implementation, using the same prompt templates as above. The graph includes a node for generating summaries which is mapped across a list of input documents. This node then flows to a second node that generates the final summary.


Details
We will need to install 
langgraph
:
%
pip install 
-
qU langgraph
import
 operator
from
 typing 
import
 Annotated
,
 List
,
 TypedDict
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langgraph
.
constants 
import
 Send
from
 langgraph
.
graph 
import
 END
,
 START
,
 StateGraph
map_template 
=
 
"Write a concise summary of the following: {context}."
reduce_template 
=
 
"""
The following is a set of summaries:
{docs}
Take these and distill it into a final, consolidated summary
of the main themes.
"""
map_prompt 
=
 ChatPromptTemplate
(
[
(
"human"
,
 map_template
)
]
)
reduce_prompt 
=
 ChatPromptTemplate
(
[
(
"human"
,
 reduce_template
)
]
)
map_chain 
=
 map_prompt 
|
 llm 
|
 StrOutputParser
(
)
reduce_chain 
=
 reduce_prompt 
|
 llm 
|
 StrOutputParser
(
)
# Graph components: define the components that will make up the graph
# This will be the overall state of the main graph.
# It will contain the input document contents, corresponding
# summaries, and a final summary.
class
 
OverallState
(
TypedDict
)
:
    
# Notice here we use the operator.add
    
# This is because we want combine all the summaries we generate
    
# from individual nodes back into one list - this is essentially
    
# the "reduce" part
    contents
:
 List
[
str
]
    summaries
:
 Annotated
[
list
,
 operator
.
add
]
    final_summary
:
 
str
# This will be the state of the node that we will "map" all
# documents to in order to generate summaries
class
 
SummaryState
(
TypedDict
)
:
    content
:
 
str
# Here we generate a summary, given a document
async
 
def
 
generate_summary
(
state
:
 SummaryState
)
:
    response 
=
 
await
 map_chain
.
ainvoke
(
state
[
"content"
]
)
    
return
 
{
"summaries"
:
 
[
response
]
}
# Here we define the logic to map out over the documents
# We will use this an edge in the graph
def
 
map_summaries
(
state
:
 OverallState
)
:
    
# We will return a list of `Send` objects
    
# Each `Send` object consists of the name of a node in the graph
    
# as well as the state to send to that node
    
return
 
[
        Send
(
"generate_summary"
,
 
{
"content"
:
 content
}
)
 
for
 content 
in
 state
[
"contents"
]
    
]
# Here we will generate the final summary
async
 
def
 
generate_final_summary
(
state
:
 OverallState
)
:
    response 
=
 
await
 reduce_chain
.
ainvoke
(
state
[
"summaries"
]
)
    
return
 
{
"final_summary"
:
 response
}
# Construct the graph: here we put everything together to construct our graph
graph 
=
 StateGraph
(
OverallState
)
graph
.
add_node
(
"generate_summary"
,
 generate_summary
)
graph
.
add_node
(
"generate_final_summary"
,
 generate_final_summary
)
graph
.
add_conditional_edges
(
START
,
 map_summaries
,
 
[
"generate_summary"
]
)
graph
.
add_edge
(
"generate_summary"
,
 
"generate_final_summary"
)
graph
.
add_edge
(
"generate_final_summary"
,
 END
)
app 
=
 graph
.
compile
(
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
Send
 | 
StateGraph
from
 IPython
.
display 
import
 Image
Image
(
app
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
Note that calling the graph in streaming mode allows us to monitor steps and potentially take action on them during execution.
# Call the graph:
async
 
for
 step 
in
 app
.
astream
(
{
"contents"
:
 
[
doc
.
page_content 
for
 doc 
in
 documents
]
}
)
:
    
print
(
step
)
{'generate_summary': {'summaries': ['Apples are typically red in color.']}}
{'generate_summary': {'summaries': ['Bananas are yellow in color.']}}
{'generate_summary': {'summaries': ['Blueberries are a type of fruit that are blue in color.']}}
{'generate_final_summary': {'final_summary': 'The main themes are the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'}}
In the 
LangSmith trace
 we recover the same four LLM calls as before.


Summarizing long documents
​


Map-reduce flows are particularly useful when texts are long compared to the context window of a LLM. 
MapReduceDocumentsChain
 supports a recursive "collapsing" of the summaries: the inputs are partitioned based on a token limit, and summaries are generated of the partitions. This step is repeated until the total length of the summaries is within a desired limit, allowing for the summarization of arbitrary-length text.


This "collapse" step is implemented as a 
while
 loop within 
MapReduceDocumentsChain
. We can demonstrate this step on a longer text, a 
LLM Powered Autonomous Agents
 blog post by Lilian Weng (as featured in the 
RAG tutorial
 and other documentation).


First we load the post and chunk it into smaller "sub documents":


from
 langchain_community
.
document_loaders 
import
 WebBaseLoader
from
 langchain_text_splitters 
import
 CharacterTextSplitter
loader 
=
 WebBaseLoader
(
"https://lilianweng.github.io/posts/2023-06-23-agent/"
)
documents 
=
 loader
.
load
(
)
text_splitter 
=
 CharacterTextSplitter
.
from_tiktoken_encoder
(
    chunk_size
=
1000
,
 chunk_overlap
=
0
)
split_docs 
=
 text_splitter
.
split_documents
(
documents
)
print
(
f"Generated 
{
len
(
split_docs
)
}
 documents."
)
API Reference:
WebBaseLoader
 | 
CharacterTextSplitter


USER_AGENT environment variable not set, consider setting it to identify your requests.
Created a chunk of size 1003, which is longer than the specified 1000
``````output
Generated 14 documents.


Legacy
​


Details
We can invoke 
MapReduceDocumentsChain
 as before:
result 
=
 map_reduce_chain
.
invoke
(
split_docs
)
print
(
result
[
"output_text"
]
)
The article discusses the use of Large Language Models (LLMs) to power autonomous agents in various tasks, showcasing their capabilities in problem-solving beyond generating written content. Key components such as planning, memory optimization, and tool use are explored, with proof-of-concept demos like AutoGPT and GPT-Engineer demonstrating the potential of LLM-powered agents. Challenges include limitations in historical information retention and natural language interface reliability, while the potential of LLMs in enhancing reasoning, problem-solving, and planning proficiency for autonomous agents is highlighted. Overall, the article emphasizes the versatility and power of LLMs in creating intelligent agents for tasks like scientific discovery and experiment design.
Consider the 
LangSmith trace
 for the above invocation. When instantiating our 
ReduceDocumentsChain
, we set a 
token_max
 of 1,000 tokens. This results in a total of 17 LLM calls:


14 calls are for summarizing the 14 sub-documents generated by our text splitter.


This generated summaries that totaled about 1,000 - 2,000 tokens. Because we set a 
token_max
 of 1,000, there are two more calls to summarize (or "collapse") these summaries.


One final call is for generating a final summary of the two "collapsed" summaries.




LangGraph
​


Details
We can extend our original map-reduce implementation in LangGraph to implement the same recursive collapsing step. We make the following changes:


Add a 
collapsed_summaries
 key to the state to store the collapsed summaries;


Update the final summarization node to summarize the collapsed summaries;


Add a 
collapse_summaries
 node that partitions a list of documents based on a token length (1,000 tokens here, as before) and generates summaries of each partition and stores the result in 
collapsed_summaries
.


We add a conditional edge from 
collapse_summaries
 to itself to form a loop: if the collapsed summaries total more than the 
token_max
, we re-run the node.
from
 typing 
import
 Literal
from
 langchain
.
chains
.
combine_documents
.
reduce
 
import
 
(
    acollapse_docs
,
    split_list_of_docs
,
)
def
 
length_function
(
documents
:
 List
[
Document
]
)
 
-
>
 
int
:
    
"""Get number of tokens for input contents."""
    
return
 
sum
(
llm
.
get_num_tokens
(
doc
.
page_content
)
 
for
 doc 
in
 documents
)
token_max 
=
 
1000
class
 
OverallState
(
TypedDict
)
:
    contents
:
 List
[
str
]
    summaries
:
 Annotated
[
list
,
 operator
.
add
]
    collapsed_summaries
:
 List
[
Document
]
  
# add key for collapsed summaries
    final_summary
:
 
str
# Add node to store summaries for collapsing
def
 
collect_summaries
(
state
:
 OverallState
)
:
    
return
 
{
        
"collapsed_summaries"
:
 
[
Document
(
summary
)
 
for
 summary 
in
 state
[
"summaries"
]
]
    
}
# Modify final summary to read off collapsed summaries
async
 
def
 
generate_final_summary
(
state
:
 OverallState
)
:
    response 
=
 
await
 reduce_chain
.
ainvoke
(
state
[
"collapsed_summaries"
]
)
    
return
 
{
"final_summary"
:
 response
}
graph 
=
 StateGraph
(
OverallState
)
graph
.
add_node
(
"generate_summary"
,
 generate_summary
)
  
# same as before
graph
.
add_node
(
"collect_summaries"
,
 collect_summaries
)
graph
.
add_node
(
"generate_final_summary"
,
 generate_final_summary
)
# Add node to collapse summaries
async
 
def
 
collapse_summaries
(
state
:
 OverallState
)
:
    doc_lists 
=
 split_list_of_docs
(
        state
[
"collapsed_summaries"
]
,
 length_function
,
 token_max
    
)
    results 
=
 
[
]
    
for
 doc_list 
in
 doc_lists
:
        results
.
append
(
await
 acollapse_docs
(
doc_list
,
 reduce_chain
.
ainvoke
)
)
    
return
 
{
"collapsed_summaries"
:
 results
}
graph
.
add_node
(
"collapse_summaries"
,
 collapse_summaries
)
def
 
should_collapse
(
    state
:
 OverallState
,
)
 
-
>
 Literal
[
"collapse_summaries"
,
 
"generate_final_summary"
]
:
    num_tokens 
=
 length_function
(
state
[
"collapsed_summaries"
]
)
    
if
 num_tokens 
>
 token_max
:
        
return
 
"collapse_summaries"
    
else
:
        
return
 
"generate_final_summary"
graph
.
add_conditional_edges
(
START
,
 map_summaries
,
 
[
"generate_summary"
]
)
graph
.
add_edge
(
"generate_summary"
,
 
"collect_summaries"
)
graph
.
add_conditional_edges
(
"collect_summaries"
,
 should_collapse
)
graph
.
add_conditional_edges
(
"collapse_summaries"
,
 should_collapse
)
graph
.
add_edge
(
"generate_final_summary"
,
 END
)
app 
=
 graph
.
compile
(
)
API Reference:
acollapse_docs
 | 
split_list_of_docs
LangGraph allows the graph structure to be plotted to help visualize its function:
from
 IPython
.
display 
import
 Image
Image
(
app
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
As before, we can stream the graph to observe its sequence of steps. Below, we will simply print out the name of the step.
Note that because we have a loop in the graph, it can be helpful to specify a 
recursion_limit
 on its execution. This is analogous to 
ReduceDocumentsChain.token_max
 to will raise a specific error when the specified limit is exceeded.
async
 
for
 step 
in
 app
.
astream
(
    
{
"contents"
:
 
[
doc
.
page_content 
for
 doc 
in
 split_docs
]
}
,
    
{
"recursion_limit"
:
 
10
}
,
)
:
    
print
(
list
(
step
.
keys
(
)
)
)
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['generate_summary']
['collect_summaries']
['collapse_summaries']
['generate_final_summary']
print
(
step
)
{'generate_final_summary': {'final_summary': 'The summaries discuss the use of Large Language Models (LLMs) to power autonomous agents in various tasks such as problem-solving, planning, and tool use. Key components like planning, memory, and task decomposition are highlighted, along with challenges such as inefficient planning and hallucination. Techniques like Algorithm Distillation and Maximum Inner Product Search are explored for optimization, while frameworks like ReAct and Reflexion show improvements in knowledge-intensive tasks. The importance of accurate interpretation of user input and well-structured code for functional autonomy is emphasized, along with the potential of LLMs in prompting, reasoning, and emergent social behavior in simulation environments. Challenges in real-world scenarios and the use of LLMs with expert-designed tools for tasks like organic synthesis and drug discovery are also discussed.'}}
In the corresponding 
LangSmith trace
 we can see the same 17 LLM calls as before, this time grouped under their respective nodes.


Next steps
​


Check out the 
LangGraph documentation
 for detail on building with LangGraph, including 
this guide
 on the details of map-reduce in LangGraph.


See 
this tutorial
 for more LLM-based summarization strategies.
Edit this page
Was this page helpful?
Previous
Migrating from LLMRouterChain
Next
Migrating from MapRerankDocumentsChain
Basic example (short documents)
Legacy
LangGraph
Summarizing long documents
Legacy
LangGraph
Next steps


--- docs_versions_migrating_chains_map_rerank_docs_chain.txt ---

Versions
Migrating from v0.0 chains
Migrating from MapRerankDocumentsChain
On this page
Migrating from MapRerankDocumentsChain


MapRerankDocumentsChain
 implements a strategy for analyzing long texts. The strategy is as follows:




Split a text into smaller documents;


Map a process to the set of documents, where the process includes generating a score;


Rank the results by score and return the maximum.




A common process in this scenario is question-answering using pieces of context from a document. Forcing the model to generate a score along with its answer helps to select for answers generated only by relevant context.


An 
LangGraph
 implementation allows for the incorporation of 
tool calling
 and other features for this problem. Below we will go through both 
MapRerankDocumentsChain
 and a corresponding LangGraph implementation on a simple example for illustrative purposes.


Example
​


Let's go through an example where we analyze a set of documents. Let's use the following 3 documents:


from
 langchain_core
.
documents 
import
 Document
documents 
=
 
[
    Document
(
page_content
=
"Alice has blue eyes"
,
 metadata
=
{
"title"
:
 
"book_chapter_2"
}
)
,
    Document
(
page_content
=
"Bob has brown eyes"
,
 metadata
=
{
"title"
:
 
"book_chapter_1"
}
)
,
    Document
(
        page_content
=
"Charlie has green eyes"
,
 metadata
=
{
"title"
:
 
"book_chapter_3"
}
    
)
,
]
API Reference:
Document


Legacy
​


Details
Below we show an implementation with 
MapRerankDocumentsChain
. We define the prompt template for a question-answering task and instantiate a 
LLMChain
 object for this purpose. We define how documents are formatted into the prompt and ensure consistency among the keys in the various prompts.
from
 langchain
.
chains 
import
 LLMChain
,
 MapRerankDocumentsChain
from
 langchain
.
output_parsers
.
regex 
import
 RegexParser
from
 langchain_core
.
prompts 
import
 PromptTemplate
from
 langchain_openai 
import
 OpenAI
document_variable_name 
=
 
"context"
llm 
=
 OpenAI
(
)
# The prompt here should take as an input variable the
# `document_variable_name`
# The actual prompt will need to be a lot more complex, this is just
# an example.
prompt_template 
=
 
(
    
"What color are Bob's eyes? "
    
"Output both your answer and a score (1-10) of how confident "
    
"you are in the format: <Answer>\nScore: <Score>.\n\n"
    
"Provide no other commentary.\n\n"
    
"Context: {context}"
)
output_parser 
=
 RegexParser
(
    regex
=
r"(.*?)\nScore: (.*)"
,
    output_keys
=
[
"answer"
,
 
"score"
]
,
)
prompt 
=
 PromptTemplate
(
    template
=
prompt_template
,
    input_variables
=
[
"context"
]
,
    output_parser
=
output_parser
,
)
llm_chain 
=
 LLMChain
(
llm
=
llm
,
 prompt
=
prompt
)
chain 
=
 MapRerankDocumentsChain
(
    llm_chain
=
llm_chain
,
    document_variable_name
=
document_variable_name
,
    rank_key
=
"score"
,
    answer_key
=
"answer"
,
)
API Reference:
LLMChain
 | 
MapRerankDocumentsChain
 | 
RegexParser
 | 
PromptTemplate
 | 
OpenAI
response 
=
 chain
.
invoke
(
documents
)
response
[
"output_text"
]
/langchain/libs/langchain/langchain/chains/llm.py:369: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(
'Brown'
Inspecting the 
LangSmith trace
 for the above run, we can see three LLM calls-- one for each document-- and that the scoring mechanism mitigated against hallucinations.


LangGraph
​


Details
Below we show a LangGraph implementation of this process. Note that our template is simplified, as we delegate the formatting instructions to the chat model's tool-calling features via the 
.with_structured_output
 method.
Here we follow a basic 
map-reduce
 workflow to execute the LLM calls in parallel.
We will need to install 
langgraph
:
pip install 
-
qU langgraph
import
 operator
from
 typing 
import
 Annotated
,
 List
,
 TypedDict
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
from
 langgraph
.
constants 
import
 Send
from
 langgraph
.
graph 
import
 END
,
 START
,
 StateGraph
class
 
AnswerWithScore
(
TypedDict
)
:
    answer
:
 
str
    score
:
 Annotated
[
int
,
 
.
.
.
,
 
"Score from 1-10."
]
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
prompt_template 
=
 
"What color are Bob's eyes?\n\n"
 
"Context: {context}"
prompt 
=
 ChatPromptTemplate
.
from_template
(
prompt_template
)
# The below chain formats context from a document into a prompt, then
# generates a response structured according to the AnswerWithScore schema.
map_chain 
=
 prompt 
|
 llm
.
with_structured_output
(
AnswerWithScore
)
# Below we define the components that will make up the graph
# This will be the overall state of the graph.
# It will contain the input document contents, corresponding
# answers with scores, and a final answer.
class
 
State
(
TypedDict
)
:
    contents
:
 List
[
str
]
    answers_with_scores
:
 Annotated
[
list
,
 operator
.
add
]
    answer
:
 
str
# This will be the state of the node that we will "map" all
# documents to in order to generate answers with scores
class
 
MapState
(
TypedDict
)
:
    content
:
 
str
# Here we define the logic to map out over the documents
# We will use this an edge in the graph
def
 
map_analyses
(
state
:
 State
)
:
    
# We will return a list of `Send` objects
    
# Each `Send` object consists of the name of a node in the graph
    
# as well as the state to send to that node
    
return
 
[
        Send
(
"generate_analysis"
,
 
{
"content"
:
 content
}
)
 
for
 content 
in
 state
[
"contents"
]
    
]
# Here we generate an answer with score, given a document
async
 
def
 
generate_analysis
(
state
:
 MapState
)
:
    response 
=
 
await
 map_chain
.
ainvoke
(
state
[
"content"
]
)
    
return
 
{
"answers_with_scores"
:
 
[
response
]
}
# Here we will select the top answer
def
 
pick_top_ranked
(
state
:
 State
)
:
    ranked_answers 
=
 
sorted
(
        state
[
"answers_with_scores"
]
,
 key
=
lambda
 x
:
 
-
int
(
x
[
"score"
]
)
    
)
    
return
 
{
"answer"
:
 ranked_answers
[
0
]
}
# Construct the graph: here we put everything together to construct our graph
graph 
=
 StateGraph
(
State
)
graph
.
add_node
(
"generate_analysis"
,
 generate_analysis
)
graph
.
add_node
(
"pick_top_ranked"
,
 pick_top_ranked
)
graph
.
add_conditional_edges
(
START
,
 map_analyses
,
 
[
"generate_analysis"
]
)
graph
.
add_edge
(
"generate_analysis"
,
 
"pick_top_ranked"
)
graph
.
add_edge
(
"pick_top_ranked"
,
 END
)
app 
=
 graph
.
compile
(
)
API Reference:
ChatPromptTemplate
 | 
ChatOpenAI
 | 
Send
 | 
StateGraph
from
 IPython
.
display 
import
 Image
Image
(
app
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
result 
=
 
await
 app
.
ainvoke
(
{
"contents"
:
 
[
doc
.
page_content 
for
 doc 
in
 documents
]
}
)
result
[
"answer"
]
{'answer': 'Bob has brown eyes.', 'score': 10}
Inspecting the 
LangSmith trace
 for the above run, we can see three LLM calls as before. Using the model's tool-calling features have also enabled us to remove the parsing step.


Next steps
​


See these 
how-to guides
 for more on question-answering tasks with RAG.


Check out the 
LangGraph documentation
 for detail on building with LangGraph, including 
this guide
 on the details of map-reduce in LangGraph.
Edit this page
Was this page helpful?
Previous
Migrating from MapReduceDocumentsChain
Next
Migrating from MultiPromptChain
Example
Legacy
LangGraph
Next steps


--- docs_versions_migrating_chains_multi_prompt_chain.txt ---

Versions
Migrating from v0.0 chains
Migrating from MultiPromptChain
On this page
Migrating from MultiPromptChain


The 
MultiPromptChain
 routed an input query to one of multiple LLMChains-- that is, given an input query, it used a LLM to select from a list of prompts, formatted the query into the prompt, and generated a response.


MultiPromptChain
 does not support common 
chat model
 features, such as message roles and 
tool calling
.


A 
LangGraph
 implementation confers a number of advantages for this problem:




Supports chat prompt templates, including messages with 
system
 and other roles;


Supports the use of tool calling for the routing step;


Supports streaming of both individual steps and output tokens.




Now let's look at them side-by-side. Note that for this guide we will 
langchain-openai >= 0.1.20


%
pip install 
-
qU langchain
-
core langchain
-
openai


import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


Legacy
​


Details
from
 langchain
.
chains
.
router
.
multi_prompt 
import
 MultiPromptChain
from
 langchain_openai 
import
 ChatOpenAI
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
prompt_1_template 
=
 
"""
You are an expert on animals. Please answer the below query:
{input}
"""
prompt_2_template 
=
 
"""
You are an expert on vegetables. Please answer the below query:
{input}
"""
prompt_infos 
=
 
[
    
{
        
"name"
:
 
"animals"
,
        
"description"
:
 
"prompt for an animal expert"
,
        
"prompt_template"
:
 prompt_1_template
,
    
}
,
    
{
        
"name"
:
 
"vegetables"
,
        
"description"
:
 
"prompt for a vegetable expert"
,
        
"prompt_template"
:
 prompt_2_template
,
    
}
,
]
chain 
=
 MultiPromptChain
.
from_prompts
(
llm
,
 prompt_infos
)
API Reference:
MultiPromptChain
 | 
ChatOpenAI
chain
.
invoke
(
{
"input"
:
 
"What color are carrots?"
}
)
{'input': 'What color are carrots?',
 'text': 'Carrots are most commonly orange, but they can also be found in a variety of other colors including purple, yellow, white, and red. The orange variety is the most popular and widely recognized.'}
In the 
LangSmith trace
 we can see the two steps of this process, including the prompts for routing the query and the final selected prompt.


LangGraph
​


Details
pip install 
-
qU langgraph
from
 operator 
import
 itemgetter
from
 typing 
import
 Literal
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnableConfig
from
 langchain_openai 
import
 ChatOpenAI
from
 langgraph
.
graph 
import
 END
,
 START
,
 StateGraph
from
 typing_extensions 
import
 TypedDict
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
)
# Define the prompts we will route to
prompt_1 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are an expert on animals."
)
,
        
(
"human"
,
 
"{input}"
)
,
    
]
)
prompt_2 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 
"You are an expert on vegetables."
)
,
        
(
"human"
,
 
"{input}"
)
,
    
]
)
# Construct the chains we will route to. These format the input query
# into the respective prompt, run it through a chat model, and cast
# the result to a string.
chain_1 
=
 prompt_1 
|
 llm 
|
 StrOutputParser
(
)
chain_2 
=
 prompt_2 
|
 llm 
|
 StrOutputParser
(
)
# Next: define the chain that selects which branch to route to.
# Here we will take advantage of tool-calling features to force
# the output to select one of two desired branches.
route_system 
=
 
"Route the user's query to either the animal or vegetable expert."
route_prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"system"
,
 route_system
)
,
        
(
"human"
,
 
"{input}"
)
,
    
]
)
# Define schema for output:
class
 
RouteQuery
(
TypedDict
)
:
    
"""Route query to destination expert."""
    destination
:
 Literal
[
"animal"
,
 
"vegetable"
]
route_chain 
=
 route_prompt 
|
 llm
.
with_structured_output
(
RouteQuery
)
# For LangGraph, we will define the state of the graph to hold the query,
# destination, and final answer.
class
 
State
(
TypedDict
)
:
    query
:
 
str
    destination
:
 RouteQuery
    answer
:
 
str
# We define functions for each node, including routing the query:
async
 
def
 
route_query
(
state
:
 State
,
 config
:
 RunnableConfig
)
:
    destination 
=
 
await
 route_chain
.
ainvoke
(
state
[
"query"
]
,
 config
)
    
return
 
{
"destination"
:
 destination
}
# And one node for each prompt
async
 
def
 
prompt_1
(
state
:
 State
,
 config
:
 RunnableConfig
)
:
    
return
 
{
"answer"
:
 
await
 chain_1
.
ainvoke
(
state
[
"query"
]
,
 config
)
}
async
 
def
 
prompt_2
(
state
:
 State
,
 config
:
 RunnableConfig
)
:
    
return
 
{
"answer"
:
 
await
 chain_2
.
ainvoke
(
state
[
"query"
]
,
 config
)
}
# We then define logic that selects the prompt based on the classification
def
 
select_node
(
state
:
 State
)
 
-
>
 Literal
[
"prompt_1"
,
 
"prompt_2"
]
:
    
if
 state
[
"destination"
]
 
==
 
"animal"
:
        
return
 
"prompt_1"
    
else
:
        
return
 
"prompt_2"
# Finally, assemble the multi-prompt chain. This is a sequence of two steps:
# 1) Select "animal" or "vegetable" via the route_chain, and collect the answer
# alongside the input query.
# 2) Route the input query to chain_1 or chain_2, based on the
# selection.
graph 
=
 StateGraph
(
State
)
graph
.
add_node
(
"route_query"
,
 route_query
)
graph
.
add_node
(
"prompt_1"
,
 prompt_1
)
graph
.
add_node
(
"prompt_2"
,
 prompt_2
)
graph
.
add_edge
(
START
,
 
"route_query"
)
graph
.
add_conditional_edges
(
"route_query"
,
 select_node
)
graph
.
add_edge
(
"prompt_1"
,
 END
)
graph
.
add_edge
(
"prompt_2"
,
 END
)
app 
=
 graph
.
compile
(
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnableConfig
 | 
ChatOpenAI
 | 
StateGraph
from
 IPython
.
display 
import
 Image
Image
(
app
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
We can invoke the chain as follows:
state 
=
 
await
 app
.
ainvoke
(
{
"query"
:
 
"what color are carrots"
}
)
print
(
state
[
"destination"
]
)
print
(
state
[
"answer"
]
)
{'destination': 'vegetable'}
Carrots are most commonly orange, but they can also come in a variety of other colors, including purple, red, yellow, and white. The different colors often indicate varying flavors and nutritional profiles. For example, purple carrots contain anthocyanins, while orange carrots are rich in beta-carotene, which is converted to vitamin A in the body.
In the 
LangSmith trace
 we can see the tool call that routed the query and the prompt that was selected to generate the answer.


Overview:
​




Under the hood, 
MultiPromptChain
 routed the query by instructing the LLM to generate JSON-formatted text, and parses out the intended destination. It took a registry of string prompt templates as input.


The LangGraph implementation, implemented above via lower-level primitives, uses tool-calling to route to arbitrary chains. In this example, the chains include chat model templates and chat models.




Next steps
​


See 
this tutorial
 for more detail on building with prompt templates, LLMs, and output parsers.


Check out the 
LangGraph documentation
 for detail on building with LangGraph.
Edit this page
Was this page helpful?
Previous
Migrating from MapRerankDocumentsChain
Next
Migrating from RefineDocumentsChain
Legacy
LangGraph
Overview:
Next steps


--- docs_versions_migrating_chains_refine_docs_chain.txt ---

Versions
Migrating from v0.0 chains
Migrating from RefineDocumentsChain
On this page
Migrating from RefineDocumentsChain


RefineDocumentsChain
 implements a strategy for analyzing long texts. The strategy is as follows:




Split a text into smaller documents;


Apply a process to the first document;


Refine or update the result based on the next document;


Repeat through the sequence of documents until finished.




A common process applied in this context is summarization, in which a running summary is modified as we proceed through chunks of a long text. This is particularly useful for texts that are large compared to the context window of a given LLM.


A 
LangGraph
 implementation confers a number of advantages for this problem:




Where 
RefineDocumentsChain
 refines the summary via a 
for
 loop inside the class, a LangGraph implementation lets you step through the execution to monitor or otherwise steer it if needed.


The LangGraph implementation supports streaming of both execution steps and individual tokens.


Because it is assembled from modular components, it is also simple to extend or modify (e.g., to incorporate 
tool calling
 or other behavior).




Below we will go through both 
RefineDocumentsChain
 and a corresponding LangGraph implementation on a simple example for illustrative purposes.


Let's first load a chat model:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Example
​


Let's go through an example where we summarize a sequence of documents. We first generate some simple documents for illustrative purposes:


from
 langchain_core
.
documents 
import
 Document
documents 
=
 
[
    Document
(
page_content
=
"Apples are red"
,
 metadata
=
{
"title"
:
 
"apple_book"
}
)
,
    Document
(
page_content
=
"Blueberries are blue"
,
 metadata
=
{
"title"
:
 
"blueberry_book"
}
)
,
    Document
(
page_content
=
"Bananas are yelow"
,
 metadata
=
{
"title"
:
 
"banana_book"
}
)
,
]
API Reference:
Document


Legacy
​


Details
Below we show an implementation with 
RefineDocumentsChain
. We define the prompt templates for the initial summarization and successive refinements, instantiate separate 
LLMChain
 objects for these two purposes, and instantiate 
RefineDocumentsChain
 with these components.
from
 langchain
.
chains 
import
 LLMChain
,
 RefineDocumentsChain
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 PromptTemplate
from
 langchain_openai 
import
 ChatOpenAI
# This controls how each document will be formatted. Specifically,
# it will be passed to `format_document` - see that function for more
# details.
document_prompt 
=
 PromptTemplate
(
    input_variables
=
[
"page_content"
]
,
 template
=
"{page_content}"
)
document_variable_name 
=
 
"context"
# The prompt here should take as an input variable the
# `document_variable_name`
summarize_prompt 
=
 ChatPromptTemplate
(
    
[
        
(
"human"
,
 
"Write a concise summary of the following: {context}"
)
,
    
]
)
initial_llm_chain 
=
 LLMChain
(
llm
=
llm
,
 prompt
=
summarize_prompt
)
initial_response_name 
=
 
"existing_answer"
# The prompt here should take as an input variable the
# `document_variable_name` as well as `initial_response_name`
refine_template 
=
 
"""
Produce a final summary.
Existing summary up to this point:
{existing_answer}
New context:
------------
{context}
------------
Given the new context, refine the original summary.
"""
refine_prompt 
=
 ChatPromptTemplate
(
[
(
"human"
,
 refine_template
)
]
)
refine_llm_chain 
=
 LLMChain
(
llm
=
llm
,
 prompt
=
refine_prompt
)
chain 
=
 RefineDocumentsChain
(
    initial_llm_chain
=
initial_llm_chain
,
    refine_llm_chain
=
refine_llm_chain
,
    document_prompt
=
document_prompt
,
    document_variable_name
=
document_variable_name
,
    initial_response_name
=
initial_response_name
,
)
API Reference:
LLMChain
 | 
RefineDocumentsChain
 | 
ChatPromptTemplate
 | 
PromptTemplate
 | 
ChatOpenAI
We can now invoke our chain:
result 
=
 chain
.
invoke
(
documents
)
result
[
"output_text"
]
'Apples are typically red in color, blueberries are blue, and bananas are yellow.'
The 
LangSmith trace
 is composed of three LLM calls: one for the initial summary, and two more updates of that summary. The process completes when we update the summary with content from the final document.


LangGraph
​


Details
Below we show a LangGraph implementation of this process:


We use the same two templates as before.


We generate a simple chain for the initial summary that plucks out the first document, formats it into a prompt and runs inference with our LLM.


We generate a second 
refine_summary_chain
 that operates on each successive document, refining the initial summary.


We will need to install 
langgraph
:
pip install 
-
qU langgraph
import
 operator
from
 typing 
import
 List
,
 Literal
,
 TypedDict
from
 langchain_core
.
output_parsers 
import
 StrOutputParser
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
runnables 
import
 RunnableConfig
from
 langchain_openai 
import
 ChatOpenAI
from
 langgraph
.
constants 
import
 Send
from
 langgraph
.
graph 
import
 END
,
 START
,
 StateGraph
llm 
=
 ChatOpenAI
(
model
=
"gpt-4o-mini"
,
 temperature
=
0
)
# Initial summary
summarize_prompt 
=
 ChatPromptTemplate
(
    
[
        
(
"human"
,
 
"Write a concise summary of the following: {context}"
)
,
    
]
)
initial_summary_chain 
=
 summarize_prompt 
|
 llm 
|
 StrOutputParser
(
)
# Refining the summary with new docs
refine_template 
=
 
"""
Produce a final summary.
Existing summary up to this point:
{existing_answer}
New context:
------------
{context}
------------
Given the new context, refine the original summary.
"""
refine_prompt 
=
 ChatPromptTemplate
(
[
(
"human"
,
 refine_template
)
]
)
refine_summary_chain 
=
 refine_prompt 
|
 llm 
|
 StrOutputParser
(
)
# For LangGraph, we will define the state of the graph to hold the query,
# destination, and final answer.
class
 
State
(
TypedDict
)
:
    contents
:
 List
[
str
]
    index
:
 
int
    summary
:
 
str
# We define functions for each node, including a node that generates
# the initial summary:
async
 
def
 
generate_initial_summary
(
state
:
 State
,
 config
:
 RunnableConfig
)
:
    summary 
=
 
await
 initial_summary_chain
.
ainvoke
(
        state
[
"contents"
]
[
0
]
,
        config
,
    
)
    
return
 
{
"summary"
:
 summary
,
 
"index"
:
 
1
}
# And a node that refines the summary based on the next document
async
 
def
 
refine_summary
(
state
:
 State
,
 config
:
 RunnableConfig
)
:
    content 
=
 state
[
"contents"
]
[
state
[
"index"
]
]
    summary 
=
 
await
 refine_summary_chain
.
ainvoke
(
        
{
"existing_answer"
:
 state
[
"summary"
]
,
 
"context"
:
 content
}
,
        config
,
    
)
    
return
 
{
"summary"
:
 summary
,
 
"index"
:
 state
[
"index"
]
 
+
 
1
}
# Here we implement logic to either exit the application or refine
# the summary.
def
 
should_refine
(
state
:
 State
)
 
-
>
 Literal
[
"refine_summary"
,
 END
]
:
    
if
 state
[
"index"
]
 
>=
 
len
(
state
[
"contents"
]
)
:
        
return
 END
    
else
:
        
return
 
"refine_summary"
graph 
=
 StateGraph
(
State
)
graph
.
add_node
(
"generate_initial_summary"
,
 generate_initial_summary
)
graph
.
add_node
(
"refine_summary"
,
 refine_summary
)
graph
.
add_edge
(
START
,
 
"generate_initial_summary"
)
graph
.
add_conditional_edges
(
"generate_initial_summary"
,
 should_refine
)
graph
.
add_conditional_edges
(
"refine_summary"
,
 should_refine
)
app 
=
 graph
.
compile
(
)
API Reference:
StrOutputParser
 | 
ChatPromptTemplate
 | 
RunnableConfig
 | 
ChatOpenAI
 | 
Send
 | 
StateGraph
from
 IPython
.
display 
import
 Image
Image
(
app
.
get_graph
(
)
.
draw_mermaid_png
(
)
)
We can step through the execution as follows, printing out the summary as it is refined:
async
 
for
 step 
in
 app
.
astream
(
    
{
"contents"
:
 
[
doc
.
page_content 
for
 doc 
in
 documents
]
}
,
    stream_mode
=
"values"
,
)
:
    
if
 summary 
:=
 step
.
get
(
"summary"
)
:
        
print
(
summary
)
Apples are typically red in color.
Apples are typically red in color, while blueberries are blue.
Apples are typically red in color, blueberries are blue, and bananas are yellow.
In the 
LangSmith trace
 we again recover three LLM calls, performing the same functions as before.
Note that we can stream tokens from the application, including from intermediate steps:
async
 
for
 event 
in
 app
.
astream_events
(
    
{
"contents"
:
 
[
doc
.
page_content 
for
 doc 
in
 documents
]
}
,
 version
=
"v2"
)
:
    kind 
=
 event
[
"event"
]
    
if
 kind 
==
 
"on_chat_model_stream"
:
        content 
=
 event
[
"data"
]
[
"chunk"
]
.
content
        
if
 content
:
            
print
(
content
,
 end
=
"|"
)
    
elif
 kind 
==
 
"on_chat_model_end"
:
        
print
(
"\n\n"
)
Ap|ples| are| characterized| by| their| red| color|.|
Ap|ples| are| characterized| by| their| red| color|,| while| blueberries| are| known| for| their| blue| hue|.|
Ap|ples| are| characterized| by| their| red| color|,| blueberries| are| known| for| their| blue| hue|,| and| bananas| are| recognized| for| their| yellow| color|.|


Next steps
​


See 
this tutorial
 for more LLM-based summarization strategies.


Check out the 
LangGraph documentation
 for detail on building with LangGraph.
Edit this page
Was this page helpful?
Previous
Migrating from MultiPromptChain
Next
Migrating from RetrievalQA
Example
Legacy
LangGraph
Next steps


--- docs_versions_migrating_chains_stuff_docs_chain.txt ---

Versions
Migrating from v0.0 chains
Migrating from StuffDocumentsChain
On this page
Migrating from StuffDocumentsChain


StuffDocumentsChain
 combines documents by concatenating them into a single context window. It is a straightforward and effective strategy for combining documents for question-answering, summarization, and other purposes.


create_stuff_documents_chain
 is the recommended alternative. It functions the same as 
StuffDocumentsChain
, with better support for streaming and batch functionality. Because it is a simple combination of 
LCEL primitives
, it is also easier to extend and incorporate into other LangChain applications.


Below we will go through both 
StuffDocumentsChain
 and 
create_stuff_documents_chain
 on a simple example for illustrative purposes.


Let's first load a chat model:




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
llm 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


Example
​


Let's go through an example where we analyze a set of documents. We first generate some simple documents for illustrative purposes:


from
 langchain_core
.
documents 
import
 Document
documents 
=
 
[
    Document
(
page_content
=
"Apples are red"
,
 metadata
=
{
"title"
:
 
"apple_book"
}
)
,
    Document
(
page_content
=
"Blueberries are blue"
,
 metadata
=
{
"title"
:
 
"blueberry_book"
}
)
,
    Document
(
page_content
=
"Bananas are yelow"
,
 metadata
=
{
"title"
:
 
"banana_book"
}
)
,
]
API Reference:
Document


Legacy
​


Details
Below we show an implementation with 
StuffDocumentsChain
. We define the prompt template for a summarization task and instantiate a 
LLMChain
 object for this purpose. We define how documents are formatted into the prompt and ensure consistency among the keys in the various prompts.
from
 langchain
.
chains 
import
 LLMChain
,
 StuffDocumentsChain
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
,
 PromptTemplate
# This controls how each document will be formatted. Specifically,
# it will be passed to `format_document` - see that function for more
# details.
document_prompt 
=
 PromptTemplate
(
    input_variables
=
[
"page_content"
]
,
 template
=
"{page_content}"
)
document_variable_name 
=
 
"context"
# The prompt here should take as an input variable the
# `document_variable_name`
prompt 
=
 ChatPromptTemplate
.
from_template
(
"Summarize this content: {context}"
)
llm_chain 
=
 LLMChain
(
llm
=
llm
,
 prompt
=
prompt
)
chain 
=
 StuffDocumentsChain
(
    llm_chain
=
llm_chain
,
    document_prompt
=
document_prompt
,
    document_variable_name
=
document_variable_name
,
)
API Reference:
LLMChain
 | 
StuffDocumentsChain
 | 
ChatPromptTemplate
 | 
PromptTemplate
We can now invoke our chain:
result 
=
 chain
.
invoke
(
documents
)
result
[
"output_text"
]
'This content describes the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'
for
 chunk 
in
 chain
.
stream
(
documents
)
:
    
print
(
chunk
)
{'input_documents': [Document(metadata={'title': 'apple_book'}, page_content='Apples are red'), Document(metadata={'title': 'blueberry_book'}, page_content='Blueberries are blue'), Document(metadata={'title': 'banana_book'}, page_content='Bananas are yelow')], 'output_text': 'This content describes the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'}


LCEL
​


Details
Below we show an implementation using 
create_stuff_documents_chain
:
from
 langchain
.
chains
.
combine_documents 
import
 create_stuff_documents_chain
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
prompt 
=
 ChatPromptTemplate
.
from_template
(
"Summarize this content: {context}"
)
chain 
=
 create_stuff_documents_chain
(
llm
,
 prompt
)
API Reference:
create_stuff_documents_chain
 | 
ChatPromptTemplate
Invoking the chain, we obtain a similar result as before:
result 
=
 chain
.
invoke
(
{
"context"
:
 documents
}
)
result
'This content describes the colors of different fruits: apples are red, blueberries are blue, and bananas are yellow.'
Note that this implementation supports streaming of output tokens:
for
 chunk 
in
 chain
.
stream
(
{
"context"
:
 documents
}
)
:
    
print
(
chunk
,
 end
=
" | "
)
 | This |  content |  describes |  the |  colors |  of |  different |  fruits | : |  apples |  are |  red | , |  blue | berries |  are |  blue | , |  and |  bananas |  are |  yellow | . |  |


Next steps
​


Check out the 
LCEL conceptual docs
 for more background information.


See these 
how-to guides
 for more on question-answering tasks with RAG.


See 
this tutorial
 for more LLM-based summarization strategies.
Edit this page
Was this page helpful?
Previous
Migrating from RetrievalQA
Next
How to migrate to LangGraph memory
Example
Legacy
LCEL
Next steps


--- docs_versions_migrating_memory.txt ---

Versions
Upgrading to LangGraph memory
On this page
How to migrate to LangGraph memory


As of the v0.3 release of LangChain, we recommend that LangChain users take advantage of 
LangGraph persistence
 to incorporate 
memory
 into their LangChain application.




Users that rely on 
RunnableWithMessageHistory
 or 
BaseChatMessageHistory
 do 
not
 need to make any changes, but are encouraged to consider using LangGraph for more complex use cases.


Users that rely on deprecated memory abstractions from LangChain 0.0.x should follow this guide to upgrade to the new LangGraph persistence feature in LangChain 0.3.x.




Why use LangGraph for memory?
​


The main advantages of persistence in LangGraph are:




Built-in support for multiple users and conversations, which is a typical requirement for real-world conversational AI applications.


Ability to save and resume complex conversations at any point. This helps with:




Error recovery


Allowing human intervention in AI workflows


Exploring different conversation paths ("time travel")






Full compatibility with both traditional 
language models
 and modern 
chat models
. Early memory implementations in LangChain weren't designed for newer chat model APIs, causing issues with features like tool-calling. LangGraph memory can persist any custom state.


Highly customizable, allowing you to fully control how memory works and use different storage backends.




Evolution of memory in LangChain
​


The concept of memory has evolved significantly in LangChain since its initial release.


LangChain 0.0.x memory
​


Broadly speaking, LangChain 0.0.x memory was used to handle three main use cases:


Use Case
Example
Managing conversation history
Keep only the last 
n
 turns of the conversation between the user and the AI.
Extraction of structured information
Extract structured information from the conversation history, such as a list of facts learned about the user.
Composite memory implementations
Combine multiple memory sources, e.g., a list of known facts about the user along with facts learned during a given conversation.


While the LangChain 0.0.x memory abstractions were useful, they were limited in their capabilities and not well suited for real-world conversational AI applications. These memory abstractions lacked built-in support for multi-user, multi-conversation scenarios, which are essential for practical conversational AI systems.


Most of these implementations have been officially deprecated in LangChain 0.3.x in favor of LangGraph persistence.


RunnableWithMessageHistory and BaseChatMessageHistory
​


note
Please see 
How to use BaseChatMessageHistory with LangGraph
, if you would like to use 
BaseChatMessageHistory
 (with or without 
RunnableWithMessageHistory
) in LangGraph.


As of LangChain v0.1, we started recommending that users rely primarily on 
BaseChatMessageHistory
. 
BaseChatMessageHistory
 serves
as a simple persistence for storing and retrieving messages in a conversation.


At that time, the only option for orchestrating LangChain chains was via 
LCEL
. To incorporate memory with 
LCEL
, users had to use the 
RunnableWithMessageHistory
 interface. While sufficient for basic chat applications, many users found the API unintuitive and challenging to use.


As of LangChain v0.3, we recommend that 
new
 code takes advantage of LangGraph for both orchestration and persistence:




Orchestration: In LangGraph, users define 
graphs
 that specify the flow of the application. This allows users to keep using 
LCEL
 within individual nodes when 
LCEL
 is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.


Persistence: Users can rely on LangGraph's 
persistence
 to store and retrieve data. LangGraph persistence is extremely flexible and can support a much wider range of use cases than the 
RunnableWithMessageHistory
 interface.




important
If you have been using 
RunnableWithMessageHistory
 or 
BaseChatMessageHistory
, you do not need to make any changes. We do not plan on deprecating either functionality in the near future. This functionality is sufficient for simple chat applications and any code that uses 
RunnableWithMessageHistory
 will continue to work as expected.


Migrations
​


Prerequisites
These guides assume some familiarity with the following concepts:


LangGraph


v0.0.x Memory


How to add persistence ("memory") to your graph




1. Managing conversation history
​


The goal of managing conversation history is to store and retrieve the history in a way that is optimal for a chat model to use.


Often this involves trimming and / or summarizing the conversation history to keep the most relevant parts of the conversation while having the conversation fit inside the context window of the chat model.


Memory classes that fall into this category include:


Memory Type
How to Migrate
Description
ConversationBufferMemory
Link to Migration Guide
A basic memory implementation that simply stores the conversation history.
ConversationStringBufferMemory
Link to Migration Guide
A special case of 
ConversationBufferMemory
 designed for LLMs and no longer relevant.
ConversationBufferWindowMemory
Link to Migration Guide
Keeps the last 
n
 turns of the conversation. Drops the oldest turn when the buffer is full.
ConversationTokenBufferMemory
Link to Migration Guide
Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.
ConversationSummaryMemory
Link to Migration Guide
Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.
ConversationSummaryBufferMemory
Link to Migration Guide
Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.
VectorStoreRetrieverMemory
See related 
long-term memory agent tutorial
Stores the conversation history in a vector store and retrieves the most relevant parts of past conversation based on the input.


2. Extraction of structured information from the conversation history
​


Please see 
long-term memory agent tutorial
 implements an agent that can extract structured information from the conversation history.


Memory classes that fall into this category include:


Memory Type
Description
BaseEntityStore
An abstract interface that resembles a key-value store. It was used for storing structured information learned during the conversation. The information had to be represented as a dictionary of key-value pairs.
ConversationEntityMemory
Combines the ability to summarize the conversation while extracting structured information from the conversation history.


And specific backend implementations of abstractions:


Memory Type
Description
InMemoryEntityStore
An implementation of 
BaseEntityStore
 that stores the information in the literal computer memory (RAM).
RedisEntityStore
A specific implementation of 
BaseEntityStore
 that uses Redis as the backend.
SQLiteEntityStore
A specific implementation of 
BaseEntityStore
 that uses SQLite as the backend.
UpstashRedisEntityStore
A specific implementation of 
BaseEntityStore
 that uses Upstash as the backend.


These abstractions have received limited development since their initial release. This is because they generally require significant customization for a specific application to be effective, making
them less widely used than the conversation history management abstractions.


For this reason, there are no migration guides for these abstractions. If you're struggling to migrate an application
that relies on these abstractions, please:




Please review this 
Long-term memory agent tutorial
 which should provide a good starting point for how to extract structured information from the conversation history.


If you're still struggling, please open an issue on the LangChain GitHub repository, explain your use case, and we'll try to provide more guidance on how to migrate these abstractions.




The general strategy for extracting structured information from the conversation history is to use a chat model with tool calling capabilities to extract structured information from the conversation history.
The extracted information can then be saved into an appropriate data structure (e.g., a dictionary), and information from it can be retrieved and added into the prompt as needed.


3. Implementations that provide composite logic on top of one or more memory implementations
​


Memory classes that fall into this category include:


Memory Type
Description
CombinedMemory
This abstraction accepted a list of 
BaseMemory
 and fetched relevant memory information from each of them based on the input.
SimpleMemory
Used to add read-only hard-coded context. Users can simply write this information into the prompt.
ReadOnlySharedMemory
Provided a read-only view of an existing 
BaseMemory
 implementation.


These implementations did not seem to be used widely or provide significant value. Users should be able
to re-implement these without too much difficulty in custom code.


Related Resources
​


Explore persistence with LangGraph:




LangGraph quickstart tutorial


How to add persistence ("memory") to your graph


How to manage conversation history


How to add summary of the conversation history




Add persistence with simple LCEL (favor langgraph for more complex use cases):




How to add message history




Working with message history:




How to trim messages


How to filter messages


How to merge message runs


Edit this page
Was this page helpful?
Previous
Migrating from StuffDocumentsChain
Next
How to migrate to LangGraph memory
Why use LangGraph for memory?
Evolution of memory in LangChain
LangChain 0.0.x memory
RunnableWithMessageHistory and BaseChatMessageHistory
Migrations
1. Managing conversation history
2. Extraction of structured information from the conversation history
3. Implementations that provide composite logic on top of one or more memory implementations
Related Resources


--- docs_versions_migrating_memory_chat_history.txt ---

Versions
Upgrading to LangGraph memory
How to use BaseChatMessageHistory with LangGraph
On this page
How to use BaseChatMessageHistory with LangGraph


Prerequisites
This guide assumes familiarity with the following concepts:


Chat History


RunnableWithMessageHistory


LangGraph


Memory




We recommend that new LangChain applications take advantage of the 
built-in LangGraph persistence
 to implement memory.


In some situations, users may need to keep using an existing persistence solution for chat message history.


Here, we will show how to use 
LangChain chat message histories
 (implementations of 
BaseChatMessageHistory
) with LangGraph.


Set up
​


%
%
capture 
-
-
no
-
stderr
%
pip install 
-
-
upgrade 
-
-
quiet langchain
-
anthropic langgraph


import
 os
from
 getpass 
import
 getpass
if
 
"ANTHROPIC_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"ANTHROPIC_API_KEY"
]
 
=
 getpass
(
)


ChatMessageHistory
​


A message history needs to be parameterized by a conversation ID or maybe by the 2-tuple of (user ID, conversation ID).


Many of the 
LangChain chat message histories
 will have either a 
session_id
 or some 
namespace
 to allow keeping track of different conversations. Please refer to the specific implementations to check how it is parameterized.


The built-in 
InMemoryChatMessageHistory
 does not contains such a parameterization, so we'll create a dictionary to keep track of the message histories.


import
 uuid
from
 langchain_core
.
chat_history 
import
 InMemoryChatMessageHistory
chats_by_session_id 
=
 
{
}
def
 
get_chat_history
(
session_id
:
 
str
)
 
-
>
 InMemoryChatMessageHistory
:
    chat_history 
=
 chats_by_session_id
.
get
(
session_id
)
    
if
 chat_history 
is
 
None
:
        chat_history 
=
 InMemoryChatMessageHistory
(
)
        chats_by_session_id
[
session_id
]
 
=
 chat_history
    
return
 chat_history
API Reference:
InMemoryChatMessageHistory


Use with LangGraph
​


Next, we'll set up a basic chat bot using LangGraph. If you're not familiar with LangGraph, you should look at the following 
Quick Start Tutorial
.


We'll create a 
LangGraph node
 for the chat model, and manually manage the conversation history, taking into account the conversation ID passed as part of the RunnableConfig.


The conversation ID can be passed as either part of the RunnableConfig (as we'll do here), or as part of the 
graph state
.


import
 uuid
from
 langchain_anthropic 
import
 ChatAnthropic
from
 langchain_core
.
messages 
import
 BaseMessage
,
 HumanMessage
from
 langchain_core
.
runnables 
import
 RunnableConfig
from
 langgraph
.
graph 
import
 START
,
 MessagesState
,
 StateGraph
# Define a new graph
builder 
=
 StateGraph
(
state_schema
=
MessagesState
)
# Define a chat model
model 
=
 ChatAnthropic
(
model
=
"claude-3-haiku-20240307"
)
# Define the function that calls the model
def
 
call_model
(
state
:
 MessagesState
,
 config
:
 RunnableConfig
)
 
-
>
 
list
[
BaseMessage
]
:
    
# Make sure that config is populated with the session id
    
if
 
"configurable"
 
not
 
in
 config 
or
 
"session_id"
 
not
 
in
 config
[
"configurable"
]
:
        
raise
 ValueError
(
            
"Make sure that the config includes the following information: {'configurable': {'session_id': 'some_value'}}"
        
)
    
# Fetch the history of messages and append to it any new messages.
    chat_history 
=
 get_chat_history
(
config
[
"configurable"
]
[
"session_id"
]
)
    messages 
=
 
list
(
chat_history
.
messages
)
 
+
 state
[
"messages"
]
    ai_message 
=
 model
.
invoke
(
messages
)
    
# Finally, update the chat message history to include
    
# the new input message from the user together with the
    
# response from the model.
    chat_history
.
add_messages
(
state
[
"messages"
]
 
+
 
[
ai_message
]
)
    
return
 
{
"messages"
:
 ai_message
}
# Define the two nodes we will cycle between
builder
.
add_edge
(
START
,
 
"model"
)
builder
.
add_node
(
"model"
,
 call_model
)
graph 
=
 builder
.
compile
(
)
# Here, we'll create a unique session ID to identify the conversation
session_id 
=
 uuid
.
uuid4
(
)
config 
=
 
{
"configurable"
:
 
{
"session_id"
:
 session_id
}
}
input_message 
=
 HumanMessage
(
content
=
"hi! I'm bob"
)
for
 event 
in
 graph
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
# Here, let's confirm that the AI remembers our name!
input_message 
=
 HumanMessage
(
content
=
"what was my name?"
)
for
 event 
in
 graph
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
API Reference:
ChatAnthropic
 | 
BaseMessage
 | 
HumanMessage
 | 
RunnableConfig
 | 
StateGraph


================================[1m Human Message [0m=================================
hi! I'm bob
==================================[1m Ai Message [0m==================================
Hello Bob! It's nice to meet you. I'm Claude, an AI assistant created by Anthropic. How are you doing today?
================================[1m Human Message [0m=================================
what was my name?
==================================[1m Ai Message [0m==================================
You introduced yourself as Bob when you said "hi! I'm bob".


tip
This also supports streaming LLM content token by token if using langgraph >= 0.2.28.


from
 langchain_core
.
messages 
import
 AIMessageChunk
first 
=
 
True
for
 msg
,
 metadata 
in
 graph
.
stream
(
    
{
"messages"
:
 input_message
}
,
 config
,
 stream_mode
=
"messages"
)
:
    
if
 msg
.
content 
and
 
not
 
isinstance
(
msg
,
 HumanMessage
)
:
        
print
(
msg
.
content
,
 end
=
"|"
,
 flush
=
True
)
API Reference:
AIMessageChunk


You| sai|d your| name was Bob.|


Using With RunnableWithMessageHistory
​


This how-to guide used the 
messages
 and 
add_messages
 interface of 
BaseChatMessageHistory
 directly.


Alternatively, you can use 
RunnableWithMessageHistory
, as 
LCEL
 can be used inside any 
LangGraph node
.


To do that replace the following code:


def
 
call_model
(
state
:
 MessagesState
,
 config
:
 RunnableConfig
)
 
-
>
 
list
[
BaseMessage
]
:
    
# Make sure that config is populated with the session id
    
if
 
"configurable"
 
not
 
in
 config 
or
 
"session_id"
 
not
 
in
 config
[
"configurable"
]
:
        
raise
 ValueError
(
            
"You make sure that the config includes the following information: {'configurable': {'session_id': 'some_value'}}"
        
)
    
# Fetch the history of messages and append to it any new messages.
    chat_history 
=
 get_chat_history
(
config
[
"configurable"
]
[
"session_id"
]
)
    messages 
=
 
list
(
chat_history
.
messages
)
 
+
 state
[
"messages"
]
    ai_message 
=
 model
.
invoke
(
messages
)
    
# Finally, update the chat message history to include
    
# the new input message from the user together with the
    
# response from the model.
    chat_history
.
add_messages
(
state
[
"messages"
]
 
+
 
[
ai_message
]
)
    
# hilight-end
    
return
 
{
"messages"
:
 ai_message
}


With the corresponding instance of 
RunnableWithMessageHistory
 defined in your current application.


runnable 
=
 RunnableWithMessageHistory
(
.
.
.
)
 
# From existing code
def
 
call_model
(
state
:
 MessagesState
,
 config
:
 RunnableConfig
)
 
-
>
 
list
[
BaseMessage
]
:
    
# RunnableWithMessageHistory takes care of reading the message history
    
# and updating it with the new human message and ai response.
    ai_message 
=
 runnable
.
invoke
(
state
[
'messages'
]
,
 config
)
    
return
 
{
        
"messages"
:
 ai_message
    
}
Edit this page
Was this page helpful?
Previous
How to migrate to LangGraph memory
Next
Migrating off ConversationBufferMemory or ConversationStringBufferMemory
Set up
ChatMessageHistory
Use with LangGraph
Using With RunnableWithMessageHistory


--- docs_versions_migrating_memory_conversation_buffer_memory.txt ---

Versions
Upgrading to LangGraph memory
Migrating off ConversationBufferMemory or ConversationStringBufferMemory
On this page
Migrating off ConversationBufferMemory or ConversationStringBufferMemory


ConversationBufferMemory

and 
ConversationStringBufferMemory

were used to keep track of a conversation between a human and an ai asstistant without any additional processing.


note
The 
ConversationStringBufferMemory
 is equivalent to 
ConversationBufferMemory
 but was targeting LLMs that were not chat models.


The methods for handling conversation history using existing modern primitives are:




Using 
LangGraph persistence
 along with appropriate processing of the message history


Using LCEL with 
RunnableWithMessageHistory
 combined with appropriate processing of the message history.




Most users will find 
LangGraph persistence
 both easier to use and configure than the equivalent LCEL, especially for more complex use cases.


Set up
​


%
%
capture 
-
-
no
-
stderr
%
pip install 
-
-
upgrade 
-
-
quiet langchain
-
openai langchain


import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


Usage with LLMChain / ConversationChain
​


This section shows how to migrate off 
ConversationBufferMemory
 or 
ConversationStringBufferMemory
 that's used together with either an 
LLMChain
 or a 
ConversationChain
.


Legacy
​


Below is example usage of 
ConversationBufferMemory
 with an 
LLMChain
 or an equivalent 
ConversationChain
.


Details
from
 langchain
.
chains 
import
 LLMChain
from
 langchain
.
memory 
import
 ConversationBufferMemory
from
 langchain_core
.
messages 
import
 SystemMessage
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
prompts
.
chat 
import
 
(
    ChatPromptTemplate
,
    HumanMessagePromptTemplate
,
    MessagesPlaceholder
,
)
from
 langchain_openai 
import
 ChatOpenAI
prompt 
=
 ChatPromptTemplate
(
    
[
        MessagesPlaceholder
(
variable_name
=
"chat_history"
)
,
        HumanMessagePromptTemplate
.
from_template
(
"{text}"
)
,
    
]
)
memory 
=
 ConversationBufferMemory
(
memory_key
=
"chat_history"
,
 return_messages
=
True
)
legacy_chain 
=
 LLMChain
(
    llm
=
ChatOpenAI
(
)
,
    prompt
=
prompt
,
    memory
=
memory
,
)
legacy_result 
=
 legacy_chain
.
invoke
(
{
"text"
:
 
"my name is bob"
}
)
print
(
legacy_result
)
legacy_result 
=
 legacy_chain
.
invoke
(
{
"text"
:
 
"what was my name"
}
)
API Reference:
LLMChain
 | 
ConversationBufferMemory
 | 
SystemMessage
 | 
ChatPromptTemplate
 | 
ChatPromptTemplate
 | 
HumanMessagePromptTemplate
 | 
MessagesPlaceholder
 | 
ChatOpenAI
{'text': 'Hello Bob! How can I assist you today?', 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})]}
legacy_result
[
"text"
]
'Your name is Bob. How can I assist you today, Bob?'
note
Note that there is no support for separating conversation threads in a single memory object


LangGraph
​


The example below shows how to use LangGraph to implement a 
ConversationChain
 or 
LLMChain
 with 
ConversationBufferMemory
.


This example assumes that you're already somewhat familiar with 
LangGraph
. If you're not, then please see the 
LangGraph Quickstart Guide
 for more details.


LangGraph
 offers a lot of additional functionality (e.g., time-travel and interrupts) and will work well for other more complex (and realistic) architectures.


Details
import
 uuid
from
 IPython
.
display 
import
 Image
,
 display
from
 langchain_core
.
messages 
import
 HumanMessage
from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
graph 
import
 START
,
 MessagesState
,
 StateGraph
# Define a new graph
workflow 
=
 StateGraph
(
state_schema
=
MessagesState
)
# Define a chat model
model 
=
 ChatOpenAI
(
)
# Define the function that calls the model
def
 
call_model
(
state
:
 MessagesState
)
:
    response 
=
 model
.
invoke
(
state
[
"messages"
]
)
    
# We return a list, because this will get added to the existing list
    
return
 
{
"messages"
:
 response
}
# Define the two nodes we will cycle between
workflow
.
add_edge
(
START
,
 
"model"
)
workflow
.
add_node
(
"model"
,
 call_model
)
# Adding memory is straight forward in langgraph!
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
    checkpointer
=
memory
)
# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
# This enables a single application to manage conversations among multiple users.
thread_id 
=
 uuid
.
uuid4
(
)
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 thread_id
}
}
input_message 
=
 HumanMessage
(
content
=
"hi! I'm bob"
)
for
 event 
in
 app
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
# Here, let's confirm that the AI remembers our name!
input_message 
=
 HumanMessage
(
content
=
"what was my name?"
)
for
 event 
in
 app
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
API Reference:
HumanMessage
 | 
MemorySaver
 | 
StateGraph
================================[1m Human Message [0m=================================
hi! I'm bob
==================================[1m Ai Message [0m==================================
Hello Bob! How can I assist you today?
================================[1m Human Message [0m=================================
what was my name?
==================================[1m Ai Message [0m==================================
Your name is Bob. How can I help you today, Bob?


LCEL RunnableWithMessageHistory
​


Alternatively, if you have a simple chain, you can wrap the chat model of the chain within a 
RunnableWithMessageHistory
.


Please refer to the following 
migration guide
 for more information.


Usage with a pre-built agent
​


This example shows usage of an Agent Executor with a pre-built agent constructed using the 
create_tool_calling_agent
 function.


If you are using one of the 
old LangChain pre-built agents
, you should be able
to replace that code with the new 
langgraph pre-built agent
 which leverages
native tool calling capabilities of chat models and will likely work better out of the box.


Legacy Usage
​


Details
from
 langchain 
import
 hub
from
 langchain
.
agents 
import
 AgentExecutor
,
 create_tool_calling_agent
from
 langchain
.
memory 
import
 ConversationBufferMemory
from
 langchain_core
.
tools 
import
 tool
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
temperature
=
0
)
@tool
def
 
get_user_age
(
name
:
 
str
)
 
-
>
 
str
:
    
"""Use this tool to find the user's age."""
    
# This is a placeholder for the actual implementation
    
if
 
"bob"
 
in
 name
.
lower
(
)
:
        
return
 
"42 years old"
    
return
 
"41 years old"
tools 
=
 
[
get_user_age
]
prompt 
=
 ChatPromptTemplate
.
from_messages
(
    
[
        
(
"placeholder"
,
 
"{chat_history}"
)
,
        
(
"human"
,
 
"{input}"
)
,
        
(
"placeholder"
,
 
"{agent_scratchpad}"
)
,
    
]
)
# Construct the Tools agent
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
)
# Instantiate memory
memory 
=
 ConversationBufferMemory
(
memory_key
=
"chat_history"
,
 return_messages
=
True
)
# Create an agent
agent 
=
 create_tool_calling_agent
(
model
,
 tools
,
 prompt
)
agent_executor 
=
 AgentExecutor
(
    agent
=
agent
,
    tools
=
tools
,
    memory
=
memory
,
  
# Pass the memory to the executor
)
# Verify that the agent can use tools
print
(
agent_executor
.
invoke
(
{
"input"
:
 
"hi! my name is bob what is my age?"
}
)
)
print
(
)
# Verify that the agent has access to conversation history.
# The agent should be able to answer that the user's name is bob.
print
(
agent_executor
.
invoke
(
{
"input"
:
 
"do you remember my name?"
}
)
)
API Reference:
hub
 | 
AgentExecutor
 | 
create_tool_calling_agent
 | 
ConversationBufferMemory
 | 
tool
 | 
ChatOpenAI
{'input': 'hi! my name is bob what is my age?', 'chat_history': [HumanMessage(content='hi! my name is bob what is my age?', additional_kwargs={}, response_metadata={}), AIMessage(content='Bob, you are 42 years old.', additional_kwargs={}, response_metadata={})], 'output': 'Bob, you are 42 years old.'}
{'input': 'do you remember my name?', 'chat_history': [HumanMessage(content='hi! my name is bob what is my age?', additional_kwargs={}, response_metadata={}), AIMessage(content='Bob, you are 42 years old.', additional_kwargs={}, response_metadata={}), HumanMessage(content='do you remember my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes, your name is Bob.', additional_kwargs={}, response_metadata={})], 'output': 'Yes, your name is Bob.'}


LangGraph
​


You can follow the standard LangChain tutorial for 
building an agent
 an in depth explanation of how this works.


This example is shown here explicitly to make it easier for users to compare the legacy implementation vs. the corresponding langgraph implementation.


This example shows how to add memory to the 
pre-built react agent
 in langgraph.


For more details, please see the 
how to add memory to the prebuilt ReAct agent
 guide in langgraph.


Details
import
 uuid
from
 langchain_core
.
messages 
import
 HumanMessage
from
 langchain_core
.
tools 
import
 tool
from
 langchain_openai 
import
 ChatOpenAI
from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
prebuilt 
import
 create_react_agent
@tool
def
 
get_user_age
(
name
:
 
str
)
 
-
>
 
str
:
    
"""Use this tool to find the user's age."""
    
# This is a placeholder for the actual implementation
    
if
 
"bob"
 
in
 name
.
lower
(
)
:
        
return
 
"42 years old"
    
return
 
"41 years old"
memory 
=
 MemorySaver
(
)
model 
=
 ChatOpenAI
(
)
app 
=
 create_react_agent
(
    model
,
    tools
=
[
get_user_age
]
,
    checkpointer
=
memory
,
)
# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
# This enables a single application to manage conversations among multiple users.
thread_id 
=
 uuid
.
uuid4
(
)
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 thread_id
}
}
# Tell the AI that our name is Bob, and ask it to use a tool to confirm
# that it's capable of working like an agent.
input_message 
=
 HumanMessage
(
content
=
"hi! I'm bob. What is my age?"
)
for
 event 
in
 app
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
# Confirm that the chat bot has access to previous conversation
# and can respond to the user saying that the user's name is Bob.
input_message 
=
 HumanMessage
(
content
=
"do you remember my name?"
)
for
 event 
in
 app
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
API Reference:
HumanMessage
 | 
tool
 | 
ChatOpenAI
 | 
MemorySaver
 | 
create_react_agent
================================[1m Human Message [0m=================================
hi! I'm bob. What is my age?
==================================[1m Ai Message [0m==================================
Tool Calls:
  get_user_age (call_oEDwEbIDNdokwqhAV6Azn47c)
 Call ID: call_oEDwEbIDNdokwqhAV6Azn47c
  Args:
    name: bob
=================================[1m Tool Message [0m=================================
Name: get_user_age
42 years old
==================================[1m Ai Message [0m==================================
Bob, you are 42 years old! If you need any more assistance or information, feel free to ask.
================================[1m Human Message [0m=================================
do you remember my name?
==================================[1m Ai Message [0m==================================
Yes, your name is Bob. If you have any other questions or need assistance, feel free to ask!
If we use a different thread ID, it'll start a new conversation and the bot will not know our name!
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 
"123456789"
}
}
input_message 
=
 HumanMessage
(
content
=
"hi! do you remember my name?"
)
for
 event 
in
 app
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
================================[1m Human Message [0m=================================
hi! do you remember my name?
==================================[1m Ai Message [0m==================================
Hello! Yes, I remember your name. It's great to see you again! How can I assist you today?


Next steps
​


Explore persistence with LangGraph:




LangGraph quickstart tutorial


How to add persistence ("memory") to your graph


How to manage conversation history


How to add summary of the conversation history




Add persistence with simple LCEL (favor langgraph for more complex use cases):




How to add message history




Working with message history:




How to trim messages


How to filter messages


How to merge message runs


Edit this page
Was this page helpful?
Previous
How to use BaseChatMessageHistory with LangGraph
Next
Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory
Set up
Usage with LLMChain / ConversationChain
Legacy
LangGraph
LCEL RunnableWithMessageHistory
Usage with a pre-built agent
Legacy Usage
LangGraph
Next steps


--- docs_versions_migrating_memory_conversation_buffer_window_memory.txt ---

Versions
Upgrading to LangGraph memory
Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory
On this page
Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory


Follow this guide if you're trying to migrate off one of the old memory classes listed below:


Memory Type
Description
ConversationBufferWindowMemory
Keeps the last 
n
 messages of the conversation. Drops the oldest messages when there are more than 
n
 messages.
ConversationTokenBufferMemory
Keeps only the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.


ConversationBufferWindowMemory
 and 
ConversationTokenBufferMemory
 apply additional processing on top of the raw conversation history to trim the conversation history to a size that fits inside the context window of a chat model.


This processing functionality can be accomplished using LangChain's built-in 
trim_messages
 function.


important
We’ll begin by exploring a straightforward method that involves applying processing logic to the entire conversation history.
While this approach is easy to implement, it has a downside: as the conversation grows, so does the latency, since the logic is re-applied to all previous exchanges in the conversation at each turn.
More advanced strategies focus on incrementally updating the conversation history to avoid redundant processing.
For instance, the langgraph 
how-to guide on summarization
 demonstrates
how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.


Set up
​


%
%
capture 
-
-
no
-
stderr
%
pip install 
-
-
upgrade 
-
-
quiet langchain
-
openai langchain


import
 os
from
 getpass 
import
 getpass
if
 
"OPENAI_API_KEY"
 
not
 
in
 os
.
environ
:
    os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
(
)


Legacy usage with LLMChain / Conversation Chain
​


Details
from
 langchain
.
chains 
import
 LLMChain
from
 langchain
.
memory 
import
 ConversationBufferWindowMemory
from
 langchain_core
.
messages 
import
 SystemMessage
from
 langchain_core
.
prompts 
import
 ChatPromptTemplate
from
 langchain_core
.
prompts
.
chat 
import
 
(
    ChatPromptTemplate
,
    HumanMessagePromptTemplate
,
    MessagesPlaceholder
,
)
from
 langchain_openai 
import
 ChatOpenAI
prompt 
=
 ChatPromptTemplate
(
    
[
        SystemMessage
(
content
=
"You are a helpful assistant."
)
,
        MessagesPlaceholder
(
variable_name
=
"chat_history"
)
,
        HumanMessagePromptTemplate
.
from_template
(
"{text}"
)
,
    
]
)
memory 
=
 ConversationBufferWindowMemory
(
memory_key
=
"chat_history"
,
 return_messages
=
True
)
legacy_chain 
=
 LLMChain
(
    llm
=
ChatOpenAI
(
)
,
    prompt
=
prompt
,
    memory
=
memory
,
)
legacy_result 
=
 legacy_chain
.
invoke
(
{
"text"
:
 
"my name is bob"
}
)
print
(
legacy_result
)
legacy_result 
=
 legacy_chain
.
invoke
(
{
"text"
:
 
"what was my name"
}
)
print
(
legacy_result
)
API Reference:
LLMChain
 | 
ConversationBufferWindowMemory
 | 
SystemMessage
 | 
ChatPromptTemplate
 | 
ChatPromptTemplate
 | 
HumanMessagePromptTemplate
 | 
MessagesPlaceholder
 | 
ChatOpenAI
{'text': 'Nice to meet you, Bob! How can I assist you today?', 'chat_history': []}
{'text': 'Your name is Bob. How can I assist you further, Bob?', 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={})]}


Reimplementing ConversationBufferWindowMemory logic
​


Let's first create appropriate logic to process the conversation history, and then we'll see how to integrate it into an application. You can later replace this basic setup with more advanced logic tailored to your specific needs.


We'll use 
trim_messages
 to implement logic that keeps the last 
n
 messages of the conversation. It will drop the oldest messages when the number of messages exceeds 
n
.


In addition, we will also keep the system message if it's present -- when present, it's the first message in a conversation that includes instructions for the chat model.


from
 langchain_core
.
messages 
import
 
(
    AIMessage
,
    BaseMessage
,
    HumanMessage
,
    SystemMessage
,
    trim_messages
,
)
from
 langchain_openai 
import
 ChatOpenAI
messages 
=
 
[
    SystemMessage
(
"you're a good assistant, you always respond with a joke."
)
,
    HumanMessage
(
"i wonder why it's called langchain"
)
,
    AIMessage
(
        
'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    
)
,
    HumanMessage
(
"and who is harrison chasing anyways"
)
,
    AIMessage
(
        
"Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"
    
)
,
    HumanMessage
(
"why is 42 always the answer?"
)
,
    AIMessage
(
        
"Because it’s the only number that’s constantly right, even when it doesn’t add up!"
    
)
,
    HumanMessage
(
"What did the cow say?"
)
,
]
API Reference:
AIMessage
 | 
BaseMessage
 | 
HumanMessage
 | 
SystemMessage
 | 
trim_messages
 | 
ChatOpenAI


from
 langchain_core
.
messages 
import
 trim_messages
selected_messages 
=
 trim_messages
(
    messages
,
    token_counter
=
len
,
  
# <-- len will simply count the number of messages rather than tokens
    max_tokens
=
5
,
  
# <-- allow up to 5 messages.
    strategy
=
"last"
,
    
# Most chat models expect that chat history starts with either:
    
# (1) a HumanMessage or
    
# (2) a SystemMessage followed by a HumanMessage
    
# start_on="human" makes sure we produce a valid chat history
    start_on
=
"human"
,
    
# Usually, we want to keep the SystemMessage
    
# if it's present in the original history.
    
# The SystemMessage has special instructions for the model.
    include_system
=
True
,
    allow_partial
=
False
,
)
for
 msg 
in
 selected_messages
:
    msg
.
pretty_print
(
)
API Reference:
trim_messages


================================[1m System Message [0m================================
you're a good assistant, you always respond with a joke.
==================================[1m Ai Message [0m==================================
Hmmm let me think.
Why, he's probably chasing after the last cup of coffee in the office!
================================[1m Human Message [0m=================================
why is 42 always the answer?
==================================[1m Ai Message [0m==================================
Because it’s the only number that’s constantly right, even when it doesn’t add up!
================================[1m Human Message [0m=================================
What did the cow say?


Reimplementing ConversationTokenBufferMemory logic
​


Here, we'll use 
trim_messages
 to keeps the system message and the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.


from
 langchain_core
.
messages 
import
 trim_messages
selected_messages 
=
 trim_messages
(
    messages
,
    
# Please see API reference for trim_messages for other ways to specify a token counter.
    token_counter
=
ChatOpenAI
(
model
=
"gpt-4o"
)
,
    max_tokens
=
80
,
  
# <-- token limit
    
# The start_on is specified
    
# Most chat models expect that chat history starts with either:
    
# (1) a HumanMessage or
    
# (2) a SystemMessage followed by a HumanMessage
    
# start_on="human" makes sure we produce a valid chat history
    start_on
=
"human"
,
    
# Usually, we want to keep the SystemMessage
    
# if it's present in the original history.
    
# The SystemMessage has special instructions for the model.
    include_system
=
True
,
    strategy
=
"last"
,
)
for
 msg 
in
 selected_messages
:
    msg
.
pretty_print
(
)
API Reference:
trim_messages


================================[1m System Message [0m================================
you're a good assistant, you always respond with a joke.
================================[1m Human Message [0m=================================
why is 42 always the answer?
==================================[1m Ai Message [0m==================================
Because it’s the only number that’s constantly right, even when it doesn’t add up!
================================[1m Human Message [0m=================================
What did the cow say?


Modern usage with LangGraph
​


The example below shows how to use LangGraph to add simple conversation pre-processing logic.


note
If you want to avoid running the computation on the entire conversation history each time, you can follow
the 
how-to guide on summarization
 that demonstrates
how to discard older messages, ensuring they aren't re-processed during later turns.


Details
import
 uuid
from
 IPython
.
display 
import
 Image
,
 display
from
 langchain_core
.
messages 
import
 HumanMessage
from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
graph 
import
 START
,
 MessagesState
,
 StateGraph
# Define a new graph
workflow 
=
 StateGraph
(
state_schema
=
MessagesState
)
# Define a chat model
model 
=
 ChatOpenAI
(
)
# Define the function that calls the model
def
 
call_model
(
state
:
 MessagesState
)
:
    selected_messages 
=
 trim_messages
(
        state
[
"messages"
]
,
        token_counter
=
len
,
  
# <-- len will simply count the number of messages rather than tokens
        max_tokens
=
5
,
  
# <-- allow up to 5 messages.
        strategy
=
"last"
,
        
# Most chat models expect that chat history starts with either:
        
# (1) a HumanMessage or
        
# (2) a SystemMessage followed by a HumanMessage
        
# start_on="human" makes sure we produce a valid chat history
        start_on
=
"human"
,
        
# Usually, we want to keep the SystemMessage
        
# if it's present in the original history.
        
# The SystemMessage has special instructions for the model.
        include_system
=
True
,
        allow_partial
=
False
,
    
)
    response 
=
 model
.
invoke
(
selected_messages
)
    
# We return a list, because this will get added to the existing list
    
return
 
{
"messages"
:
 response
}
# Define the two nodes we will cycle between
workflow
.
add_edge
(
START
,
 
"model"
)
workflow
.
add_node
(
"model"
,
 call_model
)
# Adding memory is straight forward in langgraph!
memory 
=
 MemorySaver
(
)
app 
=
 workflow
.
compile
(
    checkpointer
=
memory
)
# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
thread_id 
=
 uuid
.
uuid4
(
)
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 thread_id
}
}
input_message 
=
 HumanMessage
(
content
=
"hi! I'm bob"
)
for
 event 
in
 app
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
# Here, let's confirm that the AI remembers our name!
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 thread_id
}
}
input_message 
=
 HumanMessage
(
content
=
"what was my name?"
)
for
 event 
in
 app
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
API Reference:
HumanMessage
 | 
MemorySaver
 | 
StateGraph
================================[1m Human Message [0m=================================
hi! I'm bob
==================================[1m Ai Message [0m==================================
Hello Bob! How can I assist you today?
================================[1m Human Message [0m=================================
what was my name?
==================================[1m Ai Message [0m==================================
Your name is Bob. How can I help you, Bob?


Usage with a pre-built langgraph agent
​


This example shows usage of an Agent Executor with a pre-built agent constructed using the 
create_tool_calling_agent
 function.


If you are using one of the 
old LangChain pre-built agents
, you should be able
to replace that code with the new 
langgraph pre-built agent
 which leverages
native tool calling capabilities of chat models and will likely work better out of the box.


Details
import
 uuid
from
 langchain_core
.
messages 
import
 
(
    AIMessage
,
    BaseMessage
,
    HumanMessage
,
    SystemMessage
,
    trim_messages
,
)
from
 langchain_core
.
tools 
import
 tool
from
 langchain_openai 
import
 ChatOpenAI
from
 langgraph
.
checkpoint
.
memory 
import
 MemorySaver
from
 langgraph
.
prebuilt 
import
 create_react_agent
@tool
def
 
get_user_age
(
name
:
 
str
)
 
-
>
 
str
:
    
"""Use this tool to find the user's age."""
    
# This is a placeholder for the actual implementation
    
if
 
"bob"
 
in
 name
.
lower
(
)
:
        
return
 
"42 years old"
    
return
 
"41 years old"
memory 
=
 MemorySaver
(
)
model 
=
 ChatOpenAI
(
)
def
 
prompt
(
state
)
 
-
>
 
list
[
BaseMessage
]
:
    
"""Given the agent state, return a list of messages for the chat model."""
    
# We're using the message processor defined above.
    
return
 trim_messages
(
        state
[
"messages"
]
,
        token_counter
=
len
,
  
# <-- len will simply count the number of messages rather than tokens
        max_tokens
=
5
,
  
# <-- allow up to 5 messages.
        strategy
=
"last"
,
        
# Most chat models expect that chat history starts with either:
        
# (1) a HumanMessage or
        
# (2) a SystemMessage followed by a HumanMessage
        
# start_on="human" makes sure we produce a valid chat history
        start_on
=
"human"
,
        
# Usually, we want to keep the SystemMessage
        
# if it's present in the original history.
        
# The SystemMessage has special instructions for the model.
        include_system
=
True
,
        allow_partial
=
False
,
    
)
app 
=
 create_react_agent
(
    model
,
    tools
=
[
get_user_age
]
,
    checkpointer
=
memory
,
    prompt
=
prompt
,
)
# The thread id is a unique key that identifies
# this particular conversation.
# We'll just generate a random uuid here.
thread_id 
=
 uuid
.
uuid4
(
)
config 
=
 
{
"configurable"
:
 
{
"thread_id"
:
 thread_id
}
}
# Tell the AI that our name is Bob, and ask it to use a tool to confirm
# that it's capable of working like an agent.
input_message 
=
 HumanMessage
(
content
=
"hi! I'm bob. What is my age?"
)
for
 event 
in
 app
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
# Confirm that the chat bot has access to previous conversation
# and can respond to the user saying that the user's name is Bob.
input_message 
=
 HumanMessage
(
content
=
"do you remember my name?"
)
for
 event 
in
 app
.
stream
(
{
"messages"
:
 
[
input_message
]
}
,
 config
,
 stream_mode
=
"values"
)
:
    event
[
"messages"
]
[
-
1
]
.
pretty_print
(
)
API Reference:
AIMessage
 | 
BaseMessage
 | 
HumanMessage
 | 
SystemMessage
 | 
trim_messages
 | 
tool
 | 
ChatOpenAI
 | 
MemorySaver
 | 
create_react_agent
================================[1m Human Message [0m=================================
hi! I'm bob. What is my age?
==================================[1m Ai Message [0m==================================
Tool Calls:
  get_user_age (call_jsMvoIFv970DhqqLCJDzPKsp)
 Call ID: call_jsMvoIFv970DhqqLCJDzPKsp
  Args:
    name: bob
=================================[1m Tool Message [0m=================================
Name: get_user_age
42 years old
==================================[1m Ai Message [0m==================================
Bob, you are 42 years old.
================================[1m Human Message [0m=================================
do you remember my name?
==================================[1m Ai Message [0m==================================
Yes, your name is Bob.


LCEL: Add a preprocessing step
​


The simplest way to add complex conversation management is by introducing a pre-processing step in front of the chat model and pass the full conversation history to the pre-processing step.


This approach is conceptually simple and will work in many situations; for example, if using a 
RunnableWithMessageHistory
 instead of wrapping the chat model, wrap the chat model with the pre-processor.


The obvious downside of this approach is that latency starts to increase as the conversation history grows because of two reasons:




As the conversation gets longer, more data may need to be fetched from whatever store your'e using to store the conversation history (if not storing it in memory).


The pre-processing logic will end up doing a lot of redundant computation, repeating computation from previous steps of the conversation.




caution
If you want to use a chat model's tool calling capabilities, remember to bind the tools to the model before adding the history pre-processing step to it!


Details
from
 langchain_core
.
messages 
import
 
(
    AIMessage
,
    BaseMessage
,
    HumanMessage
,
    SystemMessage
,
    trim_messages
,
)
from
 langchain_core
.
tools 
import
 tool
from
 langchain_openai 
import
 ChatOpenAI
model 
=
 ChatOpenAI
(
)
@tool
def
 
what_did_the_cow_say
(
)
 
-
>
 
str
:
    
"""Check to see what the cow said."""
    
return
 
"foo"
message_processor 
=
 trim_messages
(
  
# Returns a Runnable if no messages are provided
    token_counter
=
len
,
  
# <-- len will simply count the number of messages rather than tokens
    max_tokens
=
5
,
  
# <-- allow up to 5 messages.
    strategy
=
"last"
,
    
# The start_on is specified
    
# to make sure we do not generate a sequence where
    
# a ToolMessage that contains the result of a tool invocation
    
# appears before the AIMessage that requested a tool invocation
    
# as this will cause some chat models to raise an error.
    start_on
=
(
"human"
,
 
"ai"
)
,
    include_system
=
True
,
  
# <-- Keep the system message
    allow_partial
=
False
,
)
# Note that we bind tools to the model first!
model_with_tools 
=
 model
.
bind_tools
(
[
what_did_the_cow_say
]
)
model_with_preprocessor 
=
 message_processor 
|
 model_with_tools
full_history 
=
 
[
    SystemMessage
(
"you're a good assistant, you always respond with a joke."
)
,
    HumanMessage
(
"i wonder why it's called langchain"
)
,
    AIMessage
(
        
'Well, I guess they thought "WordRope" and "SentenceString" just didn\'t have the same ring to it!'
    
)
,
    HumanMessage
(
"and who is harrison chasing anyways"
)
,
    AIMessage
(
        
"Hmmm let me think.\n\nWhy, he's probably chasing after the last cup of coffee in the office!"
    
)
,
    HumanMessage
(
"why is 42 always the answer?"
)
,
    AIMessage
(
        
"Because it’s the only number that’s constantly right, even when it doesn’t add up!"
    
)
,
    HumanMessage
(
"What did the cow say?"
)
,
]
# We pass it explicity to the model_with_preprocesor for illustrative purposes.
# If you're using `RunnableWithMessageHistory` the history will be automatically
# read from the source the you configure.
model_with_preprocessor
.
invoke
(
full_history
)
.
pretty_print
(
)
API Reference:
AIMessage
 | 
BaseMessage
 | 
HumanMessage
 | 
SystemMessage
 | 
trim_messages
 | 
tool
 | 
ChatOpenAI
==================================[1m Ai Message [0m==================================
Tool Calls:
  what_did_the_cow_say (call_urHTB5CShhcKz37QiVzNBlIS)
 Call ID: call_urHTB5CShhcKz37QiVzNBlIS
  Args:


If you need to implement more efficient logic and want to use 
RunnableWithMessageHistory
 for now the way to achieve this
is to subclass from 
BaseChatMessageHistory
 and
define appropriate logic for 
add_messages
 (that doesn't simply append the history, but instead re-writes it).


Unless you have a good reason to implement this solution, you should instead use LangGraph.


Next steps
​


Explore persistence with LangGraph:




LangGraph quickstart tutorial


How to add persistence ("memory") to your graph


How to manage conversation history


How to add summary of the conversation history




Add persistence with simple LCEL (favor langgraph for more complex use cases):




How to add message history




Working with message history:




How to trim messages


How to filter messages


How to merge message runs


Edit this page
Was this page helpful?
Previous
Migrating off ConversationBufferMemory or ConversationStringBufferMemory
Next
Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory
Set up
Legacy usage with LLMChain / Conversation Chain
Reimplementing ConversationBufferWindowMemory logic
Reimplementing ConversationTokenBufferMemory logic
Modern usage with LangGraph
Usage with a pre-built langgraph agent
LCEL: Add a preprocessing step
Next steps


--- docs_versions_migrating_memory_conversation_summary_memory.txt ---

Versions
Upgrading to LangGraph memory
Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory
Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory


Follow this guide if you're trying to migrate off one of the old memory classes listed below:


Memory Type
Description
ConversationSummaryMemory
Continually summarizes the conversation history. The summary is updated after each conversation turn. The abstraction returns the summary of the conversation history.
ConversationSummaryBufferMemory
Provides a running summary of the conversation together with the most recent messages in the conversation under the constraint that the total number of tokens in the conversation does not exceed a certain limit.


Please follow the following 
how-to guide on summarization
 in LangGraph.


This guide shows how to maintain a running summary of the conversation while discarding older messages, ensuring they aren't re-processed during later turns.
Edit this page
Was this page helpful?
Previous
Migrating off ConversationBufferWindowMemory or ConversationTokenBufferMemory
Next
A Long-Term Memory Agent


--- docs_versions_release_policy.txt ---

Versions
Release policy
On this page
LangChain release policy


The LangChain ecosystem is composed of different component packages (e.g., 
langchain-core
, 
langchain
, 
langchain-community
, 
langgraph
, 
langserve
, partner packages etc.)


Versioning
​


langchain
, 
langchain-core
, and integration packages
​


langchain
, 
langchain-core
, 
langchain-text-splitters
, and integration packages (
langchain-openai
, 
langchain-anthropic
, etc.) follow 
semantic versioning
 in the format of 0.
Y
.
Z
. The packages are under rapid development, and so are currently versioning the packages with a major version of 0.


Minor version increases will occur for:




Breaking changes for any public interfaces 
not
 marked as 
beta
.




Patch version increases will occur for:




Bug fixes,


New features,


Any changes to private interfaces,


Any changes to 
beta
 features.




When upgrading between minor versions, users should review the list of breaking changes and deprecations.


From time to time, we will version packages as 
release candidates
. These are versions that are intended to be released as stable versions, but we want to get feedback from the community before doing so. Release candidates will be versioned as 0.
Y
.
Z
rc
N
. For example, 0.2.0rc1. If no issues are found, the release candidate will be released as a stable version with the same version number. If issues are found, we will release a new release candidate with an incremented 
N
 value (e.g., 0.2.0rc2).


langchain-community
​


langchain-community
 is currently on version 
0.2.x
.


Minor version increases will occur for:




Updates to the major/minor versions of required 
langchain-x
 dependencies. E.g., when updating the required version of 
langchain-core
 from 
^0.2.x
 to 
0.3.0
.




Patch version increases will occur for:




Bug fixes,


New features,


Any changes to private interfaces,


Any changes to 
beta
 features,


Breaking changes to integrations to reflect breaking changes in the third-party service.




Whenever possible we will avoid making breaking changes in patch versions.
However, if an external API makes a breaking change then breaking changes to the corresponding 
langchain-community
 integration can occur in a patch version.


langchain-experimental
​


langchain-experimental
 is currently on version 
0.0.x
. All changes will be accompanied with patch version increases.


Release cadence
​


We expect to space out 
minor
 releases (e.g., from 0.2.x to 0.3.0) of 
langchain
 and 
langchain-core
 by at least 2-3 months, as such releases may contain breaking changes.


Patch versions are released frequently, up to a few times per week, as they contain bug fixes and new features.


API stability
​


The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in 
langchain
 and 
langchain-core
 will continue to evolve to better serve the needs of our users.


Even though both 
langchain
 and 
langchain-core
 are currently in a pre-1.0 state, we are committed to maintaining API stability in these packages.




Breaking changes to the public API will result in a minor version bump (the second digit)


Any bug fixes or new features will result in a patch version bump (the third digit)




We will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.


Stability of other packages
​


The stability of other packages in the LangChain ecosystem may vary:




langchain-community
 is a community maintained package that contains 3rd party integrations. While we do our best to review and test changes in 
langchain-community
, 
langchain-community
 is expected to experience more breaking changes than 
langchain
 and 
langchain-core
 as it contains many community contributions.


Partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information; however, in general these packages are expected to be stable.




What is a "API stability"?
​


API stability means:




All the public APIs (everything in this documentation) will not be moved or renamed without providing backwards-compatible aliases.


If new features are added to these APIs – which is quite possible – they will not break or change the meaning of existing methods. In other words, "stable" does not (necessarily) mean "complete."


If, for some reason, an API declared stable must be removed or replaced, it will be declared deprecated but will remain in the API for at least two minor releases. Warnings will be issued when the deprecated method is called.




APIs marked as internal
​


Certain APIs are explicitly marked as “internal” in a couple of ways:




Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.


Functions, methods, and other objects prefixed by a leading underscore (
_
). This is the standard Python convention of indicating that something is private; if any method starts with a single 
_
, it’s an internal API.




Exception:
 Certain methods are prefixed with 
_
 , but do not contain an implementation. These methods are 
meant
 to be overridden by sub-classes that provide the implementation. Such methods are generally part of the 
Public API
 of LangChain.








Deprecation policy
​


We will generally avoid deprecating features until a better alternative is available.


When a feature is deprecated, it will continue to work in the current and next minor version of 
langchain
 and 
langchain-core
. After that, the feature will be removed.


Since we're expecting to space out minor releases by at least 2-3 months, this means that a feature can be removed within 2-6 months of being deprecated.


In some situations, we may allow the feature to remain in the code base for longer periods of time, if it's not causing issues in the packages, to reduce the burden on users.
Edit this page
Was this page helpful?
Previous
A Long-Term Memory Agent
Next
Security Policy
Versioning
langchain
, 
langchain-core
, and integration packages
langchain-community
langchain-experimental
Release cadence
API stability
Stability of other packages
What is a "API stability"?
APIs marked as internal
Deprecation policy


--- docs_versions_v0_2_overview.txt ---

Versions
v0.2
Overview
On this page
Overview


What’s new in LangChain?
​


The following features have been added during the development of 0.1.x:




Better streaming support via the 
Event Streaming API
.


Standardized tool calling support


A standardized interface for 
structuring output


@chain decorator
 to more easily create 
RunnableLambdas


https://python.langchain.com/docs/expression_language/how_to/inspect/


In Python, better async support for many core abstractions (thank you 
@cbornet
!!)


Include response metadata in 
AIMessage
 to make it easy to access raw output from the underlying models


Tooling to visualize 
your runnables
 or 
your langgraph app


Interoperability of chat message histories across most providers


Over 20+ partner packages in python
 for popular integrations




What’s coming to LangChain?
​




We’ve been working hard on 
langgraph
. We will be building more capabilities on top of it and focusing on making it the go-to framework for agent architectures.


Vectorstores V2! We’ll be revisiting our vectorstores abstractions to help improve usability and reliability.


Better documentation and versioned docs!


We’re planning a breaking release (0.3.0) sometime between July-September to 
upgrade to full support of Pydantic 2
, and will drop support for Pydantic 1 (including objects originating from the 
v1
 namespace of Pydantic 2).




What changed?
​


Due to the rapidly evolving field, LangChain has also evolved rapidly.


This document serves to outline at a high level what has changed and why.


TLDR
​


As of 0.2.0:




This release completes the work that we started with release 0.1.0 by removing the dependency of 
langchain
 on 
langchain-community
.


langchain
 package no longer requires 
langchain-community
 . Instead 
langchain-community
 will now depend on 
langchain-core
 and 
langchain
 .


User code that still relies on deprecated imports from 
langchain
 will continue to work as long 
langchain_community
 is installed. These imports will start raising errors in release 0.4.x.




As of 0.1.0:




langchain
 was split into the following component packages: 
langchain-core
, 
langchain
, 
langchain-community
, 
langchain-[partner]
 to improve the usability of langchain code in production settings. You can read more about it on our 
blog
.




Ecosystem organization
​


By the release of 0.1.0, LangChain had grown to a large ecosystem with many integrations and a large community.


To improve the usability of LangChain in production, we split the single 
langchain
 package into multiple packages. This allowed us to create a good foundation architecture for the LangChain ecosystem and improve the usability of 
langchain
 in production.


Here is the high level break down of the Eco-system:




langchain-core
:  contains core abstractions involving LangChain Runnables, tooling for observability, and base implementations of important abstractions (e.g., Chat Models).


langchain:
 contains generic code that is built using interfaces defined in 
langchain-core
. This package is for code that generalizes well across different implementations of specific interfaces. For example, 
create_tool_calling_agent
 works across chat models that support 
tool calling capabilities
.


langchain-community
: community maintained 3rd party integrations. Contains integrations based on interfaces defined in 
langchain-core
. Maintained by the LangChain community.


Partner Packages (e.g., langchain-[partner])
: Partner packages are packages dedicated to especially popular integrations (e.g., 
langchain-openai
, 
langchain-anthropic
 etc.). The dedicated packages generally benefit from better reliability and support.


langgraph
: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.


langserve
: Deploy LangChain chains as REST APIs.




In the 0.1.0 release, 
langchain-community
 was retained as required a dependency of 
langchain
.


This allowed imports of vectorstores, chat models, and other integrations to continue working through 
langchain

rather than forcing users to update all of their imports to 
langchain-community
.


For the 0.2.0 release, we’re removing the dependency of 
langchain
 on 
langchain-community
. This is something we’ve been planning to do since the 0.1 release because we believe this is the right package architecture.


Old imports will continue to work as long as 
langchain-community
 is installed. These imports will be removed in the 0.4.0 release.


To understand why we think breaking the dependency of 
langchain
 on 
langchain-community
 is best we should understand what each package is meant to do.


langchain
 is meant to contain high-level chains and agent architectures. The logic in these should be specified at the level of abstractions like 
ChatModel
 and 
Retriever
, and should not be specific to any one integration. This has two main benefits:






langchain
 is fairly lightweight. Here is the full list of required dependencies (after the split)


python = ">=3.8.1,<4.0"
langchain-core = "^0.2.0"
langchain-text-splitters = ">=0.0.1,<0.1"
langsmith = "^0.1.17"
pydantic = ">=1,<3"
SQLAlchemy = ">=1.4,<3"
requests = "^2"
PyYAML = ">=5.3"
numpy = "^1"
aiohttp = "^3.8.3"
tenacity = "^8.1.0"
jsonpatch = "^1.33"






langchain
 chains/agents are largely integration-agnostic, which makes it easy to experiment with different integrations and future-proofs your code should there be issues with one specific integration.






There is also a third less tangible benefit which is that being integration-agnostic forces us to find only those very generic abstractions and architectures which generalize well across integrations. Given how general the abilities of the foundational tech are, and how quickly the space is moving, having generic architectures is a good way of future-proofing your applications.


langchain-community
 is intended to have all integration-specific components that are not yet being maintained in separate 
langchain-{partner}
 packages. Today this is still the majority of integrations and a lot of code. This code is primarily contributed by the community, while 
langchain
 is largely written by core maintainers. All of these integrations use optional dependencies and conditional imports, which prevents dependency bloat and conflicts but means compatible dependency versions are not made explicit. Given the volume of integrations in 
langchain-community
 and the speed at which integrations change, it’s very hard to follow semver versioning, and we currently don’t.


All of which is to say that there’s no large benefits to 
langchain
 depending on 
langchain-community
 and some obvious downsides: the functionality in 
langchain
 should be integration agnostic anyways, 
langchain-community
 can’t be properly versioned, and depending on 
langchain-community
 increases the 
vulnerability surface
 of 
langchain
.


For more context about the reason for the organization please see our blog: 
https://blog.langchain.dev/langchain-v0-1-0/
Edit this page
Was this page helpful?
Previous
v0.3
Next
Migration
What’s new in LangChain?
What’s coming to LangChain?
What changed?
TLDR
Ecosystem organization


--- docs_versions_v0_3.txt ---

Versions
v0.3
On this page
LangChain v0.3


Last updated: 09.16.24


What's changed
​




All packages have been upgraded from Pydantic 1 to Pydantic 2 internally. Use of Pydantic 2 in user code is fully supported with all packages without the need for bridges like 
langchain_core.pydantic_v1
 or 
pydantic.v1
.


Pydantic 1 will no longer be supported as it reached its end-of-life in June 2024.


Python 3.8 will no longer be supported as its end-of-life is October 2024.




These are the only breaking changes.


What’s new
​


The following features have been added during the development of 0.2.x:




Moved more integrations from 
langchain-community
 to their own 
langchain-x
 packages. This is a non-breaking change, as the legacy implementations are left in 
langchain-community
 and marked as deprecated. This allows us to better manage the dependencies of, test, and version these integrations. You can see all the latest integration packages in the 
API reference
.


Simplified tool definition and usage. Read more 
here
.


Added utilities for interacting with chat models: 
universal model constructor
, 
rate limiter
, 
message utilities
,


Added the ability to 
dispatch custom events
.


Revamped integration docs and API reference. Read more 
here
.


Marked as deprecated a number of legacy chains and added migration guides for all of them. These are slated for removal in 
langchain
 1.0.0. See the deprecated chains and associated 
migration guides here
.




How to update your code
​


If you're using 
langchain
 / 
langchain-community
 / 
langchain-core
 0.0 or 0.1, we recommend that you first 
upgrade to 0.2
.


If you're using 
langgraph
, upgrade to 
langgraph>=0.2.20,<0.3
. This will work with either 0.2 or 0.3 versions of all the base packages.


Here is a complete list of all packages that have been released and what we recommend upgrading your version constraints to.
Any package that now requires 
langchain-core
 0.3 had a minor version bump.
Any package that is now compatible with both 
langchain-core
 0.2 and 0.3 had a patch version bump.


You can use the 
langchain-cli
 to update deprecated imports automatically.
The CLI will handle updating deprecated imports that were introduced in LangChain 0.0.x and LangChain 0.1, as
well as updating the 
langchain_core.pydantic_v1
 and 
langchain.pydantic_v1
 imports.


Base packages
​


Package
Latest
Recommended constraint
langchain
0.3.0
>=0.3,<0.4
langchain-community
0.3.0
>=0.3,<0.4
langchain-text-splitters
0.3.0
>=0.3,<0.4
langchain-core
0.3.0
>=0.3,<0.4
langchain-experimental
0.3.0
>=0.3,<0.4


Downstream packages
​


Package
Latest
Recommended constraint
langgraph
0.2.20
>=0.2.20,<0.3
langserve
0.3.0
>=0.3,<0.4


Integration packages
​


Package
Latest
Recommended constraint
langchain-ai21
0.2.0
>=0.2,<0.3
langchain-aws
0.2.0
>=0.2,<0.3
langchain-anthropic
0.2.0
>=0.2,<0.3
langchain-astradb
0.4.1
>=0.4.1,<0.5
langchain-azure-dynamic-sessions
0.2.0
>=0.2,<0.3
langchain-box
0.2.0
>=0.2,<0.3
langchain-chroma
0.1.4
>=0.1.4,<0.2
langchain-cohere
0.3.0
>=0.3,<0.4
langchain-elasticsearch
0.3.0
>=0.3,<0.4
langchain-exa
0.2.0
>=0.2,<0.3
langchain-fireworks
0.2.0
>=0.2,<0.3
langchain-groq
0.2.0
>=0.2,<0.3
langchain-google-community
2.0.0
>=2,<3
langchain-google-genai
2.0.0
>=2,<3
langchain-google-vertexai
2.0.0
>=2,<3
langchain-huggingface
0.1.0
>=0.1,<0.2
langchain-ibm
0.3.0
>=0.3,<0.4
langchain-milvus
0.1.6
>=0.1.6,<0.2
langchain-mistralai
0.2.0
>=0.2,<0.3
langchain-mongodb
0.2.0
>=0.2,<0.3
langchain-nomic
0.1.3
>=0.1.3,<0.2
langchain-nvidia
0.3.0
>=0.3,<0.4
langchain-ollama
0.2.0
>=0.2,<0.3
langchain-openai
0.2.0
>=0.2,<0.3
langchain-pinecone
0.2.0
>=0.2,<0.3
langchain-postgres
0.0.13
>=0.0.13,<0.1
langchain-prompty
0.1.0
>=0.1,<0.2
langchain-qdrant
0.1.4
>=0.1.4,<0.2
langchain-redis
0.1.0
>=0.1,<0.2
langchain-sema4
0.2.0
>=0.2,<0.3
langchain-together
0.2.0
>=0.2,<0.3
langchain-unstructured
0.1.4
>=0.1.4,<0.2
langchain-upstage
0.3.0
>=0.3,<0.4
langchain-voyageai
0.2.0
>=0.2,<0.3
langchain-weaviate
0.0.3
>=0.0.3,<0.1


Once you've updated to recent versions of the packages, you may need to address the following issues stemming from the internal switch from Pydantic v1 to Pydantic v2:




If your code depends on Pydantic aside from LangChain, you will need to upgrade your pydantic version constraints to be 
pydantic>=2,<3
.  See 
Pydantic’s migration guide
 for help migrating your non-LangChain code to Pydantic v2 if you use pydantic v1.


There are a number of side effects to LangChain components caused by the internal switch from Pydantic v1 to v2. We have listed some of the common cases below together with the recommended solutions.




Common issues when transitioning to Pydantic 2
​


1. Do not use the 
langchain_core.pydantic_v1
 namespace
​


Replace any usage of 
langchain_core.pydantic_v1
 or 
langchain.pydantic_v1
 with
direct imports from 
pydantic
.


For example,


from
 langchain_core
.
pydantic_v1 
import
 BaseModel


to:


from
 pydantic 
import
 BaseModel


This may require you to make additional updates to your Pydantic code given that there are a number of breaking changes in Pydantic 2. See the 
Pydantic Migration
 for how to upgrade your code from Pydantic 1 to 2.


2. Passing Pydantic objects to LangChain APIs
​


Users using the following APIs:




BaseChatModel.bind_tools


BaseChatModel.with_structured_output


Tool.from_function


StructuredTool.from_function




should ensure that they are passing Pydantic 2 objects to these APIs rather than
Pydantic 1 objects (created via the 
pydantic.v1
 namespace of pydantic 2).


caution
While 
v1
 objects may be accepted by some of these APIs, users are advised to
use Pydantic 2 objects to avoid future issues.


3. Sub-classing LangChain models
​


Any sub-classing from existing LangChain models (e.g., 
BaseTool
, 
BaseChatModel
, 
LLM
)
should upgrade to use Pydantic 2 features.


For example, any user code that's relying on Pydantic 1 features (e.g., 
validator
) should
be updated to the Pydantic 2 equivalent (e.g., 
field_validator
), and any references to

pydantic.v1
, 
langchain_core.pydantic_v1
, 
langchain.pydantic_v1
 should be replaced
with imports from 
pydantic
.


from
 pydantic
.
v1 
import
 validator
,
 Field 
# if pydantic 2 is installed
# from pydantic import validator, Field # if pydantic 1 is installed
# from langchain_core.pydantic_v1 import validator, Field
# from langchain.pydantic_v1 import validator, Field
class
 
CustomTool
(
BaseTool
)
:
 
# BaseTool is v1 code
    x
:
 
int
 
=
 Field
(
default
=
1
)
    
def
 
_run
(
*
args
,
 
**
kwargs
)
:
        
return
 
"hello"
    
@validator
(
'x'
)
 
# v1 code
    
@classmethod
    
def
 
validate_x
(
cls
,
 x
:
 
int
)
 
-
>
 
int
:
        
return
 
1


Should change to:


from
 pydantic 
import
 Field
,
 field_validator 
# pydantic v2
from
 langchain_core
.
pydantic_v1 
import
 BaseTool
class
 
CustomTool
(
BaseTool
)
:
 
# BaseTool is v1 code
    x
:
 
int
 
=
 Field
(
default
=
1
)
    
def
 
_run
(
*
args
,
 
**
kwargs
)
:
        
return
 
"hello"
    
@field_validator
(
'x'
)
 
# v2 code
    
@classmethod
    
def
 
validate_x
(
cls
,
 x
:
 
int
)
 
-
>
 
int
:
        
return
 
1
CustomTool
(
    name
=
'custom_tool'
,
    description
=
"hello"
,
    x
=
1
,
)


4. model_rebuild()
​


When sub-classing from LangChain models, users may need to add relevant imports
to the file and rebuild the model.


You can read more about 
model_rebuild
 
here
.


from
 langchain_core
.
output_parsers 
import
 BaseOutputParser
class
 
FooParser
(
BaseOutputParser
)
:
    
.
.
.
API Reference:
BaseOutputParser


New code:


from
 typing 
import
 Optional 
as
 Optional
from
 langchain_core
.
output_parsers 
import
 BaseOutputParser
class
 
FooParser
(
BaseOutputParser
)
:
    
.
.
.
FooParser
.
model_rebuild
(
)
API Reference:
BaseOutputParser


Migrate using langchain-cli
​


The 
langchain-cli
 can help update deprecated LangChain imports in your code automatically.


Please note that the 
langchain-cli
 only handles deprecated LangChain imports and cannot
help to upgrade your code from pydantic 1 to pydantic 2.


For help with the Pydantic 1 to 2 migration itself please refer to the 
Pydantic Migration Guidelines
.


As of 0.0.31, the 
langchain-cli
 relies on 
gritql
 for applying code mods.


Installation
​


pip install -U langchain-cli
langchain-cli --version # <-- Make sure the version is at least 0.0.31


Usage
​


Given that the migration script is not perfect, you should make sure you have a backup of your code first (e.g., using version control like 
git
).


The 
langchain-cli
 will handle the 
langchain_core.pydantic_v1
 deprecation introduced in LangChain 0.3 as well
as older deprecations (e.g.,
from langchain.chat_models import ChatOpenAI
 which should be 
from langchain_openai import ChatOpenAI
),


You will need to run the migration script 
twice
 as it only applies one import replacement per run.


For example, say that your code is still using the old import 
from langchain.chat_models import ChatOpenAI
:


After the first run, you’ll get: 
from langchain_community.chat_models import ChatOpenAI

After the second run, you’ll get: 
from langchain_openai import ChatOpenAI


# Run a first time
# Will replace from langchain.chat_models import ChatOpenAI
langchain-cli migrate --help [path to code] # Help
langchain-cli migrate [path to code] # Apply
# Run a second time to apply more import replacements
langchain-cli migrate --diff [path to code] # Preview
langchain-cli migrate [path to code] # Apply


Other options
​


# See help menu
langchain-cli migrate --help
# Preview Changes without applying
langchain-cli migrate --diff [path to code]
# Approve changes interactively
langchain-cli migrate --interactive [path to code]
Edit this page
Was this page helpful?
Previous
Why LangChain?
Next
Overview
What's changed
What’s new
How to update your code
Base packages
Downstream packages
Integration packages
Common issues when transitioning to Pydantic 2
1. Do not use the 
langchain_core.pydantic_v1
 namespace
2. Passing Pydantic objects to LangChain APIs
3. Sub-classing LangChain models
4. model_rebuild()
Migrate using langchain-cli
Installation
Usage
Other options


--- index.txt ---

Introduction
On this page
Introduction


LangChain
 is a framework for developing applications powered by large language models (LLMs).


LangChain simplifies every stage of the LLM application lifecycle:




Development
: Build your applications using LangChain's open-source 
components
 and 
third-party integrations
.
Use 
LangGraph
 to build stateful agents with first-class streaming and human-in-the-loop support.


Productionization
: Use 
LangSmith
 to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.


Deployment
: Turn your LangGraph applications into production-ready APIs and Assistants with 
LangGraph Platform
.








LangChain implements a standard interface for large language models and related
technologies, such as embedding models and vector stores, and integrates with
hundreds of providers. See the 
integrations
 page for
more.




Select 
chat model
:
OpenAI
▾
OpenAI
Anthropic
Azure
Google Gemini
Google Vertex
AWS
Groq
Cohere
NVIDIA
Fireworks AI
Mistral AI
Together AI
IBM watsonx
Databricks
xAI
Perplexity
pip install -qU "langchain[openai]"
import
 getpass
import
 os
if
 
not
 os
.
environ
.
get
(
"OPENAI_API_KEY"
)
:
  os
.
environ
[
"OPENAI_API_KEY"
]
 
=
 getpass
.
getpass
(
"Enter API key for OpenAI: "
)
from
 langchain
.
chat_models 
import
 init_chat_model
model 
=
 init_chat_model
(
"gpt-4o-mini"
,
 model_provider
=
"openai"
)


model
.
invoke
(
"Hello, world!"
)


note
These docs focus on the Python LangChain library. 
Head here
 for docs on the JavaScript LangChain library.


Architecture
​


The LangChain framework consists of multiple open-source libraries. Read more in the

Architecture
 page.




langchain-core
: Base abstractions for chat models and other components.


Integration packages
 (e.g. 
langchain-openai
, 
langchain-anthropic
, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.


langchain
: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.


langchain-community
: Third-party integrations that are community maintained.


langgraph
: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See 
LangGraph documentation
.




Guides
​


Tutorials
​


If you're looking to build something specific or are more of a hands-on learner, check out our 
tutorials section
.
This is the best place to get started.


These are the best ones to get started with:




Build a Simple LLM Application


Build a Chatbot


Build an Agent


Introduction to LangGraph




Explore the full list of LangChain tutorials 
here
, and check out other 
LangGraph tutorials here
. To learn more about LangGraph, check out our first LangChain Academy course, 
Introduction to LangGraph
, available 
here
.


How-to guides
​


Here
 you’ll find short answers to “How do I….?” types of questions.
These how-to guides don’t cover topics in depth – you’ll find that material in the 
Tutorials
 and the 
API Reference
.
However, these guides will help you quickly accomplish common tasks using 
chat models
,

vector stores
, and other common LangChain components.


Check out 
LangGraph-specific how-tos here
.


Conceptual guide
​


Introductions to all the key parts of LangChain you’ll need to know! 
Here
 you'll find high level explanations of all LangChain concepts.


For a deeper dive into LangGraph concepts, check out 
this page
.


Integrations
​


LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.
If you're looking to get up and running quickly with 
chat models
, 
vector stores
,
or other LangChain components from a specific provider, check out our growing list of 
integrations
.


API reference
​


Head to the reference section for full documentation of all classes and methods in the LangChain Python packages.


Ecosystem
​


🦜🛠️ LangSmith
​


Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.


🦜🕸️ LangGraph
​


Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.


Additional resources
​


Versions
​


See what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.


Security
​


Read up on 
security
 best practices to make sure you're developing safely with LangChain.


Contributing
​


Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.
Edit this page
Was this page helpful?
Next
Tutorials
Architecture
Guides
Tutorials
How-to guides
Conceptual guide
Integrations
API reference
Ecosystem
🦜🛠️ LangSmith
🦜🕸️ LangGraph
Additional resources
Versions
Security
Contributing


--- v0.1_docs_get_started_introduction.txt ---

This is documentation for 
LangChain
 
v0.1
, which is no longer actively maintained.
For the current stable version, see 
this version
 (
Latest
).
Get started
On this page
Introduction
LangChain
 is a framework for developing applications powered by large language models (LLMs).
LangChain simplifies every stage of the LLM application lifecycle:
Development
: Build your applications using LangChain's open-source 
building blocks
 and 
components
. Hit the ground running using 
third-party integrations
 and 
Templates
.
Productionization
: Use 
LangSmith
 to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.
Deployment
: Turn any chain into an API with 
LangServe
.
Concretely, the framework consists of the following open-source libraries:
langchain-core
: Base abstractions and LangChain Expression Language.
langchain-community
: Third party integrations.
Partner packages (e.g. 
langchain-openai
, 
langchain-anthropic
, etc.): Some integrations have been further split into their own lightweight packages that only depend on 
langchain-core
.
langchain
: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.
langgraph
: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.
langserve
: Deploy LangChain chains as REST APIs.
The broader ecosystem includes:
LangSmith
: A developer platform that lets you debug, test, evaluate, and monitor LLM applications and seamlessly integrates with LangChain.
Get started
​
We recommend following our 
Quickstart
 guide to familiarize yourself with the framework by building your first LangChain application.
See here
 for instructions on how to install LangChain, set up your environment, and start building.
note
These docs focus on the Python LangChain library. 
Head here
 for docs on the JavaScript LangChain library.
Use cases
​
If you're looking to build something specific or are more of a hands-on learner, check out our 
use-cases
.
They're walkthroughs and techniques for common end-to-end tasks, such as:
Question answering with RAG
Extracting structured output
Chatbots
and more!
Expression Language
​
LangChain Expression Language (LCEL) is the foundation of many of LangChain's components, and is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains.
Get started
: LCEL and its benefits
Runnable interface
: The standard interface for LCEL objects
Primitives
: More on the primitives LCEL includes
and more!
Ecosystem
​
🦜🛠️ LangSmith
​
Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.
🦜🕸️ LangGraph
​
Build stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain primitives.
🦜🏓 LangServe
​
Deploy LangChain runnables and chains as REST APIs.
Security
​
Read up on our 
Security
 best practices to make sure you're developing safely with LangChain.
Additional resources
​
Components
​
LangChain provides standard, extendable interfaces and integrations for many different components, including:
Integrations
​
LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of 
integrations
.
Guides
​
Best practices for developing with LangChain.
API reference
​
Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.
Contributing
​
Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.
Help us out by providing feedback on this documentation page:
Next
Introduction
Get started
Use cases
Expression Language
Ecosystem
🦜🛠️ LangSmith
🦜🕸️ LangGraph
🦜🏓 LangServe
Security
Additional resources
Components
Integrations
Guides
API reference
Contributing


--- v0.2_docs_introduction.txt ---

This is documentation for 
LangChain
 
v0.2
, which is no longer actively maintained.
For the current stable version, see 
this version
 (
Latest
).
Introduction
On this page
Introduction
LangChain
 is a framework for developing applications powered by large language models (LLMs).
LangChain simplifies every stage of the LLM application lifecycle:
Development
: Build your applications using LangChain's open-source 
building blocks
, 
components
, and 
third-party integrations
.
Use 
LangGraph
 to build stateful agents with first-class streaming and human-in-the-loop support.
Productionization
: Use 
LangSmith
 to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.
Deployment
: Turn your LangGraph applications into production-ready APIs and Assistants with 
LangGraph Cloud
.
Concretely, the framework consists of the following open-source libraries:
langchain-core
: Base abstractions and LangChain Expression Language.
langchain-community
: Third party integrations.
Partner packages (e.g. 
langchain-openai
, 
langchain-anthropic
, etc.): Some integrations have been further split into their own lightweight packages that only depend on 
langchain-core
.
langchain
: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.
LangGraph
: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without it.
LangServe
: Deploy LangChain chains as REST APIs.
LangSmith
: A developer platform that lets you debug, test, evaluate, and monitor LLM applications.
note
These docs focus on the Python LangChain library. 
Head here
 for docs on the JavaScript LangChain library.
Tutorials
​
If you're looking to build something specific or are more of a hands-on learner, check out our 
tutorials section
.
This is the best place to get started.
These are the best ones to get started with:
Build a Simple LLM Application
Build a Chatbot
Build an Agent
Introduction to LangGraph
Explore the full list of LangChain tutorials 
here
, and check out other 
LangGraph tutorials here
. To learn more about LangGraph, check out our first LangChain Academy course, 
Introduction to LangGraph
, available 
here
.
How-to guides
​
Here
 you’ll find short answers to “How do I….?” types of questions.
These how-to guides don’t cover topics in depth – you’ll find that material in the 
Tutorials
 and the 
API Reference
.
However, these guides will help you quickly accomplish common tasks.
Check out 
LangGraph-specific how-tos here
.
Conceptual guide
​
Introductions to all the key parts of LangChain you’ll need to know! 
Here
 you'll find high level explanations of all LangChain concepts.
For a deeper dive into LangGraph concepts, check out 
this page
.
API reference
​
Head to the reference section for full documentation of all classes and methods in the LangChain Python packages.
Ecosystem
​
🦜🛠️ LangSmith
​
Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.
🦜🕸️ LangGraph
​
Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.
Additional resources
​
Versions
​
See what changed in v0.2, learn how to migrate legacy code, and read up on our release/versioning policies, and more.
Security
​
Read up on 
security
 best practices to make sure you're developing safely with LangChain.
Integrations
​
LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of 
integrations
.
Contributing
​
Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.
Edit this page
Was this page helpful?
You can also leave detailed feedback
 
on GitHub
.
Next
Tutorials
Tutorials
How-to guides
Conceptual guide
API reference
Ecosystem
🦜🛠️ LangSmith
🦜🕸️ LangGraph
Additional resources
Versions
Security
Integrations
Contributing

